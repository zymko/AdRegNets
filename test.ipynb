{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "import numpy as np\r\n",
    "import torch\r\n",
    "from torch.autograd import Variable\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from linear_regression import *\r\n",
    "\r\n",
    "%reload_ext autoreload\r\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# create dummy data for training\r\n",
    "N = 5000\r\n",
    "np.random.seed(123)\r\n",
    "\r\n",
    "x = 10*np.random.rand(N)-5\r\n",
    "x=np.array(x,dtype=np.float32)\r\n",
    "x=x.reshape(-1, 1)\r\n",
    "\r\n",
    "noise = 50*np.random.randn(N)\r\n",
    "noise = np.array(noise, dtype=np.float32)\r\n",
    "noise=noise.reshape(-1, 1)\r\n",
    "\r\n",
    "x = torch.from_numpy(x)\r\n",
    "noise = torch.from_numpy(noise)\r\n",
    "# _x =  torch.cat([x, x**2],axis=1)\r\n",
    "# noise = 0.1*x\r\n",
    "# y = 7*x - 8\r\n",
    "y = x**2\r\n",
    "\r\n",
    "y_noise = y + noise \r\n",
    "\r\n",
    "x_train, x_test, y_train, y_test, y_noise_train, y_noise_test = train_test_split(x, y, y_noise, test_size=0.2, random_state=123)\r\n",
    "\r\n",
    "X_train, X_val, Y_train, Y_val ,Y_noise_train, Y_noise_val= train_test_split(x_train, y_train, y_noise_train, test_size=0.25, random_state=123)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# Y_train.shape, Y_train\r\n",
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.7682],\n",
       "        [-2.2356],\n",
       "        [ 3.9775],\n",
       "        ...,\n",
       "        [-1.1315],\n",
       "        [-4.9944],\n",
       "        [ 2.6255]])"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    " predicted_plotting(X_train, Y_noise_train)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "inputDim = 1        # takes variable 'x' \r\n",
    "outputDim = 1       # takes variable 'y'\r\n",
    "learningRate = 0.00001 \r\n",
    "learningRate_re = 0.001 \r\n",
    "epochs = 100\r\n",
    "# epochs=30000\r\n",
    "iters=200\r\n",
    "model = QuadraticRegression(inputDim, outputDim)\r\n",
    "# model = linearRegression(inputDim, outputDim)\r\n",
    "##### For GPU #######\r\n",
    "if torch.cuda.is_available():\r\n",
    "    model.cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "# criterion = torch.nn.MSELoss() \r\n",
    "lambda_l2 = torch.nn.Parameter(torch.empty(1))\r\n",
    "torch.nn.init.uniform_(lambda_l2)\r\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\r\n",
    "optimizer = torch.optim.SGD([model.params['w1'],model.params['w2'], model.params['b']], lr=learningRate)\r\n",
    "optimizer_re = torch.optim.SGD([lambda_l2], lr=learningRate_re)\r\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=100,gamma=0.5)\r\n",
    "\r\n",
    "# lambda_l2=-3.3661\r\n",
    "# lambda_l2=0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "## training steps\r\n",
    "# train(model,epochs,optimizer, scheduler, X_train, Y_noise_train,X_val, Y_noise_val,lambda_l2)\r\n",
    "# train(model,epochs,optimizer, X_train, Y_noise_train,X_val, Y_noise_val,lambda_l2)\r\n",
    "train_with_regularization(model, iters,epochs, optimizer,optimizer_re, X_train,Y_noise_train,Y_train, X_val, Y_val, Y_noise_val, lambda_l2)\r\n",
    "# train_with_regularization(model, iters,epochs, optimizer,optimizer_re, X_train,Y_noise_train,Y_train, X_val, Y_val, Y_noise_val, lambda_l2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0, train_loss 1292.3460693359375\n",
      "epoch 0, val_loss 460.6287536621094\n",
      "epoch 1, train_loss 1292.3123779296875\n",
      "epoch 1, val_loss 460.61724853515625\n",
      "epoch 2, train_loss 1292.2786865234375\n",
      "epoch 2, val_loss 460.60577392578125\n",
      "epoch 3, train_loss 1292.244873046875\n",
      "epoch 3, val_loss 460.59423828125\n",
      "epoch 4, train_loss 1292.21142578125\n",
      "epoch 4, val_loss 460.5828552246094\n",
      "epoch 5, train_loss 1292.1781005859375\n",
      "epoch 5, val_loss 460.5715026855469\n",
      "epoch 6, train_loss 1292.144775390625\n",
      "epoch 6, val_loss 460.56011962890625\n",
      "epoch 7, train_loss 1292.111328125\n",
      "epoch 7, val_loss 460.548828125\n",
      "epoch 8, train_loss 1292.078125\n",
      "epoch 8, val_loss 460.5374450683594\n",
      "epoch 9, train_loss 1292.0450439453125\n",
      "epoch 9, val_loss 460.5262145996094\n",
      "epoch 10, train_loss 1292.01220703125\n",
      "epoch 10, val_loss 460.5149841308594\n",
      "epoch 11, train_loss 1291.9791259765625\n",
      "epoch 11, val_loss 460.503662109375\n",
      "epoch 12, train_loss 1291.9462890625\n",
      "epoch 12, val_loss 460.49261474609375\n",
      "epoch 13, train_loss 1291.9134521484375\n",
      "epoch 13, val_loss 460.48138427734375\n",
      "epoch 14, train_loss 1291.880615234375\n",
      "epoch 14, val_loss 460.47027587890625\n",
      "epoch 15, train_loss 1291.8482666015625\n",
      "epoch 15, val_loss 460.45916748046875\n",
      "epoch 16, train_loss 1291.8154296875\n",
      "epoch 16, val_loss 460.4480285644531\n",
      "epoch 17, train_loss 1291.783203125\n",
      "epoch 17, val_loss 460.4370422363281\n",
      "epoch 18, train_loss 1291.7508544921875\n",
      "epoch 18, val_loss 460.42596435546875\n",
      "epoch 19, train_loss 1291.7186279296875\n",
      "epoch 19, val_loss 460.4150390625\n",
      "epoch 20, train_loss 1291.686279296875\n",
      "epoch 20, val_loss 460.4040832519531\n",
      "epoch 21, train_loss 1291.654296875\n",
      "epoch 21, val_loss 460.3930969238281\n",
      "epoch 22, train_loss 1291.622314453125\n",
      "epoch 22, val_loss 460.38214111328125\n",
      "epoch 23, train_loss 1291.5902099609375\n",
      "epoch 23, val_loss 460.371337890625\n",
      "epoch 24, train_loss 1291.558349609375\n",
      "epoch 24, val_loss 460.36041259765625\n",
      "epoch 25, train_loss 1291.5263671875\n",
      "epoch 25, val_loss 460.34967041015625\n",
      "epoch 26, train_loss 1291.494873046875\n",
      "epoch 26, val_loss 460.3388366699219\n",
      "epoch 27, train_loss 1291.463134765625\n",
      "epoch 27, val_loss 460.3280334472656\n",
      "epoch 28, train_loss 1291.431396484375\n",
      "epoch 28, val_loss 460.31732177734375\n",
      "epoch 29, train_loss 1291.4002685546875\n",
      "epoch 29, val_loss 460.306640625\n",
      "epoch 30, train_loss 1291.368896484375\n",
      "epoch 30, val_loss 460.2959289550781\n",
      "epoch 31, train_loss 1291.33740234375\n",
      "epoch 31, val_loss 460.2852478027344\n",
      "epoch 32, train_loss 1291.30615234375\n",
      "epoch 32, val_loss 460.2746276855469\n",
      "epoch 33, train_loss 1291.2750244140625\n",
      "epoch 33, val_loss 460.2640380859375\n",
      "epoch 34, train_loss 1291.2440185546875\n",
      "epoch 34, val_loss 460.25341796875\n",
      "epoch 35, train_loss 1291.2132568359375\n",
      "epoch 35, val_loss 460.2428894042969\n",
      "epoch 36, train_loss 1291.18212890625\n",
      "epoch 36, val_loss 460.2323913574219\n",
      "epoch 37, train_loss 1291.1513671875\n",
      "epoch 37, val_loss 460.2218322753906\n",
      "epoch 38, train_loss 1291.1204833984375\n",
      "epoch 38, val_loss 460.21136474609375\n",
      "epoch 39, train_loss 1291.0897216796875\n",
      "epoch 39, val_loss 460.2009582519531\n",
      "epoch 40, train_loss 1291.0592041015625\n",
      "epoch 40, val_loss 460.1905212402344\n",
      "epoch 41, train_loss 1291.028564453125\n",
      "epoch 41, val_loss 460.1800842285156\n",
      "epoch 42, train_loss 1290.998291015625\n",
      "epoch 42, val_loss 460.169677734375\n",
      "epoch 43, train_loss 1290.9676513671875\n",
      "epoch 43, val_loss 460.1593933105469\n",
      "epoch 44, train_loss 1290.9375\n",
      "epoch 44, val_loss 460.1490478515625\n",
      "epoch 45, train_loss 1290.9072265625\n",
      "epoch 45, val_loss 460.1387939453125\n",
      "epoch 46, train_loss 1290.876953125\n",
      "epoch 46, val_loss 460.1285095214844\n",
      "epoch 47, train_loss 1290.84716796875\n",
      "epoch 47, val_loss 460.1182861328125\n",
      "epoch 48, train_loss 1290.8170166015625\n",
      "epoch 48, val_loss 460.10809326171875\n",
      "epoch 49, train_loss 1290.7872314453125\n",
      "epoch 49, val_loss 460.097900390625\n",
      "epoch 50, train_loss 1290.75732421875\n",
      "epoch 50, val_loss 460.0877380371094\n",
      "epoch 51, train_loss 1290.7275390625\n",
      "epoch 51, val_loss 460.0775451660156\n",
      "epoch 52, train_loss 1290.697998046875\n",
      "epoch 52, val_loss 460.06744384765625\n",
      "epoch 53, train_loss 1290.668212890625\n",
      "epoch 53, val_loss 460.057373046875\n",
      "epoch 54, train_loss 1290.6387939453125\n",
      "epoch 54, val_loss 460.04736328125\n",
      "epoch 55, train_loss 1290.609130859375\n",
      "epoch 55, val_loss 460.037353515625\n",
      "epoch 56, train_loss 1290.5799560546875\n",
      "epoch 56, val_loss 460.0273132324219\n",
      "epoch 57, train_loss 1290.550537109375\n",
      "epoch 57, val_loss 460.017333984375\n",
      "epoch 58, train_loss 1290.5213623046875\n",
      "epoch 58, val_loss 460.0072937011719\n",
      "epoch 59, train_loss 1290.4921875\n",
      "epoch 59, val_loss 459.9974365234375\n",
      "epoch 60, train_loss 1290.462890625\n",
      "epoch 60, val_loss 459.98748779296875\n",
      "epoch 61, train_loss 1290.4339599609375\n",
      "epoch 61, val_loss 459.97760009765625\n",
      "epoch 62, train_loss 1290.40478515625\n",
      "epoch 62, val_loss 459.9677429199219\n",
      "epoch 63, train_loss 1290.3760986328125\n",
      "epoch 63, val_loss 459.9579162597656\n",
      "epoch 64, train_loss 1290.3475341796875\n",
      "epoch 64, val_loss 459.9481201171875\n",
      "epoch 65, train_loss 1290.318603515625\n",
      "epoch 65, val_loss 459.9383850097656\n",
      "epoch 66, train_loss 1290.2899169921875\n",
      "epoch 66, val_loss 459.9286193847656\n",
      "epoch 67, train_loss 1290.2613525390625\n",
      "epoch 67, val_loss 459.9188537597656\n",
      "epoch 68, train_loss 1290.2327880859375\n",
      "epoch 68, val_loss 459.9091491699219\n",
      "epoch 69, train_loss 1290.204345703125\n",
      "epoch 69, val_loss 459.89947509765625\n",
      "epoch 70, train_loss 1290.1761474609375\n",
      "epoch 70, val_loss 459.8898010253906\n",
      "epoch 71, train_loss 1290.147705078125\n",
      "epoch 71, val_loss 459.88018798828125\n",
      "epoch 72, train_loss 1290.1195068359375\n",
      "epoch 72, val_loss 459.87054443359375\n",
      "epoch 73, train_loss 1290.09130859375\n",
      "epoch 73, val_loss 459.86102294921875\n",
      "epoch 74, train_loss 1290.0631103515625\n",
      "epoch 74, val_loss 459.8514404296875\n",
      "epoch 75, train_loss 1290.03515625\n",
      "epoch 75, val_loss 459.8419189453125\n",
      "epoch 76, train_loss 1290.007080078125\n",
      "epoch 76, val_loss 459.8323669433594\n",
      "epoch 77, train_loss 1289.9793701171875\n",
      "epoch 77, val_loss 459.8228759765625\n",
      "epoch 78, train_loss 1289.9515380859375\n",
      "epoch 78, val_loss 459.8134460449219\n",
      "epoch 79, train_loss 1289.9237060546875\n",
      "epoch 79, val_loss 459.803955078125\n",
      "epoch 80, train_loss 1289.8961181640625\n",
      "epoch 80, val_loss 459.7945556640625\n",
      "epoch 81, train_loss 1289.8685302734375\n",
      "epoch 81, val_loss 459.78521728515625\n",
      "epoch 82, train_loss 1289.841064453125\n",
      "epoch 82, val_loss 459.7757568359375\n",
      "epoch 83, train_loss 1289.8135986328125\n",
      "epoch 83, val_loss 459.7665100097656\n",
      "epoch 84, train_loss 1289.786376953125\n",
      "epoch 84, val_loss 459.7571105957031\n",
      "epoch 85, train_loss 1289.7589111328125\n",
      "epoch 85, val_loss 459.74786376953125\n",
      "epoch 86, train_loss 1289.7315673828125\n",
      "epoch 86, val_loss 459.73858642578125\n",
      "epoch 87, train_loss 1289.7044677734375\n",
      "epoch 87, val_loss 459.7292785644531\n",
      "epoch 88, train_loss 1289.67724609375\n",
      "epoch 88, val_loss 459.7200622558594\n",
      "epoch 89, train_loss 1289.6502685546875\n",
      "epoch 89, val_loss 459.71087646484375\n",
      "epoch 90, train_loss 1289.623046875\n",
      "epoch 90, val_loss 459.7017517089844\n",
      "epoch 91, train_loss 1289.596435546875\n",
      "epoch 91, val_loss 459.6925354003906\n",
      "epoch 92, train_loss 1289.5693359375\n",
      "epoch 92, val_loss 459.6834411621094\n",
      "epoch 93, train_loss 1289.5428466796875\n",
      "epoch 93, val_loss 459.67431640625\n",
      "epoch 94, train_loss 1289.51611328125\n",
      "epoch 94, val_loss 459.6652526855469\n",
      "epoch 95, train_loss 1289.4893798828125\n",
      "epoch 95, val_loss 459.6561584472656\n",
      "epoch 96, train_loss 1289.4627685546875\n",
      "epoch 96, val_loss 459.6471252441406\n",
      "epoch 97, train_loss 1289.4364013671875\n",
      "epoch 97, val_loss 459.6380615234375\n",
      "epoch 98, train_loss 1289.4100341796875\n",
      "epoch 98, val_loss 459.62908935546875\n",
      "epoch 99, train_loss 1289.383544921875\n",
      "epoch 99, val_loss 459.6201171875\n",
      "Parameter containing:\n",
      "tensor([0.1946], requires_grad=True)\n",
      "iter 0, train_loss_regularization 1.4597976207733154\n",
      "iter 0, val_loss_regularization 1.4597976207733154\n",
      "epoch 0, train_loss 1289.3287353515625\n",
      "epoch 0, val_loss 459.5826416015625\n",
      "epoch 1, train_loss 1289.302490234375\n",
      "epoch 1, val_loss 459.5736389160156\n",
      "epoch 2, train_loss 1289.2762451171875\n",
      "epoch 2, val_loss 459.5647277832031\n",
      "epoch 3, train_loss 1289.25\n",
      "epoch 3, val_loss 459.555908203125\n",
      "epoch 4, train_loss 1289.2239990234375\n",
      "epoch 4, val_loss 459.5469665527344\n",
      "epoch 5, train_loss 1289.197998046875\n",
      "epoch 5, val_loss 459.5381774902344\n",
      "epoch 6, train_loss 1289.1722412109375\n",
      "epoch 6, val_loss 459.5293273925781\n",
      "epoch 7, train_loss 1289.146240234375\n",
      "epoch 7, val_loss 459.5205078125\n",
      "epoch 8, train_loss 1289.12060546875\n",
      "epoch 8, val_loss 459.51171875\n",
      "epoch 9, train_loss 1289.0948486328125\n",
      "epoch 9, val_loss 459.5029602050781\n",
      "epoch 10, train_loss 1289.069091796875\n",
      "epoch 10, val_loss 459.4942626953125\n",
      "epoch 11, train_loss 1289.0435791015625\n",
      "epoch 11, val_loss 459.4855041503906\n",
      "epoch 12, train_loss 1289.0181884765625\n",
      "epoch 12, val_loss 459.476806640625\n",
      "epoch 13, train_loss 1288.9925537109375\n",
      "epoch 13, val_loss 459.4681396484375\n",
      "epoch 14, train_loss 1288.967041015625\n",
      "epoch 14, val_loss 459.45953369140625\n",
      "epoch 15, train_loss 1288.941650390625\n",
      "epoch 15, val_loss 459.450927734375\n",
      "epoch 16, train_loss 1288.9163818359375\n",
      "epoch 16, val_loss 459.4422607421875\n",
      "epoch 17, train_loss 1288.8912353515625\n",
      "epoch 17, val_loss 459.4336853027344\n",
      "epoch 18, train_loss 1288.865966796875\n",
      "epoch 18, val_loss 459.42510986328125\n",
      "epoch 19, train_loss 1288.8408203125\n",
      "epoch 19, val_loss 459.4166259765625\n",
      "epoch 20, train_loss 1288.81591796875\n",
      "epoch 20, val_loss 459.4080810546875\n",
      "epoch 21, train_loss 1288.7911376953125\n",
      "epoch 21, val_loss 459.3995361328125\n",
      "epoch 22, train_loss 1288.76611328125\n",
      "epoch 22, val_loss 459.3910827636719\n",
      "epoch 23, train_loss 1288.7413330078125\n",
      "epoch 23, val_loss 459.3826599121094\n",
      "epoch 24, train_loss 1288.716552734375\n",
      "epoch 24, val_loss 459.3742370605469\n",
      "epoch 25, train_loss 1288.6917724609375\n",
      "epoch 25, val_loss 459.36578369140625\n",
      "epoch 26, train_loss 1288.667236328125\n",
      "epoch 26, val_loss 459.3574523925781\n",
      "epoch 27, train_loss 1288.6427001953125\n",
      "epoch 27, val_loss 459.3489990234375\n",
      "epoch 28, train_loss 1288.617919921875\n",
      "epoch 28, val_loss 459.3406066894531\n",
      "epoch 29, train_loss 1288.593505859375\n",
      "epoch 29, val_loss 459.3323669433594\n",
      "epoch 30, train_loss 1288.5692138671875\n",
      "epoch 30, val_loss 459.3240966796875\n",
      "epoch 31, train_loss 1288.5447998046875\n",
      "epoch 31, val_loss 459.3157653808594\n",
      "epoch 32, train_loss 1288.5206298828125\n",
      "epoch 32, val_loss 459.3075256347656\n",
      "epoch 33, train_loss 1288.496337890625\n",
      "epoch 33, val_loss 459.29925537109375\n",
      "epoch 34, train_loss 1288.47216796875\n",
      "epoch 34, val_loss 459.2910461425781\n",
      "epoch 35, train_loss 1288.4481201171875\n",
      "epoch 35, val_loss 459.2828369140625\n",
      "epoch 36, train_loss 1288.424072265625\n",
      "epoch 36, val_loss 459.2746276855469\n",
      "epoch 37, train_loss 1288.400146484375\n",
      "epoch 37, val_loss 459.2664489746094\n",
      "epoch 38, train_loss 1288.3760986328125\n",
      "epoch 38, val_loss 459.2582702636719\n",
      "epoch 39, train_loss 1288.3521728515625\n",
      "epoch 39, val_loss 459.2502136230469\n",
      "epoch 40, train_loss 1288.32861328125\n",
      "epoch 40, val_loss 459.2420959472656\n",
      "epoch 41, train_loss 1288.3048095703125\n",
      "epoch 41, val_loss 459.2340087890625\n",
      "epoch 42, train_loss 1288.281005859375\n",
      "epoch 42, val_loss 459.2259521484375\n",
      "epoch 43, train_loss 1288.2574462890625\n",
      "epoch 43, val_loss 459.2179870605469\n",
      "epoch 44, train_loss 1288.23388671875\n",
      "epoch 44, val_loss 459.2099304199219\n",
      "epoch 45, train_loss 1288.21044921875\n",
      "epoch 45, val_loss 459.2019348144531\n",
      "epoch 46, train_loss 1288.1868896484375\n",
      "epoch 46, val_loss 459.1938781738281\n",
      "epoch 47, train_loss 1288.16357421875\n",
      "epoch 47, val_loss 459.18597412109375\n",
      "epoch 48, train_loss 1288.1402587890625\n",
      "epoch 48, val_loss 459.1780700683594\n",
      "epoch 49, train_loss 1288.1170654296875\n",
      "epoch 49, val_loss 459.1701354980469\n",
      "epoch 50, train_loss 1288.0938720703125\n",
      "epoch 50, val_loss 459.1622009277344\n",
      "epoch 51, train_loss 1288.070556640625\n",
      "epoch 51, val_loss 459.1543273925781\n",
      "epoch 52, train_loss 1288.0477294921875\n",
      "epoch 52, val_loss 459.146484375\n",
      "epoch 53, train_loss 1288.0245361328125\n",
      "epoch 53, val_loss 459.1386413574219\n",
      "epoch 54, train_loss 1288.001708984375\n",
      "epoch 54, val_loss 459.1308288574219\n",
      "epoch 55, train_loss 1287.978759765625\n",
      "epoch 55, val_loss 459.123046875\n",
      "epoch 56, train_loss 1287.955810546875\n",
      "epoch 56, val_loss 459.1152648925781\n",
      "epoch 57, train_loss 1287.932861328125\n",
      "epoch 57, val_loss 459.1075134277344\n",
      "epoch 58, train_loss 1287.9102783203125\n",
      "epoch 58, val_loss 459.09979248046875\n",
      "epoch 59, train_loss 1287.8875732421875\n",
      "epoch 59, val_loss 459.09210205078125\n",
      "epoch 60, train_loss 1287.864990234375\n",
      "epoch 60, val_loss 459.0843505859375\n",
      "epoch 61, train_loss 1287.8424072265625\n",
      "epoch 61, val_loss 459.0766296386719\n",
      "epoch 62, train_loss 1287.81982421875\n",
      "epoch 62, val_loss 459.0689697265625\n",
      "epoch 63, train_loss 1287.7974853515625\n",
      "epoch 63, val_loss 459.06134033203125\n",
      "epoch 64, train_loss 1287.7752685546875\n",
      "epoch 64, val_loss 459.0537109375\n",
      "epoch 65, train_loss 1287.7528076171875\n",
      "epoch 65, val_loss 459.04608154296875\n",
      "epoch 66, train_loss 1287.7303466796875\n",
      "epoch 66, val_loss 459.03857421875\n",
      "epoch 67, train_loss 1287.708251953125\n",
      "epoch 67, val_loss 459.03094482421875\n",
      "epoch 68, train_loss 1287.68603515625\n",
      "epoch 68, val_loss 459.0234069824219\n",
      "epoch 69, train_loss 1287.6639404296875\n",
      "epoch 69, val_loss 459.01593017578125\n",
      "epoch 70, train_loss 1287.6416015625\n",
      "epoch 70, val_loss 459.00836181640625\n",
      "epoch 71, train_loss 1287.619873046875\n",
      "epoch 71, val_loss 459.0008850097656\n",
      "epoch 72, train_loss 1287.597900390625\n",
      "epoch 72, val_loss 458.99334716796875\n",
      "epoch 73, train_loss 1287.575927734375\n",
      "epoch 73, val_loss 458.9859619140625\n",
      "epoch 74, train_loss 1287.5540771484375\n",
      "epoch 74, val_loss 458.9785461425781\n",
      "epoch 75, train_loss 1287.5321044921875\n",
      "epoch 75, val_loss 458.9711608886719\n",
      "epoch 76, train_loss 1287.510498046875\n",
      "epoch 76, val_loss 458.9637145996094\n",
      "epoch 77, train_loss 1287.4888916015625\n",
      "epoch 77, val_loss 458.9563293457031\n",
      "epoch 78, train_loss 1287.46728515625\n",
      "epoch 78, val_loss 458.94903564453125\n",
      "epoch 79, train_loss 1287.44580078125\n",
      "epoch 79, val_loss 458.9416198730469\n",
      "epoch 80, train_loss 1287.42431640625\n",
      "epoch 80, val_loss 458.934326171875\n",
      "epoch 81, train_loss 1287.4029541015625\n",
      "epoch 81, val_loss 458.9269714355469\n",
      "epoch 82, train_loss 1287.3812255859375\n",
      "epoch 82, val_loss 458.91973876953125\n",
      "epoch 83, train_loss 1287.3599853515625\n",
      "epoch 83, val_loss 458.9124755859375\n",
      "epoch 84, train_loss 1287.338623046875\n",
      "epoch 84, val_loss 458.90521240234375\n",
      "epoch 85, train_loss 1287.3173828125\n",
      "epoch 85, val_loss 458.89801025390625\n",
      "epoch 86, train_loss 1287.2962646484375\n",
      "epoch 86, val_loss 458.89080810546875\n",
      "epoch 87, train_loss 1287.275146484375\n",
      "epoch 87, val_loss 458.883544921875\n",
      "epoch 88, train_loss 1287.254150390625\n",
      "epoch 88, val_loss 458.8763732910156\n",
      "epoch 89, train_loss 1287.2327880859375\n",
      "epoch 89, val_loss 458.8692626953125\n",
      "epoch 90, train_loss 1287.212158203125\n",
      "epoch 90, val_loss 458.8620910644531\n",
      "epoch 91, train_loss 1287.1910400390625\n",
      "epoch 91, val_loss 458.85498046875\n",
      "epoch 92, train_loss 1287.1700439453125\n",
      "epoch 92, val_loss 458.847900390625\n",
      "epoch 93, train_loss 1287.1494140625\n",
      "epoch 93, val_loss 458.8407287597656\n",
      "epoch 94, train_loss 1287.128662109375\n",
      "epoch 94, val_loss 458.8337097167969\n",
      "epoch 95, train_loss 1287.1080322265625\n",
      "epoch 95, val_loss 458.82666015625\n",
      "epoch 96, train_loss 1287.0872802734375\n",
      "epoch 96, val_loss 458.8197021484375\n",
      "epoch 97, train_loss 1287.0667724609375\n",
      "epoch 97, val_loss 458.8125915527344\n",
      "epoch 98, train_loss 1287.0460205078125\n",
      "epoch 98, val_loss 458.80560302734375\n",
      "epoch 99, train_loss 1287.0255126953125\n",
      "epoch 99, val_loss 458.7987060546875\n",
      "Parameter containing:\n",
      "tensor([0.1372], requires_grad=True)\n",
      "iter 1, train_loss_regularization 1.409355640411377\n",
      "iter 1, val_loss_regularization 1.409355640411377\n",
      "epoch 0, train_loss 1286.990234375\n",
      "epoch 0, val_loss 458.7767639160156\n",
      "epoch 1, train_loss 1286.9697265625\n",
      "epoch 1, val_loss 458.7696838378906\n",
      "epoch 2, train_loss 1286.9493408203125\n",
      "epoch 2, val_loss 458.7628479003906\n",
      "epoch 3, train_loss 1286.928955078125\n",
      "epoch 3, val_loss 458.7558898925781\n",
      "epoch 4, train_loss 1286.9088134765625\n",
      "epoch 4, val_loss 458.74908447265625\n",
      "epoch 5, train_loss 1286.888427734375\n",
      "epoch 5, val_loss 458.74212646484375\n",
      "epoch 6, train_loss 1286.868408203125\n",
      "epoch 6, val_loss 458.7352600097656\n",
      "epoch 7, train_loss 1286.84814453125\n",
      "epoch 7, val_loss 458.7284240722656\n",
      "epoch 8, train_loss 1286.8282470703125\n",
      "epoch 8, val_loss 458.7216491699219\n",
      "epoch 9, train_loss 1286.8082275390625\n",
      "epoch 9, val_loss 458.7148132324219\n",
      "epoch 10, train_loss 1286.7882080078125\n",
      "epoch 10, val_loss 458.7080078125\n",
      "epoch 11, train_loss 1286.7684326171875\n",
      "epoch 11, val_loss 458.701171875\n",
      "epoch 12, train_loss 1286.74853515625\n",
      "epoch 12, val_loss 458.6944580078125\n",
      "epoch 13, train_loss 1286.728759765625\n",
      "epoch 13, val_loss 458.6877136230469\n",
      "epoch 14, train_loss 1286.708984375\n",
      "epoch 14, val_loss 458.6809997558594\n",
      "epoch 15, train_loss 1286.689208984375\n",
      "epoch 15, val_loss 458.6742858886719\n",
      "epoch 16, train_loss 1286.669677734375\n",
      "epoch 16, val_loss 458.6676025390625\n",
      "epoch 17, train_loss 1286.6500244140625\n",
      "epoch 17, val_loss 458.660888671875\n",
      "epoch 18, train_loss 1286.63037109375\n",
      "epoch 18, val_loss 458.6542053222656\n",
      "epoch 19, train_loss 1286.61083984375\n",
      "epoch 19, val_loss 458.64764404296875\n",
      "epoch 20, train_loss 1286.5914306640625\n",
      "epoch 20, val_loss 458.6409912109375\n",
      "epoch 21, train_loss 1286.5721435546875\n",
      "epoch 21, val_loss 458.6343688964844\n",
      "epoch 22, train_loss 1286.5526123046875\n",
      "epoch 22, val_loss 458.62774658203125\n",
      "epoch 23, train_loss 1286.5333251953125\n",
      "epoch 23, val_loss 458.6212463378906\n",
      "epoch 24, train_loss 1286.51416015625\n",
      "epoch 24, val_loss 458.6146545410156\n",
      "epoch 25, train_loss 1286.494873046875\n",
      "epoch 25, val_loss 458.608154296875\n",
      "epoch 26, train_loss 1286.475830078125\n",
      "epoch 26, val_loss 458.60162353515625\n",
      "epoch 27, train_loss 1286.45654296875\n",
      "epoch 27, val_loss 458.5950927734375\n",
      "epoch 28, train_loss 1286.4375\n",
      "epoch 28, val_loss 458.5885925292969\n",
      "epoch 29, train_loss 1286.41845703125\n",
      "epoch 29, val_loss 458.58209228515625\n",
      "epoch 30, train_loss 1286.3995361328125\n",
      "epoch 30, val_loss 458.5756530761719\n",
      "epoch 31, train_loss 1286.3804931640625\n",
      "epoch 31, val_loss 458.5692443847656\n",
      "epoch 32, train_loss 1286.3616943359375\n",
      "epoch 32, val_loss 458.5627746582031\n",
      "epoch 33, train_loss 1286.3428955078125\n",
      "epoch 33, val_loss 458.5563659667969\n",
      "epoch 34, train_loss 1286.323974609375\n",
      "epoch 34, val_loss 458.5499267578125\n",
      "epoch 35, train_loss 1286.3052978515625\n",
      "epoch 35, val_loss 458.54364013671875\n",
      "epoch 36, train_loss 1286.28662109375\n",
      "epoch 36, val_loss 458.5372314453125\n",
      "epoch 37, train_loss 1286.2679443359375\n",
      "epoch 37, val_loss 458.5308837890625\n",
      "epoch 38, train_loss 1286.249267578125\n",
      "epoch 38, val_loss 458.5245666503906\n",
      "epoch 39, train_loss 1286.2308349609375\n",
      "epoch 39, val_loss 458.5181884765625\n",
      "epoch 40, train_loss 1286.212158203125\n",
      "epoch 40, val_loss 458.5119323730469\n",
      "epoch 41, train_loss 1286.1939697265625\n",
      "epoch 41, val_loss 458.5056457519531\n",
      "epoch 42, train_loss 1286.1754150390625\n",
      "epoch 42, val_loss 458.4993591308594\n",
      "epoch 43, train_loss 1286.156982421875\n",
      "epoch 43, val_loss 458.4931640625\n",
      "epoch 44, train_loss 1286.138671875\n",
      "epoch 44, val_loss 458.48687744140625\n",
      "epoch 45, train_loss 1286.120361328125\n",
      "epoch 45, val_loss 458.480712890625\n",
      "epoch 46, train_loss 1286.102294921875\n",
      "epoch 46, val_loss 458.4744567871094\n",
      "epoch 47, train_loss 1286.083984375\n",
      "epoch 47, val_loss 458.4682922363281\n",
      "epoch 48, train_loss 1286.06591796875\n",
      "epoch 48, val_loss 458.46209716796875\n",
      "epoch 49, train_loss 1286.0479736328125\n",
      "epoch 49, val_loss 458.4559326171875\n",
      "epoch 50, train_loss 1286.0296630859375\n",
      "epoch 50, val_loss 458.4497985839844\n",
      "epoch 51, train_loss 1286.0118408203125\n",
      "epoch 51, val_loss 458.4436950683594\n",
      "epoch 52, train_loss 1285.9937744140625\n",
      "epoch 52, val_loss 458.4376220703125\n",
      "epoch 53, train_loss 1285.975830078125\n",
      "epoch 53, val_loss 458.43145751953125\n",
      "epoch 54, train_loss 1285.9580078125\n",
      "epoch 54, val_loss 458.42535400390625\n",
      "epoch 55, train_loss 1285.9400634765625\n",
      "epoch 55, val_loss 458.4192810058594\n",
      "epoch 56, train_loss 1285.9222412109375\n",
      "epoch 56, val_loss 458.4132080078125\n",
      "epoch 57, train_loss 1285.9046630859375\n",
      "epoch 57, val_loss 458.4071960449219\n",
      "epoch 58, train_loss 1285.8868408203125\n",
      "epoch 58, val_loss 458.40118408203125\n",
      "epoch 59, train_loss 1285.869140625\n",
      "epoch 59, val_loss 458.3952331542969\n",
      "epoch 60, train_loss 1285.8515625\n",
      "epoch 60, val_loss 458.3891906738281\n",
      "epoch 61, train_loss 1285.833984375\n",
      "epoch 61, val_loss 458.38323974609375\n",
      "epoch 62, train_loss 1285.81640625\n",
      "epoch 62, val_loss 458.3772277832031\n",
      "epoch 63, train_loss 1285.799072265625\n",
      "epoch 63, val_loss 458.3713073730469\n",
      "epoch 64, train_loss 1285.78173828125\n",
      "epoch 64, val_loss 458.3653564453125\n",
      "epoch 65, train_loss 1285.7642822265625\n",
      "epoch 65, val_loss 458.35943603515625\n",
      "epoch 66, train_loss 1285.7469482421875\n",
      "epoch 66, val_loss 458.3534851074219\n",
      "epoch 67, train_loss 1285.7296142578125\n",
      "epoch 67, val_loss 458.3476867675781\n",
      "epoch 68, train_loss 1285.71240234375\n",
      "epoch 68, val_loss 458.341796875\n",
      "epoch 69, train_loss 1285.695068359375\n",
      "epoch 69, val_loss 458.3359680175781\n",
      "epoch 70, train_loss 1285.677978515625\n",
      "epoch 70, val_loss 458.33013916015625\n",
      "epoch 71, train_loss 1285.6610107421875\n",
      "epoch 71, val_loss 458.32421875\n",
      "epoch 72, train_loss 1285.6439208984375\n",
      "epoch 72, val_loss 458.3184814453125\n",
      "epoch 73, train_loss 1285.6268310546875\n",
      "epoch 73, val_loss 458.3126220703125\n",
      "epoch 74, train_loss 1285.60986328125\n",
      "epoch 74, val_loss 458.306884765625\n",
      "epoch 75, train_loss 1285.5927734375\n",
      "epoch 75, val_loss 458.3011169433594\n",
      "epoch 76, train_loss 1285.575927734375\n",
      "epoch 76, val_loss 458.2953796386719\n",
      "epoch 77, train_loss 1285.5592041015625\n",
      "epoch 77, val_loss 458.2895812988281\n",
      "epoch 78, train_loss 1285.5423583984375\n",
      "epoch 78, val_loss 458.28387451171875\n",
      "epoch 79, train_loss 1285.5255126953125\n",
      "epoch 79, val_loss 458.2781677246094\n",
      "epoch 80, train_loss 1285.5087890625\n",
      "epoch 80, val_loss 458.2724609375\n",
      "epoch 81, train_loss 1285.491943359375\n",
      "epoch 81, val_loss 458.26678466796875\n",
      "epoch 82, train_loss 1285.475341796875\n",
      "epoch 82, val_loss 458.2610778808594\n",
      "epoch 83, train_loss 1285.4588623046875\n",
      "epoch 83, val_loss 458.25543212890625\n",
      "epoch 84, train_loss 1285.4422607421875\n",
      "epoch 84, val_loss 458.2498474121094\n",
      "epoch 85, train_loss 1285.425537109375\n",
      "epoch 85, val_loss 458.24420166015625\n",
      "epoch 86, train_loss 1285.4091796875\n",
      "epoch 86, val_loss 458.2384948730469\n",
      "epoch 87, train_loss 1285.392578125\n",
      "epoch 87, val_loss 458.2329406738281\n",
      "epoch 88, train_loss 1285.3763427734375\n",
      "epoch 88, val_loss 458.22735595703125\n",
      "epoch 89, train_loss 1285.35986328125\n",
      "epoch 89, val_loss 458.22186279296875\n",
      "epoch 90, train_loss 1285.3433837890625\n",
      "epoch 90, val_loss 458.21624755859375\n",
      "epoch 91, train_loss 1285.3272705078125\n",
      "epoch 91, val_loss 458.2107238769531\n",
      "epoch 92, train_loss 1285.3109130859375\n",
      "epoch 92, val_loss 458.2051696777344\n",
      "epoch 93, train_loss 1285.2947998046875\n",
      "epoch 93, val_loss 458.1996765136719\n",
      "epoch 94, train_loss 1285.2786865234375\n",
      "epoch 94, val_loss 458.1941833496094\n",
      "epoch 95, train_loss 1285.262451171875\n",
      "epoch 95, val_loss 458.1886901855469\n",
      "epoch 96, train_loss 1285.2464599609375\n",
      "epoch 96, val_loss 458.1832275390625\n",
      "epoch 97, train_loss 1285.2303466796875\n",
      "epoch 97, val_loss 458.1777648925781\n",
      "epoch 98, train_loss 1285.21435546875\n",
      "epoch 98, val_loss 458.1722717285156\n",
      "epoch 99, train_loss 1285.1983642578125\n",
      "epoch 99, val_loss 458.16680908203125\n",
      "Parameter containing:\n",
      "tensor([0.0955], requires_grad=True)\n",
      "iter 2, train_loss_regularization 1.3677009344100952\n",
      "iter 2, val_loss_regularization 1.3677009344100952\n",
      "epoch 0, train_loss 1285.1749267578125\n",
      "epoch 0, val_loss 458.1538391113281\n",
      "epoch 1, train_loss 1285.1591796875\n",
      "epoch 1, val_loss 458.14837646484375\n",
      "epoch 2, train_loss 1285.1431884765625\n",
      "epoch 2, val_loss 458.1430969238281\n",
      "epoch 3, train_loss 1285.1273193359375\n",
      "epoch 3, val_loss 458.1376647949219\n",
      "epoch 4, train_loss 1285.1116943359375\n",
      "epoch 4, val_loss 458.13226318359375\n",
      "epoch 5, train_loss 1285.095947265625\n",
      "epoch 5, val_loss 458.1269226074219\n",
      "epoch 6, train_loss 1285.080078125\n",
      "epoch 6, val_loss 458.12158203125\n",
      "epoch 7, train_loss 1285.064453125\n",
      "epoch 7, val_loss 458.1162414550781\n",
      "epoch 8, train_loss 1285.048828125\n",
      "epoch 8, val_loss 458.1109619140625\n",
      "epoch 9, train_loss 1285.033447265625\n",
      "epoch 9, val_loss 458.1056823730469\n",
      "epoch 10, train_loss 1285.017822265625\n",
      "epoch 10, val_loss 458.100341796875\n",
      "epoch 11, train_loss 1285.0025634765625\n",
      "epoch 11, val_loss 458.09503173828125\n",
      "epoch 12, train_loss 1284.98681640625\n",
      "epoch 12, val_loss 458.08984375\n",
      "epoch 13, train_loss 1284.9715576171875\n",
      "epoch 13, val_loss 458.08453369140625\n",
      "epoch 14, train_loss 1284.9560546875\n",
      "epoch 14, val_loss 458.0792541503906\n",
      "epoch 15, train_loss 1284.9407958984375\n",
      "epoch 15, val_loss 458.0740661621094\n",
      "epoch 16, train_loss 1284.925537109375\n",
      "epoch 16, val_loss 458.0688781738281\n",
      "epoch 17, train_loss 1284.91015625\n",
      "epoch 17, val_loss 458.0636901855469\n",
      "epoch 18, train_loss 1284.8948974609375\n",
      "epoch 18, val_loss 458.05853271484375\n",
      "epoch 19, train_loss 1284.879638671875\n",
      "epoch 19, val_loss 458.0533142089844\n",
      "epoch 20, train_loss 1284.864501953125\n",
      "epoch 20, val_loss 458.0482482910156\n",
      "epoch 21, train_loss 1284.849365234375\n",
      "epoch 21, val_loss 458.0430603027344\n",
      "epoch 22, train_loss 1284.8343505859375\n",
      "epoch 22, val_loss 458.03790283203125\n",
      "epoch 23, train_loss 1284.8193359375\n",
      "epoch 23, val_loss 458.0328369140625\n",
      "epoch 24, train_loss 1284.8043212890625\n",
      "epoch 24, val_loss 458.0277404785156\n",
      "epoch 25, train_loss 1284.78955078125\n",
      "epoch 25, val_loss 458.02252197265625\n",
      "epoch 26, train_loss 1284.7742919921875\n",
      "epoch 26, val_loss 458.0174865722656\n",
      "epoch 27, train_loss 1284.7596435546875\n",
      "epoch 27, val_loss 458.012451171875\n",
      "epoch 28, train_loss 1284.74462890625\n",
      "epoch 28, val_loss 458.00732421875\n",
      "epoch 29, train_loss 1284.7298583984375\n",
      "epoch 29, val_loss 458.0023193359375\n",
      "epoch 30, train_loss 1284.715087890625\n",
      "epoch 30, val_loss 457.99725341796875\n",
      "epoch 31, train_loss 1284.700439453125\n",
      "epoch 31, val_loss 457.9922790527344\n",
      "epoch 32, train_loss 1284.6856689453125\n",
      "epoch 32, val_loss 457.98724365234375\n",
      "epoch 33, train_loss 1284.6710205078125\n",
      "epoch 33, val_loss 457.9822998046875\n",
      "epoch 34, train_loss 1284.65625\n",
      "epoch 34, val_loss 457.97735595703125\n",
      "epoch 35, train_loss 1284.6419677734375\n",
      "epoch 35, val_loss 457.97235107421875\n",
      "epoch 36, train_loss 1284.6273193359375\n",
      "epoch 36, val_loss 457.9674072265625\n",
      "epoch 37, train_loss 1284.612548828125\n",
      "epoch 37, val_loss 457.9624938964844\n",
      "epoch 38, train_loss 1284.5980224609375\n",
      "epoch 38, val_loss 457.95751953125\n",
      "epoch 39, train_loss 1284.5838623046875\n",
      "epoch 39, val_loss 457.9526062011719\n",
      "epoch 40, train_loss 1284.5693359375\n",
      "epoch 40, val_loss 457.94769287109375\n",
      "epoch 41, train_loss 1284.5548095703125\n",
      "epoch 41, val_loss 457.9427795410156\n",
      "epoch 42, train_loss 1284.5406494140625\n",
      "epoch 42, val_loss 457.93792724609375\n",
      "epoch 43, train_loss 1284.5263671875\n",
      "epoch 43, val_loss 457.9330139160156\n",
      "epoch 44, train_loss 1284.5120849609375\n",
      "epoch 44, val_loss 457.92816162109375\n",
      "epoch 45, train_loss 1284.4979248046875\n",
      "epoch 45, val_loss 457.9233093261719\n",
      "epoch 46, train_loss 1284.4837646484375\n",
      "epoch 46, val_loss 457.9184875488281\n",
      "epoch 47, train_loss 1284.469482421875\n",
      "epoch 47, val_loss 457.9136047363281\n",
      "epoch 48, train_loss 1284.455322265625\n",
      "epoch 48, val_loss 457.9088439941406\n",
      "epoch 49, train_loss 1284.4412841796875\n",
      "epoch 49, val_loss 457.9040222167969\n",
      "epoch 50, train_loss 1284.4273681640625\n",
      "epoch 50, val_loss 457.8992614746094\n",
      "epoch 51, train_loss 1284.4132080078125\n",
      "epoch 51, val_loss 457.8945007324219\n",
      "epoch 52, train_loss 1284.3992919921875\n",
      "epoch 52, val_loss 457.8898010253906\n",
      "epoch 53, train_loss 1284.385498046875\n",
      "epoch 53, val_loss 457.885009765625\n",
      "epoch 54, train_loss 1284.371337890625\n",
      "epoch 54, val_loss 457.8802185058594\n",
      "epoch 55, train_loss 1284.357666015625\n",
      "epoch 55, val_loss 457.87554931640625\n",
      "epoch 56, train_loss 1284.343505859375\n",
      "epoch 56, val_loss 457.87078857421875\n",
      "epoch 57, train_loss 1284.329833984375\n",
      "epoch 57, val_loss 457.8660888671875\n",
      "epoch 58, train_loss 1284.316162109375\n",
      "epoch 58, val_loss 457.8614196777344\n",
      "epoch 59, train_loss 1284.30224609375\n",
      "epoch 59, val_loss 457.8567199707031\n",
      "epoch 60, train_loss 1284.28857421875\n",
      "epoch 60, val_loss 457.85205078125\n",
      "epoch 61, train_loss 1284.2750244140625\n",
      "epoch 61, val_loss 457.8474426269531\n",
      "epoch 62, train_loss 1284.26123046875\n",
      "epoch 62, val_loss 457.84271240234375\n",
      "epoch 63, train_loss 1284.2476806640625\n",
      "epoch 63, val_loss 457.838134765625\n",
      "epoch 64, train_loss 1284.2342529296875\n",
      "epoch 64, val_loss 457.8334655761719\n",
      "epoch 65, train_loss 1284.2205810546875\n",
      "epoch 65, val_loss 457.8288879394531\n",
      "epoch 66, train_loss 1284.2071533203125\n",
      "epoch 66, val_loss 457.8243103027344\n",
      "epoch 67, train_loss 1284.1934814453125\n",
      "epoch 67, val_loss 457.8196716308594\n",
      "epoch 68, train_loss 1284.18017578125\n",
      "epoch 68, val_loss 457.815185546875\n",
      "epoch 69, train_loss 1284.1666259765625\n",
      "epoch 69, val_loss 457.8105773925781\n",
      "epoch 70, train_loss 1284.1533203125\n",
      "epoch 70, val_loss 457.80609130859375\n",
      "epoch 71, train_loss 1284.1400146484375\n",
      "epoch 71, val_loss 457.80145263671875\n",
      "epoch 72, train_loss 1284.126708984375\n",
      "epoch 72, val_loss 457.7969970703125\n",
      "epoch 73, train_loss 1284.113525390625\n",
      "epoch 73, val_loss 457.7924499511719\n",
      "epoch 74, train_loss 1284.1002197265625\n",
      "epoch 74, val_loss 457.78790283203125\n",
      "epoch 75, train_loss 1284.0869140625\n",
      "epoch 75, val_loss 457.7834167480469\n",
      "epoch 76, train_loss 1284.0738525390625\n",
      "epoch 76, val_loss 457.7789611816406\n",
      "epoch 77, train_loss 1284.0606689453125\n",
      "epoch 77, val_loss 457.7744445800781\n",
      "epoch 78, train_loss 1284.0474853515625\n",
      "epoch 78, val_loss 457.7699890136719\n",
      "epoch 79, train_loss 1284.034423828125\n",
      "epoch 79, val_loss 457.7655334472656\n",
      "epoch 80, train_loss 1284.021484375\n",
      "epoch 80, val_loss 457.7611389160156\n",
      "epoch 81, train_loss 1284.0084228515625\n",
      "epoch 81, val_loss 457.75665283203125\n",
      "epoch 82, train_loss 1283.995361328125\n",
      "epoch 82, val_loss 457.7523193359375\n",
      "epoch 83, train_loss 1283.9825439453125\n",
      "epoch 83, val_loss 457.7478332519531\n",
      "epoch 84, train_loss 1283.9698486328125\n",
      "epoch 84, val_loss 457.7434997558594\n",
      "epoch 85, train_loss 1283.9566650390625\n",
      "epoch 85, val_loss 457.7391357421875\n",
      "epoch 86, train_loss 1283.94384765625\n",
      "epoch 86, val_loss 457.73468017578125\n",
      "epoch 87, train_loss 1283.930908203125\n",
      "epoch 87, val_loss 457.73028564453125\n",
      "epoch 88, train_loss 1283.9183349609375\n",
      "epoch 88, val_loss 457.7259826660156\n",
      "epoch 89, train_loss 1283.905517578125\n",
      "epoch 89, val_loss 457.7216491699219\n",
      "epoch 90, train_loss 1283.8927001953125\n",
      "epoch 90, val_loss 457.7173156738281\n",
      "epoch 91, train_loss 1283.880126953125\n",
      "epoch 91, val_loss 457.7129821777344\n",
      "epoch 92, train_loss 1283.867431640625\n",
      "epoch 92, val_loss 457.7087097167969\n",
      "epoch 93, train_loss 1283.854736328125\n",
      "epoch 93, val_loss 457.70440673828125\n",
      "epoch 94, train_loss 1283.8421630859375\n",
      "epoch 94, val_loss 457.7001037597656\n",
      "epoch 95, train_loss 1283.82958984375\n",
      "epoch 95, val_loss 457.69586181640625\n",
      "epoch 96, train_loss 1283.8170166015625\n",
      "epoch 96, val_loss 457.6915588378906\n",
      "epoch 97, train_loss 1283.8045654296875\n",
      "epoch 97, val_loss 457.6872863769531\n",
      "epoch 98, train_loss 1283.7921142578125\n",
      "epoch 98, val_loss 457.68304443359375\n",
      "epoch 99, train_loss 1283.7796630859375\n",
      "epoch 99, val_loss 457.6788330078125\n",
      "Parameter containing:\n",
      "tensor([0.0656], requires_grad=True)\n",
      "iter 3, train_loss_regularization 1.332927942276001\n",
      "iter 3, val_loss_regularization 1.332927942276001\n",
      "epoch 0, train_loss 1283.7635498046875\n",
      "epoch 0, val_loss 457.6708068847656\n",
      "epoch 1, train_loss 1283.751220703125\n",
      "epoch 1, val_loss 457.6665954589844\n",
      "epoch 2, train_loss 1283.7386474609375\n",
      "epoch 2, val_loss 457.66241455078125\n",
      "epoch 3, train_loss 1283.7265625\n",
      "epoch 3, val_loss 457.6582336425781\n",
      "epoch 4, train_loss 1283.7142333984375\n",
      "epoch 4, val_loss 457.6540222167969\n",
      "epoch 5, train_loss 1283.7017822265625\n",
      "epoch 5, val_loss 457.6498718261719\n",
      "epoch 6, train_loss 1283.689697265625\n",
      "epoch 6, val_loss 457.64569091796875\n",
      "epoch 7, train_loss 1283.6773681640625\n",
      "epoch 7, val_loss 457.64154052734375\n",
      "epoch 8, train_loss 1283.665283203125\n",
      "epoch 8, val_loss 457.63739013671875\n",
      "epoch 9, train_loss 1283.6531982421875\n",
      "epoch 9, val_loss 457.6332702636719\n",
      "epoch 10, train_loss 1283.6409912109375\n",
      "epoch 10, val_loss 457.6290588378906\n",
      "epoch 11, train_loss 1283.629150390625\n",
      "epoch 11, val_loss 457.625\n",
      "epoch 12, train_loss 1283.616943359375\n",
      "epoch 12, val_loss 457.620849609375\n",
      "epoch 13, train_loss 1283.6048583984375\n",
      "epoch 13, val_loss 457.6168212890625\n",
      "epoch 14, train_loss 1283.593017578125\n",
      "epoch 14, val_loss 457.6127624511719\n",
      "epoch 15, train_loss 1283.5811767578125\n",
      "epoch 15, val_loss 457.60870361328125\n",
      "epoch 16, train_loss 1283.569091796875\n",
      "epoch 16, val_loss 457.60455322265625\n",
      "epoch 17, train_loss 1283.5572509765625\n",
      "epoch 17, val_loss 457.6005554199219\n",
      "epoch 18, train_loss 1283.5452880859375\n",
      "epoch 18, val_loss 457.596435546875\n",
      "epoch 19, train_loss 1283.5335693359375\n",
      "epoch 19, val_loss 457.5924987792969\n",
      "epoch 20, train_loss 1283.521728515625\n",
      "epoch 20, val_loss 457.5884094238281\n",
      "epoch 21, train_loss 1283.5098876953125\n",
      "epoch 21, val_loss 457.58447265625\n",
      "epoch 22, train_loss 1283.4981689453125\n",
      "epoch 22, val_loss 457.5804443359375\n",
      "epoch 23, train_loss 1283.486328125\n",
      "epoch 23, val_loss 457.5764465332031\n",
      "epoch 24, train_loss 1283.4747314453125\n",
      "epoch 24, val_loss 457.5723876953125\n",
      "epoch 25, train_loss 1283.4630126953125\n",
      "epoch 25, val_loss 457.5684814453125\n",
      "epoch 26, train_loss 1283.4515380859375\n",
      "epoch 26, val_loss 457.56451416015625\n",
      "epoch 27, train_loss 1283.4398193359375\n",
      "epoch 27, val_loss 457.5605773925781\n",
      "epoch 28, train_loss 1283.4283447265625\n",
      "epoch 28, val_loss 457.5566101074219\n",
      "epoch 29, train_loss 1283.416748046875\n",
      "epoch 29, val_loss 457.5527038574219\n",
      "epoch 30, train_loss 1283.4053955078125\n",
      "epoch 30, val_loss 457.54876708984375\n",
      "epoch 31, train_loss 1283.393798828125\n",
      "epoch 31, val_loss 457.54486083984375\n",
      "epoch 32, train_loss 1283.38232421875\n",
      "epoch 32, val_loss 457.5409851074219\n",
      "epoch 33, train_loss 1283.3709716796875\n",
      "epoch 33, val_loss 457.537109375\n",
      "epoch 34, train_loss 1283.3594970703125\n",
      "epoch 34, val_loss 457.53314208984375\n",
      "epoch 35, train_loss 1283.3480224609375\n",
      "epoch 35, val_loss 457.529296875\n",
      "epoch 36, train_loss 1283.336669921875\n",
      "epoch 36, val_loss 457.5254821777344\n",
      "epoch 37, train_loss 1283.325439453125\n",
      "epoch 37, val_loss 457.5216064453125\n",
      "epoch 38, train_loss 1283.314208984375\n",
      "epoch 38, val_loss 457.5177307128906\n",
      "epoch 39, train_loss 1283.302978515625\n",
      "epoch 39, val_loss 457.5138854980469\n",
      "epoch 40, train_loss 1283.2916259765625\n",
      "epoch 40, val_loss 457.510009765625\n",
      "epoch 41, train_loss 1283.280517578125\n",
      "epoch 41, val_loss 457.5062255859375\n",
      "epoch 42, train_loss 1283.269287109375\n",
      "epoch 42, val_loss 457.5024719238281\n",
      "epoch 43, train_loss 1283.2581787109375\n",
      "epoch 43, val_loss 457.49859619140625\n",
      "epoch 44, train_loss 1283.2470703125\n",
      "epoch 44, val_loss 457.4949035644531\n",
      "epoch 45, train_loss 1283.23583984375\n",
      "epoch 45, val_loss 457.4910583496094\n",
      "epoch 46, train_loss 1283.224853515625\n",
      "epoch 46, val_loss 457.4872741699219\n",
      "epoch 47, train_loss 1283.2137451171875\n",
      "epoch 47, val_loss 457.4835205078125\n",
      "epoch 48, train_loss 1283.2027587890625\n",
      "epoch 48, val_loss 457.4797668457031\n",
      "epoch 49, train_loss 1283.1917724609375\n",
      "epoch 49, val_loss 457.47601318359375\n",
      "epoch 50, train_loss 1283.180908203125\n",
      "epoch 50, val_loss 457.4722595214844\n",
      "epoch 51, train_loss 1283.1700439453125\n",
      "epoch 51, val_loss 457.4685974121094\n",
      "epoch 52, train_loss 1283.158935546875\n",
      "epoch 52, val_loss 457.46484375\n",
      "epoch 53, train_loss 1283.14794921875\n",
      "epoch 53, val_loss 457.4611511230469\n",
      "epoch 54, train_loss 1283.1370849609375\n",
      "epoch 54, val_loss 457.45751953125\n",
      "epoch 55, train_loss 1283.12646484375\n",
      "epoch 55, val_loss 457.4537658691406\n",
      "epoch 56, train_loss 1283.115478515625\n",
      "epoch 56, val_loss 457.4500427246094\n",
      "epoch 57, train_loss 1283.104736328125\n",
      "epoch 57, val_loss 457.4464416503906\n",
      "epoch 58, train_loss 1283.093994140625\n",
      "epoch 58, val_loss 457.4427185058594\n",
      "epoch 59, train_loss 1283.083251953125\n",
      "epoch 59, val_loss 457.4390869140625\n",
      "epoch 60, train_loss 1283.0726318359375\n",
      "epoch 60, val_loss 457.43548583984375\n",
      "epoch 61, train_loss 1283.0618896484375\n",
      "epoch 61, val_loss 457.4317626953125\n",
      "epoch 62, train_loss 1283.0513916015625\n",
      "epoch 62, val_loss 457.4281921386719\n",
      "epoch 63, train_loss 1283.040771484375\n",
      "epoch 63, val_loss 457.424560546875\n",
      "epoch 64, train_loss 1283.0301513671875\n",
      "epoch 64, val_loss 457.42095947265625\n",
      "epoch 65, train_loss 1283.0196533203125\n",
      "epoch 65, val_loss 457.4173889160156\n",
      "epoch 66, train_loss 1283.0091552734375\n",
      "epoch 66, val_loss 457.4136962890625\n",
      "epoch 67, train_loss 1282.9984130859375\n",
      "epoch 67, val_loss 457.4101257324219\n",
      "epoch 68, train_loss 1282.9881591796875\n",
      "epoch 68, val_loss 457.4066467285156\n",
      "epoch 69, train_loss 1282.9776611328125\n",
      "epoch 69, val_loss 457.4031066894531\n",
      "epoch 70, train_loss 1282.9671630859375\n",
      "epoch 70, val_loss 457.39947509765625\n",
      "epoch 71, train_loss 1282.956787109375\n",
      "epoch 71, val_loss 457.39593505859375\n",
      "epoch 72, train_loss 1282.9464111328125\n",
      "epoch 72, val_loss 457.39239501953125\n",
      "epoch 73, train_loss 1282.9359130859375\n",
      "epoch 73, val_loss 457.3889465332031\n",
      "epoch 74, train_loss 1282.9256591796875\n",
      "epoch 74, val_loss 457.3853759765625\n",
      "epoch 75, train_loss 1282.9154052734375\n",
      "epoch 75, val_loss 457.3818054199219\n",
      "epoch 76, train_loss 1282.9052734375\n",
      "epoch 76, val_loss 457.3782958984375\n",
      "epoch 77, train_loss 1282.89501953125\n",
      "epoch 77, val_loss 457.37493896484375\n",
      "epoch 78, train_loss 1282.884521484375\n",
      "epoch 78, val_loss 457.371337890625\n",
      "epoch 79, train_loss 1282.87451171875\n",
      "epoch 79, val_loss 457.3679504394531\n",
      "epoch 80, train_loss 1282.8642578125\n",
      "epoch 80, val_loss 457.364501953125\n",
      "epoch 81, train_loss 1282.8541259765625\n",
      "epoch 81, val_loss 457.3609924316406\n",
      "epoch 82, train_loss 1282.843994140625\n",
      "epoch 82, val_loss 457.3575134277344\n",
      "epoch 83, train_loss 1282.8338623046875\n",
      "epoch 83, val_loss 457.3541259765625\n",
      "epoch 84, train_loss 1282.82373046875\n",
      "epoch 84, val_loss 457.3506774902344\n",
      "epoch 85, train_loss 1282.8138427734375\n",
      "epoch 85, val_loss 457.3471984863281\n",
      "epoch 86, train_loss 1282.8037109375\n",
      "epoch 86, val_loss 457.3438415527344\n",
      "epoch 87, train_loss 1282.7935791015625\n",
      "epoch 87, val_loss 457.34039306640625\n",
      "epoch 88, train_loss 1282.7838134765625\n",
      "epoch 88, val_loss 457.3370056152344\n",
      "epoch 89, train_loss 1282.7738037109375\n",
      "epoch 89, val_loss 457.33367919921875\n",
      "epoch 90, train_loss 1282.763916015625\n",
      "epoch 90, val_loss 457.33026123046875\n",
      "epoch 91, train_loss 1282.7537841796875\n",
      "epoch 91, val_loss 457.3268737792969\n",
      "epoch 92, train_loss 1282.7440185546875\n",
      "epoch 92, val_loss 457.3234558105469\n",
      "epoch 93, train_loss 1282.734130859375\n",
      "epoch 93, val_loss 457.32012939453125\n",
      "epoch 94, train_loss 1282.7244873046875\n",
      "epoch 94, val_loss 457.3167419433594\n",
      "epoch 95, train_loss 1282.714599609375\n",
      "epoch 95, val_loss 457.31341552734375\n",
      "epoch 96, train_loss 1282.704833984375\n",
      "epoch 96, val_loss 457.31011962890625\n",
      "epoch 97, train_loss 1282.6951904296875\n",
      "epoch 97, val_loss 457.3067932128906\n",
      "epoch 98, train_loss 1282.6854248046875\n",
      "epoch 98, val_loss 457.303466796875\n",
      "epoch 99, train_loss 1282.6756591796875\n",
      "epoch 99, val_loss 457.3001403808594\n",
      "Parameter containing:\n",
      "tensor([0.0446], requires_grad=True)\n",
      "iter 4, train_loss_regularization 1.3035602569580078\n",
      "iter 4, val_loss_regularization 1.3035602569580078\n",
      "epoch 0, train_loss 1282.6639404296875\n",
      "epoch 0, val_loss 457.2950744628906\n",
      "epoch 1, train_loss 1282.654541015625\n",
      "epoch 1, val_loss 457.291748046875\n",
      "epoch 2, train_loss 1282.644775390625\n",
      "epoch 2, val_loss 457.2884521484375\n",
      "epoch 3, train_loss 1282.63525390625\n",
      "epoch 3, val_loss 457.28515625\n",
      "epoch 4, train_loss 1282.6256103515625\n",
      "epoch 4, val_loss 457.28192138671875\n",
      "epoch 5, train_loss 1282.6160888671875\n",
      "epoch 5, val_loss 457.2787170410156\n",
      "epoch 6, train_loss 1282.606689453125\n",
      "epoch 6, val_loss 457.2753601074219\n",
      "epoch 7, train_loss 1282.596923828125\n",
      "epoch 7, val_loss 457.2721252441406\n",
      "epoch 8, train_loss 1282.5875244140625\n",
      "epoch 8, val_loss 457.2689514160156\n",
      "epoch 9, train_loss 1282.5780029296875\n",
      "epoch 9, val_loss 457.2657165527344\n",
      "epoch 10, train_loss 1282.568603515625\n",
      "epoch 10, val_loss 457.262451171875\n",
      "epoch 11, train_loss 1282.559326171875\n",
      "epoch 11, val_loss 457.25933837890625\n",
      "epoch 12, train_loss 1282.5496826171875\n",
      "epoch 12, val_loss 457.25604248046875\n",
      "epoch 13, train_loss 1282.540283203125\n",
      "epoch 13, val_loss 457.2528381347656\n",
      "epoch 14, train_loss 1282.531005859375\n",
      "epoch 14, val_loss 457.2496643066406\n",
      "epoch 15, train_loss 1282.5216064453125\n",
      "epoch 15, val_loss 457.2464599609375\n",
      "epoch 16, train_loss 1282.5123291015625\n",
      "epoch 16, val_loss 457.2432861328125\n",
      "epoch 17, train_loss 1282.5030517578125\n",
      "epoch 17, val_loss 457.24017333984375\n",
      "epoch 18, train_loss 1282.49365234375\n",
      "epoch 18, val_loss 457.23699951171875\n",
      "epoch 19, train_loss 1282.4844970703125\n",
      "epoch 19, val_loss 457.2337951660156\n",
      "epoch 20, train_loss 1282.475341796875\n",
      "epoch 20, val_loss 457.23065185546875\n",
      "epoch 21, train_loss 1282.466064453125\n",
      "epoch 21, val_loss 457.2275390625\n",
      "epoch 22, train_loss 1282.4569091796875\n",
      "epoch 22, val_loss 457.22442626953125\n",
      "epoch 23, train_loss 1282.447998046875\n",
      "epoch 23, val_loss 457.2213134765625\n",
      "epoch 24, train_loss 1282.4388427734375\n",
      "epoch 24, val_loss 457.2181091308594\n",
      "epoch 25, train_loss 1282.4296875\n",
      "epoch 25, val_loss 457.215087890625\n",
      "epoch 26, train_loss 1282.42041015625\n",
      "epoch 26, val_loss 457.2119445800781\n",
      "epoch 27, train_loss 1282.4114990234375\n",
      "epoch 27, val_loss 457.20892333984375\n",
      "epoch 28, train_loss 1282.402587890625\n",
      "epoch 28, val_loss 457.2058410644531\n",
      "epoch 29, train_loss 1282.3934326171875\n",
      "epoch 29, val_loss 457.2027587890625\n",
      "epoch 30, train_loss 1282.384521484375\n",
      "epoch 30, val_loss 457.1996154785156\n",
      "epoch 31, train_loss 1282.375244140625\n",
      "epoch 31, val_loss 457.1966247558594\n",
      "epoch 32, train_loss 1282.366455078125\n",
      "epoch 32, val_loss 457.1934509277344\n",
      "epoch 33, train_loss 1282.357421875\n",
      "epoch 33, val_loss 457.19049072265625\n",
      "epoch 34, train_loss 1282.3485107421875\n",
      "epoch 34, val_loss 457.18743896484375\n",
      "epoch 35, train_loss 1282.3397216796875\n",
      "epoch 35, val_loss 457.1843566894531\n",
      "epoch 36, train_loss 1282.3309326171875\n",
      "epoch 36, val_loss 457.1814270019531\n",
      "epoch 37, train_loss 1282.322021484375\n",
      "epoch 37, val_loss 457.1783752441406\n",
      "epoch 38, train_loss 1282.3133544921875\n",
      "epoch 38, val_loss 457.1754455566406\n",
      "epoch 39, train_loss 1282.3045654296875\n",
      "epoch 39, val_loss 457.17236328125\n",
      "epoch 40, train_loss 1282.2955322265625\n",
      "epoch 40, val_loss 457.16937255859375\n",
      "epoch 41, train_loss 1282.286865234375\n",
      "epoch 41, val_loss 457.1664123535156\n",
      "epoch 42, train_loss 1282.2781982421875\n",
      "epoch 42, val_loss 457.1634826660156\n",
      "epoch 43, train_loss 1282.269287109375\n",
      "epoch 43, val_loss 457.1604919433594\n",
      "epoch 44, train_loss 1282.2607421875\n",
      "epoch 44, val_loss 457.1575012207031\n",
      "epoch 45, train_loss 1282.2520751953125\n",
      "epoch 45, val_loss 457.1545104980469\n",
      "epoch 46, train_loss 1282.2435302734375\n",
      "epoch 46, val_loss 457.1515808105469\n",
      "epoch 47, train_loss 1282.2347412109375\n",
      "epoch 47, val_loss 457.14862060546875\n",
      "epoch 48, train_loss 1282.226318359375\n",
      "epoch 48, val_loss 457.14569091796875\n",
      "epoch 49, train_loss 1282.2177734375\n",
      "epoch 49, val_loss 457.142822265625\n",
      "epoch 50, train_loss 1282.208984375\n",
      "epoch 50, val_loss 457.13983154296875\n",
      "epoch 51, train_loss 1282.2005615234375\n",
      "epoch 51, val_loss 457.136962890625\n",
      "epoch 52, train_loss 1282.1920166015625\n",
      "epoch 52, val_loss 457.1340026855469\n",
      "epoch 53, train_loss 1282.1834716796875\n",
      "epoch 53, val_loss 457.1311950683594\n",
      "epoch 54, train_loss 1282.175048828125\n",
      "epoch 54, val_loss 457.12823486328125\n",
      "epoch 55, train_loss 1282.166748046875\n",
      "epoch 55, val_loss 457.12542724609375\n",
      "epoch 56, train_loss 1282.158203125\n",
      "epoch 56, val_loss 457.12249755859375\n",
      "epoch 57, train_loss 1282.149658203125\n",
      "epoch 57, val_loss 457.11962890625\n",
      "epoch 58, train_loss 1282.141357421875\n",
      "epoch 58, val_loss 457.11676025390625\n",
      "epoch 59, train_loss 1282.133056640625\n",
      "epoch 59, val_loss 457.1139221191406\n",
      "epoch 60, train_loss 1282.1246337890625\n",
      "epoch 60, val_loss 457.111083984375\n",
      "epoch 61, train_loss 1282.116455078125\n",
      "epoch 61, val_loss 457.108154296875\n",
      "epoch 62, train_loss 1282.1080322265625\n",
      "epoch 62, val_loss 457.1053161621094\n",
      "epoch 63, train_loss 1282.099853515625\n",
      "epoch 63, val_loss 457.1025390625\n",
      "epoch 64, train_loss 1282.0914306640625\n",
      "epoch 64, val_loss 457.0997009277344\n",
      "epoch 65, train_loss 1282.0831298828125\n",
      "epoch 65, val_loss 457.096923828125\n",
      "epoch 66, train_loss 1282.0750732421875\n",
      "epoch 66, val_loss 457.0940246582031\n",
      "epoch 67, train_loss 1282.0667724609375\n",
      "epoch 67, val_loss 457.0912780761719\n",
      "epoch 68, train_loss 1282.05859375\n",
      "epoch 68, val_loss 457.08843994140625\n",
      "epoch 69, train_loss 1282.05029296875\n",
      "epoch 69, val_loss 457.0856628417969\n",
      "epoch 70, train_loss 1282.0423583984375\n",
      "epoch 70, val_loss 457.0829162597656\n",
      "epoch 71, train_loss 1282.0341796875\n",
      "epoch 71, val_loss 457.0801696777344\n",
      "epoch 72, train_loss 1282.0260009765625\n",
      "epoch 72, val_loss 457.07733154296875\n",
      "epoch 73, train_loss 1282.0179443359375\n",
      "epoch 73, val_loss 457.0746765136719\n",
      "epoch 74, train_loss 1282.009765625\n",
      "epoch 74, val_loss 457.07177734375\n",
      "epoch 75, train_loss 1282.0018310546875\n",
      "epoch 75, val_loss 457.069091796875\n",
      "epoch 76, train_loss 1281.9937744140625\n",
      "epoch 76, val_loss 457.0663757324219\n",
      "epoch 77, train_loss 1281.98583984375\n",
      "epoch 77, val_loss 457.0636291503906\n",
      "epoch 78, train_loss 1281.9776611328125\n",
      "epoch 78, val_loss 457.0608215332031\n",
      "epoch 79, train_loss 1281.9698486328125\n",
      "epoch 79, val_loss 457.05810546875\n",
      "epoch 80, train_loss 1281.9617919921875\n",
      "epoch 80, val_loss 457.055419921875\n",
      "epoch 81, train_loss 1281.9539794921875\n",
      "epoch 81, val_loss 457.052734375\n",
      "epoch 82, train_loss 1281.946044921875\n",
      "epoch 82, val_loss 457.050048828125\n",
      "epoch 83, train_loss 1281.93798828125\n",
      "epoch 83, val_loss 457.04736328125\n",
      "epoch 84, train_loss 1281.93017578125\n",
      "epoch 84, val_loss 457.044677734375\n",
      "epoch 85, train_loss 1281.92236328125\n",
      "epoch 85, val_loss 457.04205322265625\n",
      "epoch 86, train_loss 1281.9144287109375\n",
      "epoch 86, val_loss 457.039306640625\n",
      "epoch 87, train_loss 1281.9066162109375\n",
      "epoch 87, val_loss 457.03662109375\n",
      "epoch 88, train_loss 1281.8990478515625\n",
      "epoch 88, val_loss 457.03399658203125\n",
      "epoch 89, train_loss 1281.89111328125\n",
      "epoch 89, val_loss 457.03131103515625\n",
      "epoch 90, train_loss 1281.8831787109375\n",
      "epoch 90, val_loss 457.0286865234375\n",
      "epoch 91, train_loss 1281.8756103515625\n",
      "epoch 91, val_loss 457.0260314941406\n",
      "epoch 92, train_loss 1281.867919921875\n",
      "epoch 92, val_loss 457.0233459472656\n",
      "epoch 93, train_loss 1281.860107421875\n",
      "epoch 93, val_loss 457.02081298828125\n",
      "epoch 94, train_loss 1281.8524169921875\n",
      "epoch 94, val_loss 457.0181579589844\n",
      "epoch 95, train_loss 1281.8447265625\n",
      "epoch 95, val_loss 457.0155334472656\n",
      "epoch 96, train_loss 1281.837158203125\n",
      "epoch 96, val_loss 457.012939453125\n",
      "epoch 97, train_loss 1281.82958984375\n",
      "epoch 97, val_loss 457.0102844238281\n",
      "epoch 98, train_loss 1281.8218994140625\n",
      "epoch 98, val_loss 457.0077209472656\n",
      "epoch 99, train_loss 1281.8143310546875\n",
      "epoch 99, val_loss 457.0050964355469\n",
      "Parameter containing:\n",
      "tensor([0.0299], requires_grad=True)\n",
      "iter 5, train_loss_regularization 1.2784500122070312\n",
      "iter 5, val_loss_regularization 1.2784500122070312\n",
      "epoch 0, train_loss 1281.805908203125\n",
      "epoch 0, val_loss 457.0016174316406\n",
      "epoch 1, train_loss 1281.7984619140625\n",
      "epoch 1, val_loss 456.99908447265625\n",
      "epoch 2, train_loss 1281.7908935546875\n",
      "epoch 2, val_loss 456.9964904785156\n",
      "epoch 3, train_loss 1281.783447265625\n",
      "epoch 3, val_loss 456.9940185546875\n",
      "epoch 4, train_loss 1281.7757568359375\n",
      "epoch 4, val_loss 456.99139404296875\n",
      "epoch 5, train_loss 1281.768310546875\n",
      "epoch 5, val_loss 456.98883056640625\n",
      "epoch 6, train_loss 1281.7607421875\n",
      "epoch 6, val_loss 456.9862976074219\n",
      "epoch 7, train_loss 1281.75341796875\n",
      "epoch 7, val_loss 456.9837646484375\n",
      "epoch 8, train_loss 1281.74609375\n",
      "epoch 8, val_loss 456.9811706542969\n",
      "epoch 9, train_loss 1281.7386474609375\n",
      "epoch 9, val_loss 456.97869873046875\n",
      "epoch 10, train_loss 1281.731201171875\n",
      "epoch 10, val_loss 456.97613525390625\n",
      "epoch 11, train_loss 1281.723876953125\n",
      "epoch 11, val_loss 456.9736328125\n",
      "epoch 12, train_loss 1281.716552734375\n",
      "epoch 12, val_loss 456.97113037109375\n",
      "epoch 13, train_loss 1281.708984375\n",
      "epoch 13, val_loss 456.9685363769531\n",
      "epoch 14, train_loss 1281.7017822265625\n",
      "epoch 14, val_loss 456.96612548828125\n",
      "epoch 15, train_loss 1281.6947021484375\n",
      "epoch 15, val_loss 456.9635925292969\n",
      "epoch 16, train_loss 1281.6873779296875\n",
      "epoch 16, val_loss 456.9610900878906\n",
      "epoch 17, train_loss 1281.6800537109375\n",
      "epoch 17, val_loss 456.95867919921875\n",
      "epoch 18, train_loss 1281.6728515625\n",
      "epoch 18, val_loss 456.95611572265625\n",
      "epoch 19, train_loss 1281.6656494140625\n",
      "epoch 19, val_loss 456.9537048339844\n",
      "epoch 20, train_loss 1281.658447265625\n",
      "epoch 20, val_loss 456.9512634277344\n",
      "epoch 21, train_loss 1281.6512451171875\n",
      "epoch 21, val_loss 456.9486999511719\n",
      "epoch 22, train_loss 1281.64404296875\n",
      "epoch 22, val_loss 456.9462890625\n",
      "epoch 23, train_loss 1281.6368408203125\n",
      "epoch 23, val_loss 456.94378662109375\n",
      "epoch 24, train_loss 1281.6297607421875\n",
      "epoch 24, val_loss 456.94146728515625\n",
      "epoch 25, train_loss 1281.6224365234375\n",
      "epoch 25, val_loss 456.9390563964844\n",
      "epoch 26, train_loss 1281.6156005859375\n",
      "epoch 26, val_loss 456.93658447265625\n",
      "epoch 27, train_loss 1281.6083984375\n",
      "epoch 27, val_loss 456.93414306640625\n",
      "epoch 28, train_loss 1281.601318359375\n",
      "epoch 28, val_loss 456.93170166015625\n",
      "epoch 29, train_loss 1281.59423828125\n",
      "epoch 29, val_loss 456.9292907714844\n",
      "epoch 30, train_loss 1281.5872802734375\n",
      "epoch 30, val_loss 456.9269104003906\n",
      "epoch 31, train_loss 1281.5802001953125\n",
      "epoch 31, val_loss 456.924560546875\n",
      "epoch 32, train_loss 1281.5733642578125\n",
      "epoch 32, val_loss 456.92218017578125\n",
      "epoch 33, train_loss 1281.5662841796875\n",
      "epoch 33, val_loss 456.919677734375\n",
      "epoch 34, train_loss 1281.5592041015625\n",
      "epoch 34, val_loss 456.9173278808594\n",
      "epoch 35, train_loss 1281.5523681640625\n",
      "epoch 35, val_loss 456.91497802734375\n",
      "epoch 36, train_loss 1281.54541015625\n",
      "epoch 36, val_loss 456.91259765625\n",
      "epoch 37, train_loss 1281.5384521484375\n",
      "epoch 37, val_loss 456.9102478027344\n",
      "epoch 38, train_loss 1281.5316162109375\n",
      "epoch 38, val_loss 456.9078674316406\n",
      "epoch 39, train_loss 1281.5247802734375\n",
      "epoch 39, val_loss 456.905517578125\n",
      "epoch 40, train_loss 1281.517822265625\n",
      "epoch 40, val_loss 456.9031677246094\n",
      "epoch 41, train_loss 1281.5108642578125\n",
      "epoch 41, val_loss 456.9008483886719\n",
      "epoch 42, train_loss 1281.5040283203125\n",
      "epoch 42, val_loss 456.8985595703125\n",
      "epoch 43, train_loss 1281.4974365234375\n",
      "epoch 43, val_loss 456.8961181640625\n",
      "epoch 44, train_loss 1281.4906005859375\n",
      "epoch 44, val_loss 456.8938293457031\n",
      "epoch 45, train_loss 1281.4837646484375\n",
      "epoch 45, val_loss 456.8915100097656\n",
      "epoch 46, train_loss 1281.4769287109375\n",
      "epoch 46, val_loss 456.88922119140625\n",
      "epoch 47, train_loss 1281.4703369140625\n",
      "epoch 47, val_loss 456.8868408203125\n",
      "epoch 48, train_loss 1281.46337890625\n",
      "epoch 48, val_loss 456.88458251953125\n",
      "epoch 49, train_loss 1281.456787109375\n",
      "epoch 49, val_loss 456.8822937011719\n",
      "epoch 50, train_loss 1281.4501953125\n",
      "epoch 50, val_loss 456.8799743652344\n",
      "epoch 51, train_loss 1281.443359375\n",
      "epoch 51, val_loss 456.877685546875\n",
      "epoch 52, train_loss 1281.436767578125\n",
      "epoch 52, val_loss 456.87542724609375\n",
      "epoch 53, train_loss 1281.429931640625\n",
      "epoch 53, val_loss 456.8731384277344\n",
      "epoch 54, train_loss 1281.42333984375\n",
      "epoch 54, val_loss 456.8708801269531\n",
      "epoch 55, train_loss 1281.4168701171875\n",
      "epoch 55, val_loss 456.8686828613281\n",
      "epoch 56, train_loss 1281.4102783203125\n",
      "epoch 56, val_loss 456.8663330078125\n",
      "epoch 57, train_loss 1281.4036865234375\n",
      "epoch 57, val_loss 456.8641357421875\n",
      "epoch 58, train_loss 1281.39697265625\n",
      "epoch 58, val_loss 456.8618469238281\n",
      "epoch 59, train_loss 1281.3905029296875\n",
      "epoch 59, val_loss 456.8595886230469\n",
      "epoch 60, train_loss 1281.3839111328125\n",
      "epoch 60, val_loss 456.8573303222656\n",
      "epoch 61, train_loss 1281.37744140625\n",
      "epoch 61, val_loss 456.8551330566406\n",
      "epoch 62, train_loss 1281.3709716796875\n",
      "epoch 62, val_loss 456.85296630859375\n",
      "epoch 63, train_loss 1281.3643798828125\n",
      "epoch 63, val_loss 456.8507080078125\n",
      "epoch 64, train_loss 1281.3580322265625\n",
      "epoch 64, val_loss 456.8485107421875\n",
      "epoch 65, train_loss 1281.3514404296875\n",
      "epoch 65, val_loss 456.8462219238281\n",
      "epoch 66, train_loss 1281.3448486328125\n",
      "epoch 66, val_loss 456.843994140625\n",
      "epoch 67, train_loss 1281.3385009765625\n",
      "epoch 67, val_loss 456.8418884277344\n",
      "epoch 68, train_loss 1281.332275390625\n",
      "epoch 68, val_loss 456.8396301269531\n",
      "epoch 69, train_loss 1281.32568359375\n",
      "epoch 69, val_loss 456.8374328613281\n",
      "epoch 70, train_loss 1281.3194580078125\n",
      "epoch 70, val_loss 456.83526611328125\n",
      "epoch 71, train_loss 1281.3131103515625\n",
      "epoch 71, val_loss 456.8331298828125\n",
      "epoch 72, train_loss 1281.3067626953125\n",
      "epoch 72, val_loss 456.8309326171875\n",
      "epoch 73, train_loss 1281.300537109375\n",
      "epoch 73, val_loss 456.8287048339844\n",
      "epoch 74, train_loss 1281.2939453125\n",
      "epoch 74, val_loss 456.82659912109375\n",
      "epoch 75, train_loss 1281.28759765625\n",
      "epoch 75, val_loss 456.8244323730469\n",
      "epoch 76, train_loss 1281.2813720703125\n",
      "epoch 76, val_loss 456.822265625\n",
      "epoch 77, train_loss 1281.2750244140625\n",
      "epoch 77, val_loss 456.8200988769531\n",
      "epoch 78, train_loss 1281.268798828125\n",
      "epoch 78, val_loss 456.8179626464844\n",
      "epoch 79, train_loss 1281.2625732421875\n",
      "epoch 79, val_loss 456.8157958984375\n",
      "epoch 80, train_loss 1281.25634765625\n",
      "epoch 80, val_loss 456.813720703125\n",
      "epoch 81, train_loss 1281.2501220703125\n",
      "epoch 81, val_loss 456.8116149902344\n",
      "epoch 82, train_loss 1281.2440185546875\n",
      "epoch 82, val_loss 456.8094482421875\n",
      "epoch 83, train_loss 1281.2379150390625\n",
      "epoch 83, val_loss 456.807373046875\n",
      "epoch 84, train_loss 1281.231689453125\n",
      "epoch 84, val_loss 456.8052062988281\n",
      "epoch 85, train_loss 1281.2255859375\n",
      "epoch 85, val_loss 456.8031311035156\n",
      "epoch 86, train_loss 1281.219482421875\n",
      "epoch 86, val_loss 456.8010559082031\n",
      "epoch 87, train_loss 1281.2132568359375\n",
      "epoch 87, val_loss 456.7989196777344\n",
      "epoch 88, train_loss 1281.20703125\n",
      "epoch 88, val_loss 456.7967834472656\n",
      "epoch 89, train_loss 1281.200927734375\n",
      "epoch 89, val_loss 456.7947692871094\n",
      "epoch 90, train_loss 1281.1949462890625\n",
      "epoch 90, val_loss 456.79266357421875\n",
      "epoch 91, train_loss 1281.1888427734375\n",
      "epoch 91, val_loss 456.79058837890625\n",
      "epoch 92, train_loss 1281.182861328125\n",
      "epoch 92, val_loss 456.7885437011719\n",
      "epoch 93, train_loss 1281.1767578125\n",
      "epoch 93, val_loss 456.7864074707031\n",
      "epoch 94, train_loss 1281.1707763671875\n",
      "epoch 94, val_loss 456.7843322753906\n",
      "epoch 95, train_loss 1281.1646728515625\n",
      "epoch 95, val_loss 456.7823791503906\n",
      "epoch 96, train_loss 1281.1588134765625\n",
      "epoch 96, val_loss 456.7802429199219\n",
      "epoch 97, train_loss 1281.15283203125\n",
      "epoch 97, val_loss 456.77825927734375\n",
      "epoch 98, train_loss 1281.1468505859375\n",
      "epoch 98, val_loss 456.776123046875\n",
      "epoch 99, train_loss 1281.140869140625\n",
      "epoch 99, val_loss 456.7741394042969\n",
      "Parameter containing:\n",
      "tensor([0.0199], requires_grad=True)\n",
      "iter 6, train_loss_regularization 1.2567048072814941\n",
      "iter 6, val_loss_regularization 1.2567048072814941\n",
      "epoch 0, train_loss 1281.134521484375\n",
      "epoch 0, val_loss 456.771728515625\n",
      "epoch 1, train_loss 1281.128662109375\n",
      "epoch 1, val_loss 456.76971435546875\n",
      "epoch 2, train_loss 1281.12255859375\n",
      "epoch 2, val_loss 456.76763916015625\n",
      "epoch 3, train_loss 1281.11669921875\n",
      "epoch 3, val_loss 456.76568603515625\n",
      "epoch 4, train_loss 1281.11083984375\n",
      "epoch 4, val_loss 456.763671875\n",
      "epoch 5, train_loss 1281.1048583984375\n",
      "epoch 5, val_loss 456.7616271972656\n",
      "epoch 6, train_loss 1281.09912109375\n",
      "epoch 6, val_loss 456.75958251953125\n",
      "epoch 7, train_loss 1281.09326171875\n",
      "epoch 7, val_loss 456.7575988769531\n",
      "epoch 8, train_loss 1281.0875244140625\n",
      "epoch 8, val_loss 456.7556457519531\n",
      "epoch 9, train_loss 1281.0816650390625\n",
      "epoch 9, val_loss 456.7536315917969\n",
      "epoch 10, train_loss 1281.075927734375\n",
      "epoch 10, val_loss 456.75164794921875\n",
      "epoch 11, train_loss 1281.070068359375\n",
      "epoch 11, val_loss 456.7497253417969\n",
      "epoch 12, train_loss 1281.064208984375\n",
      "epoch 12, val_loss 456.7477111816406\n",
      "epoch 13, train_loss 1281.0587158203125\n",
      "epoch 13, val_loss 456.7457275390625\n",
      "epoch 14, train_loss 1281.0526123046875\n",
      "epoch 14, val_loss 456.7438049316406\n",
      "epoch 15, train_loss 1281.047119140625\n",
      "epoch 15, val_loss 456.74176025390625\n",
      "epoch 16, train_loss 1281.0413818359375\n",
      "epoch 16, val_loss 456.73980712890625\n",
      "epoch 17, train_loss 1281.0355224609375\n",
      "epoch 17, val_loss 456.7378845214844\n",
      "epoch 18, train_loss 1281.030029296875\n",
      "epoch 18, val_loss 456.7359619140625\n",
      "epoch 19, train_loss 1281.0242919921875\n",
      "epoch 19, val_loss 456.7339782714844\n",
      "epoch 20, train_loss 1281.0186767578125\n",
      "epoch 20, val_loss 456.7320861816406\n",
      "epoch 21, train_loss 1281.0130615234375\n",
      "epoch 21, val_loss 456.7301940917969\n",
      "epoch 22, train_loss 1281.0074462890625\n",
      "epoch 22, val_loss 456.72821044921875\n",
      "epoch 23, train_loss 1281.001708984375\n",
      "epoch 23, val_loss 456.726318359375\n",
      "epoch 24, train_loss 1280.9962158203125\n",
      "epoch 24, val_loss 456.72430419921875\n",
      "epoch 25, train_loss 1280.9906005859375\n",
      "epoch 25, val_loss 456.7224426269531\n",
      "epoch 26, train_loss 1280.985107421875\n",
      "epoch 26, val_loss 456.720458984375\n",
      "epoch 27, train_loss 1280.9794921875\n",
      "epoch 27, val_loss 456.7186279296875\n",
      "epoch 28, train_loss 1280.9737548828125\n",
      "epoch 28, val_loss 456.71673583984375\n",
      "epoch 29, train_loss 1280.9683837890625\n",
      "epoch 29, val_loss 456.7148132324219\n",
      "epoch 30, train_loss 1280.962890625\n",
      "epoch 30, val_loss 456.712890625\n",
      "epoch 31, train_loss 1280.9573974609375\n",
      "epoch 31, val_loss 456.7110595703125\n",
      "epoch 32, train_loss 1280.9520263671875\n",
      "epoch 32, val_loss 456.7091064453125\n",
      "epoch 33, train_loss 1280.946533203125\n",
      "epoch 33, val_loss 456.7073059082031\n",
      "epoch 34, train_loss 1280.94091796875\n",
      "epoch 34, val_loss 456.70538330078125\n",
      "epoch 35, train_loss 1280.935546875\n",
      "epoch 35, val_loss 456.7035217285156\n",
      "epoch 36, train_loss 1280.929931640625\n",
      "epoch 36, val_loss 456.7016296386719\n",
      "epoch 37, train_loss 1280.924560546875\n",
      "epoch 37, val_loss 456.6997985839844\n",
      "epoch 38, train_loss 1280.919189453125\n",
      "epoch 38, val_loss 456.6978759765625\n",
      "epoch 39, train_loss 1280.9139404296875\n",
      "epoch 39, val_loss 456.69610595703125\n",
      "epoch 40, train_loss 1280.908447265625\n",
      "epoch 40, val_loss 456.6942138671875\n",
      "epoch 41, train_loss 1280.903076171875\n",
      "epoch 41, val_loss 456.6923522949219\n",
      "epoch 42, train_loss 1280.897705078125\n",
      "epoch 42, val_loss 456.6904602050781\n",
      "epoch 43, train_loss 1280.8924560546875\n",
      "epoch 43, val_loss 456.6886901855469\n",
      "epoch 44, train_loss 1280.886962890625\n",
      "epoch 44, val_loss 456.6868591308594\n",
      "epoch 45, train_loss 1280.8817138671875\n",
      "epoch 45, val_loss 456.68505859375\n",
      "epoch 46, train_loss 1280.8763427734375\n",
      "epoch 46, val_loss 456.68316650390625\n",
      "epoch 47, train_loss 1280.87109375\n",
      "epoch 47, val_loss 456.681396484375\n",
      "epoch 48, train_loss 1280.8656005859375\n",
      "epoch 48, val_loss 456.6795654296875\n",
      "epoch 49, train_loss 1280.8604736328125\n",
      "epoch 49, val_loss 456.6777648925781\n",
      "epoch 50, train_loss 1280.8551025390625\n",
      "epoch 50, val_loss 456.67596435546875\n",
      "epoch 51, train_loss 1280.849853515625\n",
      "epoch 51, val_loss 456.6741943359375\n",
      "epoch 52, train_loss 1280.8446044921875\n",
      "epoch 52, val_loss 456.6723327636719\n",
      "epoch 53, train_loss 1280.839599609375\n",
      "epoch 53, val_loss 456.6705627441406\n",
      "epoch 54, train_loss 1280.8343505859375\n",
      "epoch 54, val_loss 456.6687927246094\n",
      "epoch 55, train_loss 1280.8289794921875\n",
      "epoch 55, val_loss 456.6670227050781\n",
      "epoch 56, train_loss 1280.8238525390625\n",
      "epoch 56, val_loss 456.66522216796875\n",
      "epoch 57, train_loss 1280.8187255859375\n",
      "epoch 57, val_loss 456.6634826660156\n",
      "epoch 58, train_loss 1280.8135986328125\n",
      "epoch 58, val_loss 456.6616516113281\n",
      "epoch 59, train_loss 1280.808349609375\n",
      "epoch 59, val_loss 456.65985107421875\n",
      "epoch 60, train_loss 1280.8031005859375\n",
      "epoch 60, val_loss 456.65814208984375\n",
      "epoch 61, train_loss 1280.7982177734375\n",
      "epoch 61, val_loss 456.6563415527344\n",
      "epoch 62, train_loss 1280.7930908203125\n",
      "epoch 62, val_loss 456.65460205078125\n",
      "epoch 63, train_loss 1280.7879638671875\n",
      "epoch 63, val_loss 456.65283203125\n",
      "epoch 64, train_loss 1280.782958984375\n",
      "epoch 64, val_loss 456.6510925292969\n",
      "epoch 65, train_loss 1280.7777099609375\n",
      "epoch 65, val_loss 456.64935302734375\n",
      "epoch 66, train_loss 1280.772705078125\n",
      "epoch 66, val_loss 456.64764404296875\n",
      "epoch 67, train_loss 1280.767578125\n",
      "epoch 67, val_loss 456.6458740234375\n",
      "epoch 68, train_loss 1280.7628173828125\n",
      "epoch 68, val_loss 456.6441345214844\n",
      "epoch 69, train_loss 1280.7576904296875\n",
      "epoch 69, val_loss 456.6424255371094\n",
      "epoch 70, train_loss 1280.7525634765625\n",
      "epoch 70, val_loss 456.6407165527344\n",
      "epoch 71, train_loss 1280.7474365234375\n",
      "epoch 71, val_loss 456.6390686035156\n",
      "epoch 72, train_loss 1280.7425537109375\n",
      "epoch 72, val_loss 456.63720703125\n",
      "epoch 73, train_loss 1280.737548828125\n",
      "epoch 73, val_loss 456.63555908203125\n",
      "epoch 74, train_loss 1280.732666015625\n",
      "epoch 74, val_loss 456.63385009765625\n",
      "epoch 75, train_loss 1280.7275390625\n",
      "epoch 75, val_loss 456.63214111328125\n",
      "epoch 76, train_loss 1280.72265625\n",
      "epoch 76, val_loss 456.63043212890625\n",
      "epoch 77, train_loss 1280.7177734375\n",
      "epoch 77, val_loss 456.6287536621094\n",
      "epoch 78, train_loss 1280.7130126953125\n",
      "epoch 78, val_loss 456.62701416015625\n",
      "epoch 79, train_loss 1280.7081298828125\n",
      "epoch 79, val_loss 456.6253967285156\n",
      "epoch 80, train_loss 1280.703125\n",
      "epoch 80, val_loss 456.62371826171875\n",
      "epoch 81, train_loss 1280.6982421875\n",
      "epoch 81, val_loss 456.62200927734375\n",
      "epoch 82, train_loss 1280.693115234375\n",
      "epoch 82, val_loss 456.620361328125\n",
      "epoch 83, train_loss 1280.6883544921875\n",
      "epoch 83, val_loss 456.6186828613281\n",
      "epoch 84, train_loss 1280.68359375\n",
      "epoch 84, val_loss 456.6169738769531\n",
      "epoch 85, train_loss 1280.6787109375\n",
      "epoch 85, val_loss 456.6153869628906\n",
      "epoch 86, train_loss 1280.6739501953125\n",
      "epoch 86, val_loss 456.6136474609375\n",
      "epoch 87, train_loss 1280.669189453125\n",
      "epoch 87, val_loss 456.6119689941406\n",
      "epoch 88, train_loss 1280.6644287109375\n",
      "epoch 88, val_loss 456.6103515625\n",
      "epoch 89, train_loss 1280.65966796875\n",
      "epoch 89, val_loss 456.6087341308594\n",
      "epoch 90, train_loss 1280.65478515625\n",
      "epoch 90, val_loss 456.6070861816406\n",
      "epoch 91, train_loss 1280.64990234375\n",
      "epoch 91, val_loss 456.60546875\n",
      "epoch 92, train_loss 1280.6451416015625\n",
      "epoch 92, val_loss 456.6037902832031\n",
      "epoch 93, train_loss 1280.640625\n",
      "epoch 93, val_loss 456.6021728515625\n",
      "epoch 94, train_loss 1280.6357421875\n",
      "epoch 94, val_loss 456.6005554199219\n",
      "epoch 95, train_loss 1280.630859375\n",
      "epoch 95, val_loss 456.598876953125\n",
      "epoch 96, train_loss 1280.62646484375\n",
      "epoch 96, val_loss 456.5972595214844\n",
      "epoch 97, train_loss 1280.62158203125\n",
      "epoch 97, val_loss 456.59564208984375\n",
      "epoch 98, train_loss 1280.616943359375\n",
      "epoch 98, val_loss 456.59405517578125\n",
      "epoch 99, train_loss 1280.6123046875\n",
      "epoch 99, val_loss 456.5924377441406\n",
      "Parameter containing:\n",
      "tensor([0.0131], requires_grad=True)\n",
      "iter 7, train_loss_regularization 1.2376292943954468\n",
      "iter 7, val_loss_regularization 1.2376292943954468\n",
      "epoch 0, train_loss 1280.607666015625\n",
      "epoch 0, val_loss 456.59063720703125\n",
      "epoch 1, train_loss 1280.602783203125\n",
      "epoch 1, val_loss 456.5889892578125\n",
      "epoch 2, train_loss 1280.5982666015625\n",
      "epoch 2, val_loss 456.58746337890625\n",
      "epoch 3, train_loss 1280.5936279296875\n",
      "epoch 3, val_loss 456.5858154296875\n",
      "epoch 4, train_loss 1280.5888671875\n",
      "epoch 4, val_loss 456.5842590332031\n",
      "epoch 5, train_loss 1280.5843505859375\n",
      "epoch 5, val_loss 456.5826721191406\n",
      "epoch 6, train_loss 1280.5797119140625\n",
      "epoch 6, val_loss 456.5810241699219\n",
      "epoch 7, train_loss 1280.5751953125\n",
      "epoch 7, val_loss 456.5794982910156\n",
      "epoch 8, train_loss 1280.5704345703125\n",
      "epoch 8, val_loss 456.5779113769531\n",
      "epoch 9, train_loss 1280.56591796875\n",
      "epoch 9, val_loss 456.5763854980469\n",
      "epoch 10, train_loss 1280.561279296875\n",
      "epoch 10, val_loss 456.5747375488281\n",
      "epoch 11, train_loss 1280.5570068359375\n",
      "epoch 11, val_loss 456.5732116699219\n",
      "epoch 12, train_loss 1280.55224609375\n",
      "epoch 12, val_loss 456.5716552734375\n",
      "epoch 13, train_loss 1280.5477294921875\n",
      "epoch 13, val_loss 456.570068359375\n",
      "epoch 14, train_loss 1280.54345703125\n",
      "epoch 14, val_loss 456.56854248046875\n",
      "epoch 15, train_loss 1280.538818359375\n",
      "epoch 15, val_loss 456.5670471191406\n",
      "epoch 16, train_loss 1280.5341796875\n",
      "epoch 16, val_loss 456.56549072265625\n",
      "epoch 17, train_loss 1280.5299072265625\n",
      "epoch 17, val_loss 456.56390380859375\n",
      "epoch 18, train_loss 1280.525390625\n",
      "epoch 18, val_loss 456.5623779296875\n",
      "epoch 19, train_loss 1280.520751953125\n",
      "epoch 19, val_loss 456.5608215332031\n",
      "epoch 20, train_loss 1280.5164794921875\n",
      "epoch 20, val_loss 456.5592956542969\n",
      "epoch 21, train_loss 1280.5120849609375\n",
      "epoch 21, val_loss 456.5577087402344\n",
      "epoch 22, train_loss 1280.507568359375\n",
      "epoch 22, val_loss 456.5562438964844\n",
      "epoch 23, train_loss 1280.503173828125\n",
      "epoch 23, val_loss 456.55474853515625\n",
      "epoch 24, train_loss 1280.4986572265625\n",
      "epoch 24, val_loss 456.5531921386719\n",
      "epoch 25, train_loss 1280.4945068359375\n",
      "epoch 25, val_loss 456.5516662597656\n",
      "epoch 26, train_loss 1280.489990234375\n",
      "epoch 26, val_loss 456.55010986328125\n",
      "epoch 27, train_loss 1280.485595703125\n",
      "epoch 27, val_loss 456.5486755371094\n",
      "epoch 28, train_loss 1280.481201171875\n",
      "epoch 28, val_loss 456.5471496582031\n",
      "epoch 29, train_loss 1280.476806640625\n",
      "epoch 29, val_loss 456.54571533203125\n",
      "epoch 30, train_loss 1280.472412109375\n",
      "epoch 30, val_loss 456.54412841796875\n",
      "epoch 31, train_loss 1280.4681396484375\n",
      "epoch 31, val_loss 456.54266357421875\n",
      "epoch 32, train_loss 1280.4639892578125\n",
      "epoch 32, val_loss 456.5411071777344\n",
      "epoch 33, train_loss 1280.459716796875\n",
      "epoch 33, val_loss 456.5396728515625\n",
      "epoch 34, train_loss 1280.4552001953125\n",
      "epoch 34, val_loss 456.5381774902344\n",
      "epoch 35, train_loss 1280.450927734375\n",
      "epoch 35, val_loss 456.5367126464844\n",
      "epoch 36, train_loss 1280.4466552734375\n",
      "epoch 36, val_loss 456.5352478027344\n",
      "epoch 37, train_loss 1280.4423828125\n",
      "epoch 37, val_loss 456.53375244140625\n",
      "epoch 38, train_loss 1280.4381103515625\n",
      "epoch 38, val_loss 456.53228759765625\n",
      "epoch 39, train_loss 1280.4339599609375\n",
      "epoch 39, val_loss 456.5307922363281\n",
      "epoch 40, train_loss 1280.4295654296875\n",
      "epoch 40, val_loss 456.5293273925781\n",
      "epoch 41, train_loss 1280.42529296875\n",
      "epoch 41, val_loss 456.5279235839844\n",
      "epoch 42, train_loss 1280.4212646484375\n",
      "epoch 42, val_loss 456.52642822265625\n",
      "epoch 43, train_loss 1280.4168701171875\n",
      "epoch 43, val_loss 456.5249938964844\n",
      "epoch 44, train_loss 1280.4127197265625\n",
      "epoch 44, val_loss 456.5235290527344\n",
      "epoch 45, train_loss 1280.4085693359375\n",
      "epoch 45, val_loss 456.5220947265625\n",
      "epoch 46, train_loss 1280.4044189453125\n",
      "epoch 46, val_loss 456.5206298828125\n",
      "epoch 47, train_loss 1280.400146484375\n",
      "epoch 47, val_loss 456.5191955566406\n",
      "epoch 48, train_loss 1280.39599609375\n",
      "epoch 48, val_loss 456.5177917480469\n",
      "epoch 49, train_loss 1280.3919677734375\n",
      "epoch 49, val_loss 456.5163269042969\n",
      "epoch 50, train_loss 1280.3878173828125\n",
      "epoch 50, val_loss 456.51483154296875\n",
      "epoch 51, train_loss 1280.3836669921875\n",
      "epoch 51, val_loss 456.5135498046875\n",
      "epoch 52, train_loss 1280.3795166015625\n",
      "epoch 52, val_loss 456.51202392578125\n",
      "epoch 53, train_loss 1280.375244140625\n",
      "epoch 53, val_loss 456.5106201171875\n",
      "epoch 54, train_loss 1280.3712158203125\n",
      "epoch 54, val_loss 456.50921630859375\n",
      "epoch 55, train_loss 1280.3671875\n",
      "epoch 55, val_loss 456.5077819824219\n",
      "epoch 56, train_loss 1280.3631591796875\n",
      "epoch 56, val_loss 456.50640869140625\n",
      "epoch 57, train_loss 1280.3590087890625\n",
      "epoch 57, val_loss 456.50494384765625\n",
      "epoch 58, train_loss 1280.354736328125\n",
      "epoch 58, val_loss 456.5035400390625\n",
      "epoch 59, train_loss 1280.350830078125\n",
      "epoch 59, val_loss 456.5021667480469\n",
      "epoch 60, train_loss 1280.346923828125\n",
      "epoch 60, val_loss 456.500732421875\n",
      "epoch 61, train_loss 1280.3427734375\n",
      "epoch 61, val_loss 456.4993591308594\n",
      "epoch 62, train_loss 1280.338623046875\n",
      "epoch 62, val_loss 456.4979553222656\n",
      "epoch 63, train_loss 1280.3348388671875\n",
      "epoch 63, val_loss 456.49658203125\n",
      "epoch 64, train_loss 1280.3306884765625\n",
      "epoch 64, val_loss 456.4952087402344\n",
      "epoch 65, train_loss 1280.32666015625\n",
      "epoch 65, val_loss 456.49383544921875\n",
      "epoch 66, train_loss 1280.322998046875\n",
      "epoch 66, val_loss 456.4924011230469\n",
      "epoch 67, train_loss 1280.31884765625\n",
      "epoch 67, val_loss 456.49102783203125\n",
      "epoch 68, train_loss 1280.3148193359375\n",
      "epoch 68, val_loss 456.4896545410156\n",
      "epoch 69, train_loss 1280.3109130859375\n",
      "epoch 69, val_loss 456.4883728027344\n",
      "epoch 70, train_loss 1280.3067626953125\n",
      "epoch 70, val_loss 456.48699951171875\n",
      "epoch 71, train_loss 1280.302978515625\n",
      "epoch 71, val_loss 456.48553466796875\n",
      "epoch 72, train_loss 1280.299072265625\n",
      "epoch 72, val_loss 456.4842529296875\n",
      "epoch 73, train_loss 1280.295166015625\n",
      "epoch 73, val_loss 456.4828796386719\n",
      "epoch 74, train_loss 1280.2911376953125\n",
      "epoch 74, val_loss 456.4815673828125\n",
      "epoch 75, train_loss 1280.2872314453125\n",
      "epoch 75, val_loss 456.4801940917969\n",
      "epoch 76, train_loss 1280.283203125\n",
      "epoch 76, val_loss 456.47882080078125\n",
      "epoch 77, train_loss 1280.279541015625\n",
      "epoch 77, val_loss 456.4775085449219\n",
      "epoch 78, train_loss 1280.2757568359375\n",
      "epoch 78, val_loss 456.47613525390625\n",
      "epoch 79, train_loss 1280.271728515625\n",
      "epoch 79, val_loss 456.47479248046875\n",
      "epoch 80, train_loss 1280.2679443359375\n",
      "epoch 80, val_loss 456.47344970703125\n",
      "epoch 81, train_loss 1280.2640380859375\n",
      "epoch 81, val_loss 456.47210693359375\n",
      "epoch 82, train_loss 1280.2601318359375\n",
      "epoch 82, val_loss 456.4707946777344\n",
      "epoch 83, train_loss 1280.2564697265625\n",
      "epoch 83, val_loss 456.4694519042969\n",
      "epoch 84, train_loss 1280.25244140625\n",
      "epoch 84, val_loss 456.4681091308594\n",
      "epoch 85, train_loss 1280.2486572265625\n",
      "epoch 85, val_loss 456.4668273925781\n",
      "epoch 86, train_loss 1280.2449951171875\n",
      "epoch 86, val_loss 456.4654846191406\n",
      "epoch 87, train_loss 1280.2410888671875\n",
      "epoch 87, val_loss 456.46417236328125\n",
      "epoch 88, train_loss 1280.2371826171875\n",
      "epoch 88, val_loss 456.4628601074219\n",
      "epoch 89, train_loss 1280.2333984375\n",
      "epoch 89, val_loss 456.4615783691406\n",
      "epoch 90, train_loss 1280.229736328125\n",
      "epoch 90, val_loss 456.4602966308594\n",
      "epoch 91, train_loss 1280.22607421875\n",
      "epoch 91, val_loss 456.45892333984375\n",
      "epoch 92, train_loss 1280.2222900390625\n",
      "epoch 92, val_loss 456.4576721191406\n",
      "epoch 93, train_loss 1280.218505859375\n",
      "epoch 93, val_loss 456.45635986328125\n",
      "epoch 94, train_loss 1280.21484375\n",
      "epoch 94, val_loss 456.455078125\n",
      "epoch 95, train_loss 1280.2109375\n",
      "epoch 95, val_loss 456.45379638671875\n",
      "epoch 96, train_loss 1280.207275390625\n",
      "epoch 96, val_loss 456.4524841308594\n",
      "epoch 97, train_loss 1280.20361328125\n",
      "epoch 97, val_loss 456.4512023925781\n",
      "epoch 98, train_loss 1280.199951171875\n",
      "epoch 98, val_loss 456.4499206542969\n",
      "epoch 99, train_loss 1280.1961669921875\n",
      "epoch 99, val_loss 456.44866943359375\n",
      "Parameter containing:\n",
      "tensor([0.0086], requires_grad=True)\n",
      "iter 8, train_loss_regularization 1.220680832862854\n",
      "iter 8, val_loss_regularization 1.220680832862854\n",
      "epoch 0, train_loss 1280.1923828125\n",
      "epoch 0, val_loss 456.447265625\n",
      "epoch 1, train_loss 1280.188720703125\n",
      "epoch 1, val_loss 456.4460144042969\n",
      "epoch 2, train_loss 1280.18505859375\n",
      "epoch 2, val_loss 456.4447326660156\n",
      "epoch 3, train_loss 1280.1812744140625\n",
      "epoch 3, val_loss 456.4434814453125\n",
      "epoch 4, train_loss 1280.1776123046875\n",
      "epoch 4, val_loss 456.4422302246094\n",
      "epoch 5, train_loss 1280.1741943359375\n",
      "epoch 5, val_loss 456.44097900390625\n",
      "epoch 6, train_loss 1280.1705322265625\n",
      "epoch 6, val_loss 456.4397277832031\n",
      "epoch 7, train_loss 1280.1668701171875\n",
      "epoch 7, val_loss 456.4384460449219\n",
      "epoch 8, train_loss 1280.1632080078125\n",
      "epoch 8, val_loss 456.43719482421875\n",
      "epoch 9, train_loss 1280.1597900390625\n",
      "epoch 9, val_loss 456.4359436035156\n",
      "epoch 10, train_loss 1280.1561279296875\n",
      "epoch 10, val_loss 456.4346923828125\n",
      "epoch 11, train_loss 1280.1524658203125\n",
      "epoch 11, val_loss 456.4334716796875\n",
      "epoch 12, train_loss 1280.1488037109375\n",
      "epoch 12, val_loss 456.4321594238281\n",
      "epoch 13, train_loss 1280.1453857421875\n",
      "epoch 13, val_loss 456.43096923828125\n",
      "epoch 14, train_loss 1280.1417236328125\n",
      "epoch 14, val_loss 456.4297790527344\n",
      "epoch 15, train_loss 1280.13818359375\n",
      "epoch 15, val_loss 456.4284362792969\n",
      "epoch 16, train_loss 1280.1346435546875\n",
      "epoch 16, val_loss 456.4272766113281\n",
      "epoch 17, train_loss 1280.131103515625\n",
      "epoch 17, val_loss 456.4260559082031\n",
      "epoch 18, train_loss 1280.12744140625\n",
      "epoch 18, val_loss 456.4247741699219\n",
      "epoch 19, train_loss 1280.1240234375\n",
      "epoch 19, val_loss 456.4236145019531\n",
      "epoch 20, train_loss 1280.1204833984375\n",
      "epoch 20, val_loss 456.42236328125\n",
      "epoch 21, train_loss 1280.1170654296875\n",
      "epoch 21, val_loss 456.421142578125\n",
      "epoch 22, train_loss 1280.113525390625\n",
      "epoch 22, val_loss 456.419921875\n",
      "epoch 23, train_loss 1280.110107421875\n",
      "epoch 23, val_loss 456.418701171875\n",
      "epoch 24, train_loss 1280.1064453125\n",
      "epoch 24, val_loss 456.41748046875\n",
      "epoch 25, train_loss 1280.10302734375\n",
      "epoch 25, val_loss 456.4163513183594\n",
      "epoch 26, train_loss 1280.099609375\n",
      "epoch 26, val_loss 456.4151306152344\n",
      "epoch 27, train_loss 1280.09619140625\n",
      "epoch 27, val_loss 456.41387939453125\n",
      "epoch 28, train_loss 1280.0926513671875\n",
      "epoch 28, val_loss 456.4127197265625\n",
      "epoch 29, train_loss 1280.08935546875\n",
      "epoch 29, val_loss 456.41156005859375\n",
      "epoch 30, train_loss 1280.085693359375\n",
      "epoch 30, val_loss 456.41033935546875\n",
      "epoch 31, train_loss 1280.0823974609375\n",
      "epoch 31, val_loss 456.4090881347656\n",
      "epoch 32, train_loss 1280.0789794921875\n",
      "epoch 32, val_loss 456.4079284667969\n",
      "epoch 33, train_loss 1280.075439453125\n",
      "epoch 33, val_loss 456.4067687988281\n",
      "epoch 34, train_loss 1280.0721435546875\n",
      "epoch 34, val_loss 456.405517578125\n",
      "epoch 35, train_loss 1280.0687255859375\n",
      "epoch 35, val_loss 456.4043884277344\n",
      "epoch 36, train_loss 1280.0654296875\n",
      "epoch 36, val_loss 456.4032287597656\n",
      "epoch 37, train_loss 1280.0618896484375\n",
      "epoch 37, val_loss 456.4020080566406\n",
      "epoch 38, train_loss 1280.058837890625\n",
      "epoch 38, val_loss 456.40081787109375\n",
      "epoch 39, train_loss 1280.05517578125\n",
      "epoch 39, val_loss 456.39971923828125\n",
      "epoch 40, train_loss 1280.052001953125\n",
      "epoch 40, val_loss 456.3985595703125\n",
      "epoch 41, train_loss 1280.0484619140625\n",
      "epoch 41, val_loss 456.3973388671875\n",
      "epoch 42, train_loss 1280.0452880859375\n",
      "epoch 42, val_loss 456.3961486816406\n",
      "epoch 43, train_loss 1280.0418701171875\n",
      "epoch 43, val_loss 456.39501953125\n",
      "epoch 44, train_loss 1280.03857421875\n",
      "epoch 44, val_loss 456.39385986328125\n",
      "epoch 45, train_loss 1280.0352783203125\n",
      "epoch 45, val_loss 456.39276123046875\n",
      "epoch 46, train_loss 1280.031982421875\n",
      "epoch 46, val_loss 456.3916015625\n",
      "epoch 47, train_loss 1280.028564453125\n",
      "epoch 47, val_loss 456.390380859375\n",
      "epoch 48, train_loss 1280.025390625\n",
      "epoch 48, val_loss 456.3892822265625\n",
      "epoch 49, train_loss 1280.02197265625\n",
      "epoch 49, val_loss 456.3881530761719\n",
      "epoch 50, train_loss 1280.018798828125\n",
      "epoch 50, val_loss 456.38702392578125\n",
      "epoch 51, train_loss 1280.0155029296875\n",
      "epoch 51, val_loss 456.38580322265625\n",
      "epoch 52, train_loss 1280.01220703125\n",
      "epoch 52, val_loss 456.384765625\n",
      "epoch 53, train_loss 1280.0091552734375\n",
      "epoch 53, val_loss 456.38360595703125\n",
      "epoch 54, train_loss 1280.005615234375\n",
      "epoch 54, val_loss 456.38250732421875\n",
      "epoch 55, train_loss 1280.00244140625\n",
      "epoch 55, val_loss 456.38140869140625\n",
      "epoch 56, train_loss 1279.9993896484375\n",
      "epoch 56, val_loss 456.3802185058594\n",
      "epoch 57, train_loss 1279.99609375\n",
      "epoch 57, val_loss 456.37908935546875\n",
      "epoch 58, train_loss 1279.9927978515625\n",
      "epoch 58, val_loss 456.3779296875\n",
      "epoch 59, train_loss 1279.9896240234375\n",
      "epoch 59, val_loss 456.3768005371094\n",
      "epoch 60, train_loss 1279.9864501953125\n",
      "epoch 60, val_loss 456.3757629394531\n",
      "epoch 61, train_loss 1279.9832763671875\n",
      "epoch 61, val_loss 456.3746337890625\n",
      "epoch 62, train_loss 1279.97998046875\n",
      "epoch 62, val_loss 456.3735046386719\n",
      "epoch 63, train_loss 1279.976806640625\n",
      "epoch 63, val_loss 456.3723449707031\n",
      "epoch 64, train_loss 1279.9737548828125\n",
      "epoch 64, val_loss 456.371337890625\n",
      "epoch 65, train_loss 1279.970458984375\n",
      "epoch 65, val_loss 456.37017822265625\n",
      "epoch 66, train_loss 1279.9674072265625\n",
      "epoch 66, val_loss 456.3691101074219\n",
      "epoch 67, train_loss 1279.96435546875\n",
      "epoch 67, val_loss 456.36798095703125\n",
      "epoch 68, train_loss 1279.9609375\n",
      "epoch 68, val_loss 456.366943359375\n",
      "epoch 69, train_loss 1279.9580078125\n",
      "epoch 69, val_loss 456.3658142089844\n",
      "epoch 70, train_loss 1279.9547119140625\n",
      "epoch 70, val_loss 456.36468505859375\n",
      "epoch 71, train_loss 1279.95166015625\n",
      "epoch 71, val_loss 456.3636474609375\n",
      "epoch 72, train_loss 1279.948486328125\n",
      "epoch 72, val_loss 456.36248779296875\n",
      "epoch 73, train_loss 1279.9454345703125\n",
      "epoch 73, val_loss 456.3614501953125\n",
      "epoch 74, train_loss 1279.9423828125\n",
      "epoch 74, val_loss 456.3603515625\n",
      "epoch 75, train_loss 1279.939208984375\n",
      "epoch 75, val_loss 456.3592834472656\n",
      "epoch 76, train_loss 1279.936279296875\n",
      "epoch 76, val_loss 456.3581848144531\n",
      "epoch 77, train_loss 1279.93310546875\n",
      "epoch 77, val_loss 456.357177734375\n",
      "epoch 78, train_loss 1279.9300537109375\n",
      "epoch 78, val_loss 456.35601806640625\n",
      "epoch 79, train_loss 1279.9268798828125\n",
      "epoch 79, val_loss 456.3550720214844\n",
      "epoch 80, train_loss 1279.923828125\n",
      "epoch 80, val_loss 456.35394287109375\n",
      "epoch 81, train_loss 1279.9207763671875\n",
      "epoch 81, val_loss 456.35284423828125\n",
      "epoch 82, train_loss 1279.917724609375\n",
      "epoch 82, val_loss 456.351806640625\n",
      "epoch 83, train_loss 1279.914794921875\n",
      "epoch 83, val_loss 456.35076904296875\n",
      "epoch 84, train_loss 1279.91162109375\n",
      "epoch 84, val_loss 456.3497009277344\n",
      "epoch 85, train_loss 1279.90869140625\n",
      "epoch 85, val_loss 456.3486328125\n",
      "epoch 86, train_loss 1279.9056396484375\n",
      "epoch 86, val_loss 456.34765625\n",
      "epoch 87, train_loss 1279.9027099609375\n",
      "epoch 87, val_loss 456.3465576171875\n",
      "epoch 88, train_loss 1279.899658203125\n",
      "epoch 88, val_loss 456.3454284667969\n",
      "epoch 89, train_loss 1279.8966064453125\n",
      "epoch 89, val_loss 456.34442138671875\n",
      "epoch 90, train_loss 1279.8935546875\n",
      "epoch 90, val_loss 456.3433837890625\n",
      "epoch 91, train_loss 1279.890625\n",
      "epoch 91, val_loss 456.3424072265625\n",
      "epoch 92, train_loss 1279.8876953125\n",
      "epoch 92, val_loss 456.34130859375\n",
      "epoch 93, train_loss 1279.884765625\n",
      "epoch 93, val_loss 456.3402404785156\n",
      "epoch 94, train_loss 1279.8818359375\n",
      "epoch 94, val_loss 456.3392639160156\n",
      "epoch 95, train_loss 1279.87890625\n",
      "epoch 95, val_loss 456.338134765625\n",
      "epoch 96, train_loss 1279.8759765625\n",
      "epoch 96, val_loss 456.33709716796875\n",
      "epoch 97, train_loss 1279.8729248046875\n",
      "epoch 97, val_loss 456.336181640625\n",
      "epoch 98, train_loss 1279.8699951171875\n",
      "epoch 98, val_loss 456.3351135253906\n",
      "epoch 99, train_loss 1279.866943359375\n",
      "epoch 99, val_loss 456.3341064453125\n",
      "Parameter containing:\n",
      "tensor([0.0056], requires_grad=True)\n",
      "iter 9, train_loss_regularization 1.2054355144500732\n",
      "iter 9, val_loss_regularization 1.2054355144500732\n",
      "epoch 0, train_loss 1279.8641357421875\n",
      "epoch 0, val_loss 456.3330993652344\n",
      "epoch 1, train_loss 1279.861328125\n",
      "epoch 1, val_loss 456.33203125\n",
      "epoch 2, train_loss 1279.8583984375\n",
      "epoch 2, val_loss 456.3310241699219\n",
      "epoch 3, train_loss 1279.85546875\n",
      "epoch 3, val_loss 456.3299865722656\n",
      "epoch 4, train_loss 1279.8525390625\n",
      "epoch 4, val_loss 456.3289794921875\n",
      "epoch 5, train_loss 1279.849609375\n",
      "epoch 5, val_loss 456.3280334472656\n",
      "epoch 6, train_loss 1279.8468017578125\n",
      "epoch 6, val_loss 456.326904296875\n",
      "epoch 7, train_loss 1279.8438720703125\n",
      "epoch 7, val_loss 456.3258972167969\n",
      "epoch 8, train_loss 1279.8409423828125\n",
      "epoch 8, val_loss 456.324951171875\n",
      "epoch 9, train_loss 1279.8382568359375\n",
      "epoch 9, val_loss 456.32391357421875\n",
      "epoch 10, train_loss 1279.8353271484375\n",
      "epoch 10, val_loss 456.32293701171875\n",
      "epoch 11, train_loss 1279.8323974609375\n",
      "epoch 11, val_loss 456.32196044921875\n",
      "epoch 12, train_loss 1279.82958984375\n",
      "epoch 12, val_loss 456.3209533691406\n",
      "epoch 13, train_loss 1279.8267822265625\n",
      "epoch 13, val_loss 456.3199157714844\n",
      "epoch 14, train_loss 1279.823974609375\n",
      "epoch 14, val_loss 456.3189392089844\n",
      "epoch 15, train_loss 1279.8211669921875\n",
      "epoch 15, val_loss 456.3180236816406\n",
      "epoch 16, train_loss 1279.818359375\n",
      "epoch 16, val_loss 456.3170166015625\n",
      "epoch 17, train_loss 1279.8155517578125\n",
      "epoch 17, val_loss 456.3160400390625\n",
      "epoch 18, train_loss 1279.812744140625\n",
      "epoch 18, val_loss 456.3150329589844\n",
      "epoch 19, train_loss 1279.8099365234375\n",
      "epoch 19, val_loss 456.3139953613281\n",
      "epoch 20, train_loss 1279.806884765625\n",
      "epoch 20, val_loss 456.3130798339844\n",
      "epoch 21, train_loss 1279.804443359375\n",
      "epoch 21, val_loss 456.31207275390625\n",
      "epoch 22, train_loss 1279.801513671875\n",
      "epoch 22, val_loss 456.3111267089844\n",
      "epoch 23, train_loss 1279.798828125\n",
      "epoch 23, val_loss 456.3101501464844\n",
      "epoch 24, train_loss 1279.796142578125\n",
      "epoch 24, val_loss 456.3092041015625\n",
      "epoch 25, train_loss 1279.7933349609375\n",
      "epoch 25, val_loss 456.3082275390625\n",
      "epoch 26, train_loss 1279.7906494140625\n",
      "epoch 26, val_loss 456.30718994140625\n",
      "epoch 27, train_loss 1279.7877197265625\n",
      "epoch 27, val_loss 456.3062744140625\n",
      "epoch 28, train_loss 1279.7850341796875\n",
      "epoch 28, val_loss 456.3053283691406\n",
      "epoch 29, train_loss 1279.782470703125\n",
      "epoch 29, val_loss 456.3043212890625\n",
      "epoch 30, train_loss 1279.779541015625\n",
      "epoch 30, val_loss 456.3034362792969\n",
      "epoch 31, train_loss 1279.7767333984375\n",
      "epoch 31, val_loss 456.302490234375\n",
      "epoch 32, train_loss 1279.7742919921875\n",
      "epoch 32, val_loss 456.3014831542969\n",
      "epoch 33, train_loss 1279.7713623046875\n",
      "epoch 33, val_loss 456.300537109375\n",
      "epoch 34, train_loss 1279.7686767578125\n",
      "epoch 34, val_loss 456.2996520996094\n",
      "epoch 35, train_loss 1279.76611328125\n",
      "epoch 35, val_loss 456.29864501953125\n",
      "epoch 36, train_loss 1279.7633056640625\n",
      "epoch 36, val_loss 456.2976989746094\n",
      "epoch 37, train_loss 1279.7606201171875\n",
      "epoch 37, val_loss 456.2967834472656\n",
      "epoch 38, train_loss 1279.758056640625\n",
      "epoch 38, val_loss 456.29583740234375\n",
      "epoch 39, train_loss 1279.7552490234375\n",
      "epoch 39, val_loss 456.2948913574219\n",
      "epoch 40, train_loss 1279.75244140625\n",
      "epoch 40, val_loss 456.2939147949219\n",
      "epoch 41, train_loss 1279.7498779296875\n",
      "epoch 41, val_loss 456.2929382324219\n",
      "epoch 42, train_loss 1279.747314453125\n",
      "epoch 42, val_loss 456.2920837402344\n",
      "epoch 43, train_loss 1279.7447509765625\n",
      "epoch 43, val_loss 456.29119873046875\n",
      "epoch 44, train_loss 1279.741943359375\n",
      "epoch 44, val_loss 456.2901916503906\n",
      "epoch 45, train_loss 1279.7393798828125\n",
      "epoch 45, val_loss 456.2892761230469\n",
      "epoch 46, train_loss 1279.7366943359375\n",
      "epoch 46, val_loss 456.2884826660156\n",
      "epoch 47, train_loss 1279.734130859375\n",
      "epoch 47, val_loss 456.2874450683594\n",
      "epoch 48, train_loss 1279.7315673828125\n",
      "epoch 48, val_loss 456.2865295410156\n",
      "epoch 49, train_loss 1279.7288818359375\n",
      "epoch 49, val_loss 456.28558349609375\n",
      "epoch 50, train_loss 1279.726318359375\n",
      "epoch 50, val_loss 456.2846984863281\n",
      "epoch 51, train_loss 1279.7237548828125\n",
      "epoch 51, val_loss 456.2838134765625\n",
      "epoch 52, train_loss 1279.72119140625\n",
      "epoch 52, val_loss 456.28289794921875\n",
      "epoch 53, train_loss 1279.718505859375\n",
      "epoch 53, val_loss 456.2819519042969\n",
      "epoch 54, train_loss 1279.7159423828125\n",
      "epoch 54, val_loss 456.2809753417969\n",
      "epoch 55, train_loss 1279.71337890625\n",
      "epoch 55, val_loss 456.2801208496094\n",
      "epoch 56, train_loss 1279.7109375\n",
      "epoch 56, val_loss 456.2792663574219\n",
      "epoch 57, train_loss 1279.708251953125\n",
      "epoch 57, val_loss 456.2783508300781\n",
      "epoch 58, train_loss 1279.705810546875\n",
      "epoch 58, val_loss 456.2774353027344\n",
      "epoch 59, train_loss 1279.703125\n",
      "epoch 59, val_loss 456.2765808105469\n",
      "epoch 60, train_loss 1279.700439453125\n",
      "epoch 60, val_loss 456.2756042480469\n",
      "epoch 61, train_loss 1279.6981201171875\n",
      "epoch 61, val_loss 456.2747497558594\n",
      "epoch 62, train_loss 1279.6954345703125\n",
      "epoch 62, val_loss 456.27386474609375\n",
      "epoch 63, train_loss 1279.6929931640625\n",
      "epoch 63, val_loss 456.27294921875\n",
      "epoch 64, train_loss 1279.6904296875\n",
      "epoch 64, val_loss 456.2720642089844\n",
      "epoch 65, train_loss 1279.6881103515625\n",
      "epoch 65, val_loss 456.2711486816406\n",
      "epoch 66, train_loss 1279.685302734375\n",
      "epoch 66, val_loss 456.27032470703125\n",
      "epoch 67, train_loss 1279.6829833984375\n",
      "epoch 67, val_loss 456.2694396972656\n",
      "epoch 68, train_loss 1279.680419921875\n",
      "epoch 68, val_loss 456.26849365234375\n",
      "epoch 69, train_loss 1279.6778564453125\n",
      "epoch 69, val_loss 456.2676086425781\n",
      "epoch 70, train_loss 1279.675537109375\n",
      "epoch 70, val_loss 456.26678466796875\n",
      "epoch 71, train_loss 1279.6728515625\n",
      "epoch 71, val_loss 456.265869140625\n",
      "epoch 72, train_loss 1279.6705322265625\n",
      "epoch 72, val_loss 456.26495361328125\n",
      "epoch 73, train_loss 1279.66796875\n",
      "epoch 73, val_loss 456.26416015625\n",
      "epoch 74, train_loss 1279.66552734375\n",
      "epoch 74, val_loss 456.2633361816406\n",
      "epoch 75, train_loss 1279.6629638671875\n",
      "epoch 75, val_loss 456.2624206542969\n",
      "epoch 76, train_loss 1279.6605224609375\n",
      "epoch 76, val_loss 456.2615661621094\n",
      "epoch 77, train_loss 1279.658203125\n",
      "epoch 77, val_loss 456.2606201171875\n",
      "epoch 78, train_loss 1279.6556396484375\n",
      "epoch 78, val_loss 456.25982666015625\n",
      "epoch 79, train_loss 1279.6531982421875\n",
      "epoch 79, val_loss 456.2589416503906\n",
      "epoch 80, train_loss 1279.65087890625\n",
      "epoch 80, val_loss 456.2580871582031\n",
      "epoch 81, train_loss 1279.6483154296875\n",
      "epoch 81, val_loss 456.2572021484375\n",
      "epoch 82, train_loss 1279.64599609375\n",
      "epoch 82, val_loss 456.25640869140625\n",
      "epoch 83, train_loss 1279.6435546875\n",
      "epoch 83, val_loss 456.2555847167969\n",
      "epoch 84, train_loss 1279.64111328125\n",
      "epoch 84, val_loss 456.25469970703125\n",
      "epoch 85, train_loss 1279.6387939453125\n",
      "epoch 85, val_loss 456.2538146972656\n",
      "epoch 86, train_loss 1279.6363525390625\n",
      "epoch 86, val_loss 456.25299072265625\n",
      "epoch 87, train_loss 1279.6337890625\n",
      "epoch 87, val_loss 456.2520446777344\n",
      "epoch 88, train_loss 1279.6314697265625\n",
      "epoch 88, val_loss 456.2512512207031\n",
      "epoch 89, train_loss 1279.6290283203125\n",
      "epoch 89, val_loss 456.2503967285156\n",
      "epoch 90, train_loss 1279.626708984375\n",
      "epoch 90, val_loss 456.2496032714844\n",
      "epoch 91, train_loss 1279.6243896484375\n",
      "epoch 91, val_loss 456.2486877441406\n",
      "epoch 92, train_loss 1279.6219482421875\n",
      "epoch 92, val_loss 456.24786376953125\n",
      "epoch 93, train_loss 1279.61962890625\n",
      "epoch 93, val_loss 456.2470703125\n",
      "epoch 94, train_loss 1279.6171875\n",
      "epoch 94, val_loss 456.2461853027344\n",
      "epoch 95, train_loss 1279.614990234375\n",
      "epoch 95, val_loss 456.2453918457031\n",
      "epoch 96, train_loss 1279.6126708984375\n",
      "epoch 96, val_loss 456.24456787109375\n",
      "epoch 97, train_loss 1279.6102294921875\n",
      "epoch 97, val_loss 456.2437438964844\n",
      "epoch 98, train_loss 1279.6077880859375\n",
      "epoch 98, val_loss 456.242919921875\n",
      "epoch 99, train_loss 1279.6055908203125\n",
      "epoch 99, val_loss 456.24212646484375\n",
      "Parameter containing:\n",
      "tensor([0.0036], requires_grad=True)\n",
      "iter 10, train_loss_regularization 1.191562294960022\n",
      "iter 10, val_loss_regularization 1.191562294960022\n",
      "epoch 0, train_loss 1279.603271484375\n",
      "epoch 0, val_loss 456.2412414550781\n",
      "epoch 1, train_loss 1279.600830078125\n",
      "epoch 1, val_loss 456.2403869628906\n",
      "epoch 2, train_loss 1279.5986328125\n",
      "epoch 2, val_loss 456.2395935058594\n",
      "epoch 3, train_loss 1279.5963134765625\n",
      "epoch 3, val_loss 456.2388000488281\n",
      "epoch 4, train_loss 1279.5941162109375\n",
      "epoch 4, val_loss 456.2379150390625\n",
      "epoch 5, train_loss 1279.5916748046875\n",
      "epoch 5, val_loss 456.23712158203125\n",
      "epoch 6, train_loss 1279.58935546875\n",
      "epoch 6, val_loss 456.2362976074219\n",
      "epoch 7, train_loss 1279.5870361328125\n",
      "epoch 7, val_loss 456.2355041503906\n",
      "epoch 8, train_loss 1279.5848388671875\n",
      "epoch 8, val_loss 456.23468017578125\n",
      "epoch 9, train_loss 1279.58251953125\n",
      "epoch 9, val_loss 456.23388671875\n",
      "epoch 10, train_loss 1279.580322265625\n",
      "epoch 10, val_loss 456.23309326171875\n",
      "epoch 11, train_loss 1279.5777587890625\n",
      "epoch 11, val_loss 456.23223876953125\n",
      "epoch 12, train_loss 1279.57568359375\n",
      "epoch 12, val_loss 456.2314453125\n",
      "epoch 13, train_loss 1279.5733642578125\n",
      "epoch 13, val_loss 456.2306213378906\n",
      "epoch 14, train_loss 1279.5711669921875\n",
      "epoch 14, val_loss 456.2298889160156\n",
      "epoch 15, train_loss 1279.5687255859375\n",
      "epoch 15, val_loss 456.2290344238281\n",
      "epoch 16, train_loss 1279.566650390625\n",
      "epoch 16, val_loss 456.2282409667969\n",
      "epoch 17, train_loss 1279.564453125\n",
      "epoch 17, val_loss 456.2275085449219\n",
      "epoch 18, train_loss 1279.5621337890625\n",
      "epoch 18, val_loss 456.2267150878906\n",
      "epoch 19, train_loss 1279.56005859375\n",
      "epoch 19, val_loss 456.2259216308594\n",
      "epoch 20, train_loss 1279.5576171875\n",
      "epoch 20, val_loss 456.2251281738281\n",
      "epoch 21, train_loss 1279.5555419921875\n",
      "epoch 21, val_loss 456.2243347167969\n",
      "epoch 22, train_loss 1279.55322265625\n",
      "epoch 22, val_loss 456.2235412597656\n",
      "epoch 23, train_loss 1279.551025390625\n",
      "epoch 23, val_loss 456.22271728515625\n",
      "epoch 24, train_loss 1279.548828125\n",
      "epoch 24, val_loss 456.22198486328125\n",
      "epoch 25, train_loss 1279.5467529296875\n",
      "epoch 25, val_loss 456.2211608886719\n",
      "epoch 26, train_loss 1279.54443359375\n",
      "epoch 26, val_loss 456.2203674316406\n",
      "epoch 27, train_loss 1279.5421142578125\n",
      "epoch 27, val_loss 456.2196350097656\n",
      "epoch 28, train_loss 1279.5401611328125\n",
      "epoch 28, val_loss 456.2188415527344\n",
      "epoch 29, train_loss 1279.5379638671875\n",
      "epoch 29, val_loss 456.2180480957031\n",
      "epoch 30, train_loss 1279.5357666015625\n",
      "epoch 30, val_loss 456.21728515625\n",
      "epoch 31, train_loss 1279.533447265625\n",
      "epoch 31, val_loss 456.2164611816406\n",
      "epoch 32, train_loss 1279.5313720703125\n",
      "epoch 32, val_loss 456.21575927734375\n",
      "epoch 33, train_loss 1279.529296875\n",
      "epoch 33, val_loss 456.2149658203125\n",
      "epoch 34, train_loss 1279.527099609375\n",
      "epoch 34, val_loss 456.2142028808594\n",
      "epoch 35, train_loss 1279.5247802734375\n",
      "epoch 35, val_loss 456.2134094238281\n",
      "epoch 36, train_loss 1279.522705078125\n",
      "epoch 36, val_loss 456.2126770019531\n",
      "epoch 37, train_loss 1279.5206298828125\n",
      "epoch 37, val_loss 456.2118225097656\n",
      "epoch 38, train_loss 1279.5185546875\n",
      "epoch 38, val_loss 456.211181640625\n",
      "epoch 39, train_loss 1279.516357421875\n",
      "epoch 39, val_loss 456.21038818359375\n",
      "epoch 40, train_loss 1279.51416015625\n",
      "epoch 40, val_loss 456.2095947265625\n",
      "epoch 41, train_loss 1279.5120849609375\n",
      "epoch 41, val_loss 456.2088623046875\n",
      "epoch 42, train_loss 1279.510009765625\n",
      "epoch 42, val_loss 456.20806884765625\n",
      "epoch 43, train_loss 1279.5076904296875\n",
      "epoch 43, val_loss 456.2073669433594\n",
      "epoch 44, train_loss 1279.505859375\n",
      "epoch 44, val_loss 456.2065734863281\n",
      "epoch 45, train_loss 1279.5035400390625\n",
      "epoch 45, val_loss 456.2058410644531\n",
      "epoch 46, train_loss 1279.50146484375\n",
      "epoch 46, val_loss 456.2050476074219\n",
      "epoch 47, train_loss 1279.499267578125\n",
      "epoch 47, val_loss 456.204345703125\n",
      "epoch 48, train_loss 1279.49755859375\n",
      "epoch 48, val_loss 456.2035827636719\n",
      "epoch 49, train_loss 1279.4952392578125\n",
      "epoch 49, val_loss 456.202880859375\n",
      "epoch 50, train_loss 1279.4931640625\n",
      "epoch 50, val_loss 456.2021179199219\n",
      "epoch 51, train_loss 1279.4912109375\n",
      "epoch 51, val_loss 456.2013854980469\n",
      "epoch 52, train_loss 1279.4888916015625\n",
      "epoch 52, val_loss 456.2006530761719\n",
      "epoch 53, train_loss 1279.487060546875\n",
      "epoch 53, val_loss 456.1999206542969\n",
      "epoch 54, train_loss 1279.48486328125\n",
      "epoch 54, val_loss 456.1991271972656\n",
      "epoch 55, train_loss 1279.4825439453125\n",
      "epoch 55, val_loss 456.1984558105469\n",
      "epoch 56, train_loss 1279.480712890625\n",
      "epoch 56, val_loss 456.1977233886719\n",
      "epoch 57, train_loss 1279.4786376953125\n",
      "epoch 57, val_loss 456.19696044921875\n",
      "epoch 58, train_loss 1279.4765625\n",
      "epoch 58, val_loss 456.1962585449219\n",
      "epoch 59, train_loss 1279.474609375\n",
      "epoch 59, val_loss 456.195556640625\n",
      "epoch 60, train_loss 1279.4725341796875\n",
      "epoch 60, val_loss 456.19476318359375\n",
      "epoch 61, train_loss 1279.4705810546875\n",
      "epoch 61, val_loss 456.194091796875\n",
      "epoch 62, train_loss 1279.468505859375\n",
      "epoch 62, val_loss 456.19329833984375\n",
      "epoch 63, train_loss 1279.46630859375\n",
      "epoch 63, val_loss 456.1925964355469\n",
      "epoch 64, train_loss 1279.4644775390625\n",
      "epoch 64, val_loss 456.1919250488281\n",
      "epoch 65, train_loss 1279.46240234375\n",
      "epoch 65, val_loss 456.1911315917969\n",
      "epoch 66, train_loss 1279.4603271484375\n",
      "epoch 66, val_loss 456.1904296875\n",
      "epoch 67, train_loss 1279.4583740234375\n",
      "epoch 67, val_loss 456.18975830078125\n",
      "epoch 68, train_loss 1279.456298828125\n",
      "epoch 68, val_loss 456.18896484375\n",
      "epoch 69, train_loss 1279.4544677734375\n",
      "epoch 69, val_loss 456.1882629394531\n",
      "epoch 70, train_loss 1279.4525146484375\n",
      "epoch 70, val_loss 456.1875915527344\n",
      "epoch 71, train_loss 1279.450439453125\n",
      "epoch 71, val_loss 456.1867980957031\n",
      "epoch 72, train_loss 1279.448486328125\n",
      "epoch 72, val_loss 456.1861572265625\n",
      "epoch 73, train_loss 1279.4464111328125\n",
      "epoch 73, val_loss 456.1854553222656\n",
      "epoch 74, train_loss 1279.4444580078125\n",
      "epoch 74, val_loss 456.1847229003906\n",
      "epoch 75, train_loss 1279.442626953125\n",
      "epoch 75, val_loss 456.1840515136719\n",
      "epoch 76, train_loss 1279.4405517578125\n",
      "epoch 76, val_loss 456.1833190917969\n",
      "epoch 77, train_loss 1279.4384765625\n",
      "epoch 77, val_loss 456.1825866699219\n",
      "epoch 78, train_loss 1279.4365234375\n",
      "epoch 78, val_loss 456.181884765625\n",
      "epoch 79, train_loss 1279.4344482421875\n",
      "epoch 79, val_loss 456.18121337890625\n",
      "epoch 80, train_loss 1279.4327392578125\n",
      "epoch 80, val_loss 456.1804504394531\n",
      "epoch 81, train_loss 1279.4307861328125\n",
      "epoch 81, val_loss 456.1797790527344\n",
      "epoch 82, train_loss 1279.4287109375\n",
      "epoch 82, val_loss 456.17913818359375\n",
      "epoch 83, train_loss 1279.4268798828125\n",
      "epoch 83, val_loss 456.17840576171875\n",
      "epoch 84, train_loss 1279.4248046875\n",
      "epoch 84, val_loss 456.1777648925781\n",
      "epoch 85, train_loss 1279.4229736328125\n",
      "epoch 85, val_loss 456.1770935058594\n",
      "epoch 86, train_loss 1279.421142578125\n",
      "epoch 86, val_loss 456.17633056640625\n",
      "epoch 87, train_loss 1279.419189453125\n",
      "epoch 87, val_loss 456.1756286621094\n",
      "epoch 88, train_loss 1279.417236328125\n",
      "epoch 88, val_loss 456.1749572753906\n",
      "epoch 89, train_loss 1279.415283203125\n",
      "epoch 89, val_loss 456.1742858886719\n",
      "epoch 90, train_loss 1279.413330078125\n",
      "epoch 90, val_loss 456.1735534667969\n",
      "epoch 91, train_loss 1279.411376953125\n",
      "epoch 91, val_loss 456.1728820800781\n",
      "epoch 92, train_loss 1279.4095458984375\n",
      "epoch 92, val_loss 456.1722106933594\n",
      "epoch 93, train_loss 1279.40771484375\n",
      "epoch 93, val_loss 456.1715393066406\n",
      "epoch 94, train_loss 1279.40576171875\n",
      "epoch 94, val_loss 456.17083740234375\n",
      "epoch 95, train_loss 1279.40380859375\n",
      "epoch 95, val_loss 456.1701965332031\n",
      "epoch 96, train_loss 1279.4019775390625\n",
      "epoch 96, val_loss 456.16949462890625\n",
      "epoch 97, train_loss 1279.4000244140625\n",
      "epoch 97, val_loss 456.1688232421875\n",
      "epoch 98, train_loss 1279.398193359375\n",
      "epoch 98, val_loss 456.1681213378906\n",
      "epoch 99, train_loss 1279.3963623046875\n",
      "epoch 99, val_loss 456.16741943359375\n",
      "Parameter containing:\n",
      "tensor([0.0023], requires_grad=True)\n",
      "iter 11, train_loss_regularization 1.1788012981414795\n",
      "iter 11, val_loss_regularization 1.1788012981414795\n",
      "epoch 0, train_loss 1279.39453125\n",
      "epoch 0, val_loss 456.1667785644531\n",
      "epoch 1, train_loss 1279.392578125\n",
      "epoch 1, val_loss 456.1661682128906\n",
      "epoch 2, train_loss 1279.3907470703125\n",
      "epoch 2, val_loss 456.1654968261719\n",
      "epoch 3, train_loss 1279.3887939453125\n",
      "epoch 3, val_loss 456.1647644042969\n",
      "epoch 4, train_loss 1279.38720703125\n",
      "epoch 4, val_loss 456.16412353515625\n",
      "epoch 5, train_loss 1279.3851318359375\n",
      "epoch 5, val_loss 456.1634521484375\n",
      "epoch 6, train_loss 1279.38330078125\n",
      "epoch 6, val_loss 456.16278076171875\n",
      "epoch 7, train_loss 1279.381591796875\n",
      "epoch 7, val_loss 456.1621398925781\n",
      "epoch 8, train_loss 1279.379638671875\n",
      "epoch 8, val_loss 456.1614990234375\n",
      "epoch 9, train_loss 1279.3778076171875\n",
      "epoch 9, val_loss 456.16082763671875\n",
      "epoch 10, train_loss 1279.3760986328125\n",
      "epoch 10, val_loss 456.16021728515625\n",
      "epoch 11, train_loss 1279.3743896484375\n",
      "epoch 11, val_loss 456.1594543457031\n",
      "epoch 12, train_loss 1279.3721923828125\n",
      "epoch 12, val_loss 456.1588439941406\n",
      "epoch 13, train_loss 1279.3704833984375\n",
      "epoch 13, val_loss 456.1581726074219\n",
      "epoch 14, train_loss 1279.36865234375\n",
      "epoch 14, val_loss 456.15753173828125\n",
      "epoch 15, train_loss 1279.366943359375\n",
      "epoch 15, val_loss 456.1568603515625\n",
      "epoch 16, train_loss 1279.364990234375\n",
      "epoch 16, val_loss 456.15625\n",
      "epoch 17, train_loss 1279.36328125\n",
      "epoch 17, val_loss 456.15557861328125\n",
      "epoch 18, train_loss 1279.3614501953125\n",
      "epoch 18, val_loss 456.15496826171875\n",
      "epoch 19, train_loss 1279.359619140625\n",
      "epoch 19, val_loss 456.154296875\n",
      "epoch 20, train_loss 1279.3580322265625\n",
      "epoch 20, val_loss 456.1535949707031\n",
      "epoch 21, train_loss 1279.356201171875\n",
      "epoch 21, val_loss 456.15301513671875\n",
      "epoch 22, train_loss 1279.3543701171875\n",
      "epoch 22, val_loss 456.1523742675781\n",
      "epoch 23, train_loss 1279.3526611328125\n",
      "epoch 23, val_loss 456.1517028808594\n",
      "epoch 24, train_loss 1279.350830078125\n",
      "epoch 24, val_loss 456.1510009765625\n",
      "epoch 25, train_loss 1279.3489990234375\n",
      "epoch 25, val_loss 456.1504211425781\n",
      "epoch 26, train_loss 1279.34716796875\n",
      "epoch 26, val_loss 456.14984130859375\n",
      "epoch 27, train_loss 1279.345458984375\n",
      "epoch 27, val_loss 456.1491394042969\n",
      "epoch 28, train_loss 1279.3438720703125\n",
      "epoch 28, val_loss 456.1485290527344\n",
      "epoch 29, train_loss 1279.342041015625\n",
      "epoch 29, val_loss 456.14788818359375\n",
      "epoch 30, train_loss 1279.3402099609375\n",
      "epoch 30, val_loss 456.147216796875\n",
      "epoch 31, train_loss 1279.3385009765625\n",
      "epoch 31, val_loss 456.1465759277344\n",
      "epoch 32, train_loss 1279.3367919921875\n",
      "epoch 32, val_loss 456.1459655761719\n",
      "epoch 33, train_loss 1279.3349609375\n",
      "epoch 33, val_loss 456.1452941894531\n",
      "epoch 34, train_loss 1279.333251953125\n",
      "epoch 34, val_loss 456.1446533203125\n",
      "epoch 35, train_loss 1279.33154296875\n",
      "epoch 35, val_loss 456.1440124511719\n",
      "epoch 36, train_loss 1279.329833984375\n",
      "epoch 36, val_loss 456.1434631347656\n",
      "epoch 37, train_loss 1279.328125\n",
      "epoch 37, val_loss 456.1429138183594\n",
      "epoch 38, train_loss 1279.3262939453125\n",
      "epoch 38, val_loss 456.1422119140625\n",
      "epoch 39, train_loss 1279.3245849609375\n",
      "epoch 39, val_loss 456.14154052734375\n",
      "epoch 40, train_loss 1279.322998046875\n",
      "epoch 40, val_loss 456.1409606933594\n",
      "epoch 41, train_loss 1279.3212890625\n",
      "epoch 41, val_loss 456.14031982421875\n",
      "epoch 42, train_loss 1279.3194580078125\n",
      "epoch 42, val_loss 456.1397399902344\n",
      "epoch 43, train_loss 1279.3177490234375\n",
      "epoch 43, val_loss 456.1390686035156\n",
      "epoch 44, train_loss 1279.3160400390625\n",
      "epoch 44, val_loss 456.13848876953125\n",
      "epoch 45, train_loss 1279.314453125\n",
      "epoch 45, val_loss 456.1379089355469\n",
      "epoch 46, train_loss 1279.3126220703125\n",
      "epoch 46, val_loss 456.1372375488281\n",
      "epoch 47, train_loss 1279.31103515625\n",
      "epoch 47, val_loss 456.1366271972656\n",
      "epoch 48, train_loss 1279.3092041015625\n",
      "epoch 48, val_loss 456.135986328125\n",
      "epoch 49, train_loss 1279.3076171875\n",
      "epoch 49, val_loss 456.1353759765625\n",
      "epoch 50, train_loss 1279.3060302734375\n",
      "epoch 50, val_loss 456.13482666015625\n",
      "epoch 51, train_loss 1279.304443359375\n",
      "epoch 51, val_loss 456.1341552734375\n",
      "epoch 52, train_loss 1279.3026123046875\n",
      "epoch 52, val_loss 456.1336364746094\n",
      "epoch 53, train_loss 1279.301025390625\n",
      "epoch 53, val_loss 456.13299560546875\n",
      "epoch 54, train_loss 1279.2991943359375\n",
      "epoch 54, val_loss 456.13238525390625\n",
      "epoch 55, train_loss 1279.2974853515625\n",
      "epoch 55, val_loss 456.1317443847656\n",
      "epoch 56, train_loss 1279.2958984375\n",
      "epoch 56, val_loss 456.1311950683594\n",
      "epoch 57, train_loss 1279.294189453125\n",
      "epoch 57, val_loss 456.130615234375\n",
      "epoch 58, train_loss 1279.292724609375\n",
      "epoch 58, val_loss 456.12994384765625\n",
      "epoch 59, train_loss 1279.291015625\n",
      "epoch 59, val_loss 456.1293640136719\n",
      "epoch 60, train_loss 1279.289306640625\n",
      "epoch 60, val_loss 456.12884521484375\n",
      "epoch 61, train_loss 1279.2877197265625\n",
      "epoch 61, val_loss 456.128173828125\n",
      "epoch 62, train_loss 1279.2861328125\n",
      "epoch 62, val_loss 456.1275939941406\n",
      "epoch 63, train_loss 1279.2845458984375\n",
      "epoch 63, val_loss 456.126953125\n",
      "epoch 64, train_loss 1279.2828369140625\n",
      "epoch 64, val_loss 456.12640380859375\n",
      "epoch 65, train_loss 1279.2811279296875\n",
      "epoch 65, val_loss 456.12579345703125\n",
      "epoch 66, train_loss 1279.279541015625\n",
      "epoch 66, val_loss 456.125244140625\n",
      "epoch 67, train_loss 1279.2779541015625\n",
      "epoch 67, val_loss 456.12469482421875\n",
      "epoch 68, train_loss 1279.276123046875\n",
      "epoch 68, val_loss 456.1240539550781\n",
      "epoch 69, train_loss 1279.274658203125\n",
      "epoch 69, val_loss 456.1235046386719\n",
      "epoch 70, train_loss 1279.27294921875\n",
      "epoch 70, val_loss 456.1229248046875\n",
      "epoch 71, train_loss 1279.271484375\n",
      "epoch 71, val_loss 456.1222839355469\n",
      "epoch 72, train_loss 1279.269775390625\n",
      "epoch 72, val_loss 456.1217041015625\n",
      "epoch 73, train_loss 1279.268310546875\n",
      "epoch 73, val_loss 456.1211242675781\n",
      "epoch 74, train_loss 1279.2666015625\n",
      "epoch 74, val_loss 456.1205139160156\n",
      "epoch 75, train_loss 1279.2650146484375\n",
      "epoch 75, val_loss 456.1199035644531\n",
      "epoch 76, train_loss 1279.263427734375\n",
      "epoch 76, val_loss 456.11932373046875\n",
      "epoch 77, train_loss 1279.2618408203125\n",
      "epoch 77, val_loss 456.1188049316406\n",
      "epoch 78, train_loss 1279.2603759765625\n",
      "epoch 78, val_loss 456.1182556152344\n",
      "epoch 79, train_loss 1279.2586669921875\n",
      "epoch 79, val_loss 456.11761474609375\n",
      "epoch 80, train_loss 1279.2572021484375\n",
      "epoch 80, val_loss 456.1170959472656\n",
      "epoch 81, train_loss 1279.2554931640625\n",
      "epoch 81, val_loss 456.1164855957031\n",
      "epoch 82, train_loss 1279.25390625\n",
      "epoch 82, val_loss 456.1158752441406\n",
      "epoch 83, train_loss 1279.25244140625\n",
      "epoch 83, val_loss 456.1153869628906\n",
      "epoch 84, train_loss 1279.2508544921875\n",
      "epoch 84, val_loss 456.11474609375\n",
      "epoch 85, train_loss 1279.249267578125\n",
      "epoch 85, val_loss 456.11419677734375\n",
      "epoch 86, train_loss 1279.2476806640625\n",
      "epoch 86, val_loss 456.1136169433594\n",
      "epoch 87, train_loss 1279.2462158203125\n",
      "epoch 87, val_loss 456.11309814453125\n",
      "epoch 88, train_loss 1279.24462890625\n",
      "epoch 88, val_loss 456.11248779296875\n",
      "epoch 89, train_loss 1279.2430419921875\n",
      "epoch 89, val_loss 456.11187744140625\n",
      "epoch 90, train_loss 1279.241455078125\n",
      "epoch 90, val_loss 456.11138916015625\n",
      "epoch 91, train_loss 1279.2401123046875\n",
      "epoch 91, val_loss 456.11077880859375\n",
      "epoch 92, train_loss 1279.238525390625\n",
      "epoch 92, val_loss 456.1102600097656\n",
      "epoch 93, train_loss 1279.2369384765625\n",
      "epoch 93, val_loss 456.10968017578125\n",
      "epoch 94, train_loss 1279.2352294921875\n",
      "epoch 94, val_loss 456.109130859375\n",
      "epoch 95, train_loss 1279.2340087890625\n",
      "epoch 95, val_loss 456.10858154296875\n",
      "epoch 96, train_loss 1279.2322998046875\n",
      "epoch 96, val_loss 456.1080017089844\n",
      "epoch 97, train_loss 1279.2308349609375\n",
      "epoch 97, val_loss 456.1074523925781\n",
      "epoch 98, train_loss 1279.2291259765625\n",
      "epoch 98, val_loss 456.10687255859375\n",
      "epoch 99, train_loss 1279.227783203125\n",
      "epoch 99, val_loss 456.1063232421875\n",
      "Parameter containing:\n",
      "tensor([0.0015], requires_grad=True)\n",
      "iter 12, train_loss_regularization 1.166948676109314\n",
      "iter 12, val_loss_regularization 1.166948676109314\n",
      "epoch 0, train_loss 1279.2261962890625\n",
      "epoch 0, val_loss 456.1057434082031\n",
      "epoch 1, train_loss 1279.2247314453125\n",
      "epoch 1, val_loss 456.1052551269531\n",
      "epoch 2, train_loss 1279.22314453125\n",
      "epoch 2, val_loss 456.1046142578125\n",
      "epoch 3, train_loss 1279.2218017578125\n",
      "epoch 3, val_loss 456.1041259765625\n",
      "epoch 4, train_loss 1279.22021484375\n",
      "epoch 4, val_loss 456.10357666015625\n",
      "epoch 5, train_loss 1279.2186279296875\n",
      "epoch 5, val_loss 456.10296630859375\n",
      "epoch 6, train_loss 1279.21728515625\n",
      "epoch 6, val_loss 456.1025085449219\n",
      "epoch 7, train_loss 1279.2158203125\n",
      "epoch 7, val_loss 456.10186767578125\n",
      "epoch 8, train_loss 1279.21435546875\n",
      "epoch 8, val_loss 456.10137939453125\n",
      "epoch 9, train_loss 1279.212890625\n",
      "epoch 9, val_loss 456.100830078125\n",
      "epoch 10, train_loss 1279.211181640625\n",
      "epoch 10, val_loss 456.10028076171875\n",
      "epoch 11, train_loss 1279.2098388671875\n",
      "epoch 11, val_loss 456.0997619628906\n",
      "epoch 12, train_loss 1279.2083740234375\n",
      "epoch 12, val_loss 456.0992126464844\n",
      "epoch 13, train_loss 1279.2069091796875\n",
      "epoch 13, val_loss 456.0986633300781\n",
      "epoch 14, train_loss 1279.2054443359375\n",
      "epoch 14, val_loss 456.09808349609375\n",
      "epoch 15, train_loss 1279.2039794921875\n",
      "epoch 15, val_loss 456.09759521484375\n",
      "epoch 16, train_loss 1279.202392578125\n",
      "epoch 16, val_loss 456.09698486328125\n",
      "epoch 17, train_loss 1279.200927734375\n",
      "epoch 17, val_loss 456.09649658203125\n",
      "epoch 18, train_loss 1279.199462890625\n",
      "epoch 18, val_loss 456.095947265625\n",
      "epoch 19, train_loss 1279.1981201171875\n",
      "epoch 19, val_loss 456.0953674316406\n",
      "epoch 20, train_loss 1279.1966552734375\n",
      "epoch 20, val_loss 456.0948791503906\n",
      "epoch 21, train_loss 1279.195068359375\n",
      "epoch 21, val_loss 456.0943603515625\n",
      "epoch 22, train_loss 1279.19384765625\n",
      "epoch 22, val_loss 456.0938720703125\n",
      "epoch 23, train_loss 1279.1923828125\n",
      "epoch 23, val_loss 456.0932922363281\n",
      "epoch 24, train_loss 1279.1910400390625\n",
      "epoch 24, val_loss 456.0927429199219\n",
      "epoch 25, train_loss 1279.189453125\n",
      "epoch 25, val_loss 456.0922546386719\n",
      "epoch 26, train_loss 1279.1878662109375\n",
      "epoch 26, val_loss 456.0917053222656\n",
      "epoch 27, train_loss 1279.1865234375\n",
      "epoch 27, val_loss 456.0911560058594\n",
      "epoch 28, train_loss 1279.1851806640625\n",
      "epoch 28, val_loss 456.0906677246094\n",
      "epoch 29, train_loss 1279.1837158203125\n",
      "epoch 29, val_loss 456.0901184082031\n",
      "epoch 30, train_loss 1279.1822509765625\n",
      "epoch 30, val_loss 456.0896301269531\n",
      "epoch 31, train_loss 1279.180908203125\n",
      "epoch 31, val_loss 456.0890808105469\n",
      "epoch 32, train_loss 1279.1795654296875\n",
      "epoch 32, val_loss 456.0885009765625\n",
      "epoch 33, train_loss 1279.177978515625\n",
      "epoch 33, val_loss 456.0880126953125\n",
      "epoch 34, train_loss 1279.176513671875\n",
      "epoch 34, val_loss 456.0874938964844\n",
      "epoch 35, train_loss 1279.1751708984375\n",
      "epoch 35, val_loss 456.0870361328125\n",
      "epoch 36, train_loss 1279.173828125\n",
      "epoch 36, val_loss 456.08642578125\n",
      "epoch 37, train_loss 1279.17236328125\n",
      "epoch 37, val_loss 456.0859069824219\n",
      "epoch 38, train_loss 1279.171142578125\n",
      "epoch 38, val_loss 456.08544921875\n",
      "epoch 39, train_loss 1279.1695556640625\n",
      "epoch 39, val_loss 456.0849304199219\n",
      "epoch 40, train_loss 1279.168212890625\n",
      "epoch 40, val_loss 456.08441162109375\n",
      "epoch 41, train_loss 1279.1668701171875\n",
      "epoch 41, val_loss 456.08392333984375\n",
      "epoch 42, train_loss 1279.16552734375\n",
      "epoch 42, val_loss 456.0833740234375\n",
      "epoch 43, train_loss 1279.1640625\n",
      "epoch 43, val_loss 456.08282470703125\n",
      "epoch 44, train_loss 1279.1627197265625\n",
      "epoch 44, val_loss 456.08233642578125\n",
      "epoch 45, train_loss 1279.1612548828125\n",
      "epoch 45, val_loss 456.081787109375\n",
      "epoch 46, train_loss 1279.1600341796875\n",
      "epoch 46, val_loss 456.0813293457031\n",
      "epoch 47, train_loss 1279.158447265625\n",
      "epoch 47, val_loss 456.0808410644531\n",
      "epoch 48, train_loss 1279.1572265625\n",
      "epoch 48, val_loss 456.08026123046875\n",
      "epoch 49, train_loss 1279.1558837890625\n",
      "epoch 49, val_loss 456.0798034667969\n",
      "epoch 50, train_loss 1279.154541015625\n",
      "epoch 50, val_loss 456.0792541503906\n",
      "epoch 51, train_loss 1279.1531982421875\n",
      "epoch 51, val_loss 456.07879638671875\n",
      "epoch 52, train_loss 1279.15185546875\n",
      "epoch 52, val_loss 456.0782775878906\n",
      "epoch 53, train_loss 1279.150390625\n",
      "epoch 53, val_loss 456.07781982421875\n",
      "epoch 54, train_loss 1279.14892578125\n",
      "epoch 54, val_loss 456.0773010253906\n",
      "epoch 55, train_loss 1279.1475830078125\n",
      "epoch 55, val_loss 456.0767822265625\n",
      "epoch 56, train_loss 1279.1463623046875\n",
      "epoch 56, val_loss 456.0762634277344\n",
      "epoch 57, train_loss 1279.1448974609375\n",
      "epoch 57, val_loss 456.07574462890625\n",
      "epoch 58, train_loss 1279.1436767578125\n",
      "epoch 58, val_loss 456.07525634765625\n",
      "epoch 59, train_loss 1279.1422119140625\n",
      "epoch 59, val_loss 456.0747375488281\n",
      "epoch 60, train_loss 1279.140869140625\n",
      "epoch 60, val_loss 456.0742492675781\n",
      "epoch 61, train_loss 1279.1396484375\n",
      "epoch 61, val_loss 456.0738220214844\n",
      "epoch 62, train_loss 1279.1383056640625\n",
      "epoch 62, val_loss 456.0733337402344\n",
      "epoch 63, train_loss 1279.1370849609375\n",
      "epoch 63, val_loss 456.0728454589844\n",
      "epoch 64, train_loss 1279.135498046875\n",
      "epoch 64, val_loss 456.07232666015625\n",
      "epoch 65, train_loss 1279.13427734375\n",
      "epoch 65, val_loss 456.07183837890625\n",
      "epoch 66, train_loss 1279.133056640625\n",
      "epoch 66, val_loss 456.0713195800781\n",
      "epoch 67, train_loss 1279.1317138671875\n",
      "epoch 67, val_loss 456.07086181640625\n",
      "epoch 68, train_loss 1279.13037109375\n",
      "epoch 68, val_loss 456.07037353515625\n",
      "epoch 69, train_loss 1279.1290283203125\n",
      "epoch 69, val_loss 456.06988525390625\n",
      "epoch 70, train_loss 1279.127685546875\n",
      "epoch 70, val_loss 456.0693664550781\n",
      "epoch 71, train_loss 1279.12646484375\n",
      "epoch 71, val_loss 456.06890869140625\n",
      "epoch 72, train_loss 1279.1251220703125\n",
      "epoch 72, val_loss 456.0683898925781\n",
      "epoch 73, train_loss 1279.123779296875\n",
      "epoch 73, val_loss 456.06787109375\n",
      "epoch 74, train_loss 1279.12255859375\n",
      "epoch 74, val_loss 456.0674133300781\n",
      "epoch 75, train_loss 1279.1212158203125\n",
      "epoch 75, val_loss 456.0669250488281\n",
      "epoch 76, train_loss 1279.1199951171875\n",
      "epoch 76, val_loss 456.06640625\n",
      "epoch 77, train_loss 1279.1185302734375\n",
      "epoch 77, val_loss 456.06591796875\n",
      "epoch 78, train_loss 1279.1173095703125\n",
      "epoch 78, val_loss 456.06549072265625\n",
      "epoch 79, train_loss 1279.1162109375\n",
      "epoch 79, val_loss 456.06500244140625\n",
      "epoch 80, train_loss 1279.1148681640625\n",
      "epoch 80, val_loss 456.06451416015625\n",
      "epoch 81, train_loss 1279.1134033203125\n",
      "epoch 81, val_loss 456.0639953613281\n",
      "epoch 82, train_loss 1279.1121826171875\n",
      "epoch 82, val_loss 456.06353759765625\n",
      "epoch 83, train_loss 1279.1109619140625\n",
      "epoch 83, val_loss 456.0630798339844\n",
      "epoch 84, train_loss 1279.109619140625\n",
      "epoch 84, val_loss 456.0625915527344\n",
      "epoch 85, train_loss 1279.1082763671875\n",
      "epoch 85, val_loss 456.06207275390625\n",
      "epoch 86, train_loss 1279.1070556640625\n",
      "epoch 86, val_loss 456.0616149902344\n",
      "epoch 87, train_loss 1279.1058349609375\n",
      "epoch 87, val_loss 456.06121826171875\n",
      "epoch 88, train_loss 1279.1044921875\n",
      "epoch 88, val_loss 456.0606994628906\n",
      "epoch 89, train_loss 1279.103271484375\n",
      "epoch 89, val_loss 456.0602111816406\n",
      "epoch 90, train_loss 1279.1019287109375\n",
      "epoch 90, val_loss 456.05975341796875\n",
      "epoch 91, train_loss 1279.100830078125\n",
      "epoch 91, val_loss 456.0592956542969\n",
      "epoch 92, train_loss 1279.099609375\n",
      "epoch 92, val_loss 456.05877685546875\n",
      "epoch 93, train_loss 1279.098388671875\n",
      "epoch 93, val_loss 456.05828857421875\n",
      "epoch 94, train_loss 1279.0970458984375\n",
      "epoch 94, val_loss 456.0578308105469\n",
      "epoch 95, train_loss 1279.095703125\n",
      "epoch 95, val_loss 456.0573425292969\n",
      "epoch 96, train_loss 1279.094482421875\n",
      "epoch 96, val_loss 456.0570068359375\n",
      "epoch 97, train_loss 1279.0933837890625\n",
      "epoch 97, val_loss 456.05645751953125\n",
      "epoch 98, train_loss 1279.092041015625\n",
      "epoch 98, val_loss 456.0559997558594\n",
      "epoch 99, train_loss 1279.0908203125\n",
      "epoch 99, val_loss 456.0555725097656\n",
      "Parameter containing:\n",
      "tensor([0.0009], requires_grad=True)\n",
      "iter 13, train_loss_regularization 1.155843734741211\n",
      "iter 13, val_loss_regularization 1.155843734741211\n",
      "epoch 0, train_loss 1279.0894775390625\n",
      "epoch 0, val_loss 456.0550842285156\n",
      "epoch 1, train_loss 1279.08837890625\n",
      "epoch 1, val_loss 456.0545349121094\n",
      "epoch 2, train_loss 1279.0870361328125\n",
      "epoch 2, val_loss 456.05413818359375\n",
      "epoch 3, train_loss 1279.0860595703125\n",
      "epoch 3, val_loss 456.0537414550781\n",
      "epoch 4, train_loss 1279.0845947265625\n",
      "epoch 4, val_loss 456.0532531738281\n",
      "epoch 5, train_loss 1279.0833740234375\n",
      "epoch 5, val_loss 456.05279541015625\n",
      "epoch 6, train_loss 1279.0821533203125\n",
      "epoch 6, val_loss 456.0522766113281\n",
      "epoch 7, train_loss 1279.0809326171875\n",
      "epoch 7, val_loss 456.0518798828125\n",
      "epoch 8, train_loss 1279.0797119140625\n",
      "epoch 8, val_loss 456.0513610839844\n",
      "epoch 9, train_loss 1279.0784912109375\n",
      "epoch 9, val_loss 456.05096435546875\n",
      "epoch 10, train_loss 1279.0772705078125\n",
      "epoch 10, val_loss 456.0505065917969\n",
      "epoch 11, train_loss 1279.0760498046875\n",
      "epoch 11, val_loss 456.050048828125\n",
      "epoch 12, train_loss 1279.0750732421875\n",
      "epoch 12, val_loss 456.0495300292969\n",
      "epoch 13, train_loss 1279.07373046875\n",
      "epoch 13, val_loss 456.0491638183594\n",
      "epoch 14, train_loss 1279.072509765625\n",
      "epoch 14, val_loss 456.0486755371094\n",
      "epoch 15, train_loss 1279.0712890625\n",
      "epoch 15, val_loss 456.0482482910156\n",
      "epoch 16, train_loss 1279.0699462890625\n",
      "epoch 16, val_loss 456.04779052734375\n",
      "epoch 17, train_loss 1279.0689697265625\n",
      "epoch 17, val_loss 456.0473327636719\n",
      "epoch 18, train_loss 1279.06787109375\n",
      "epoch 18, val_loss 456.0469055175781\n",
      "epoch 19, train_loss 1279.06640625\n",
      "epoch 19, val_loss 456.0464172363281\n",
      "epoch 20, train_loss 1279.0653076171875\n",
      "epoch 20, val_loss 456.04595947265625\n",
      "epoch 21, train_loss 1279.064208984375\n",
      "epoch 21, val_loss 456.0455017089844\n",
      "epoch 22, train_loss 1279.06298828125\n",
      "epoch 22, val_loss 456.0451354980469\n",
      "epoch 23, train_loss 1279.0618896484375\n",
      "epoch 23, val_loss 456.044677734375\n",
      "epoch 24, train_loss 1279.060791015625\n",
      "epoch 24, val_loss 456.0442199707031\n",
      "epoch 25, train_loss 1279.0594482421875\n",
      "epoch 25, val_loss 456.04376220703125\n",
      "epoch 26, train_loss 1279.058349609375\n",
      "epoch 26, val_loss 456.0433349609375\n",
      "epoch 27, train_loss 1279.05712890625\n",
      "epoch 27, val_loss 456.0428466796875\n",
      "epoch 28, train_loss 1279.055908203125\n",
      "epoch 28, val_loss 456.0423889160156\n",
      "epoch 29, train_loss 1279.0548095703125\n",
      "epoch 29, val_loss 456.0419616699219\n",
      "epoch 30, train_loss 1279.0535888671875\n",
      "epoch 30, val_loss 456.04150390625\n",
      "epoch 31, train_loss 1279.0523681640625\n",
      "epoch 31, val_loss 456.0411376953125\n",
      "epoch 32, train_loss 1279.05126953125\n",
      "epoch 32, val_loss 456.0406799316406\n",
      "epoch 33, train_loss 1279.050048828125\n",
      "epoch 33, val_loss 456.0402526855469\n",
      "epoch 34, train_loss 1279.048828125\n",
      "epoch 34, val_loss 456.039794921875\n",
      "epoch 35, train_loss 1279.0477294921875\n",
      "epoch 35, val_loss 456.0393371582031\n",
      "epoch 36, train_loss 1279.046630859375\n",
      "epoch 36, val_loss 456.0389099121094\n",
      "epoch 37, train_loss 1279.0455322265625\n",
      "epoch 37, val_loss 456.0384521484375\n",
      "epoch 38, train_loss 1279.044189453125\n",
      "epoch 38, val_loss 456.0379943847656\n",
      "epoch 39, train_loss 1279.043212890625\n",
      "epoch 39, val_loss 456.0376281738281\n",
      "epoch 40, train_loss 1279.0419921875\n",
      "epoch 40, val_loss 456.0371398925781\n",
      "epoch 41, train_loss 1279.0408935546875\n",
      "epoch 41, val_loss 456.0366516113281\n",
      "epoch 42, train_loss 1279.039794921875\n",
      "epoch 42, val_loss 456.036376953125\n",
      "epoch 43, train_loss 1279.0386962890625\n",
      "epoch 43, val_loss 456.0359191894531\n",
      "epoch 44, train_loss 1279.03759765625\n",
      "epoch 44, val_loss 456.0354309082031\n",
      "epoch 45, train_loss 1279.0361328125\n",
      "epoch 45, val_loss 456.0350341796875\n",
      "epoch 46, train_loss 1279.0350341796875\n",
      "epoch 46, val_loss 456.0345764160156\n",
      "epoch 47, train_loss 1279.0340576171875\n",
      "epoch 47, val_loss 456.0342102050781\n",
      "epoch 48, train_loss 1279.032958984375\n",
      "epoch 48, val_loss 456.0337219238281\n",
      "epoch 49, train_loss 1279.031982421875\n",
      "epoch 49, val_loss 456.0332946777344\n",
      "epoch 50, train_loss 1279.03076171875\n",
      "epoch 50, val_loss 456.0328369140625\n",
      "epoch 51, train_loss 1279.029541015625\n",
      "epoch 51, val_loss 456.03240966796875\n",
      "epoch 52, train_loss 1279.0284423828125\n",
      "epoch 52, val_loss 456.0320129394531\n",
      "epoch 53, train_loss 1279.02734375\n",
      "epoch 53, val_loss 456.0315856933594\n",
      "epoch 54, train_loss 1279.0263671875\n",
      "epoch 54, val_loss 456.0311279296875\n",
      "epoch 55, train_loss 1279.0250244140625\n",
      "epoch 55, val_loss 456.03070068359375\n",
      "epoch 56, train_loss 1279.0240478515625\n",
      "epoch 56, val_loss 456.0303039550781\n",
      "epoch 57, train_loss 1279.02294921875\n",
      "epoch 57, val_loss 456.0299072265625\n",
      "epoch 58, train_loss 1279.021728515625\n",
      "epoch 58, val_loss 456.0294189453125\n",
      "epoch 59, train_loss 1279.0206298828125\n",
      "epoch 59, val_loss 456.0290832519531\n",
      "epoch 60, train_loss 1279.0196533203125\n",
      "epoch 60, val_loss 456.0286560058594\n",
      "epoch 61, train_loss 1279.0185546875\n",
      "epoch 61, val_loss 456.0281982421875\n",
      "epoch 62, train_loss 1279.017333984375\n",
      "epoch 62, val_loss 456.0278625488281\n",
      "epoch 63, train_loss 1279.016357421875\n",
      "epoch 63, val_loss 456.0273742675781\n",
      "epoch 64, train_loss 1279.015380859375\n",
      "epoch 64, val_loss 456.0270080566406\n",
      "epoch 65, train_loss 1279.0142822265625\n",
      "epoch 65, val_loss 456.0265808105469\n",
      "epoch 66, train_loss 1279.0130615234375\n",
      "epoch 66, val_loss 456.026123046875\n",
      "epoch 67, train_loss 1279.0120849609375\n",
      "epoch 67, val_loss 456.0257873535156\n",
      "epoch 68, train_loss 1279.0111083984375\n",
      "epoch 68, val_loss 456.0253601074219\n",
      "epoch 69, train_loss 1279.010009765625\n",
      "epoch 69, val_loss 456.0248718261719\n",
      "epoch 70, train_loss 1279.0087890625\n",
      "epoch 70, val_loss 456.0245361328125\n",
      "epoch 71, train_loss 1279.0076904296875\n",
      "epoch 71, val_loss 456.0240783691406\n",
      "epoch 72, train_loss 1279.006591796875\n",
      "epoch 72, val_loss 456.023681640625\n",
      "epoch 73, train_loss 1279.005615234375\n",
      "epoch 73, val_loss 456.02325439453125\n",
      "epoch 74, train_loss 1279.0042724609375\n",
      "epoch 74, val_loss 456.0229187011719\n",
      "epoch 75, train_loss 1279.00341796875\n",
      "epoch 75, val_loss 456.0224609375\n",
      "epoch 76, train_loss 1279.002197265625\n",
      "epoch 76, val_loss 456.0220031738281\n",
      "epoch 77, train_loss 1279.001220703125\n",
      "epoch 77, val_loss 456.0216369628906\n",
      "epoch 78, train_loss 1279.0\n",
      "epoch 78, val_loss 456.021240234375\n",
      "epoch 79, train_loss 1278.9991455078125\n",
      "epoch 79, val_loss 456.0208435058594\n",
      "epoch 80, train_loss 1278.998291015625\n",
      "epoch 80, val_loss 456.0203857421875\n",
      "epoch 81, train_loss 1278.9969482421875\n",
      "epoch 81, val_loss 456.0199890136719\n",
      "epoch 82, train_loss 1278.99609375\n",
      "epoch 82, val_loss 456.01953125\n",
      "epoch 83, train_loss 1278.994873046875\n",
      "epoch 83, val_loss 456.0191345214844\n",
      "epoch 84, train_loss 1278.9940185546875\n",
      "epoch 84, val_loss 456.018798828125\n",
      "epoch 85, train_loss 1278.9927978515625\n",
      "epoch 85, val_loss 456.0183410644531\n",
      "epoch 86, train_loss 1278.99169921875\n",
      "epoch 86, val_loss 456.01800537109375\n",
      "epoch 87, train_loss 1278.9906005859375\n",
      "epoch 87, val_loss 456.0175476074219\n",
      "epoch 88, train_loss 1278.9896240234375\n",
      "epoch 88, val_loss 456.0171203613281\n",
      "epoch 89, train_loss 1278.988525390625\n",
      "epoch 89, val_loss 456.01678466796875\n",
      "epoch 90, train_loss 1278.987548828125\n",
      "epoch 90, val_loss 456.0163879394531\n",
      "epoch 91, train_loss 1278.986572265625\n",
      "epoch 91, val_loss 456.0159912109375\n",
      "epoch 92, train_loss 1278.9854736328125\n",
      "epoch 92, val_loss 456.0155334472656\n",
      "epoch 93, train_loss 1278.984375\n",
      "epoch 93, val_loss 456.0151672363281\n",
      "epoch 94, train_loss 1278.9835205078125\n",
      "epoch 94, val_loss 456.0147399902344\n",
      "epoch 95, train_loss 1278.9825439453125\n",
      "epoch 95, val_loss 456.014404296875\n",
      "epoch 96, train_loss 1278.9814453125\n",
      "epoch 96, val_loss 456.0140075683594\n",
      "epoch 97, train_loss 1278.9803466796875\n",
      "epoch 97, val_loss 456.01361083984375\n",
      "epoch 98, train_loss 1278.979248046875\n",
      "epoch 98, val_loss 456.0132141113281\n",
      "epoch 99, train_loss 1278.9783935546875\n",
      "epoch 99, val_loss 456.0128479003906\n",
      "Parameter containing:\n",
      "tensor([0.0006], requires_grad=True)\n",
      "iter 14, train_loss_regularization 1.1453603506088257\n",
      "iter 14, val_loss_regularization 1.1453603506088257\n",
      "epoch 0, train_loss 1278.977294921875\n",
      "epoch 0, val_loss 456.012451171875\n",
      "epoch 1, train_loss 1278.976318359375\n",
      "epoch 1, val_loss 456.0120544433594\n",
      "epoch 2, train_loss 1278.975341796875\n",
      "epoch 2, val_loss 456.0115966796875\n",
      "epoch 3, train_loss 1278.974365234375\n",
      "epoch 3, val_loss 456.0112609863281\n",
      "epoch 4, train_loss 1278.97314453125\n",
      "epoch 4, val_loss 456.0108337402344\n",
      "epoch 5, train_loss 1278.97216796875\n",
      "epoch 5, val_loss 456.0104064941406\n",
      "epoch 6, train_loss 1278.97119140625\n",
      "epoch 6, val_loss 456.01007080078125\n",
      "epoch 7, train_loss 1278.97021484375\n",
      "epoch 7, val_loss 456.0096130371094\n",
      "epoch 8, train_loss 1278.9691162109375\n",
      "epoch 8, val_loss 456.0092468261719\n",
      "epoch 9, train_loss 1278.9681396484375\n",
      "epoch 9, val_loss 456.0089111328125\n",
      "epoch 10, train_loss 1278.9671630859375\n",
      "epoch 10, val_loss 456.008544921875\n",
      "epoch 11, train_loss 1278.9661865234375\n",
      "epoch 11, val_loss 456.00811767578125\n",
      "epoch 12, train_loss 1278.9652099609375\n",
      "epoch 12, val_loss 456.0077819824219\n",
      "epoch 13, train_loss 1278.9642333984375\n",
      "epoch 13, val_loss 456.00732421875\n",
      "epoch 14, train_loss 1278.963134765625\n",
      "epoch 14, val_loss 456.0069885253906\n",
      "epoch 15, train_loss 1278.962158203125\n",
      "epoch 15, val_loss 456.006591796875\n",
      "epoch 16, train_loss 1278.9613037109375\n",
      "epoch 16, val_loss 456.00616455078125\n",
      "epoch 17, train_loss 1278.9603271484375\n",
      "epoch 17, val_loss 456.0058288574219\n",
      "epoch 18, train_loss 1278.9593505859375\n",
      "epoch 18, val_loss 456.0054016113281\n",
      "epoch 19, train_loss 1278.9583740234375\n",
      "epoch 19, val_loss 456.0050964355469\n",
      "epoch 20, train_loss 1278.9573974609375\n",
      "epoch 20, val_loss 456.0046691894531\n",
      "epoch 21, train_loss 1278.9564208984375\n",
      "epoch 21, val_loss 456.0043029785156\n",
      "epoch 22, train_loss 1278.9552001953125\n",
      "epoch 22, val_loss 456.00390625\n",
      "epoch 23, train_loss 1278.9542236328125\n",
      "epoch 23, val_loss 456.0035095214844\n",
      "epoch 24, train_loss 1278.953369140625\n",
      "epoch 24, val_loss 456.003173828125\n",
      "epoch 25, train_loss 1278.952392578125\n",
      "epoch 25, val_loss 456.0027770996094\n",
      "epoch 26, train_loss 1278.951416015625\n",
      "epoch 26, val_loss 456.00238037109375\n",
      "epoch 27, train_loss 1278.9503173828125\n",
      "epoch 27, val_loss 456.0020751953125\n",
      "epoch 28, train_loss 1278.949462890625\n",
      "epoch 28, val_loss 456.0016784667969\n",
      "epoch 29, train_loss 1278.948486328125\n",
      "epoch 29, val_loss 456.001220703125\n",
      "epoch 30, train_loss 1278.9476318359375\n",
      "epoch 30, val_loss 456.0008239746094\n",
      "epoch 31, train_loss 1278.946533203125\n",
      "epoch 31, val_loss 456.00048828125\n",
      "epoch 32, train_loss 1278.9456787109375\n",
      "epoch 32, val_loss 456.0001525878906\n",
      "epoch 33, train_loss 1278.9447021484375\n",
      "epoch 33, val_loss 455.999755859375\n",
      "epoch 34, train_loss 1278.94384765625\n",
      "epoch 34, val_loss 455.9994201660156\n",
      "epoch 35, train_loss 1278.94287109375\n",
      "epoch 35, val_loss 455.9990539550781\n",
      "epoch 36, train_loss 1278.9417724609375\n",
      "epoch 36, val_loss 455.9986267089844\n",
      "epoch 37, train_loss 1278.9407958984375\n",
      "epoch 37, val_loss 455.998291015625\n",
      "epoch 38, train_loss 1278.93994140625\n",
      "epoch 38, val_loss 455.9979248046875\n",
      "epoch 39, train_loss 1278.93896484375\n",
      "epoch 39, val_loss 455.99749755859375\n",
      "epoch 40, train_loss 1278.9378662109375\n",
      "epoch 40, val_loss 455.9971618652344\n",
      "epoch 41, train_loss 1278.9371337890625\n",
      "epoch 41, val_loss 455.996826171875\n",
      "epoch 42, train_loss 1278.9361572265625\n",
      "epoch 42, val_loss 455.9964294433594\n",
      "epoch 43, train_loss 1278.9351806640625\n",
      "epoch 43, val_loss 455.99609375\n",
      "epoch 44, train_loss 1278.9342041015625\n",
      "epoch 44, val_loss 455.99566650390625\n",
      "epoch 45, train_loss 1278.9332275390625\n",
      "epoch 45, val_loss 455.99530029296875\n",
      "epoch 46, train_loss 1278.932373046875\n",
      "epoch 46, val_loss 455.9949645996094\n",
      "epoch 47, train_loss 1278.9315185546875\n",
      "epoch 47, val_loss 455.9945983886719\n",
      "epoch 48, train_loss 1278.9305419921875\n",
      "epoch 48, val_loss 455.9941711425781\n",
      "epoch 49, train_loss 1278.9295654296875\n",
      "epoch 49, val_loss 455.99383544921875\n",
      "epoch 50, train_loss 1278.9287109375\n",
      "epoch 50, val_loss 455.99346923828125\n",
      "epoch 51, train_loss 1278.9276123046875\n",
      "epoch 51, val_loss 455.9931640625\n",
      "epoch 52, train_loss 1278.9267578125\n",
      "epoch 52, val_loss 455.9927978515625\n",
      "epoch 53, train_loss 1278.92578125\n",
      "epoch 53, val_loss 455.99237060546875\n",
      "epoch 54, train_loss 1278.9249267578125\n",
      "epoch 54, val_loss 455.9920349121094\n",
      "epoch 55, train_loss 1278.924072265625\n",
      "epoch 55, val_loss 455.9916687011719\n",
      "epoch 56, train_loss 1278.923095703125\n",
      "epoch 56, val_loss 455.9913330078125\n",
      "epoch 57, train_loss 1278.922119140625\n",
      "epoch 57, val_loss 455.99090576171875\n",
      "epoch 58, train_loss 1278.9212646484375\n",
      "epoch 58, val_loss 455.9905700683594\n",
      "epoch 59, train_loss 1278.9202880859375\n",
      "epoch 59, val_loss 455.9902648925781\n",
      "epoch 60, train_loss 1278.9195556640625\n",
      "epoch 60, val_loss 455.9898681640625\n",
      "epoch 61, train_loss 1278.9185791015625\n",
      "epoch 61, val_loss 455.9895324707031\n",
      "epoch 62, train_loss 1278.917724609375\n",
      "epoch 62, val_loss 455.9891357421875\n",
      "epoch 63, train_loss 1278.916748046875\n",
      "epoch 63, val_loss 455.9888000488281\n",
      "epoch 64, train_loss 1278.916015625\n",
      "epoch 64, val_loss 455.9884033203125\n",
      "epoch 65, train_loss 1278.914794921875\n",
      "epoch 65, val_loss 455.9881286621094\n",
      "epoch 66, train_loss 1278.9139404296875\n",
      "epoch 66, val_loss 455.9877624511719\n",
      "epoch 67, train_loss 1278.9129638671875\n",
      "epoch 67, val_loss 455.98736572265625\n",
      "epoch 68, train_loss 1278.912109375\n",
      "epoch 68, val_loss 455.9869689941406\n",
      "epoch 69, train_loss 1278.911376953125\n",
      "epoch 69, val_loss 455.9866638183594\n",
      "epoch 70, train_loss 1278.910400390625\n",
      "epoch 70, val_loss 455.9862976074219\n",
      "epoch 71, train_loss 1278.9093017578125\n",
      "epoch 71, val_loss 455.9859313964844\n",
      "epoch 72, train_loss 1278.908447265625\n",
      "epoch 72, val_loss 455.985595703125\n",
      "epoch 73, train_loss 1278.90771484375\n",
      "epoch 73, val_loss 455.9851989746094\n",
      "epoch 74, train_loss 1278.9066162109375\n",
      "epoch 74, val_loss 455.98492431640625\n",
      "epoch 75, train_loss 1278.9058837890625\n",
      "epoch 75, val_loss 455.9845886230469\n",
      "epoch 76, train_loss 1278.905029296875\n",
      "epoch 76, val_loss 455.9841613769531\n",
      "epoch 77, train_loss 1278.9041748046875\n",
      "epoch 77, val_loss 455.9837951660156\n",
      "epoch 78, train_loss 1278.9031982421875\n",
      "epoch 78, val_loss 455.9835510253906\n",
      "epoch 79, train_loss 1278.9022216796875\n",
      "epoch 79, val_loss 455.983154296875\n",
      "epoch 80, train_loss 1278.9013671875\n",
      "epoch 80, val_loss 455.9827880859375\n",
      "epoch 81, train_loss 1278.900634765625\n",
      "epoch 81, val_loss 455.982421875\n",
      "epoch 82, train_loss 1278.8997802734375\n",
      "epoch 82, val_loss 455.98211669921875\n",
      "epoch 83, train_loss 1278.8988037109375\n",
      "epoch 83, val_loss 455.98175048828125\n",
      "epoch 84, train_loss 1278.89794921875\n",
      "epoch 84, val_loss 455.9813232421875\n",
      "epoch 85, train_loss 1278.8970947265625\n",
      "epoch 85, val_loss 455.9810791015625\n",
      "epoch 86, train_loss 1278.8963623046875\n",
      "epoch 86, val_loss 455.980712890625\n",
      "epoch 87, train_loss 1278.8955078125\n",
      "epoch 87, val_loss 455.9803466796875\n",
      "epoch 88, train_loss 1278.89453125\n",
      "epoch 88, val_loss 455.9800109863281\n",
      "epoch 89, train_loss 1278.8935546875\n",
      "epoch 89, val_loss 455.9797058105469\n",
      "epoch 90, train_loss 1278.892822265625\n",
      "epoch 90, val_loss 455.9792785644531\n",
      "epoch 91, train_loss 1278.8919677734375\n",
      "epoch 91, val_loss 455.9789733886719\n",
      "epoch 92, train_loss 1278.8909912109375\n",
      "epoch 92, val_loss 455.9786682128906\n",
      "epoch 93, train_loss 1278.89013671875\n",
      "epoch 93, val_loss 455.97833251953125\n",
      "epoch 94, train_loss 1278.8892822265625\n",
      "epoch 94, val_loss 455.9779052734375\n",
      "epoch 95, train_loss 1278.8883056640625\n",
      "epoch 95, val_loss 455.9776306152344\n",
      "epoch 96, train_loss 1278.8875732421875\n",
      "epoch 96, val_loss 455.9773254394531\n",
      "epoch 97, train_loss 1278.88671875\n",
      "epoch 97, val_loss 455.9769592285156\n",
      "epoch 98, train_loss 1278.8858642578125\n",
      "epoch 98, val_loss 455.97662353515625\n",
      "epoch 99, train_loss 1278.885009765625\n",
      "epoch 99, val_loss 455.9762878417969\n",
      "Parameter containing:\n",
      "tensor([0.0004], requires_grad=True)\n",
      "iter 15, train_loss_regularization 1.1353983879089355\n",
      "iter 15, val_loss_regularization 1.1353983879089355\n",
      "epoch 0, train_loss 1278.8841552734375\n",
      "epoch 0, val_loss 455.9759521484375\n",
      "epoch 1, train_loss 1278.88330078125\n",
      "epoch 1, val_loss 455.9755859375\n",
      "epoch 2, train_loss 1278.8824462890625\n",
      "epoch 2, val_loss 455.9752502441406\n",
      "epoch 3, train_loss 1278.8817138671875\n",
      "epoch 3, val_loss 455.97491455078125\n",
      "epoch 4, train_loss 1278.8807373046875\n",
      "epoch 4, val_loss 455.9745788574219\n",
      "epoch 5, train_loss 1278.8798828125\n",
      "epoch 5, val_loss 455.9742126464844\n",
      "epoch 6, train_loss 1278.8790283203125\n",
      "epoch 6, val_loss 455.9739074707031\n",
      "epoch 7, train_loss 1278.878173828125\n",
      "epoch 7, val_loss 455.97357177734375\n",
      "epoch 8, train_loss 1278.87744140625\n",
      "epoch 8, val_loss 455.97320556640625\n",
      "epoch 9, train_loss 1278.8765869140625\n",
      "epoch 9, val_loss 455.9729309082031\n",
      "epoch 10, train_loss 1278.8756103515625\n",
      "epoch 10, val_loss 455.9725341796875\n",
      "epoch 11, train_loss 1278.8748779296875\n",
      "epoch 11, val_loss 455.9722595214844\n",
      "epoch 12, train_loss 1278.8740234375\n",
      "epoch 12, val_loss 455.97186279296875\n",
      "epoch 13, train_loss 1278.8731689453125\n",
      "epoch 13, val_loss 455.9715270996094\n",
      "epoch 14, train_loss 1278.872314453125\n",
      "epoch 14, val_loss 455.9712219238281\n",
      "epoch 15, train_loss 1278.8714599609375\n",
      "epoch 15, val_loss 455.97088623046875\n",
      "epoch 16, train_loss 1278.8707275390625\n",
      "epoch 16, val_loss 455.9705505371094\n",
      "epoch 17, train_loss 1278.8699951171875\n",
      "epoch 17, val_loss 455.9702453613281\n",
      "epoch 18, train_loss 1278.869140625\n",
      "epoch 18, val_loss 455.9698791503906\n",
      "epoch 19, train_loss 1278.8681640625\n",
      "epoch 19, val_loss 455.9695739746094\n",
      "epoch 20, train_loss 1278.8673095703125\n",
      "epoch 20, val_loss 455.96923828125\n",
      "epoch 21, train_loss 1278.86669921875\n",
      "epoch 21, val_loss 455.9689025878906\n",
      "epoch 22, train_loss 1278.8658447265625\n",
      "epoch 22, val_loss 455.9685974121094\n",
      "epoch 23, train_loss 1278.864990234375\n",
      "epoch 23, val_loss 455.96826171875\n",
      "epoch 24, train_loss 1278.8643798828125\n",
      "epoch 24, val_loss 455.96795654296875\n",
      "epoch 25, train_loss 1278.86328125\n",
      "epoch 25, val_loss 455.9676208496094\n",
      "epoch 26, train_loss 1278.862548828125\n",
      "epoch 26, val_loss 455.96728515625\n",
      "epoch 27, train_loss 1278.8616943359375\n",
      "epoch 27, val_loss 455.9669189453125\n",
      "epoch 28, train_loss 1278.8609619140625\n",
      "epoch 28, val_loss 455.9665832519531\n",
      "epoch 29, train_loss 1278.860107421875\n",
      "epoch 29, val_loss 455.9663391113281\n",
      "epoch 30, train_loss 1278.859375\n",
      "epoch 30, val_loss 455.9659729003906\n",
      "epoch 31, train_loss 1278.8583984375\n",
      "epoch 31, val_loss 455.965576171875\n",
      "epoch 32, train_loss 1278.8575439453125\n",
      "epoch 32, val_loss 455.96533203125\n",
      "epoch 33, train_loss 1278.8568115234375\n",
      "epoch 33, val_loss 455.9649963378906\n",
      "epoch 34, train_loss 1278.8560791015625\n",
      "epoch 34, val_loss 455.96466064453125\n",
      "epoch 35, train_loss 1278.8553466796875\n",
      "epoch 35, val_loss 455.9643249511719\n",
      "epoch 36, train_loss 1278.8543701171875\n",
      "epoch 36, val_loss 455.9639892578125\n",
      "epoch 37, train_loss 1278.8536376953125\n",
      "epoch 37, val_loss 455.9637145996094\n",
      "epoch 38, train_loss 1278.8529052734375\n",
      "epoch 38, val_loss 455.96337890625\n",
      "epoch 39, train_loss 1278.85205078125\n",
      "epoch 39, val_loss 455.96307373046875\n",
      "epoch 40, train_loss 1278.851318359375\n",
      "epoch 40, val_loss 455.9627380371094\n",
      "epoch 41, train_loss 1278.8505859375\n",
      "epoch 41, val_loss 455.96240234375\n",
      "epoch 42, train_loss 1278.849609375\n",
      "epoch 42, val_loss 455.9621276855469\n",
      "epoch 43, train_loss 1278.8487548828125\n",
      "epoch 43, val_loss 455.9617614746094\n",
      "epoch 44, train_loss 1278.8480224609375\n",
      "epoch 44, val_loss 455.9614562988281\n",
      "epoch 45, train_loss 1278.8472900390625\n",
      "epoch 45, val_loss 455.96112060546875\n",
      "epoch 46, train_loss 1278.8465576171875\n",
      "epoch 46, val_loss 455.9608459472656\n",
      "epoch 47, train_loss 1278.8455810546875\n",
      "epoch 47, val_loss 455.96051025390625\n",
      "epoch 48, train_loss 1278.844970703125\n",
      "epoch 48, val_loss 455.9601745605469\n",
      "epoch 49, train_loss 1278.8441162109375\n",
      "epoch 49, val_loss 455.9598388671875\n",
      "epoch 50, train_loss 1278.8433837890625\n",
      "epoch 50, val_loss 455.9595947265625\n",
      "epoch 51, train_loss 1278.842529296875\n",
      "epoch 51, val_loss 455.9592590332031\n",
      "epoch 52, train_loss 1278.841796875\n",
      "epoch 52, val_loss 455.95892333984375\n",
      "epoch 53, train_loss 1278.841064453125\n",
      "epoch 53, val_loss 455.9586181640625\n",
      "epoch 54, train_loss 1278.84033203125\n",
      "epoch 54, val_loss 455.9583435058594\n",
      "epoch 55, train_loss 1278.8394775390625\n",
      "epoch 55, val_loss 455.95794677734375\n",
      "epoch 56, train_loss 1278.838623046875\n",
      "epoch 56, val_loss 455.9576721191406\n",
      "epoch 57, train_loss 1278.837890625\n",
      "epoch 57, val_loss 455.9573669433594\n",
      "epoch 58, train_loss 1278.8372802734375\n",
      "epoch 58, val_loss 455.95703125\n",
      "epoch 59, train_loss 1278.83642578125\n",
      "epoch 59, val_loss 455.9567565917969\n",
      "epoch 60, train_loss 1278.835693359375\n",
      "epoch 60, val_loss 455.9564208984375\n",
      "epoch 61, train_loss 1278.8348388671875\n",
      "epoch 61, val_loss 455.95611572265625\n",
      "epoch 62, train_loss 1278.8341064453125\n",
      "epoch 62, val_loss 455.9557800292969\n",
      "epoch 63, train_loss 1278.833251953125\n",
      "epoch 63, val_loss 455.95550537109375\n",
      "epoch 64, train_loss 1278.83251953125\n",
      "epoch 64, val_loss 455.9552001953125\n",
      "epoch 65, train_loss 1278.831787109375\n",
      "epoch 65, val_loss 455.954833984375\n",
      "epoch 66, train_loss 1278.8311767578125\n",
      "epoch 66, val_loss 455.95458984375\n",
      "epoch 67, train_loss 1278.830322265625\n",
      "epoch 67, val_loss 455.9541931152344\n",
      "epoch 68, train_loss 1278.8294677734375\n",
      "epoch 68, val_loss 455.9539489746094\n",
      "epoch 69, train_loss 1278.828857421875\n",
      "epoch 69, val_loss 455.9535827636719\n",
      "epoch 70, train_loss 1278.8280029296875\n",
      "epoch 70, val_loss 455.9532775878906\n",
      "epoch 71, train_loss 1278.8272705078125\n",
      "epoch 71, val_loss 455.9530029296875\n",
      "epoch 72, train_loss 1278.826416015625\n",
      "epoch 72, val_loss 455.9526672363281\n",
      "epoch 73, train_loss 1278.8258056640625\n",
      "epoch 73, val_loss 455.9523620605469\n",
      "epoch 74, train_loss 1278.824951171875\n",
      "epoch 74, val_loss 455.95208740234375\n",
      "epoch 75, train_loss 1278.8243408203125\n",
      "epoch 75, val_loss 455.9517517089844\n",
      "epoch 76, train_loss 1278.8233642578125\n",
      "epoch 76, val_loss 455.9514465332031\n",
      "epoch 77, train_loss 1278.8226318359375\n",
      "epoch 77, val_loss 455.9512634277344\n",
      "epoch 78, train_loss 1278.822021484375\n",
      "epoch 78, val_loss 455.9508361816406\n",
      "epoch 79, train_loss 1278.821044921875\n",
      "epoch 79, val_loss 455.9505310058594\n",
      "epoch 80, train_loss 1278.820556640625\n",
      "epoch 80, val_loss 455.9502868652344\n",
      "epoch 81, train_loss 1278.8197021484375\n",
      "epoch 81, val_loss 455.949951171875\n",
      "epoch 82, train_loss 1278.8189697265625\n",
      "epoch 82, val_loss 455.9496154785156\n",
      "epoch 83, train_loss 1278.818359375\n",
      "epoch 83, val_loss 455.9493408203125\n",
      "epoch 84, train_loss 1278.817626953125\n",
      "epoch 84, val_loss 455.9490966796875\n",
      "epoch 85, train_loss 1278.8167724609375\n",
      "epoch 85, val_loss 455.9487609863281\n",
      "epoch 86, train_loss 1278.8160400390625\n",
      "epoch 86, val_loss 455.94842529296875\n",
      "epoch 87, train_loss 1278.8153076171875\n",
      "epoch 87, val_loss 455.9481201171875\n",
      "epoch 88, train_loss 1278.814697265625\n",
      "epoch 88, val_loss 455.9478454589844\n",
      "epoch 89, train_loss 1278.813720703125\n",
      "epoch 89, val_loss 455.947509765625\n",
      "epoch 90, train_loss 1278.8131103515625\n",
      "epoch 90, val_loss 455.94720458984375\n",
      "epoch 91, train_loss 1278.812255859375\n",
      "epoch 91, val_loss 455.9469299316406\n",
      "epoch 92, train_loss 1278.8115234375\n",
      "epoch 92, val_loss 455.9466247558594\n",
      "epoch 93, train_loss 1278.81103515625\n",
      "epoch 93, val_loss 455.9463195800781\n",
      "epoch 94, train_loss 1278.8101806640625\n",
      "epoch 94, val_loss 455.9460144042969\n",
      "epoch 95, train_loss 1278.8096923828125\n",
      "epoch 95, val_loss 455.94573974609375\n",
      "epoch 96, train_loss 1278.80859375\n",
      "epoch 96, val_loss 455.9454650878906\n",
      "epoch 97, train_loss 1278.8079833984375\n",
      "epoch 97, val_loss 455.945068359375\n",
      "epoch 98, train_loss 1278.807373046875\n",
      "epoch 98, val_loss 455.94482421875\n",
      "epoch 99, train_loss 1278.806640625\n",
      "epoch 99, val_loss 455.9444885253906\n",
      "Parameter containing:\n",
      "tensor([0.0002], requires_grad=True)\n",
      "iter 16, train_loss_regularization 1.1258790493011475\n",
      "iter 16, val_loss_regularization 1.1258790493011475\n",
      "epoch 0, train_loss 1278.8057861328125\n",
      "epoch 0, val_loss 455.9442443847656\n",
      "epoch 1, train_loss 1278.80517578125\n",
      "epoch 1, val_loss 455.9440002441406\n",
      "epoch 2, train_loss 1278.8043212890625\n",
      "epoch 2, val_loss 455.94366455078125\n",
      "epoch 3, train_loss 1278.8037109375\n",
      "epoch 3, val_loss 455.9433288574219\n",
      "epoch 4, train_loss 1278.8028564453125\n",
      "epoch 4, val_loss 455.94305419921875\n",
      "epoch 5, train_loss 1278.8021240234375\n",
      "epoch 5, val_loss 455.9427185058594\n",
      "epoch 6, train_loss 1278.8016357421875\n",
      "epoch 6, val_loss 455.9425048828125\n",
      "epoch 7, train_loss 1278.80078125\n",
      "epoch 7, val_loss 455.9421691894531\n",
      "epoch 8, train_loss 1278.800048828125\n",
      "epoch 8, val_loss 455.9418640136719\n",
      "epoch 9, train_loss 1278.7994384765625\n",
      "epoch 9, val_loss 455.94158935546875\n",
      "epoch 10, train_loss 1278.7987060546875\n",
      "epoch 10, val_loss 455.9412841796875\n",
      "epoch 11, train_loss 1278.798095703125\n",
      "epoch 11, val_loss 455.9410095214844\n",
      "epoch 12, train_loss 1278.79736328125\n",
      "epoch 12, val_loss 455.940673828125\n",
      "epoch 13, train_loss 1278.79638671875\n",
      "epoch 13, val_loss 455.9404296875\n",
      "epoch 14, train_loss 1278.7958984375\n",
      "epoch 14, val_loss 455.94012451171875\n",
      "epoch 15, train_loss 1278.795166015625\n",
      "epoch 15, val_loss 455.9397888183594\n",
      "epoch 16, train_loss 1278.7945556640625\n",
      "epoch 16, val_loss 455.9395751953125\n",
      "epoch 17, train_loss 1278.7939453125\n",
      "epoch 17, val_loss 455.9392395019531\n",
      "epoch 18, train_loss 1278.7930908203125\n",
      "epoch 18, val_loss 455.93896484375\n",
      "epoch 19, train_loss 1278.7923583984375\n",
      "epoch 19, val_loss 455.93865966796875\n",
      "epoch 20, train_loss 1278.7918701171875\n",
      "epoch 20, val_loss 455.9383850097656\n",
      "epoch 21, train_loss 1278.791015625\n",
      "epoch 21, val_loss 455.9380798339844\n",
      "epoch 22, train_loss 1278.790283203125\n",
      "epoch 22, val_loss 455.9378356933594\n",
      "epoch 23, train_loss 1278.78955078125\n",
      "epoch 23, val_loss 455.9374694824219\n",
      "epoch 24, train_loss 1278.7889404296875\n",
      "epoch 24, val_loss 455.93719482421875\n",
      "epoch 25, train_loss 1278.7882080078125\n",
      "epoch 25, val_loss 455.9369201660156\n",
      "epoch 26, train_loss 1278.7874755859375\n",
      "epoch 26, val_loss 455.9366149902344\n",
      "epoch 27, train_loss 1278.786865234375\n",
      "epoch 27, val_loss 455.9363708496094\n",
      "epoch 28, train_loss 1278.7861328125\n",
      "epoch 28, val_loss 455.93609619140625\n",
      "epoch 29, train_loss 1278.7855224609375\n",
      "epoch 29, val_loss 455.935791015625\n",
      "epoch 30, train_loss 1278.7847900390625\n",
      "epoch 30, val_loss 455.935546875\n",
      "epoch 31, train_loss 1278.7840576171875\n",
      "epoch 31, val_loss 455.93524169921875\n",
      "epoch 32, train_loss 1278.7833251953125\n",
      "epoch 32, val_loss 455.9348449707031\n",
      "epoch 33, train_loss 1278.78271484375\n",
      "epoch 33, val_loss 455.93463134765625\n",
      "epoch 34, train_loss 1278.781982421875\n",
      "epoch 34, val_loss 455.9342956542969\n",
      "epoch 35, train_loss 1278.7813720703125\n",
      "epoch 35, val_loss 455.9340515136719\n",
      "epoch 36, train_loss 1278.7806396484375\n",
      "epoch 36, val_loss 455.93377685546875\n",
      "epoch 37, train_loss 1278.780029296875\n",
      "epoch 37, val_loss 455.9334716796875\n",
      "epoch 38, train_loss 1278.779296875\n",
      "epoch 38, val_loss 455.9331970214844\n",
      "epoch 39, train_loss 1278.7784423828125\n",
      "epoch 39, val_loss 455.9329528808594\n",
      "epoch 40, train_loss 1278.7779541015625\n",
      "epoch 40, val_loss 455.9326171875\n",
      "epoch 41, train_loss 1278.7772216796875\n",
      "epoch 41, val_loss 455.932373046875\n",
      "epoch 42, train_loss 1278.776611328125\n",
      "epoch 42, val_loss 455.9320983886719\n",
      "epoch 43, train_loss 1278.77587890625\n",
      "epoch 43, val_loss 455.9317932128906\n",
      "epoch 44, train_loss 1278.7752685546875\n",
      "epoch 44, val_loss 455.9314880371094\n",
      "epoch 45, train_loss 1278.774658203125\n",
      "epoch 45, val_loss 455.9313049316406\n",
      "epoch 46, train_loss 1278.7740478515625\n",
      "epoch 46, val_loss 455.93096923828125\n",
      "epoch 47, train_loss 1278.7733154296875\n",
      "epoch 47, val_loss 455.9306945800781\n",
      "epoch 48, train_loss 1278.7724609375\n",
      "epoch 48, val_loss 455.9304504394531\n",
      "epoch 49, train_loss 1278.77197265625\n",
      "epoch 49, val_loss 455.9300842285156\n",
      "epoch 50, train_loss 1278.7713623046875\n",
      "epoch 50, val_loss 455.929931640625\n",
      "epoch 51, train_loss 1278.7706298828125\n",
      "epoch 51, val_loss 455.9295349121094\n",
      "epoch 52, train_loss 1278.769775390625\n",
      "epoch 52, val_loss 455.92926025390625\n",
      "epoch 53, train_loss 1278.769287109375\n",
      "epoch 53, val_loss 455.9289855957031\n",
      "epoch 54, train_loss 1278.7685546875\n",
      "epoch 54, val_loss 455.9287414550781\n",
      "epoch 55, train_loss 1278.767822265625\n",
      "epoch 55, val_loss 455.928466796875\n",
      "epoch 56, train_loss 1278.7672119140625\n",
      "epoch 56, val_loss 455.92816162109375\n",
      "epoch 57, train_loss 1278.7667236328125\n",
      "epoch 57, val_loss 455.92791748046875\n",
      "epoch 58, train_loss 1278.7659912109375\n",
      "epoch 58, val_loss 455.9276123046875\n",
      "epoch 59, train_loss 1278.7652587890625\n",
      "epoch 59, val_loss 455.9273681640625\n",
      "epoch 60, train_loss 1278.7646484375\n",
      "epoch 60, val_loss 455.9270324707031\n",
      "epoch 61, train_loss 1278.7640380859375\n",
      "epoch 61, val_loss 455.92681884765625\n",
      "epoch 62, train_loss 1278.763427734375\n",
      "epoch 62, val_loss 455.92645263671875\n",
      "epoch 63, train_loss 1278.7625732421875\n",
      "epoch 63, val_loss 455.9262390136719\n",
      "epoch 64, train_loss 1278.761962890625\n",
      "epoch 64, val_loss 455.92596435546875\n",
      "epoch 65, train_loss 1278.76123046875\n",
      "epoch 65, val_loss 455.92572021484375\n",
      "epoch 66, train_loss 1278.7608642578125\n",
      "epoch 66, val_loss 455.9253845214844\n",
      "epoch 67, train_loss 1278.7601318359375\n",
      "epoch 67, val_loss 455.92510986328125\n",
      "epoch 68, train_loss 1278.75927734375\n",
      "epoch 68, val_loss 455.9248352050781\n",
      "epoch 69, train_loss 1278.7587890625\n",
      "epoch 69, val_loss 455.92462158203125\n",
      "epoch 70, train_loss 1278.7581787109375\n",
      "epoch 70, val_loss 455.9242858886719\n",
      "epoch 71, train_loss 1278.7574462890625\n",
      "epoch 71, val_loss 455.92401123046875\n",
      "epoch 72, train_loss 1278.7568359375\n",
      "epoch 72, val_loss 455.9237365722656\n",
      "epoch 73, train_loss 1278.7559814453125\n",
      "epoch 73, val_loss 455.9234924316406\n",
      "epoch 74, train_loss 1278.7554931640625\n",
      "epoch 74, val_loss 455.9232177734375\n",
      "epoch 75, train_loss 1278.7548828125\n",
      "epoch 75, val_loss 455.9230041503906\n",
      "epoch 76, train_loss 1278.7542724609375\n",
      "epoch 76, val_loss 455.9226989746094\n",
      "epoch 77, train_loss 1278.753662109375\n",
      "epoch 77, val_loss 455.92242431640625\n",
      "epoch 78, train_loss 1278.7529296875\n",
      "epoch 78, val_loss 455.922119140625\n",
      "epoch 79, train_loss 1278.7523193359375\n",
      "epoch 79, val_loss 455.921875\n",
      "epoch 80, train_loss 1278.751708984375\n",
      "epoch 80, val_loss 455.9215393066406\n",
      "epoch 81, train_loss 1278.7509765625\n",
      "epoch 81, val_loss 455.92132568359375\n",
      "epoch 82, train_loss 1278.7503662109375\n",
      "epoch 82, val_loss 455.9211120605469\n",
      "epoch 83, train_loss 1278.7498779296875\n",
      "epoch 83, val_loss 455.9208068847656\n",
      "epoch 84, train_loss 1278.7490234375\n",
      "epoch 84, val_loss 455.92047119140625\n",
      "epoch 85, train_loss 1278.7484130859375\n",
      "epoch 85, val_loss 455.9202575683594\n",
      "epoch 86, train_loss 1278.748046875\n",
      "epoch 86, val_loss 455.9200439453125\n",
      "epoch 87, train_loss 1278.747314453125\n",
      "epoch 87, val_loss 455.919677734375\n",
      "epoch 88, train_loss 1278.7467041015625\n",
      "epoch 88, val_loss 455.9194641113281\n",
      "epoch 89, train_loss 1278.7459716796875\n",
      "epoch 89, val_loss 455.91912841796875\n",
      "epoch 90, train_loss 1278.745361328125\n",
      "epoch 90, val_loss 455.9189147949219\n",
      "epoch 91, train_loss 1278.7447509765625\n",
      "epoch 91, val_loss 455.9186706542969\n",
      "epoch 92, train_loss 1278.744140625\n",
      "epoch 92, val_loss 455.9183654785156\n",
      "epoch 93, train_loss 1278.7435302734375\n",
      "epoch 93, val_loss 455.9180908203125\n",
      "epoch 94, train_loss 1278.7427978515625\n",
      "epoch 94, val_loss 455.9178771972656\n",
      "epoch 95, train_loss 1278.7423095703125\n",
      "epoch 95, val_loss 455.9175720214844\n",
      "epoch 96, train_loss 1278.74169921875\n",
      "epoch 96, val_loss 455.9173278808594\n",
      "epoch 97, train_loss 1278.7410888671875\n",
      "epoch 97, val_loss 455.91705322265625\n",
      "epoch 98, train_loss 1278.7403564453125\n",
      "epoch 98, val_loss 455.9168395996094\n",
      "epoch 99, train_loss 1278.7398681640625\n",
      "epoch 99, val_loss 455.9165344238281\n",
      "Parameter containing:\n",
      "tensor([0.0001], requires_grad=True)\n",
      "iter 17, train_loss_regularization 1.1167391538619995\n",
      "iter 17, val_loss_regularization 1.1167391538619995\n",
      "epoch 0, train_loss 1278.7391357421875\n",
      "epoch 0, val_loss 455.91619873046875\n",
      "epoch 1, train_loss 1278.738525390625\n",
      "epoch 1, val_loss 455.9159851074219\n",
      "epoch 2, train_loss 1278.738037109375\n",
      "epoch 2, val_loss 455.9157409667969\n",
      "epoch 3, train_loss 1278.7374267578125\n",
      "epoch 3, val_loss 455.91546630859375\n",
      "epoch 4, train_loss 1278.7366943359375\n",
      "epoch 4, val_loss 455.91522216796875\n",
      "epoch 5, train_loss 1278.7362060546875\n",
      "epoch 5, val_loss 455.9149475097656\n",
      "epoch 6, train_loss 1278.7354736328125\n",
      "epoch 6, val_loss 455.9147033691406\n",
      "epoch 7, train_loss 1278.73486328125\n",
      "epoch 7, val_loss 455.9144287109375\n",
      "epoch 8, train_loss 1278.734130859375\n",
      "epoch 8, val_loss 455.91412353515625\n",
      "epoch 9, train_loss 1278.733642578125\n",
      "epoch 9, val_loss 455.9139099121094\n",
      "epoch 10, train_loss 1278.7330322265625\n",
      "epoch 10, val_loss 455.9136657714844\n",
      "epoch 11, train_loss 1278.732421875\n",
      "epoch 11, val_loss 455.913330078125\n",
      "epoch 12, train_loss 1278.7318115234375\n",
      "epoch 12, val_loss 455.9131774902344\n",
      "epoch 13, train_loss 1278.731201171875\n",
      "epoch 13, val_loss 455.9128723144531\n",
      "epoch 14, train_loss 1278.7305908203125\n",
      "epoch 14, val_loss 455.91259765625\n",
      "epoch 15, train_loss 1278.730224609375\n",
      "epoch 15, val_loss 455.91229248046875\n",
      "epoch 16, train_loss 1278.7293701171875\n",
      "epoch 16, val_loss 455.9120788574219\n",
      "epoch 17, train_loss 1278.728759765625\n",
      "epoch 17, val_loss 455.9118347167969\n",
      "epoch 18, train_loss 1278.728271484375\n",
      "epoch 18, val_loss 455.9115295410156\n",
      "epoch 19, train_loss 1278.7275390625\n",
      "epoch 19, val_loss 455.9112548828125\n",
      "epoch 20, train_loss 1278.72705078125\n",
      "epoch 20, val_loss 455.91107177734375\n",
      "epoch 21, train_loss 1278.7264404296875\n",
      "epoch 21, val_loss 455.91082763671875\n",
      "epoch 22, train_loss 1278.7257080078125\n",
      "epoch 22, val_loss 455.9105529785156\n",
      "epoch 23, train_loss 1278.725341796875\n",
      "epoch 23, val_loss 455.9102478027344\n",
      "epoch 24, train_loss 1278.724609375\n",
      "epoch 24, val_loss 455.9100036621094\n",
      "epoch 25, train_loss 1278.7239990234375\n",
      "epoch 25, val_loss 455.9097595214844\n",
      "epoch 26, train_loss 1278.7235107421875\n",
      "epoch 26, val_loss 455.9094543457031\n",
      "epoch 27, train_loss 1278.72265625\n",
      "epoch 27, val_loss 455.9092102050781\n",
      "epoch 28, train_loss 1278.7222900390625\n",
      "epoch 28, val_loss 455.90899658203125\n",
      "epoch 29, train_loss 1278.7218017578125\n",
      "epoch 29, val_loss 455.9087219238281\n",
      "epoch 30, train_loss 1278.7210693359375\n",
      "epoch 30, val_loss 455.908447265625\n",
      "epoch 31, train_loss 1278.7203369140625\n",
      "epoch 31, val_loss 455.908203125\n",
      "epoch 32, train_loss 1278.719970703125\n",
      "epoch 32, val_loss 455.9079895019531\n",
      "epoch 33, train_loss 1278.71923828125\n",
      "epoch 33, val_loss 455.90771484375\n",
      "epoch 34, train_loss 1278.7186279296875\n",
      "epoch 34, val_loss 455.90740966796875\n",
      "epoch 35, train_loss 1278.7181396484375\n",
      "epoch 35, val_loss 455.90716552734375\n",
      "epoch 36, train_loss 1278.717529296875\n",
      "epoch 36, val_loss 455.9069519042969\n",
      "epoch 37, train_loss 1278.717041015625\n",
      "epoch 37, val_loss 455.90667724609375\n",
      "epoch 38, train_loss 1278.71630859375\n",
      "epoch 38, val_loss 455.9064025878906\n",
      "epoch 39, train_loss 1278.7156982421875\n",
      "epoch 39, val_loss 455.9062194824219\n",
      "epoch 40, train_loss 1278.7152099609375\n",
      "epoch 40, val_loss 455.90594482421875\n",
      "epoch 41, train_loss 1278.714599609375\n",
      "epoch 41, val_loss 455.9056396484375\n",
      "epoch 42, train_loss 1278.7139892578125\n",
      "epoch 42, val_loss 455.90545654296875\n",
      "epoch 43, train_loss 1278.71337890625\n",
      "epoch 43, val_loss 455.90521240234375\n",
      "epoch 44, train_loss 1278.712890625\n",
      "epoch 44, val_loss 455.9049072265625\n",
      "epoch 45, train_loss 1278.7122802734375\n",
      "epoch 45, val_loss 455.9046630859375\n",
      "epoch 46, train_loss 1278.711669921875\n",
      "epoch 46, val_loss 455.9044189453125\n",
      "epoch 47, train_loss 1278.711181640625\n",
      "epoch 47, val_loss 455.90411376953125\n",
      "epoch 48, train_loss 1278.71044921875\n",
      "epoch 48, val_loss 455.9039306640625\n",
      "epoch 49, train_loss 1278.7099609375\n",
      "epoch 49, val_loss 455.90362548828125\n",
      "epoch 50, train_loss 1278.7093505859375\n",
      "epoch 50, val_loss 455.90338134765625\n",
      "epoch 51, train_loss 1278.7088623046875\n",
      "epoch 51, val_loss 455.9031982421875\n",
      "epoch 52, train_loss 1278.708251953125\n",
      "epoch 52, val_loss 455.9029541015625\n",
      "epoch 53, train_loss 1278.7076416015625\n",
      "epoch 53, val_loss 455.9026184082031\n",
      "epoch 54, train_loss 1278.70703125\n",
      "epoch 54, val_loss 455.90240478515625\n",
      "epoch 55, train_loss 1278.70654296875\n",
      "epoch 55, val_loss 455.90216064453125\n",
      "epoch 56, train_loss 1278.705810546875\n",
      "epoch 56, val_loss 455.9020080566406\n",
      "epoch 57, train_loss 1278.7054443359375\n",
      "epoch 57, val_loss 455.90167236328125\n",
      "epoch 58, train_loss 1278.7047119140625\n",
      "epoch 58, val_loss 455.9013671875\n",
      "epoch 59, train_loss 1278.7042236328125\n",
      "epoch 59, val_loss 455.9011535644531\n",
      "epoch 60, train_loss 1278.70361328125\n",
      "epoch 60, val_loss 455.9009094238281\n",
      "epoch 61, train_loss 1278.703125\n",
      "epoch 61, val_loss 455.900634765625\n",
      "epoch 62, train_loss 1278.7025146484375\n",
      "epoch 62, val_loss 455.90045166015625\n",
      "epoch 63, train_loss 1278.701904296875\n",
      "epoch 63, val_loss 455.9001770019531\n",
      "epoch 64, train_loss 1278.7015380859375\n",
      "epoch 64, val_loss 455.89990234375\n",
      "epoch 65, train_loss 1278.7008056640625\n",
      "epoch 65, val_loss 455.899658203125\n",
      "epoch 66, train_loss 1278.7003173828125\n",
      "epoch 66, val_loss 455.8994140625\n",
      "epoch 67, train_loss 1278.6998291015625\n",
      "epoch 67, val_loss 455.8992004394531\n",
      "epoch 68, train_loss 1278.698974609375\n",
      "epoch 68, val_loss 455.8989562988281\n",
      "epoch 69, train_loss 1278.6986083984375\n",
      "epoch 69, val_loss 455.89862060546875\n",
      "epoch 70, train_loss 1278.6981201171875\n",
      "epoch 70, val_loss 455.8984069824219\n",
      "epoch 71, train_loss 1278.6973876953125\n",
      "epoch 71, val_loss 455.898193359375\n",
      "epoch 72, train_loss 1278.6968994140625\n",
      "epoch 72, val_loss 455.89794921875\n",
      "epoch 73, train_loss 1278.6962890625\n",
      "epoch 73, val_loss 455.897705078125\n",
      "epoch 74, train_loss 1278.69580078125\n",
      "epoch 74, val_loss 455.8974914550781\n",
      "epoch 75, train_loss 1278.6951904296875\n",
      "epoch 75, val_loss 455.897216796875\n",
      "epoch 76, train_loss 1278.6947021484375\n",
      "epoch 76, val_loss 455.89697265625\n",
      "epoch 77, train_loss 1278.6942138671875\n",
      "epoch 77, val_loss 455.8967590332031\n",
      "epoch 78, train_loss 1278.693603515625\n",
      "epoch 78, val_loss 455.8965148925781\n",
      "epoch 79, train_loss 1278.693115234375\n",
      "epoch 79, val_loss 455.896240234375\n",
      "epoch 80, train_loss 1278.6925048828125\n",
      "epoch 80, val_loss 455.8959655761719\n",
      "epoch 81, train_loss 1278.692138671875\n",
      "epoch 81, val_loss 455.8957824707031\n",
      "epoch 82, train_loss 1278.69140625\n",
      "epoch 82, val_loss 455.89556884765625\n",
      "epoch 83, train_loss 1278.6907958984375\n",
      "epoch 83, val_loss 455.8952941894531\n",
      "epoch 84, train_loss 1278.6903076171875\n",
      "epoch 84, val_loss 455.8950500488281\n",
      "epoch 85, train_loss 1278.689697265625\n",
      "epoch 85, val_loss 455.8948059082031\n",
      "epoch 86, train_loss 1278.689208984375\n",
      "epoch 86, val_loss 455.89453125\n",
      "epoch 87, train_loss 1278.6884765625\n",
      "epoch 87, val_loss 455.894287109375\n",
      "epoch 88, train_loss 1278.68798828125\n",
      "epoch 88, val_loss 455.8940124511719\n",
      "epoch 89, train_loss 1278.6873779296875\n",
      "epoch 89, val_loss 455.8938293457031\n",
      "epoch 90, train_loss 1278.6871337890625\n",
      "epoch 90, val_loss 455.8935852050781\n",
      "epoch 91, train_loss 1278.6864013671875\n",
      "epoch 91, val_loss 455.8933410644531\n",
      "epoch 92, train_loss 1278.6859130859375\n",
      "epoch 92, val_loss 455.8930969238281\n",
      "epoch 93, train_loss 1278.685546875\n",
      "epoch 93, val_loss 455.892822265625\n",
      "epoch 94, train_loss 1278.684814453125\n",
      "epoch 94, val_loss 455.89263916015625\n",
      "epoch 95, train_loss 1278.684326171875\n",
      "epoch 95, val_loss 455.8923645019531\n",
      "epoch 96, train_loss 1278.6837158203125\n",
      "epoch 96, val_loss 455.8921203613281\n",
      "epoch 97, train_loss 1278.6832275390625\n",
      "epoch 97, val_loss 455.891845703125\n",
      "epoch 98, train_loss 1278.6827392578125\n",
      "epoch 98, val_loss 455.8916931152344\n",
      "epoch 99, train_loss 1278.68212890625\n",
      "epoch 99, val_loss 455.89141845703125\n",
      "Parameter containing:\n",
      "tensor([9.3100e-05], requires_grad=True)\n",
      "iter 18, train_loss_regularization 1.1079282760620117\n",
      "iter 18, val_loss_regularization 1.1079282760620117\n",
      "epoch 0, train_loss 1278.681640625\n",
      "epoch 0, val_loss 455.89117431640625\n",
      "epoch 1, train_loss 1278.6810302734375\n",
      "epoch 1, val_loss 455.8909606933594\n",
      "epoch 2, train_loss 1278.6805419921875\n",
      "epoch 2, val_loss 455.8906555175781\n",
      "epoch 3, train_loss 1278.6800537109375\n",
      "epoch 3, val_loss 455.8904724121094\n",
      "epoch 4, train_loss 1278.679443359375\n",
      "epoch 4, val_loss 455.89019775390625\n",
      "epoch 5, train_loss 1278.678955078125\n",
      "epoch 5, val_loss 455.8900146484375\n",
      "epoch 6, train_loss 1278.678466796875\n",
      "epoch 6, val_loss 455.8897399902344\n",
      "epoch 7, train_loss 1278.6778564453125\n",
      "epoch 7, val_loss 455.88946533203125\n",
      "epoch 8, train_loss 1278.6773681640625\n",
      "epoch 8, val_loss 455.8892517089844\n",
      "epoch 9, train_loss 1278.6768798828125\n",
      "epoch 9, val_loss 455.8890380859375\n",
      "epoch 10, train_loss 1278.6763916015625\n",
      "epoch 10, val_loss 455.8887939453125\n",
      "epoch 11, train_loss 1278.67578125\n",
      "epoch 11, val_loss 455.88848876953125\n",
      "epoch 12, train_loss 1278.67529296875\n",
      "epoch 12, val_loss 455.8883361816406\n",
      "epoch 13, train_loss 1278.674560546875\n",
      "epoch 13, val_loss 455.8880310058594\n",
      "epoch 14, train_loss 1278.6741943359375\n",
      "epoch 14, val_loss 455.8878479003906\n",
      "epoch 15, train_loss 1278.673583984375\n",
      "epoch 15, val_loss 455.8875732421875\n",
      "epoch 16, train_loss 1278.673095703125\n",
      "epoch 16, val_loss 455.88739013671875\n",
      "epoch 17, train_loss 1278.672607421875\n",
      "epoch 17, val_loss 455.8871765136719\n",
      "epoch 18, train_loss 1278.6719970703125\n",
      "epoch 18, val_loss 455.88690185546875\n",
      "epoch 19, train_loss 1278.671630859375\n",
      "epoch 19, val_loss 455.88671875\n",
      "epoch 20, train_loss 1278.6707763671875\n",
      "epoch 20, val_loss 455.8864440917969\n",
      "epoch 21, train_loss 1278.6705322265625\n",
      "epoch 21, val_loss 455.8861999511719\n",
      "epoch 22, train_loss 1278.6697998046875\n",
      "epoch 22, val_loss 455.885986328125\n",
      "epoch 23, train_loss 1278.6695556640625\n",
      "epoch 23, val_loss 455.8857421875\n",
      "epoch 24, train_loss 1278.6688232421875\n",
      "epoch 24, val_loss 455.8855285644531\n",
      "epoch 25, train_loss 1278.66845703125\n",
      "epoch 25, val_loss 455.8852233886719\n",
      "epoch 26, train_loss 1278.667724609375\n",
      "epoch 26, val_loss 455.8850402832031\n",
      "epoch 27, train_loss 1278.6673583984375\n",
      "epoch 27, val_loss 455.88482666015625\n",
      "epoch 28, train_loss 1278.666748046875\n",
      "epoch 28, val_loss 455.8845520019531\n",
      "epoch 29, train_loss 1278.666259765625\n",
      "epoch 29, val_loss 455.8842468261719\n",
      "epoch 30, train_loss 1278.665771484375\n",
      "epoch 30, val_loss 455.88409423828125\n",
      "epoch 31, train_loss 1278.665283203125\n",
      "epoch 31, val_loss 455.88397216796875\n",
      "epoch 32, train_loss 1278.664794921875\n",
      "epoch 32, val_loss 455.8836669921875\n",
      "epoch 33, train_loss 1278.664306640625\n",
      "epoch 33, val_loss 455.8834228515625\n",
      "epoch 34, train_loss 1278.6636962890625\n",
      "epoch 34, val_loss 455.8831787109375\n",
      "epoch 35, train_loss 1278.663330078125\n",
      "epoch 35, val_loss 455.88299560546875\n",
      "epoch 36, train_loss 1278.6627197265625\n",
      "epoch 36, val_loss 455.88275146484375\n",
      "epoch 37, train_loss 1278.662109375\n",
      "epoch 37, val_loss 455.8824462890625\n",
      "epoch 38, train_loss 1278.66162109375\n",
      "epoch 38, val_loss 455.88226318359375\n",
      "epoch 39, train_loss 1278.6611328125\n",
      "epoch 39, val_loss 455.8819885253906\n",
      "epoch 40, train_loss 1278.6605224609375\n",
      "epoch 40, val_loss 455.8818054199219\n",
      "epoch 41, train_loss 1278.6602783203125\n",
      "epoch 41, val_loss 455.8816223144531\n",
      "epoch 42, train_loss 1278.65966796875\n",
      "epoch 42, val_loss 455.88134765625\n",
      "epoch 43, train_loss 1278.6591796875\n",
      "epoch 43, val_loss 455.8810729980469\n",
      "epoch 44, train_loss 1278.65869140625\n",
      "epoch 44, val_loss 455.8808898925781\n",
      "epoch 45, train_loss 1278.658203125\n",
      "epoch 45, val_loss 455.8805847167969\n",
      "epoch 46, train_loss 1278.65771484375\n",
      "epoch 46, val_loss 455.8804016113281\n",
      "epoch 47, train_loss 1278.6572265625\n",
      "epoch 47, val_loss 455.8801574707031\n",
      "epoch 48, train_loss 1278.6566162109375\n",
      "epoch 48, val_loss 455.87994384765625\n",
      "epoch 49, train_loss 1278.656005859375\n",
      "epoch 49, val_loss 455.8797607421875\n",
      "epoch 50, train_loss 1278.6556396484375\n",
      "epoch 50, val_loss 455.8794860839844\n",
      "epoch 51, train_loss 1278.6551513671875\n",
      "epoch 51, val_loss 455.8792419433594\n",
      "epoch 52, train_loss 1278.654541015625\n",
      "epoch 52, val_loss 455.8790283203125\n",
      "epoch 53, train_loss 1278.654052734375\n",
      "epoch 53, val_loss 455.87884521484375\n",
      "epoch 54, train_loss 1278.6536865234375\n",
      "epoch 54, val_loss 455.8785705566406\n",
      "epoch 55, train_loss 1278.653076171875\n",
      "epoch 55, val_loss 455.8783264160156\n",
      "epoch 56, train_loss 1278.6524658203125\n",
      "epoch 56, val_loss 455.878173828125\n",
      "epoch 57, train_loss 1278.652099609375\n",
      "epoch 57, val_loss 455.8779296875\n",
      "epoch 58, train_loss 1278.651611328125\n",
      "epoch 58, val_loss 455.8777160644531\n",
      "epoch 59, train_loss 1278.6512451171875\n",
      "epoch 59, val_loss 455.8774719238281\n",
      "epoch 60, train_loss 1278.6505126953125\n",
      "epoch 60, val_loss 455.8772888183594\n",
      "epoch 61, train_loss 1278.6500244140625\n",
      "epoch 61, val_loss 455.87701416015625\n",
      "epoch 62, train_loss 1278.649658203125\n",
      "epoch 62, val_loss 455.8768005371094\n",
      "epoch 63, train_loss 1278.649169921875\n",
      "epoch 63, val_loss 455.8765563964844\n",
      "epoch 64, train_loss 1278.6484375\n",
      "epoch 64, val_loss 455.8763427734375\n",
      "epoch 65, train_loss 1278.648193359375\n",
      "epoch 65, val_loss 455.87615966796875\n",
      "epoch 66, train_loss 1278.647705078125\n",
      "epoch 66, val_loss 455.8758850097656\n",
      "epoch 67, train_loss 1278.647216796875\n",
      "epoch 67, val_loss 455.8757019042969\n",
      "epoch 68, train_loss 1278.646484375\n",
      "epoch 68, val_loss 455.8754577636719\n",
      "epoch 69, train_loss 1278.6461181640625\n",
      "epoch 69, val_loss 455.8751525878906\n",
      "epoch 70, train_loss 1278.6456298828125\n",
      "epoch 70, val_loss 455.875\n",
      "epoch 71, train_loss 1278.6451416015625\n",
      "epoch 71, val_loss 455.874755859375\n",
      "epoch 72, train_loss 1278.64453125\n",
      "epoch 72, val_loss 455.8745422363281\n",
      "epoch 73, train_loss 1278.6441650390625\n",
      "epoch 73, val_loss 455.87432861328125\n",
      "epoch 74, train_loss 1278.6436767578125\n",
      "epoch 74, val_loss 455.87408447265625\n",
      "epoch 75, train_loss 1278.6431884765625\n",
      "epoch 75, val_loss 455.8738708496094\n",
      "epoch 76, train_loss 1278.642578125\n",
      "epoch 76, val_loss 455.87359619140625\n",
      "epoch 77, train_loss 1278.6422119140625\n",
      "epoch 77, val_loss 455.8734130859375\n",
      "epoch 78, train_loss 1278.6417236328125\n",
      "epoch 78, val_loss 455.8731994628906\n",
      "epoch 79, train_loss 1278.64111328125\n",
      "epoch 79, val_loss 455.87298583984375\n",
      "epoch 80, train_loss 1278.640625\n",
      "epoch 80, val_loss 455.87274169921875\n",
      "epoch 81, train_loss 1278.640380859375\n",
      "epoch 81, val_loss 455.87249755859375\n",
      "epoch 82, train_loss 1278.639892578125\n",
      "epoch 82, val_loss 455.8723449707031\n",
      "epoch 83, train_loss 1278.6392822265625\n",
      "epoch 83, val_loss 455.8720397949219\n",
      "epoch 84, train_loss 1278.6387939453125\n",
      "epoch 84, val_loss 455.87188720703125\n",
      "epoch 85, train_loss 1278.6383056640625\n",
      "epoch 85, val_loss 455.87158203125\n",
      "epoch 86, train_loss 1278.6376953125\n",
      "epoch 86, val_loss 455.8714294433594\n",
      "epoch 87, train_loss 1278.6373291015625\n",
      "epoch 87, val_loss 455.8712158203125\n",
      "epoch 88, train_loss 1278.636962890625\n",
      "epoch 88, val_loss 455.87091064453125\n",
      "epoch 89, train_loss 1278.6363525390625\n",
      "epoch 89, val_loss 455.8706970214844\n",
      "epoch 90, train_loss 1278.6358642578125\n",
      "epoch 90, val_loss 455.8705139160156\n",
      "epoch 91, train_loss 1278.6353759765625\n",
      "epoch 91, val_loss 455.87030029296875\n",
      "epoch 92, train_loss 1278.6348876953125\n",
      "epoch 92, val_loss 455.8700866699219\n",
      "epoch 93, train_loss 1278.6346435546875\n",
      "epoch 93, val_loss 455.869873046875\n",
      "epoch 94, train_loss 1278.6337890625\n",
      "epoch 94, val_loss 455.8696594238281\n",
      "epoch 95, train_loss 1278.6336669921875\n",
      "epoch 95, val_loss 455.8695068359375\n",
      "epoch 96, train_loss 1278.633056640625\n",
      "epoch 96, val_loss 455.86920166015625\n",
      "epoch 97, train_loss 1278.632568359375\n",
      "epoch 97, val_loss 455.86895751953125\n",
      "epoch 98, train_loss 1278.6319580078125\n",
      "epoch 98, val_loss 455.86871337890625\n",
      "epoch 99, train_loss 1278.6318359375\n",
      "epoch 99, val_loss 455.8685302734375\n",
      "Parameter containing:\n",
      "tensor([5.8494e-05], requires_grad=True)\n",
      "iter 19, train_loss_regularization 1.0994064807891846\n",
      "iter 19, val_loss_regularization 1.0994064807891846\n",
      "epoch 0, train_loss 1278.6312255859375\n",
      "epoch 0, val_loss 455.86834716796875\n",
      "epoch 1, train_loss 1278.630615234375\n",
      "epoch 1, val_loss 455.8680725097656\n",
      "epoch 2, train_loss 1278.630126953125\n",
      "epoch 2, val_loss 455.8678894042969\n",
      "epoch 3, train_loss 1278.6297607421875\n",
      "epoch 3, val_loss 455.86767578125\n",
      "epoch 4, train_loss 1278.6292724609375\n",
      "epoch 4, val_loss 455.867431640625\n",
      "epoch 5, train_loss 1278.6287841796875\n",
      "epoch 5, val_loss 455.8672790527344\n",
      "epoch 6, train_loss 1278.6282958984375\n",
      "epoch 6, val_loss 455.8670349121094\n",
      "epoch 7, train_loss 1278.6280517578125\n",
      "epoch 7, val_loss 455.86676025390625\n",
      "epoch 8, train_loss 1278.62744140625\n",
      "epoch 8, val_loss 455.8665771484375\n",
      "epoch 9, train_loss 1278.6268310546875\n",
      "epoch 9, val_loss 455.8663635253906\n",
      "epoch 10, train_loss 1278.62646484375\n",
      "epoch 10, val_loss 455.8661193847656\n",
      "epoch 11, train_loss 1278.6260986328125\n",
      "epoch 11, val_loss 455.86590576171875\n",
      "epoch 12, train_loss 1278.6256103515625\n",
      "epoch 12, val_loss 455.86572265625\n",
      "epoch 13, train_loss 1278.625\n",
      "epoch 13, val_loss 455.8654479980469\n",
      "epoch 14, train_loss 1278.6246337890625\n",
      "epoch 14, val_loss 455.86529541015625\n",
      "epoch 15, train_loss 1278.6240234375\n",
      "epoch 15, val_loss 455.8650817871094\n",
      "epoch 16, train_loss 1278.62353515625\n",
      "epoch 16, val_loss 455.8648681640625\n",
      "epoch 17, train_loss 1278.6231689453125\n",
      "epoch 17, val_loss 455.8646545410156\n",
      "epoch 18, train_loss 1278.6226806640625\n",
      "epoch 18, val_loss 455.8644104003906\n",
      "epoch 19, train_loss 1278.6221923828125\n",
      "epoch 19, val_loss 455.86419677734375\n",
      "epoch 20, train_loss 1278.6217041015625\n",
      "epoch 20, val_loss 455.86395263671875\n",
      "epoch 21, train_loss 1278.6214599609375\n",
      "epoch 21, val_loss 455.8637390136719\n",
      "epoch 22, train_loss 1278.620849609375\n",
      "epoch 22, val_loss 455.8635559082031\n",
      "epoch 23, train_loss 1278.620361328125\n",
      "epoch 23, val_loss 455.86334228515625\n",
      "epoch 24, train_loss 1278.619873046875\n",
      "epoch 24, val_loss 455.86309814453125\n",
      "epoch 25, train_loss 1278.6195068359375\n",
      "epoch 25, val_loss 455.8629150390625\n",
      "epoch 26, train_loss 1278.618896484375\n",
      "epoch 26, val_loss 455.8627014160156\n",
      "epoch 27, train_loss 1278.6185302734375\n",
      "epoch 27, val_loss 455.8624572753906\n",
      "epoch 28, train_loss 1278.6180419921875\n",
      "epoch 28, val_loss 455.8623046875\n",
      "epoch 29, train_loss 1278.6177978515625\n",
      "epoch 29, val_loss 455.8620910644531\n",
      "epoch 30, train_loss 1278.6173095703125\n",
      "epoch 30, val_loss 455.8618469238281\n",
      "epoch 31, train_loss 1278.6165771484375\n",
      "epoch 31, val_loss 455.8616638183594\n",
      "epoch 32, train_loss 1278.6162109375\n",
      "epoch 32, val_loss 455.8614501953125\n",
      "epoch 33, train_loss 1278.61572265625\n",
      "epoch 33, val_loss 455.8611755371094\n",
      "epoch 34, train_loss 1278.6153564453125\n",
      "epoch 34, val_loss 455.8609619140625\n",
      "epoch 35, train_loss 1278.6148681640625\n",
      "epoch 35, val_loss 455.8607177734375\n",
      "epoch 36, train_loss 1278.614501953125\n",
      "epoch 36, val_loss 455.86053466796875\n",
      "epoch 37, train_loss 1278.614013671875\n",
      "epoch 37, val_loss 455.8603210449219\n",
      "epoch 38, train_loss 1278.613525390625\n",
      "epoch 38, val_loss 455.86016845703125\n",
      "epoch 39, train_loss 1278.613037109375\n",
      "epoch 39, val_loss 455.85992431640625\n",
      "epoch 40, train_loss 1278.6126708984375\n",
      "epoch 40, val_loss 455.8597106933594\n",
      "epoch 41, train_loss 1278.6123046875\n",
      "epoch 41, val_loss 455.8594970703125\n",
      "epoch 42, train_loss 1278.61181640625\n",
      "epoch 42, val_loss 455.8593444824219\n",
      "epoch 43, train_loss 1278.6112060546875\n",
      "epoch 43, val_loss 455.8590087890625\n",
      "epoch 44, train_loss 1278.61083984375\n",
      "epoch 44, val_loss 455.85882568359375\n",
      "epoch 45, train_loss 1278.6104736328125\n",
      "epoch 45, val_loss 455.8586730957031\n",
      "epoch 46, train_loss 1278.60986328125\n",
      "epoch 46, val_loss 455.8584289550781\n",
      "epoch 47, train_loss 1278.6094970703125\n",
      "epoch 47, val_loss 455.8582458496094\n",
      "epoch 48, train_loss 1278.60888671875\n",
      "epoch 48, val_loss 455.8580017089844\n",
      "epoch 49, train_loss 1278.6083984375\n",
      "epoch 49, val_loss 455.8577880859375\n",
      "epoch 50, train_loss 1278.608154296875\n",
      "epoch 50, val_loss 455.8576354980469\n",
      "epoch 51, train_loss 1278.607666015625\n",
      "epoch 51, val_loss 455.857421875\n",
      "epoch 52, train_loss 1278.607177734375\n",
      "epoch 52, val_loss 455.857177734375\n",
      "epoch 53, train_loss 1278.606689453125\n",
      "epoch 53, val_loss 455.8569030761719\n",
      "epoch 54, train_loss 1278.6063232421875\n",
      "epoch 54, val_loss 455.85675048828125\n",
      "epoch 55, train_loss 1278.605712890625\n",
      "epoch 55, val_loss 455.8565368652344\n",
      "epoch 56, train_loss 1278.60546875\n",
      "epoch 56, val_loss 455.85638427734375\n",
      "epoch 57, train_loss 1278.60498046875\n",
      "epoch 57, val_loss 455.85614013671875\n",
      "epoch 58, train_loss 1278.6043701171875\n",
      "epoch 58, val_loss 455.85595703125\n",
      "epoch 59, train_loss 1278.6041259765625\n",
      "epoch 59, val_loss 455.85565185546875\n",
      "epoch 60, train_loss 1278.6036376953125\n",
      "epoch 60, val_loss 455.8554992675781\n",
      "epoch 61, train_loss 1278.6031494140625\n",
      "epoch 61, val_loss 455.85528564453125\n",
      "epoch 62, train_loss 1278.602783203125\n",
      "epoch 62, val_loss 455.8550720214844\n",
      "epoch 63, train_loss 1278.602294921875\n",
      "epoch 63, val_loss 455.8548889160156\n",
      "epoch 64, train_loss 1278.6019287109375\n",
      "epoch 64, val_loss 455.8545837402344\n",
      "epoch 65, train_loss 1278.6014404296875\n",
      "epoch 65, val_loss 455.85443115234375\n",
      "epoch 66, train_loss 1278.60107421875\n",
      "epoch 66, val_loss 455.8542175292969\n",
      "epoch 67, train_loss 1278.6005859375\n",
      "epoch 67, val_loss 455.85400390625\n",
      "epoch 68, train_loss 1278.60009765625\n",
      "epoch 68, val_loss 455.85382080078125\n",
      "epoch 69, train_loss 1278.5997314453125\n",
      "epoch 69, val_loss 455.8536376953125\n",
      "epoch 70, train_loss 1278.5992431640625\n",
      "epoch 70, val_loss 455.85345458984375\n",
      "epoch 71, train_loss 1278.598876953125\n",
      "epoch 71, val_loss 455.85321044921875\n",
      "epoch 72, train_loss 1278.598388671875\n",
      "epoch 72, val_loss 455.85302734375\n",
      "epoch 73, train_loss 1278.5980224609375\n",
      "epoch 73, val_loss 455.852783203125\n",
      "epoch 74, train_loss 1278.5975341796875\n",
      "epoch 74, val_loss 455.8525390625\n",
      "epoch 75, train_loss 1278.5970458984375\n",
      "epoch 75, val_loss 455.8523864746094\n",
      "epoch 76, train_loss 1278.5968017578125\n",
      "epoch 76, val_loss 455.8521728515625\n",
      "epoch 77, train_loss 1278.5963134765625\n",
      "epoch 77, val_loss 455.8519592285156\n",
      "epoch 78, train_loss 1278.595703125\n",
      "epoch 78, val_loss 455.85174560546875\n",
      "epoch 79, train_loss 1278.5953369140625\n",
      "epoch 79, val_loss 455.8515930175781\n",
      "epoch 80, train_loss 1278.5947265625\n",
      "epoch 80, val_loss 455.851318359375\n",
      "epoch 81, train_loss 1278.5946044921875\n",
      "epoch 81, val_loss 455.85113525390625\n",
      "epoch 82, train_loss 1278.5938720703125\n",
      "epoch 82, val_loss 455.8509216308594\n",
      "epoch 83, train_loss 1278.5936279296875\n",
      "epoch 83, val_loss 455.8507385253906\n",
      "epoch 84, train_loss 1278.5931396484375\n",
      "epoch 84, val_loss 455.8505554199219\n",
      "epoch 85, train_loss 1278.5926513671875\n",
      "epoch 85, val_loss 455.850341796875\n",
      "epoch 86, train_loss 1278.59228515625\n",
      "epoch 86, val_loss 455.85009765625\n",
      "epoch 87, train_loss 1278.591796875\n",
      "epoch 87, val_loss 455.8498840332031\n",
      "epoch 88, train_loss 1278.59130859375\n",
      "epoch 88, val_loss 455.8497009277344\n",
      "epoch 89, train_loss 1278.591064453125\n",
      "epoch 89, val_loss 455.8494873046875\n",
      "epoch 90, train_loss 1278.5906982421875\n",
      "epoch 90, val_loss 455.84930419921875\n",
      "epoch 91, train_loss 1278.5902099609375\n",
      "epoch 91, val_loss 455.8490295410156\n",
      "epoch 92, train_loss 1278.5897216796875\n",
      "epoch 92, val_loss 455.848876953125\n",
      "epoch 93, train_loss 1278.5892333984375\n",
      "epoch 93, val_loss 455.8486633300781\n",
      "epoch 94, train_loss 1278.5887451171875\n",
      "epoch 94, val_loss 455.84844970703125\n",
      "epoch 95, train_loss 1278.58837890625\n",
      "epoch 95, val_loss 455.8482971191406\n",
      "epoch 96, train_loss 1278.588134765625\n",
      "epoch 96, val_loss 455.84808349609375\n",
      "epoch 97, train_loss 1278.5875244140625\n",
      "epoch 97, val_loss 455.8477783203125\n",
      "epoch 98, train_loss 1278.5870361328125\n",
      "epoch 98, val_loss 455.8476257324219\n",
      "epoch 99, train_loss 1278.5867919921875\n",
      "epoch 99, val_loss 455.847412109375\n",
      "Parameter containing:\n",
      "tensor([3.6735e-05], requires_grad=True)\n",
      "iter 20, train_loss_regularization 1.0911409854888916\n",
      "iter 20, val_loss_regularization 1.0911409854888916\n",
      "epoch 0, train_loss 1278.5863037109375\n",
      "epoch 0, val_loss 455.8471984863281\n",
      "epoch 1, train_loss 1278.5859375\n",
      "epoch 1, val_loss 455.8470458984375\n",
      "epoch 2, train_loss 1278.58544921875\n",
      "epoch 2, val_loss 455.84686279296875\n",
      "epoch 3, train_loss 1278.5849609375\n",
      "epoch 3, val_loss 455.8465881347656\n",
      "epoch 4, train_loss 1278.5848388671875\n",
      "epoch 4, val_loss 455.8464050292969\n",
      "epoch 5, train_loss 1278.584228515625\n",
      "epoch 5, val_loss 455.8462219238281\n",
      "epoch 6, train_loss 1278.5838623046875\n",
      "epoch 6, val_loss 455.8460693359375\n",
      "epoch 7, train_loss 1278.5833740234375\n",
      "epoch 7, val_loss 455.8458251953125\n",
      "epoch 8, train_loss 1278.5828857421875\n",
      "epoch 8, val_loss 455.8456115722656\n",
      "epoch 9, train_loss 1278.5826416015625\n",
      "epoch 9, val_loss 455.845458984375\n",
      "epoch 10, train_loss 1278.58203125\n",
      "epoch 10, val_loss 455.8452453613281\n",
      "epoch 11, train_loss 1278.5816650390625\n",
      "epoch 11, val_loss 455.84503173828125\n",
      "epoch 12, train_loss 1278.581298828125\n",
      "epoch 12, val_loss 455.8448181152344\n",
      "epoch 13, train_loss 1278.580810546875\n",
      "epoch 13, val_loss 455.8445739746094\n",
      "epoch 14, train_loss 1278.5804443359375\n",
      "epoch 14, val_loss 455.84442138671875\n",
      "epoch 15, train_loss 1278.579833984375\n",
      "epoch 15, val_loss 455.84417724609375\n",
      "epoch 16, train_loss 1278.5794677734375\n",
      "epoch 16, val_loss 455.843994140625\n",
      "epoch 17, train_loss 1278.5791015625\n",
      "epoch 17, val_loss 455.8437805175781\n",
      "epoch 18, train_loss 1278.5787353515625\n",
      "epoch 18, val_loss 455.8435974121094\n",
      "epoch 19, train_loss 1278.5782470703125\n",
      "epoch 19, val_loss 455.8434143066406\n",
      "epoch 20, train_loss 1278.5780029296875\n",
      "epoch 20, val_loss 455.8431701660156\n",
      "epoch 21, train_loss 1278.5775146484375\n",
      "epoch 21, val_loss 455.8430480957031\n",
      "epoch 22, train_loss 1278.5770263671875\n",
      "epoch 22, val_loss 455.8428039550781\n",
      "epoch 23, train_loss 1278.57666015625\n",
      "epoch 23, val_loss 455.8426208496094\n",
      "epoch 24, train_loss 1278.576171875\n",
      "epoch 24, val_loss 455.8424072265625\n",
      "epoch 25, train_loss 1278.5758056640625\n",
      "epoch 25, val_loss 455.8422546386719\n",
      "epoch 26, train_loss 1278.575439453125\n",
      "epoch 26, val_loss 455.8419494628906\n",
      "epoch 27, train_loss 1278.574951171875\n",
      "epoch 27, val_loss 455.841796875\n",
      "epoch 28, train_loss 1278.5745849609375\n",
      "epoch 28, val_loss 455.84161376953125\n",
      "epoch 29, train_loss 1278.57421875\n",
      "epoch 29, val_loss 455.8414306640625\n",
      "epoch 30, train_loss 1278.57373046875\n",
      "epoch 30, val_loss 455.8412170410156\n",
      "epoch 31, train_loss 1278.5733642578125\n",
      "epoch 31, val_loss 455.8409729003906\n",
      "epoch 32, train_loss 1278.572998046875\n",
      "epoch 32, val_loss 455.84075927734375\n",
      "epoch 33, train_loss 1278.5726318359375\n",
      "epoch 33, val_loss 455.840576171875\n",
      "epoch 34, train_loss 1278.5721435546875\n",
      "epoch 34, val_loss 455.8404235839844\n",
      "epoch 35, train_loss 1278.57177734375\n",
      "epoch 35, val_loss 455.8402099609375\n",
      "epoch 36, train_loss 1278.5714111328125\n",
      "epoch 36, val_loss 455.8399963378906\n",
      "epoch 37, train_loss 1278.57080078125\n",
      "epoch 37, val_loss 455.83978271484375\n",
      "epoch 38, train_loss 1278.5706787109375\n",
      "epoch 38, val_loss 455.8396301269531\n",
      "epoch 39, train_loss 1278.570068359375\n",
      "epoch 39, val_loss 455.83941650390625\n",
      "epoch 40, train_loss 1278.5697021484375\n",
      "epoch 40, val_loss 455.8392028808594\n",
      "epoch 41, train_loss 1278.5694580078125\n",
      "epoch 41, val_loss 455.8390808105469\n",
      "epoch 42, train_loss 1278.5689697265625\n",
      "epoch 42, val_loss 455.8388366699219\n",
      "epoch 43, train_loss 1278.568359375\n",
      "epoch 43, val_loss 455.838623046875\n",
      "epoch 44, train_loss 1278.56787109375\n",
      "epoch 44, val_loss 455.8384094238281\n",
      "epoch 45, train_loss 1278.567626953125\n",
      "epoch 45, val_loss 455.8381652832031\n",
      "epoch 46, train_loss 1278.5672607421875\n",
      "epoch 46, val_loss 455.8380126953125\n",
      "epoch 47, train_loss 1278.5667724609375\n",
      "epoch 47, val_loss 455.8377990722656\n",
      "epoch 48, train_loss 1278.56640625\n",
      "epoch 48, val_loss 455.83758544921875\n",
      "epoch 49, train_loss 1278.5662841796875\n",
      "epoch 49, val_loss 455.83746337890625\n",
      "epoch 50, train_loss 1278.5655517578125\n",
      "epoch 50, val_loss 455.8372497558594\n",
      "epoch 51, train_loss 1278.5653076171875\n",
      "epoch 51, val_loss 455.8370056152344\n",
      "epoch 52, train_loss 1278.56494140625\n",
      "epoch 52, val_loss 455.8368225097656\n",
      "epoch 53, train_loss 1278.564453125\n",
      "epoch 53, val_loss 455.836669921875\n",
      "epoch 54, train_loss 1278.5640869140625\n",
      "epoch 54, val_loss 455.8364562988281\n",
      "epoch 55, train_loss 1278.563720703125\n",
      "epoch 55, val_loss 455.83624267578125\n",
      "epoch 56, train_loss 1278.563232421875\n",
      "epoch 56, val_loss 455.8360900878906\n",
      "epoch 57, train_loss 1278.5628662109375\n",
      "epoch 57, val_loss 455.8359069824219\n",
      "epoch 58, train_loss 1278.5625\n",
      "epoch 58, val_loss 455.8356628417969\n",
      "epoch 59, train_loss 1278.5621337890625\n",
      "epoch 59, val_loss 455.8355407714844\n",
      "epoch 60, train_loss 1278.5616455078125\n",
      "epoch 60, val_loss 455.8352355957031\n",
      "epoch 61, train_loss 1278.5611572265625\n",
      "epoch 61, val_loss 455.8351135253906\n",
      "epoch 62, train_loss 1278.560791015625\n",
      "epoch 62, val_loss 455.8349304199219\n",
      "epoch 63, train_loss 1278.560546875\n",
      "epoch 63, val_loss 455.834716796875\n",
      "epoch 64, train_loss 1278.56005859375\n",
      "epoch 64, val_loss 455.83447265625\n",
      "epoch 65, train_loss 1278.5596923828125\n",
      "epoch 65, val_loss 455.8342590332031\n",
      "epoch 66, train_loss 1278.5592041015625\n",
      "epoch 66, val_loss 455.8340759277344\n",
      "epoch 67, train_loss 1278.5587158203125\n",
      "epoch 67, val_loss 455.8338623046875\n",
      "epoch 68, train_loss 1278.558349609375\n",
      "epoch 68, val_loss 455.8337097167969\n",
      "epoch 69, train_loss 1278.5579833984375\n",
      "epoch 69, val_loss 455.83349609375\n",
      "epoch 70, train_loss 1278.5576171875\n",
      "epoch 70, val_loss 455.8332824707031\n",
      "epoch 71, train_loss 1278.557373046875\n",
      "epoch 71, val_loss 455.83306884765625\n",
      "epoch 72, train_loss 1278.5567626953125\n",
      "epoch 72, val_loss 455.8329162597656\n",
      "epoch 73, train_loss 1278.5562744140625\n",
      "epoch 73, val_loss 455.8327941894531\n",
      "epoch 74, train_loss 1278.5560302734375\n",
      "epoch 74, val_loss 455.83258056640625\n",
      "epoch 75, train_loss 1278.5556640625\n",
      "epoch 75, val_loss 455.83233642578125\n",
      "epoch 76, train_loss 1278.5552978515625\n",
      "epoch 76, val_loss 455.8321533203125\n",
      "epoch 77, train_loss 1278.5550537109375\n",
      "epoch 77, val_loss 455.8320007324219\n",
      "epoch 78, train_loss 1278.554443359375\n",
      "epoch 78, val_loss 455.831787109375\n",
      "epoch 79, train_loss 1278.55419921875\n",
      "epoch 79, val_loss 455.8315734863281\n",
      "epoch 80, train_loss 1278.5538330078125\n",
      "epoch 80, val_loss 455.8314208984375\n",
      "epoch 81, train_loss 1278.5533447265625\n",
      "epoch 81, val_loss 455.8311767578125\n",
      "epoch 82, train_loss 1278.5528564453125\n",
      "epoch 82, val_loss 455.83099365234375\n",
      "epoch 83, train_loss 1278.552490234375\n",
      "epoch 83, val_loss 455.8308410644531\n",
      "epoch 84, train_loss 1278.55224609375\n",
      "epoch 84, val_loss 455.8305969238281\n",
      "epoch 85, train_loss 1278.5518798828125\n",
      "epoch 85, val_loss 455.8304138183594\n",
      "epoch 86, train_loss 1278.5513916015625\n",
      "epoch 86, val_loss 455.830322265625\n",
      "epoch 87, train_loss 1278.551025390625\n",
      "epoch 87, val_loss 455.8299865722656\n",
      "epoch 88, train_loss 1278.550537109375\n",
      "epoch 88, val_loss 455.8298645019531\n",
      "epoch 89, train_loss 1278.55029296875\n",
      "epoch 89, val_loss 455.8296813964844\n",
      "epoch 90, train_loss 1278.5498046875\n",
      "epoch 90, val_loss 455.8294982910156\n",
      "epoch 91, train_loss 1278.54931640625\n",
      "epoch 91, val_loss 455.8292541503906\n",
      "epoch 92, train_loss 1278.5489501953125\n",
      "epoch 92, val_loss 455.82904052734375\n",
      "epoch 93, train_loss 1278.5487060546875\n",
      "epoch 93, val_loss 455.8288269042969\n",
      "epoch 94, train_loss 1278.54833984375\n",
      "epoch 94, val_loss 455.82861328125\n",
      "epoch 95, train_loss 1278.5478515625\n",
      "epoch 95, val_loss 455.82855224609375\n",
      "epoch 96, train_loss 1278.54736328125\n",
      "epoch 96, val_loss 455.8283386230469\n",
      "epoch 97, train_loss 1278.5472412109375\n",
      "epoch 97, val_loss 455.8280944824219\n",
      "epoch 98, train_loss 1278.546630859375\n",
      "epoch 98, val_loss 455.827880859375\n",
      "epoch 99, train_loss 1278.546142578125\n",
      "epoch 99, val_loss 455.8277587890625\n",
      "Parameter containing:\n",
      "tensor([2.3067e-05], requires_grad=True)\n",
      "iter 21, train_loss_regularization 1.0831058025360107\n",
      "iter 21, val_loss_regularization 1.0831058025360107\n",
      "epoch 0, train_loss 1278.5460205078125\n",
      "epoch 0, val_loss 455.82757568359375\n",
      "epoch 1, train_loss 1278.5455322265625\n",
      "epoch 1, val_loss 455.82733154296875\n",
      "epoch 2, train_loss 1278.545166015625\n",
      "epoch 2, val_loss 455.8271179199219\n",
      "epoch 3, train_loss 1278.5447998046875\n",
      "epoch 3, val_loss 455.82696533203125\n",
      "epoch 4, train_loss 1278.5445556640625\n",
      "epoch 4, val_loss 455.8267822265625\n",
      "epoch 5, train_loss 1278.5439453125\n",
      "epoch 5, val_loss 455.8265380859375\n",
      "epoch 6, train_loss 1278.543701171875\n",
      "epoch 6, val_loss 455.826416015625\n",
      "epoch 7, train_loss 1278.54345703125\n",
      "epoch 7, val_loss 455.826171875\n",
      "epoch 8, train_loss 1278.542724609375\n",
      "epoch 8, val_loss 455.82598876953125\n",
      "epoch 9, train_loss 1278.5423583984375\n",
      "epoch 9, val_loss 455.8258361816406\n",
      "epoch 10, train_loss 1278.5421142578125\n",
      "epoch 10, val_loss 455.8255920410156\n",
      "epoch 11, train_loss 1278.5419921875\n",
      "epoch 11, val_loss 455.8254089355469\n",
      "epoch 12, train_loss 1278.541259765625\n",
      "epoch 12, val_loss 455.8252868652344\n",
      "epoch 13, train_loss 1278.5411376953125\n",
      "epoch 13, val_loss 455.8250427246094\n",
      "epoch 14, train_loss 1278.54052734375\n",
      "epoch 14, val_loss 455.82489013671875\n",
      "epoch 15, train_loss 1278.5400390625\n",
      "epoch 15, val_loss 455.82470703125\n",
      "epoch 16, train_loss 1278.5399169921875\n",
      "epoch 16, val_loss 455.824462890625\n",
      "epoch 17, train_loss 1278.53955078125\n",
      "epoch 17, val_loss 455.82427978515625\n",
      "epoch 18, train_loss 1278.5389404296875\n",
      "epoch 18, val_loss 455.82415771484375\n",
      "epoch 19, train_loss 1278.53857421875\n",
      "epoch 19, val_loss 455.82391357421875\n",
      "epoch 20, train_loss 1278.538330078125\n",
      "epoch 20, val_loss 455.8237609863281\n",
      "epoch 21, train_loss 1278.5379638671875\n",
      "epoch 21, val_loss 455.823486328125\n",
      "epoch 22, train_loss 1278.5377197265625\n",
      "epoch 22, val_loss 455.8233642578125\n",
      "epoch 23, train_loss 1278.537109375\n",
      "epoch 23, val_loss 455.82318115234375\n",
      "epoch 24, train_loss 1278.536865234375\n",
      "epoch 24, val_loss 455.8229675292969\n",
      "epoch 25, train_loss 1278.5364990234375\n",
      "epoch 25, val_loss 455.8228454589844\n",
      "epoch 26, train_loss 1278.5361328125\n",
      "epoch 26, val_loss 455.8226623535156\n",
      "epoch 27, train_loss 1278.5355224609375\n",
      "epoch 27, val_loss 455.8224182128906\n",
      "epoch 28, train_loss 1278.5352783203125\n",
      "epoch 28, val_loss 455.8222351074219\n",
      "epoch 29, train_loss 1278.5350341796875\n",
      "epoch 29, val_loss 455.82208251953125\n",
      "epoch 30, train_loss 1278.534423828125\n",
      "epoch 30, val_loss 455.8219299316406\n",
      "epoch 31, train_loss 1278.5340576171875\n",
      "epoch 31, val_loss 455.8216552734375\n",
      "epoch 32, train_loss 1278.5338134765625\n",
      "epoch 32, val_loss 455.8215026855469\n",
      "epoch 33, train_loss 1278.533447265625\n",
      "epoch 33, val_loss 455.8212890625\n",
      "epoch 34, train_loss 1278.5330810546875\n",
      "epoch 34, val_loss 455.8210754394531\n",
      "epoch 35, train_loss 1278.53271484375\n",
      "epoch 35, val_loss 455.8209533691406\n",
      "epoch 36, train_loss 1278.5323486328125\n",
      "epoch 36, val_loss 455.8207092285156\n",
      "epoch 37, train_loss 1278.5318603515625\n",
      "epoch 37, val_loss 455.820556640625\n",
      "epoch 38, train_loss 1278.531494140625\n",
      "epoch 38, val_loss 455.8204040527344\n",
      "epoch 39, train_loss 1278.53125\n",
      "epoch 39, val_loss 455.8201599121094\n",
      "epoch 40, train_loss 1278.53076171875\n",
      "epoch 40, val_loss 455.8199462890625\n",
      "epoch 41, train_loss 1278.5303955078125\n",
      "epoch 41, val_loss 455.8197937011719\n",
      "epoch 42, train_loss 1278.530029296875\n",
      "epoch 42, val_loss 455.8196105957031\n",
      "epoch 43, train_loss 1278.52978515625\n",
      "epoch 43, val_loss 455.8194580078125\n",
      "epoch 44, train_loss 1278.5294189453125\n",
      "epoch 44, val_loss 455.8192138671875\n",
      "epoch 45, train_loss 1278.529052734375\n",
      "epoch 45, val_loss 455.81903076171875\n",
      "epoch 46, train_loss 1278.5286865234375\n",
      "epoch 46, val_loss 455.8188781738281\n",
      "epoch 47, train_loss 1278.5281982421875\n",
      "epoch 47, val_loss 455.8186950683594\n",
      "epoch 48, train_loss 1278.5279541015625\n",
      "epoch 48, val_loss 455.8185119628906\n",
      "epoch 49, train_loss 1278.5274658203125\n",
      "epoch 49, val_loss 455.8183288574219\n",
      "epoch 50, train_loss 1278.5272216796875\n",
      "epoch 50, val_loss 455.818115234375\n",
      "epoch 51, train_loss 1278.5267333984375\n",
      "epoch 51, val_loss 455.8179016113281\n",
      "epoch 52, train_loss 1278.5263671875\n",
      "epoch 52, val_loss 455.8177490234375\n",
      "epoch 53, train_loss 1278.52587890625\n",
      "epoch 53, val_loss 455.8175964355469\n",
      "epoch 54, train_loss 1278.5255126953125\n",
      "epoch 54, val_loss 455.8173828125\n",
      "epoch 55, train_loss 1278.525390625\n",
      "epoch 55, val_loss 455.8171691894531\n",
      "epoch 56, train_loss 1278.52490234375\n",
      "epoch 56, val_loss 455.8170471191406\n",
      "epoch 57, train_loss 1278.5245361328125\n",
      "epoch 57, val_loss 455.81683349609375\n",
      "epoch 58, train_loss 1278.524169921875\n",
      "epoch 58, val_loss 455.8166809082031\n",
      "epoch 59, train_loss 1278.52392578125\n",
      "epoch 59, val_loss 455.81640625\n",
      "epoch 60, train_loss 1278.5233154296875\n",
      "epoch 60, val_loss 455.81622314453125\n",
      "epoch 61, train_loss 1278.523193359375\n",
      "epoch 61, val_loss 455.8160705566406\n",
      "epoch 62, train_loss 1278.522705078125\n",
      "epoch 62, val_loss 455.81591796875\n",
      "epoch 63, train_loss 1278.5223388671875\n",
      "epoch 63, val_loss 455.8157043457031\n",
      "epoch 64, train_loss 1278.52197265625\n",
      "epoch 64, val_loss 455.8155517578125\n",
      "epoch 65, train_loss 1278.5216064453125\n",
      "epoch 65, val_loss 455.8153381347656\n",
      "epoch 66, train_loss 1278.5213623046875\n",
      "epoch 66, val_loss 455.81512451171875\n",
      "epoch 67, train_loss 1278.5208740234375\n",
      "epoch 67, val_loss 455.81500244140625\n",
      "epoch 68, train_loss 1278.5206298828125\n",
      "epoch 68, val_loss 455.81475830078125\n",
      "epoch 69, train_loss 1278.520263671875\n",
      "epoch 69, val_loss 455.81463623046875\n",
      "epoch 70, train_loss 1278.519775390625\n",
      "epoch 70, val_loss 455.8144226074219\n",
      "epoch 71, train_loss 1278.51953125\n",
      "epoch 71, val_loss 455.814208984375\n",
      "epoch 72, train_loss 1278.51904296875\n",
      "epoch 72, val_loss 455.8140563964844\n",
      "epoch 73, train_loss 1278.518798828125\n",
      "epoch 73, val_loss 455.8138427734375\n",
      "epoch 74, train_loss 1278.5184326171875\n",
      "epoch 74, val_loss 455.813720703125\n",
      "epoch 75, train_loss 1278.5179443359375\n",
      "epoch 75, val_loss 455.8135070800781\n",
      "epoch 76, train_loss 1278.5177001953125\n",
      "epoch 76, val_loss 455.8133239746094\n",
      "epoch 77, train_loss 1278.5174560546875\n",
      "epoch 77, val_loss 455.81317138671875\n",
      "epoch 78, train_loss 1278.5169677734375\n",
      "epoch 78, val_loss 455.81292724609375\n",
      "epoch 79, train_loss 1278.5164794921875\n",
      "epoch 79, val_loss 455.81280517578125\n",
      "epoch 80, train_loss 1278.51611328125\n",
      "epoch 80, val_loss 455.8125915527344\n",
      "epoch 81, train_loss 1278.515869140625\n",
      "epoch 81, val_loss 455.8123474121094\n",
      "epoch 82, train_loss 1278.515380859375\n",
      "epoch 82, val_loss 455.812255859375\n",
      "epoch 83, train_loss 1278.51513671875\n",
      "epoch 83, val_loss 455.81201171875\n",
      "epoch 84, train_loss 1278.5146484375\n",
      "epoch 84, val_loss 455.81182861328125\n",
      "epoch 85, train_loss 1278.514404296875\n",
      "epoch 85, val_loss 455.8116760253906\n",
      "epoch 86, train_loss 1278.51416015625\n",
      "epoch 86, val_loss 455.8114318847656\n",
      "epoch 87, train_loss 1278.5137939453125\n",
      "epoch 87, val_loss 455.81134033203125\n",
      "epoch 88, train_loss 1278.5133056640625\n",
      "epoch 88, val_loss 455.8111267089844\n",
      "epoch 89, train_loss 1278.512939453125\n",
      "epoch 89, val_loss 455.8109130859375\n",
      "epoch 90, train_loss 1278.5126953125\n",
      "epoch 90, val_loss 455.8106994628906\n",
      "epoch 91, train_loss 1278.512451171875\n",
      "epoch 91, val_loss 455.8105773925781\n",
      "epoch 92, train_loss 1278.5118408203125\n",
      "epoch 92, val_loss 455.81036376953125\n",
      "epoch 93, train_loss 1278.51171875\n",
      "epoch 93, val_loss 455.8101806640625\n",
      "epoch 94, train_loss 1278.5113525390625\n",
      "epoch 94, val_loss 455.80999755859375\n",
      "epoch 95, train_loss 1278.5108642578125\n",
      "epoch 95, val_loss 455.8097839355469\n",
      "epoch 96, train_loss 1278.5106201171875\n",
      "epoch 96, val_loss 455.8096618652344\n",
      "epoch 97, train_loss 1278.5101318359375\n",
      "epoch 97, val_loss 455.8094482421875\n",
      "epoch 98, train_loss 1278.509765625\n",
      "epoch 98, val_loss 455.80926513671875\n",
      "epoch 99, train_loss 1278.50927734375\n",
      "epoch 99, val_loss 455.80908203125\n",
      "Parameter containing:\n",
      "tensor([1.4486e-05], requires_grad=True)\n",
      "iter 22, train_loss_regularization 1.0752794742584229\n",
      "iter 22, val_loss_regularization 1.0752794742584229\n",
      "epoch 0, train_loss 1278.509033203125\n",
      "epoch 0, val_loss 455.8089294433594\n",
      "epoch 1, train_loss 1278.5087890625\n",
      "epoch 1, val_loss 455.80877685546875\n",
      "epoch 2, train_loss 1278.5084228515625\n",
      "epoch 2, val_loss 455.80853271484375\n",
      "epoch 3, train_loss 1278.508056640625\n",
      "epoch 3, val_loss 455.8083801269531\n",
      "epoch 4, train_loss 1278.5076904296875\n",
      "epoch 4, val_loss 455.8081359863281\n",
      "epoch 5, train_loss 1278.5074462890625\n",
      "epoch 5, val_loss 455.80804443359375\n",
      "epoch 6, train_loss 1278.5068359375\n",
      "epoch 6, val_loss 455.80780029296875\n",
      "epoch 7, train_loss 1278.5067138671875\n",
      "epoch 7, val_loss 455.80767822265625\n",
      "epoch 8, train_loss 1278.5064697265625\n",
      "epoch 8, val_loss 455.8074645996094\n",
      "epoch 9, train_loss 1278.5059814453125\n",
      "epoch 9, val_loss 455.8072509765625\n",
      "epoch 10, train_loss 1278.5054931640625\n",
      "epoch 10, val_loss 455.80712890625\n",
      "epoch 11, train_loss 1278.50537109375\n",
      "epoch 11, val_loss 455.806884765625\n",
      "epoch 12, train_loss 1278.5050048828125\n",
      "epoch 12, val_loss 455.8067626953125\n",
      "epoch 13, train_loss 1278.504638671875\n",
      "epoch 13, val_loss 455.80657958984375\n",
      "epoch 14, train_loss 1278.504150390625\n",
      "epoch 14, val_loss 455.8063659667969\n",
      "epoch 15, train_loss 1278.50390625\n",
      "epoch 15, val_loss 455.80621337890625\n",
      "epoch 16, train_loss 1278.503662109375\n",
      "epoch 16, val_loss 455.80596923828125\n",
      "epoch 17, train_loss 1278.503173828125\n",
      "epoch 17, val_loss 455.8058776855469\n",
      "epoch 18, train_loss 1278.5028076171875\n",
      "epoch 18, val_loss 455.8056640625\n",
      "epoch 19, train_loss 1278.50244140625\n",
      "epoch 19, val_loss 455.8054504394531\n",
      "epoch 20, train_loss 1278.502197265625\n",
      "epoch 20, val_loss 455.8053283691406\n",
      "epoch 21, train_loss 1278.5018310546875\n",
      "epoch 21, val_loss 455.80511474609375\n",
      "epoch 22, train_loss 1278.50146484375\n",
      "epoch 22, val_loss 455.804931640625\n",
      "epoch 23, train_loss 1278.501220703125\n",
      "epoch 23, val_loss 455.80474853515625\n",
      "epoch 24, train_loss 1278.5006103515625\n",
      "epoch 24, val_loss 455.8045959472656\n",
      "epoch 25, train_loss 1278.5003662109375\n",
      "epoch 25, val_loss 455.8044128417969\n",
      "epoch 26, train_loss 1278.5001220703125\n",
      "epoch 26, val_loss 455.80426025390625\n",
      "epoch 27, train_loss 1278.4996337890625\n",
      "epoch 27, val_loss 455.8040466308594\n",
      "epoch 28, train_loss 1278.49951171875\n",
      "epoch 28, val_loss 455.8038635253906\n",
      "epoch 29, train_loss 1278.4989013671875\n",
      "epoch 29, val_loss 455.8036804199219\n",
      "epoch 30, train_loss 1278.4986572265625\n",
      "epoch 30, val_loss 455.8034973144531\n",
      "epoch 31, train_loss 1278.498291015625\n",
      "epoch 31, val_loss 455.8033447265625\n",
      "epoch 32, train_loss 1278.498046875\n",
      "epoch 32, val_loss 455.80316162109375\n",
      "epoch 33, train_loss 1278.4976806640625\n",
      "epoch 33, val_loss 455.8029479980469\n",
      "epoch 34, train_loss 1278.4971923828125\n",
      "epoch 34, val_loss 455.8027648925781\n",
      "epoch 35, train_loss 1278.4969482421875\n",
      "epoch 35, val_loss 455.8026123046875\n",
      "epoch 36, train_loss 1278.49658203125\n",
      "epoch 36, val_loss 455.80242919921875\n",
      "epoch 37, train_loss 1278.4962158203125\n",
      "epoch 37, val_loss 455.8022766113281\n",
      "epoch 38, train_loss 1278.4959716796875\n",
      "epoch 38, val_loss 455.8020324707031\n",
      "epoch 39, train_loss 1278.49560546875\n",
      "epoch 39, val_loss 455.8019104003906\n",
      "epoch 40, train_loss 1278.4952392578125\n",
      "epoch 40, val_loss 455.80169677734375\n",
      "epoch 41, train_loss 1278.494873046875\n",
      "epoch 41, val_loss 455.8015441894531\n",
      "epoch 42, train_loss 1278.49462890625\n",
      "epoch 42, val_loss 455.80133056640625\n",
      "epoch 43, train_loss 1278.4942626953125\n",
      "epoch 43, val_loss 455.80120849609375\n",
      "epoch 44, train_loss 1278.493896484375\n",
      "epoch 44, val_loss 455.80096435546875\n",
      "epoch 45, train_loss 1278.49365234375\n",
      "epoch 45, val_loss 455.80084228515625\n",
      "epoch 46, train_loss 1278.4930419921875\n",
      "epoch 46, val_loss 455.8006286621094\n",
      "epoch 47, train_loss 1278.4927978515625\n",
      "epoch 47, val_loss 455.8004455566406\n",
      "epoch 48, train_loss 1278.4925537109375\n",
      "epoch 48, val_loss 455.8003234863281\n",
      "epoch 49, train_loss 1278.4921875\n",
      "epoch 49, val_loss 455.8001708984375\n",
      "epoch 50, train_loss 1278.49169921875\n",
      "epoch 50, val_loss 455.7999267578125\n",
      "epoch 51, train_loss 1278.491455078125\n",
      "epoch 51, val_loss 455.7998352050781\n",
      "epoch 52, train_loss 1278.4912109375\n",
      "epoch 52, val_loss 455.7995910644531\n",
      "epoch 53, train_loss 1278.49072265625\n",
      "epoch 53, val_loss 455.7994689941406\n",
      "epoch 54, train_loss 1278.490478515625\n",
      "epoch 54, val_loss 455.7992858886719\n",
      "epoch 55, train_loss 1278.489990234375\n",
      "epoch 55, val_loss 455.799072265625\n",
      "epoch 56, train_loss 1278.4898681640625\n",
      "epoch 56, val_loss 455.7989196777344\n",
      "epoch 57, train_loss 1278.4893798828125\n",
      "epoch 57, val_loss 455.7987365722656\n",
      "epoch 58, train_loss 1278.489013671875\n",
      "epoch 58, val_loss 455.7985534667969\n",
      "epoch 59, train_loss 1278.48876953125\n",
      "epoch 59, val_loss 455.7983703613281\n",
      "epoch 60, train_loss 1278.488525390625\n",
      "epoch 60, val_loss 455.7982177734375\n",
      "epoch 61, train_loss 1278.4881591796875\n",
      "epoch 61, val_loss 455.7979431152344\n",
      "epoch 62, train_loss 1278.48779296875\n",
      "epoch 62, val_loss 455.7978820800781\n",
      "epoch 63, train_loss 1278.487548828125\n",
      "epoch 63, val_loss 455.7976379394531\n",
      "epoch 64, train_loss 1278.487060546875\n",
      "epoch 64, val_loss 455.7974853515625\n",
      "epoch 65, train_loss 1278.4866943359375\n",
      "epoch 65, val_loss 455.7972412109375\n",
      "epoch 66, train_loss 1278.4864501953125\n",
      "epoch 66, val_loss 455.797119140625\n",
      "epoch 67, train_loss 1278.486083984375\n",
      "epoch 67, val_loss 455.7969665527344\n",
      "epoch 68, train_loss 1278.4859619140625\n",
      "epoch 68, val_loss 455.7967834472656\n",
      "epoch 69, train_loss 1278.4854736328125\n",
      "epoch 69, val_loss 455.79656982421875\n",
      "epoch 70, train_loss 1278.4852294921875\n",
      "epoch 70, val_loss 455.79638671875\n",
      "epoch 71, train_loss 1278.484619140625\n",
      "epoch 71, val_loss 455.7962646484375\n",
      "epoch 72, train_loss 1278.484375\n",
      "epoch 72, val_loss 455.79608154296875\n",
      "epoch 73, train_loss 1278.4840087890625\n",
      "epoch 73, val_loss 455.7959289550781\n",
      "epoch 74, train_loss 1278.483642578125\n",
      "epoch 74, val_loss 455.7957458496094\n",
      "epoch 75, train_loss 1278.4833984375\n",
      "epoch 75, val_loss 455.7955322265625\n",
      "epoch 76, train_loss 1278.4830322265625\n",
      "epoch 76, val_loss 455.7953796386719\n",
      "epoch 77, train_loss 1278.4827880859375\n",
      "epoch 77, val_loss 455.795166015625\n",
      "epoch 78, train_loss 1278.4825439453125\n",
      "epoch 78, val_loss 455.7950439453125\n",
      "epoch 79, train_loss 1278.4820556640625\n",
      "epoch 79, val_loss 455.7948303222656\n",
      "epoch 80, train_loss 1278.481689453125\n",
      "epoch 80, val_loss 455.7947082519531\n",
      "epoch 81, train_loss 1278.4814453125\n",
      "epoch 81, val_loss 455.7944030761719\n",
      "epoch 82, train_loss 1278.481201171875\n",
      "epoch 82, val_loss 455.7943420410156\n",
      "epoch 83, train_loss 1278.480712890625\n",
      "epoch 83, val_loss 455.79412841796875\n",
      "epoch 84, train_loss 1278.48046875\n",
      "epoch 84, val_loss 455.7939453125\n",
      "epoch 85, train_loss 1278.480224609375\n",
      "epoch 85, val_loss 455.7937927246094\n",
      "epoch 86, train_loss 1278.479736328125\n",
      "epoch 86, val_loss 455.7935791015625\n",
      "epoch 87, train_loss 1278.4794921875\n",
      "epoch 87, val_loss 455.7934265136719\n",
      "epoch 88, train_loss 1278.47900390625\n",
      "epoch 88, val_loss 455.7932434082031\n",
      "epoch 89, train_loss 1278.4786376953125\n",
      "epoch 89, val_loss 455.7930908203125\n",
      "epoch 90, train_loss 1278.4783935546875\n",
      "epoch 90, val_loss 455.7928771972656\n",
      "epoch 91, train_loss 1278.47802734375\n",
      "epoch 91, val_loss 455.7927551269531\n",
      "epoch 92, train_loss 1278.477783203125\n",
      "epoch 92, val_loss 455.79254150390625\n",
      "epoch 93, train_loss 1278.4774169921875\n",
      "epoch 93, val_loss 455.7923889160156\n",
      "epoch 94, train_loss 1278.4771728515625\n",
      "epoch 94, val_loss 455.792236328125\n",
      "epoch 95, train_loss 1278.476806640625\n",
      "epoch 95, val_loss 455.7920837402344\n",
      "epoch 96, train_loss 1278.4764404296875\n",
      "epoch 96, val_loss 455.7918701171875\n",
      "epoch 97, train_loss 1278.4761962890625\n",
      "epoch 97, val_loss 455.7917175292969\n",
      "epoch 98, train_loss 1278.4759521484375\n",
      "epoch 98, val_loss 455.7915344238281\n",
      "epoch 99, train_loss 1278.4754638671875\n",
      "epoch 99, val_loss 455.7913818359375\n",
      "Parameter containing:\n",
      "tensor([9.1003e-06], requires_grad=True)\n",
      "iter 23, train_loss_regularization 1.0676445960998535\n",
      "iter 23, val_loss_regularization 1.0676445960998535\n",
      "epoch 0, train_loss 1278.47509765625\n",
      "epoch 0, val_loss 455.79119873046875\n",
      "epoch 1, train_loss 1278.474853515625\n",
      "epoch 1, val_loss 455.7909851074219\n",
      "epoch 2, train_loss 1278.4744873046875\n",
      "epoch 2, val_loss 455.7908020019531\n",
      "epoch 3, train_loss 1278.47412109375\n",
      "epoch 3, val_loss 455.7906799316406\n",
      "epoch 4, train_loss 1278.473876953125\n",
      "epoch 4, val_loss 455.79046630859375\n",
      "epoch 5, train_loss 1278.473388671875\n",
      "epoch 5, val_loss 455.79034423828125\n",
      "epoch 6, train_loss 1278.4730224609375\n",
      "epoch 6, val_loss 455.7901306152344\n",
      "epoch 7, train_loss 1278.4727783203125\n",
      "epoch 7, val_loss 455.7899475097656\n",
      "epoch 8, train_loss 1278.47265625\n",
      "epoch 8, val_loss 455.7897644042969\n",
      "epoch 9, train_loss 1278.4722900390625\n",
      "epoch 9, val_loss 455.78961181640625\n",
      "epoch 10, train_loss 1278.4718017578125\n",
      "epoch 10, val_loss 455.78948974609375\n",
      "epoch 11, train_loss 1278.4715576171875\n",
      "epoch 11, val_loss 455.789306640625\n",
      "epoch 12, train_loss 1278.47119140625\n",
      "epoch 12, val_loss 455.78912353515625\n",
      "epoch 13, train_loss 1278.470703125\n",
      "epoch 13, val_loss 455.7889099121094\n",
      "epoch 14, train_loss 1278.470458984375\n",
      "epoch 14, val_loss 455.7887878417969\n",
      "epoch 15, train_loss 1278.47021484375\n",
      "epoch 15, val_loss 455.78857421875\n",
      "epoch 16, train_loss 1278.469970703125\n",
      "epoch 16, val_loss 455.7884216308594\n",
      "epoch 17, train_loss 1278.4697265625\n",
      "epoch 17, val_loss 455.7882080078125\n",
      "epoch 18, train_loss 1278.4693603515625\n",
      "epoch 18, val_loss 455.7880554199219\n",
      "epoch 19, train_loss 1278.4688720703125\n",
      "epoch 19, val_loss 455.787841796875\n",
      "epoch 20, train_loss 1278.468505859375\n",
      "epoch 20, val_loss 455.78765869140625\n",
      "epoch 21, train_loss 1278.46826171875\n",
      "epoch 21, val_loss 455.7875061035156\n",
      "epoch 22, train_loss 1278.4681396484375\n",
      "epoch 22, val_loss 455.7873229980469\n",
      "epoch 23, train_loss 1278.467529296875\n",
      "epoch 23, val_loss 455.78717041015625\n",
      "epoch 24, train_loss 1278.4674072265625\n",
      "epoch 24, val_loss 455.7869873046875\n",
      "epoch 25, train_loss 1278.467041015625\n",
      "epoch 25, val_loss 455.786865234375\n",
      "epoch 26, train_loss 1278.466796875\n",
      "epoch 26, val_loss 455.7867126464844\n",
      "epoch 27, train_loss 1278.46630859375\n",
      "epoch 27, val_loss 455.7864990234375\n",
      "epoch 28, train_loss 1278.4659423828125\n",
      "epoch 28, val_loss 455.7863464355469\n",
      "epoch 29, train_loss 1278.4656982421875\n",
      "epoch 29, val_loss 455.7861633300781\n",
      "epoch 30, train_loss 1278.46533203125\n",
      "epoch 30, val_loss 455.7860107421875\n",
      "epoch 31, train_loss 1278.465087890625\n",
      "epoch 31, val_loss 455.7857971191406\n",
      "epoch 32, train_loss 1278.4647216796875\n",
      "epoch 32, val_loss 455.7856750488281\n",
      "epoch 33, train_loss 1278.4644775390625\n",
      "epoch 33, val_loss 455.7854309082031\n",
      "epoch 34, train_loss 1278.4638671875\n",
      "epoch 34, val_loss 455.78533935546875\n",
      "epoch 35, train_loss 1278.4638671875\n",
      "epoch 35, val_loss 455.7851257324219\n",
      "epoch 36, train_loss 1278.4635009765625\n",
      "epoch 36, val_loss 455.78497314453125\n",
      "epoch 37, train_loss 1278.463134765625\n",
      "epoch 37, val_loss 455.7847900390625\n",
      "epoch 38, train_loss 1278.462890625\n",
      "epoch 38, val_loss 455.7845764160156\n",
      "epoch 39, train_loss 1278.462646484375\n",
      "epoch 39, val_loss 455.784423828125\n",
      "epoch 40, train_loss 1278.4622802734375\n",
      "epoch 40, val_loss 455.7843017578125\n",
      "epoch 41, train_loss 1278.4617919921875\n",
      "epoch 41, val_loss 455.7841796875\n",
      "epoch 42, train_loss 1278.4615478515625\n",
      "epoch 42, val_loss 455.7839050292969\n",
      "epoch 43, train_loss 1278.4613037109375\n",
      "epoch 43, val_loss 455.7838439941406\n",
      "epoch 44, train_loss 1278.4609375\n",
      "epoch 44, val_loss 455.78363037109375\n",
      "epoch 45, train_loss 1278.4605712890625\n",
      "epoch 45, val_loss 455.78338623046875\n",
      "epoch 46, train_loss 1278.460205078125\n",
      "epoch 46, val_loss 455.78326416015625\n",
      "epoch 47, train_loss 1278.4599609375\n",
      "epoch 47, val_loss 455.7830810546875\n",
      "epoch 48, train_loss 1278.459716796875\n",
      "epoch 48, val_loss 455.7828674316406\n",
      "epoch 49, train_loss 1278.459228515625\n",
      "epoch 49, val_loss 455.7827453613281\n",
      "epoch 50, train_loss 1278.4591064453125\n",
      "epoch 50, val_loss 455.7825927734375\n",
      "epoch 51, train_loss 1278.458740234375\n",
      "epoch 51, val_loss 455.7823181152344\n",
      "epoch 52, train_loss 1278.4583740234375\n",
      "epoch 52, val_loss 455.78216552734375\n",
      "epoch 53, train_loss 1278.4580078125\n",
      "epoch 53, val_loss 455.7820739746094\n",
      "epoch 54, train_loss 1278.457763671875\n",
      "epoch 54, val_loss 455.7818603515625\n",
      "epoch 55, train_loss 1278.4573974609375\n",
      "epoch 55, val_loss 455.7817077636719\n",
      "epoch 56, train_loss 1278.4571533203125\n",
      "epoch 56, val_loss 455.7815856933594\n",
      "epoch 57, train_loss 1278.456787109375\n",
      "epoch 57, val_loss 455.7813720703125\n",
      "epoch 58, train_loss 1278.456298828125\n",
      "epoch 58, val_loss 455.7812194824219\n",
      "epoch 59, train_loss 1278.4561767578125\n",
      "epoch 59, val_loss 455.7810363769531\n",
      "epoch 60, train_loss 1278.455810546875\n",
      "epoch 60, val_loss 455.78082275390625\n",
      "epoch 61, train_loss 1278.45556640625\n",
      "epoch 61, val_loss 455.7806701660156\n",
      "epoch 62, train_loss 1278.455078125\n",
      "epoch 62, val_loss 455.7804870605469\n",
      "epoch 63, train_loss 1278.4549560546875\n",
      "epoch 63, val_loss 455.7803039550781\n",
      "epoch 64, train_loss 1278.45458984375\n",
      "epoch 64, val_loss 455.7801818847656\n",
      "epoch 65, train_loss 1278.4542236328125\n",
      "epoch 65, val_loss 455.7799987792969\n",
      "epoch 66, train_loss 1278.453857421875\n",
      "epoch 66, val_loss 455.77984619140625\n",
      "epoch 67, train_loss 1278.45361328125\n",
      "epoch 67, val_loss 455.7796630859375\n",
      "epoch 68, train_loss 1278.4532470703125\n",
      "epoch 68, val_loss 455.779541015625\n",
      "epoch 69, train_loss 1278.452880859375\n",
      "epoch 69, val_loss 455.7793273925781\n",
      "epoch 70, train_loss 1278.4527587890625\n",
      "epoch 70, val_loss 455.7791748046875\n",
      "epoch 71, train_loss 1278.452392578125\n",
      "epoch 71, val_loss 455.7789611816406\n",
      "epoch 72, train_loss 1278.4520263671875\n",
      "epoch 72, val_loss 455.7788391113281\n",
      "epoch 73, train_loss 1278.4517822265625\n",
      "epoch 73, val_loss 455.7786560058594\n",
      "epoch 74, train_loss 1278.4515380859375\n",
      "epoch 74, val_loss 455.77850341796875\n",
      "epoch 75, train_loss 1278.4512939453125\n",
      "epoch 75, val_loss 455.7782897949219\n",
      "epoch 76, train_loss 1278.450927734375\n",
      "epoch 76, val_loss 455.77813720703125\n",
      "epoch 77, train_loss 1278.450439453125\n",
      "epoch 77, val_loss 455.7779541015625\n",
      "epoch 78, train_loss 1278.4500732421875\n",
      "epoch 78, val_loss 455.7778015136719\n",
      "epoch 79, train_loss 1278.4498291015625\n",
      "epoch 79, val_loss 455.7777099609375\n",
      "epoch 80, train_loss 1278.44970703125\n",
      "epoch 80, val_loss 455.7774963378906\n",
      "epoch 81, train_loss 1278.4493408203125\n",
      "epoch 81, val_loss 455.7772521972656\n",
      "epoch 82, train_loss 1278.448974609375\n",
      "epoch 82, val_loss 455.77716064453125\n",
      "epoch 83, train_loss 1278.448486328125\n",
      "epoch 83, val_loss 455.7769470214844\n",
      "epoch 84, train_loss 1278.4483642578125\n",
      "epoch 84, val_loss 455.77679443359375\n",
      "epoch 85, train_loss 1278.4481201171875\n",
      "epoch 85, val_loss 455.7765808105469\n",
      "epoch 86, train_loss 1278.44775390625\n",
      "epoch 86, val_loss 455.7764587402344\n",
      "epoch 87, train_loss 1278.4473876953125\n",
      "epoch 87, val_loss 455.7763366699219\n",
      "epoch 88, train_loss 1278.4471435546875\n",
      "epoch 88, val_loss 455.7760925292969\n",
      "epoch 89, train_loss 1278.44677734375\n",
      "epoch 89, val_loss 455.7760009765625\n",
      "epoch 90, train_loss 1278.446533203125\n",
      "epoch 90, val_loss 455.7758483886719\n",
      "epoch 91, train_loss 1278.4461669921875\n",
      "epoch 91, val_loss 455.775634765625\n",
      "epoch 92, train_loss 1278.44580078125\n",
      "epoch 92, val_loss 455.77545166015625\n",
      "epoch 93, train_loss 1278.445556640625\n",
      "epoch 93, val_loss 455.77532958984375\n",
      "epoch 94, train_loss 1278.4451904296875\n",
      "epoch 94, val_loss 455.7751159667969\n",
      "epoch 95, train_loss 1278.44482421875\n",
      "epoch 95, val_loss 455.77496337890625\n",
      "epoch 96, train_loss 1278.444580078125\n",
      "epoch 96, val_loss 455.7747497558594\n",
      "epoch 97, train_loss 1278.4444580078125\n",
      "epoch 97, val_loss 455.774658203125\n",
      "epoch 98, train_loss 1278.444091796875\n",
      "epoch 98, val_loss 455.7744140625\n",
      "epoch 99, train_loss 1278.4434814453125\n",
      "epoch 99, val_loss 455.7742614746094\n",
      "Parameter containing:\n",
      "tensor([5.7201e-06], requires_grad=True)\n",
      "iter 24, train_loss_regularization 1.0601866245269775\n",
      "iter 24, val_loss_regularization 1.0601866245269775\n",
      "epoch 0, train_loss 1278.4432373046875\n",
      "epoch 0, val_loss 455.7740783691406\n",
      "epoch 1, train_loss 1278.443115234375\n",
      "epoch 1, val_loss 455.77392578125\n",
      "epoch 2, train_loss 1278.4427490234375\n",
      "epoch 2, val_loss 455.7738342285156\n",
      "epoch 3, train_loss 1278.4423828125\n",
      "epoch 3, val_loss 455.77362060546875\n",
      "epoch 4, train_loss 1278.4422607421875\n",
      "epoch 4, val_loss 455.7734680175781\n",
      "epoch 5, train_loss 1278.4417724609375\n",
      "epoch 5, val_loss 455.7732849121094\n",
      "epoch 6, train_loss 1278.4415283203125\n",
      "epoch 6, val_loss 455.77313232421875\n",
      "epoch 7, train_loss 1278.441162109375\n",
      "epoch 7, val_loss 455.77288818359375\n",
      "epoch 8, train_loss 1278.44091796875\n",
      "epoch 8, val_loss 455.7727355957031\n",
      "epoch 9, train_loss 1278.440673828125\n",
      "epoch 9, val_loss 455.7725830078125\n",
      "epoch 10, train_loss 1278.4404296875\n",
      "epoch 10, val_loss 455.7724609375\n",
      "epoch 11, train_loss 1278.43994140625\n",
      "epoch 11, val_loss 455.7723388671875\n",
      "epoch 12, train_loss 1278.439697265625\n",
      "epoch 12, val_loss 455.7720947265625\n",
      "epoch 13, train_loss 1278.439453125\n",
      "epoch 13, val_loss 455.77197265625\n",
      "epoch 14, train_loss 1278.43896484375\n",
      "epoch 14, val_loss 455.77178955078125\n",
      "epoch 15, train_loss 1278.4388427734375\n",
      "epoch 15, val_loss 455.7716369628906\n",
      "epoch 16, train_loss 1278.4384765625\n",
      "epoch 16, val_loss 455.7714538574219\n",
      "epoch 17, train_loss 1278.4381103515625\n",
      "epoch 17, val_loss 455.77130126953125\n",
      "epoch 18, train_loss 1278.4378662109375\n",
      "epoch 18, val_loss 455.7711181640625\n",
      "epoch 19, train_loss 1278.4375\n",
      "epoch 19, val_loss 455.7709045410156\n",
      "epoch 20, train_loss 1278.437255859375\n",
      "epoch 20, val_loss 455.770751953125\n",
      "epoch 21, train_loss 1278.43701171875\n",
      "epoch 21, val_loss 455.7706298828125\n",
      "epoch 22, train_loss 1278.4366455078125\n",
      "epoch 22, val_loss 455.77044677734375\n",
      "epoch 23, train_loss 1278.4364013671875\n",
      "epoch 23, val_loss 455.770263671875\n",
      "epoch 24, train_loss 1278.4361572265625\n",
      "epoch 24, val_loss 455.7701110839844\n",
      "epoch 25, train_loss 1278.435791015625\n",
      "epoch 25, val_loss 455.76995849609375\n",
      "epoch 26, train_loss 1278.435546875\n",
      "epoch 26, val_loss 455.7697448730469\n",
      "epoch 27, train_loss 1278.4351806640625\n",
      "epoch 27, val_loss 455.7696228027344\n",
      "epoch 28, train_loss 1278.434814453125\n",
      "epoch 28, val_loss 455.7693786621094\n",
      "epoch 29, train_loss 1278.4344482421875\n",
      "epoch 29, val_loss 455.7692565917969\n",
      "epoch 30, train_loss 1278.434326171875\n",
      "epoch 30, val_loss 455.7690734863281\n",
      "epoch 31, train_loss 1278.4339599609375\n",
      "epoch 31, val_loss 455.7689208984375\n",
      "epoch 32, train_loss 1278.433837890625\n",
      "epoch 32, val_loss 455.7688293457031\n",
      "epoch 33, train_loss 1278.4332275390625\n",
      "epoch 33, val_loss 455.7686767578125\n",
      "epoch 34, train_loss 1278.4329833984375\n",
      "epoch 34, val_loss 455.7684631347656\n",
      "epoch 35, train_loss 1278.432861328125\n",
      "epoch 35, val_loss 455.7682800292969\n",
      "epoch 36, train_loss 1278.4324951171875\n",
      "epoch 36, val_loss 455.76812744140625\n",
      "epoch 37, train_loss 1278.4322509765625\n",
      "epoch 37, val_loss 455.7679443359375\n",
      "epoch 38, train_loss 1278.431884765625\n",
      "epoch 38, val_loss 455.76776123046875\n",
      "epoch 39, train_loss 1278.431640625\n",
      "epoch 39, val_loss 455.767578125\n",
      "epoch 40, train_loss 1278.43115234375\n",
      "epoch 40, val_loss 455.7674560546875\n",
      "epoch 41, train_loss 1278.430908203125\n",
      "epoch 41, val_loss 455.7673034667969\n",
      "epoch 42, train_loss 1278.4307861328125\n",
      "epoch 42, val_loss 455.7671813964844\n",
      "epoch 43, train_loss 1278.4302978515625\n",
      "epoch 43, val_loss 455.7669982910156\n",
      "epoch 44, train_loss 1278.4300537109375\n",
      "epoch 44, val_loss 455.76678466796875\n",
      "epoch 45, train_loss 1278.429931640625\n",
      "epoch 45, val_loss 455.7666320800781\n",
      "epoch 46, train_loss 1278.429443359375\n",
      "epoch 46, val_loss 455.7665100097656\n",
      "epoch 47, train_loss 1278.4290771484375\n",
      "epoch 47, val_loss 455.7663269042969\n",
      "epoch 48, train_loss 1278.4288330078125\n",
      "epoch 48, val_loss 455.76611328125\n",
      "epoch 49, train_loss 1278.4285888671875\n",
      "epoch 49, val_loss 455.7659912109375\n",
      "epoch 50, train_loss 1278.42822265625\n",
      "epoch 50, val_loss 455.7657775878906\n",
      "epoch 51, train_loss 1278.427978515625\n",
      "epoch 51, val_loss 455.7656555175781\n",
      "epoch 52, train_loss 1278.427734375\n",
      "epoch 52, val_loss 455.7655334472656\n",
      "epoch 53, train_loss 1278.4273681640625\n",
      "epoch 53, val_loss 455.76531982421875\n",
      "epoch 54, train_loss 1278.4271240234375\n",
      "epoch 54, val_loss 455.76513671875\n",
      "epoch 55, train_loss 1278.4268798828125\n",
      "epoch 55, val_loss 455.7650146484375\n",
      "epoch 56, train_loss 1278.426513671875\n",
      "epoch 56, val_loss 455.76483154296875\n",
      "epoch 57, train_loss 1278.426025390625\n",
      "epoch 57, val_loss 455.7646789550781\n",
      "epoch 58, train_loss 1278.4259033203125\n",
      "epoch 58, val_loss 455.7644958496094\n",
      "epoch 59, train_loss 1278.425537109375\n",
      "epoch 59, val_loss 455.7642822265625\n",
      "epoch 60, train_loss 1278.42529296875\n",
      "epoch 60, val_loss 455.76416015625\n",
      "epoch 61, train_loss 1278.425048828125\n",
      "epoch 61, val_loss 455.7640380859375\n",
      "epoch 62, train_loss 1278.4248046875\n",
      "epoch 62, val_loss 455.7638854980469\n",
      "epoch 63, train_loss 1278.4244384765625\n",
      "epoch 63, val_loss 455.763671875\n",
      "epoch 64, train_loss 1278.424072265625\n",
      "epoch 64, val_loss 455.76348876953125\n",
      "epoch 65, train_loss 1278.4237060546875\n",
      "epoch 65, val_loss 455.7633361816406\n",
      "epoch 66, train_loss 1278.4234619140625\n",
      "epoch 66, val_loss 455.76312255859375\n",
      "epoch 67, train_loss 1278.42333984375\n",
      "epoch 67, val_loss 455.7630310058594\n",
      "epoch 68, train_loss 1278.4229736328125\n",
      "epoch 68, val_loss 455.76287841796875\n",
      "epoch 69, train_loss 1278.422607421875\n",
      "epoch 69, val_loss 455.76263427734375\n",
      "epoch 70, train_loss 1278.42236328125\n",
      "epoch 70, val_loss 455.7625427246094\n",
      "epoch 71, train_loss 1278.422119140625\n",
      "epoch 71, val_loss 455.7623291015625\n",
      "epoch 72, train_loss 1278.4217529296875\n",
      "epoch 72, val_loss 455.76220703125\n",
      "epoch 73, train_loss 1278.42138671875\n",
      "epoch 73, val_loss 455.7619934082031\n",
      "epoch 74, train_loss 1278.421142578125\n",
      "epoch 74, val_loss 455.7618408203125\n",
      "epoch 75, train_loss 1278.4207763671875\n",
      "epoch 75, val_loss 455.76165771484375\n",
      "epoch 76, train_loss 1278.420654296875\n",
      "epoch 76, val_loss 455.7615051269531\n",
      "epoch 77, train_loss 1278.420166015625\n",
      "epoch 77, val_loss 455.7613220214844\n",
      "epoch 78, train_loss 1278.4200439453125\n",
      "epoch 78, val_loss 455.7611999511719\n",
      "epoch 79, train_loss 1278.4197998046875\n",
      "epoch 79, val_loss 455.76104736328125\n",
      "epoch 80, train_loss 1278.41943359375\n",
      "epoch 80, val_loss 455.7609558105469\n",
      "epoch 81, train_loss 1278.4190673828125\n",
      "epoch 81, val_loss 455.7607116699219\n",
      "epoch 82, train_loss 1278.418701171875\n",
      "epoch 82, val_loss 455.7605895996094\n",
      "epoch 83, train_loss 1278.4183349609375\n",
      "epoch 83, val_loss 455.7603759765625\n",
      "epoch 84, train_loss 1278.418212890625\n",
      "epoch 84, val_loss 455.7602233886719\n",
      "epoch 85, train_loss 1278.4178466796875\n",
      "epoch 85, val_loss 455.7600402832031\n",
      "epoch 86, train_loss 1278.41748046875\n",
      "epoch 86, val_loss 455.7599182128906\n",
      "epoch 87, train_loss 1278.4173583984375\n",
      "epoch 87, val_loss 455.7597351074219\n",
      "epoch 88, train_loss 1278.4171142578125\n",
      "epoch 88, val_loss 455.75958251953125\n",
      "epoch 89, train_loss 1278.416748046875\n",
      "epoch 89, val_loss 455.7594299316406\n",
      "epoch 90, train_loss 1278.4163818359375\n",
      "epoch 90, val_loss 455.7592468261719\n",
      "epoch 91, train_loss 1278.4161376953125\n",
      "epoch 91, val_loss 455.7591247558594\n",
      "epoch 92, train_loss 1278.415771484375\n",
      "epoch 92, val_loss 455.7589111328125\n",
      "epoch 93, train_loss 1278.41552734375\n",
      "epoch 93, val_loss 455.7587890625\n",
      "epoch 94, train_loss 1278.415283203125\n",
      "epoch 94, val_loss 455.7585754394531\n",
      "epoch 95, train_loss 1278.4149169921875\n",
      "epoch 95, val_loss 455.7585144042969\n",
      "epoch 96, train_loss 1278.4146728515625\n",
      "epoch 96, val_loss 455.75830078125\n",
      "epoch 97, train_loss 1278.414306640625\n",
      "epoch 97, val_loss 455.7580871582031\n",
      "epoch 98, train_loss 1278.4140625\n",
      "epoch 98, val_loss 455.7579040527344\n",
      "epoch 99, train_loss 1278.4136962890625\n",
      "epoch 99, val_loss 455.7577819824219\n",
      "Parameter containing:\n",
      "tensor([3.5980e-06], requires_grad=True)\n",
      "iter 25, train_loss_regularization 1.0528936386108398\n",
      "iter 25, val_loss_regularization 1.0528936386108398\n",
      "epoch 0, train_loss 1278.41357421875\n",
      "epoch 0, val_loss 455.757568359375\n",
      "epoch 1, train_loss 1278.413330078125\n",
      "epoch 1, val_loss 455.7574462890625\n",
      "epoch 2, train_loss 1278.4129638671875\n",
      "epoch 2, val_loss 455.7572937011719\n",
      "epoch 3, train_loss 1278.4127197265625\n",
      "epoch 3, val_loss 455.7571105957031\n",
      "epoch 4, train_loss 1278.412353515625\n",
      "epoch 4, val_loss 455.7569580078125\n",
      "epoch 5, train_loss 1278.412109375\n",
      "epoch 5, val_loss 455.7568359375\n",
      "epoch 6, train_loss 1278.411865234375\n",
      "epoch 6, val_loss 455.75665283203125\n",
      "epoch 7, train_loss 1278.4114990234375\n",
      "epoch 7, val_loss 455.7564697265625\n",
      "epoch 8, train_loss 1278.4111328125\n",
      "epoch 8, val_loss 455.75628662109375\n",
      "epoch 9, train_loss 1278.410888671875\n",
      "epoch 9, val_loss 455.75616455078125\n",
      "epoch 10, train_loss 1278.41064453125\n",
      "epoch 10, val_loss 455.7559509277344\n",
      "epoch 11, train_loss 1278.4102783203125\n",
      "epoch 11, val_loss 455.7558288574219\n",
      "epoch 12, train_loss 1278.409912109375\n",
      "epoch 12, val_loss 455.7557373046875\n",
      "epoch 13, train_loss 1278.4097900390625\n",
      "epoch 13, val_loss 455.75555419921875\n",
      "epoch 14, train_loss 1278.4095458984375\n",
      "epoch 14, val_loss 455.7553405761719\n",
      "epoch 15, train_loss 1278.4091796875\n",
      "epoch 15, val_loss 455.7552490234375\n",
      "epoch 16, train_loss 1278.4088134765625\n",
      "epoch 16, val_loss 455.7550048828125\n",
      "epoch 17, train_loss 1278.40869140625\n",
      "epoch 17, val_loss 455.75482177734375\n",
      "epoch 18, train_loss 1278.408203125\n",
      "epoch 18, val_loss 455.7546691894531\n",
      "epoch 19, train_loss 1278.407958984375\n",
      "epoch 19, val_loss 455.7545471191406\n",
      "epoch 20, train_loss 1278.40771484375\n",
      "epoch 20, val_loss 455.75445556640625\n",
      "epoch 21, train_loss 1278.407470703125\n",
      "epoch 21, val_loss 455.7542419433594\n",
      "epoch 22, train_loss 1278.4072265625\n",
      "epoch 22, val_loss 455.75408935546875\n",
      "epoch 23, train_loss 1278.4068603515625\n",
      "epoch 23, val_loss 455.7538757324219\n",
      "epoch 24, train_loss 1278.4066162109375\n",
      "epoch 24, val_loss 455.7537536621094\n",
      "epoch 25, train_loss 1278.40625\n",
      "epoch 25, val_loss 455.7535400390625\n",
      "epoch 26, train_loss 1278.4058837890625\n",
      "epoch 26, val_loss 455.75341796875\n",
      "epoch 27, train_loss 1278.40576171875\n",
      "epoch 27, val_loss 455.75323486328125\n",
      "epoch 28, train_loss 1278.405517578125\n",
      "epoch 28, val_loss 455.75311279296875\n",
      "epoch 29, train_loss 1278.4051513671875\n",
      "epoch 29, val_loss 455.75299072265625\n",
      "epoch 30, train_loss 1278.405029296875\n",
      "epoch 30, val_loss 455.75274658203125\n",
      "epoch 31, train_loss 1278.404541015625\n",
      "epoch 31, val_loss 455.7525939941406\n",
      "epoch 32, train_loss 1278.404296875\n",
      "epoch 32, val_loss 455.75238037109375\n",
      "epoch 33, train_loss 1278.404052734375\n",
      "epoch 33, val_loss 455.7522888183594\n",
      "epoch 34, train_loss 1278.4036865234375\n",
      "epoch 34, val_loss 455.7520751953125\n",
      "epoch 35, train_loss 1278.4034423828125\n",
      "epoch 35, val_loss 455.7519226074219\n",
      "epoch 36, train_loss 1278.403076171875\n",
      "epoch 36, val_loss 455.7518005371094\n",
      "epoch 37, train_loss 1278.4029541015625\n",
      "epoch 37, val_loss 455.751708984375\n",
      "epoch 38, train_loss 1278.4024658203125\n",
      "epoch 38, val_loss 455.7514953613281\n",
      "epoch 39, train_loss 1278.4022216796875\n",
      "epoch 39, val_loss 455.7513427734375\n",
      "epoch 40, train_loss 1278.402099609375\n",
      "epoch 40, val_loss 455.75115966796875\n",
      "epoch 41, train_loss 1278.40185546875\n",
      "epoch 41, val_loss 455.7510070800781\n",
      "epoch 42, train_loss 1278.4014892578125\n",
      "epoch 42, val_loss 455.7508239746094\n",
      "epoch 43, train_loss 1278.401123046875\n",
      "epoch 43, val_loss 455.7507019042969\n",
      "epoch 44, train_loss 1278.4010009765625\n",
      "epoch 44, val_loss 455.75054931640625\n",
      "epoch 45, train_loss 1278.400634765625\n",
      "epoch 45, val_loss 455.7503356933594\n",
      "epoch 46, train_loss 1278.400390625\n",
      "epoch 46, val_loss 455.7502136230469\n",
      "epoch 47, train_loss 1278.400146484375\n",
      "epoch 47, val_loss 455.7500305175781\n",
      "epoch 48, train_loss 1278.3997802734375\n",
      "epoch 48, val_loss 455.7498779296875\n",
      "epoch 49, train_loss 1278.3995361328125\n",
      "epoch 49, val_loss 455.7496643066406\n",
      "epoch 50, train_loss 1278.3992919921875\n",
      "epoch 50, val_loss 455.74951171875\n",
      "epoch 51, train_loss 1278.3990478515625\n",
      "epoch 51, val_loss 455.7494201660156\n",
      "epoch 52, train_loss 1278.398681640625\n",
      "epoch 52, val_loss 455.7492370605469\n",
      "epoch 53, train_loss 1278.3983154296875\n",
      "epoch 53, val_loss 455.74908447265625\n",
      "epoch 54, train_loss 1278.3980712890625\n",
      "epoch 54, val_loss 455.7489318847656\n",
      "epoch 55, train_loss 1278.397705078125\n",
      "epoch 55, val_loss 455.748779296875\n",
      "epoch 56, train_loss 1278.3974609375\n",
      "epoch 56, val_loss 455.74859619140625\n",
      "epoch 57, train_loss 1278.397216796875\n",
      "epoch 57, val_loss 455.7483825683594\n",
      "epoch 58, train_loss 1278.39697265625\n",
      "epoch 58, val_loss 455.7483215332031\n",
      "epoch 59, train_loss 1278.3966064453125\n",
      "epoch 59, val_loss 455.7481384277344\n",
      "epoch 60, train_loss 1278.396240234375\n",
      "epoch 60, val_loss 455.7479553222656\n",
      "epoch 61, train_loss 1278.39599609375\n",
      "epoch 61, val_loss 455.74774169921875\n",
      "epoch 62, train_loss 1278.395751953125\n",
      "epoch 62, val_loss 455.74761962890625\n",
      "epoch 63, train_loss 1278.3956298828125\n",
      "epoch 63, val_loss 455.7474670410156\n",
      "epoch 64, train_loss 1278.3953857421875\n",
      "epoch 64, val_loss 455.7473449707031\n",
      "epoch 65, train_loss 1278.3948974609375\n",
      "epoch 65, val_loss 455.7471618652344\n",
      "epoch 66, train_loss 1278.3946533203125\n",
      "epoch 66, val_loss 455.7470397949219\n",
      "epoch 67, train_loss 1278.3944091796875\n",
      "epoch 67, val_loss 455.74688720703125\n",
      "epoch 68, train_loss 1278.39404296875\n",
      "epoch 68, val_loss 455.7467041015625\n",
      "epoch 69, train_loss 1278.3936767578125\n",
      "epoch 69, val_loss 455.7464904785156\n",
      "epoch 70, train_loss 1278.3936767578125\n",
      "epoch 70, val_loss 455.7463684082031\n",
      "epoch 71, train_loss 1278.393310546875\n",
      "epoch 71, val_loss 455.7462158203125\n",
      "epoch 72, train_loss 1278.3929443359375\n",
      "epoch 72, val_loss 455.7460021972656\n",
      "epoch 73, train_loss 1278.3927001953125\n",
      "epoch 73, val_loss 455.7458190917969\n",
      "epoch 74, train_loss 1278.3924560546875\n",
      "epoch 74, val_loss 455.7457580566406\n",
      "epoch 75, train_loss 1278.3922119140625\n",
      "epoch 75, val_loss 455.7455749511719\n",
      "epoch 76, train_loss 1278.391845703125\n",
      "epoch 76, val_loss 455.745361328125\n",
      "epoch 77, train_loss 1278.3916015625\n",
      "epoch 77, val_loss 455.7452392578125\n",
      "epoch 78, train_loss 1278.3912353515625\n",
      "epoch 78, val_loss 455.74505615234375\n",
      "epoch 79, train_loss 1278.39111328125\n",
      "epoch 79, val_loss 455.7449645996094\n",
      "epoch 80, train_loss 1278.3907470703125\n",
      "epoch 80, val_loss 455.7447509765625\n",
      "epoch 81, train_loss 1278.390625\n",
      "epoch 81, val_loss 455.7445983886719\n",
      "epoch 82, train_loss 1278.3902587890625\n",
      "epoch 82, val_loss 455.7445068359375\n",
      "epoch 83, train_loss 1278.389892578125\n",
      "epoch 83, val_loss 455.74432373046875\n",
      "epoch 84, train_loss 1278.3897705078125\n",
      "epoch 84, val_loss 455.7441101074219\n",
      "epoch 85, train_loss 1278.389404296875\n",
      "epoch 85, val_loss 455.74395751953125\n",
      "epoch 86, train_loss 1278.3890380859375\n",
      "epoch 86, val_loss 455.74383544921875\n",
      "epoch 87, train_loss 1278.3887939453125\n",
      "epoch 87, val_loss 455.74365234375\n",
      "epoch 88, train_loss 1278.3885498046875\n",
      "epoch 88, val_loss 455.7434997558594\n",
      "epoch 89, train_loss 1278.3883056640625\n",
      "epoch 89, val_loss 455.74334716796875\n",
      "epoch 90, train_loss 1278.387939453125\n",
      "epoch 90, val_loss 455.7431945800781\n",
      "epoch 91, train_loss 1278.3876953125\n",
      "epoch 91, val_loss 455.7430419921875\n",
      "epoch 92, train_loss 1278.387451171875\n",
      "epoch 92, val_loss 455.7428894042969\n",
      "epoch 93, train_loss 1278.3870849609375\n",
      "epoch 93, val_loss 455.74273681640625\n",
      "epoch 94, train_loss 1278.3868408203125\n",
      "epoch 94, val_loss 455.7425842285156\n",
      "epoch 95, train_loss 1278.386474609375\n",
      "epoch 95, val_loss 455.742431640625\n",
      "epoch 96, train_loss 1278.3863525390625\n",
      "epoch 96, val_loss 455.74224853515625\n",
      "epoch 97, train_loss 1278.3861083984375\n",
      "epoch 97, val_loss 455.7420959472656\n",
      "epoch 98, train_loss 1278.3858642578125\n",
      "epoch 98, val_loss 455.7419128417969\n",
      "epoch 99, train_loss 1278.3856201171875\n",
      "epoch 99, val_loss 455.74176025390625\n",
      "Parameter containing:\n",
      "tensor([2.2652e-06], requires_grad=True)\n",
      "iter 26, train_loss_regularization 1.045755386352539\n",
      "iter 26, val_loss_regularization 1.045755386352539\n",
      "epoch 0, train_loss 1278.3851318359375\n",
      "epoch 0, val_loss 455.7416687011719\n",
      "epoch 1, train_loss 1278.3848876953125\n",
      "epoch 1, val_loss 455.7414245605469\n",
      "epoch 2, train_loss 1278.3846435546875\n",
      "epoch 2, val_loss 455.7412414550781\n",
      "epoch 3, train_loss 1278.38427734375\n",
      "epoch 3, val_loss 455.7411193847656\n",
      "epoch 4, train_loss 1278.38427734375\n",
      "epoch 4, val_loss 455.7409973144531\n",
      "epoch 5, train_loss 1278.3837890625\n",
      "epoch 5, val_loss 455.7407531738281\n",
      "epoch 6, train_loss 1278.383544921875\n",
      "epoch 6, val_loss 455.7405700683594\n",
      "epoch 7, train_loss 1278.38330078125\n",
      "epoch 7, val_loss 455.7405090332031\n",
      "epoch 8, train_loss 1278.383056640625\n",
      "epoch 8, val_loss 455.7403259277344\n",
      "epoch 9, train_loss 1278.3826904296875\n",
      "epoch 9, val_loss 455.74017333984375\n",
      "epoch 10, train_loss 1278.3822021484375\n",
      "epoch 10, val_loss 455.739990234375\n",
      "epoch 11, train_loss 1278.3822021484375\n",
      "epoch 11, val_loss 455.7398376464844\n",
      "epoch 12, train_loss 1278.3818359375\n",
      "epoch 12, val_loss 455.7397155761719\n",
      "epoch 13, train_loss 1278.381591796875\n",
      "epoch 13, val_loss 455.739501953125\n",
      "epoch 14, train_loss 1278.3812255859375\n",
      "epoch 14, val_loss 455.7393798828125\n",
      "epoch 15, train_loss 1278.3809814453125\n",
      "epoch 15, val_loss 455.7391662597656\n",
      "epoch 16, train_loss 1278.380859375\n",
      "epoch 16, val_loss 455.7390441894531\n",
      "epoch 17, train_loss 1278.380615234375\n",
      "epoch 17, val_loss 455.7388610839844\n",
      "epoch 18, train_loss 1278.3802490234375\n",
      "epoch 18, val_loss 455.7387390136719\n",
      "epoch 19, train_loss 1278.3800048828125\n",
      "epoch 19, val_loss 455.73858642578125\n",
      "epoch 20, train_loss 1278.379638671875\n",
      "epoch 20, val_loss 455.73846435546875\n",
      "epoch 21, train_loss 1278.37939453125\n",
      "epoch 21, val_loss 455.73828125\n",
      "epoch 22, train_loss 1278.3790283203125\n",
      "epoch 22, val_loss 455.7381591796875\n",
      "epoch 23, train_loss 1278.3787841796875\n",
      "epoch 23, val_loss 455.7379455566406\n",
      "epoch 24, train_loss 1278.3785400390625\n",
      "epoch 24, val_loss 455.73779296875\n",
      "epoch 25, train_loss 1278.3782958984375\n",
      "epoch 25, val_loss 455.7376708984375\n",
      "epoch 26, train_loss 1278.3780517578125\n",
      "epoch 26, val_loss 455.73748779296875\n",
      "epoch 27, train_loss 1278.3778076171875\n",
      "epoch 27, val_loss 455.7373352050781\n",
      "epoch 28, train_loss 1278.3773193359375\n",
      "epoch 28, val_loss 455.7371520996094\n",
      "epoch 29, train_loss 1278.377197265625\n",
      "epoch 29, val_loss 455.73699951171875\n",
      "epoch 30, train_loss 1278.376953125\n",
      "epoch 30, val_loss 455.7368469238281\n",
      "epoch 31, train_loss 1278.376708984375\n",
      "epoch 31, val_loss 455.7366943359375\n",
      "epoch 32, train_loss 1278.3763427734375\n",
      "epoch 32, val_loss 455.73651123046875\n",
      "epoch 33, train_loss 1278.376220703125\n",
      "epoch 33, val_loss 455.736328125\n",
      "epoch 34, train_loss 1278.3759765625\n",
      "epoch 34, val_loss 455.7362060546875\n",
      "epoch 35, train_loss 1278.3756103515625\n",
      "epoch 35, val_loss 455.7360534667969\n",
      "epoch 36, train_loss 1278.375244140625\n",
      "epoch 36, val_loss 455.7359313964844\n",
      "epoch 37, train_loss 1278.3748779296875\n",
      "epoch 37, val_loss 455.7357482910156\n",
      "epoch 38, train_loss 1278.3746337890625\n",
      "epoch 38, val_loss 455.735595703125\n",
      "epoch 39, train_loss 1278.3746337890625\n",
      "epoch 39, val_loss 455.73541259765625\n",
      "epoch 40, train_loss 1278.3741455078125\n",
      "epoch 40, val_loss 455.7353210449219\n",
      "epoch 41, train_loss 1278.3739013671875\n",
      "epoch 41, val_loss 455.73516845703125\n",
      "epoch 42, train_loss 1278.37353515625\n",
      "epoch 42, val_loss 455.7349853515625\n",
      "epoch 43, train_loss 1278.373291015625\n",
      "epoch 43, val_loss 455.7348327636719\n",
      "epoch 44, train_loss 1278.373046875\n",
      "epoch 44, val_loss 455.7347106933594\n",
      "epoch 45, train_loss 1278.3729248046875\n",
      "epoch 45, val_loss 455.7345275878906\n",
      "epoch 46, train_loss 1278.37255859375\n",
      "epoch 46, val_loss 455.734375\n",
      "epoch 47, train_loss 1278.372314453125\n",
      "epoch 47, val_loss 455.7342224121094\n",
      "epoch 48, train_loss 1278.3719482421875\n",
      "epoch 48, val_loss 455.73406982421875\n",
      "epoch 49, train_loss 1278.3719482421875\n",
      "epoch 49, val_loss 455.7339172363281\n",
      "epoch 50, train_loss 1278.3714599609375\n",
      "epoch 50, val_loss 455.7337646484375\n",
      "epoch 51, train_loss 1278.3712158203125\n",
      "epoch 51, val_loss 455.73358154296875\n",
      "epoch 52, train_loss 1278.3709716796875\n",
      "epoch 52, val_loss 455.73333740234375\n",
      "epoch 53, train_loss 1278.3707275390625\n",
      "epoch 53, val_loss 455.7333068847656\n",
      "epoch 54, train_loss 1278.370361328125\n",
      "epoch 54, val_loss 455.7331237792969\n",
      "epoch 55, train_loss 1278.3701171875\n",
      "epoch 55, val_loss 455.73297119140625\n",
      "epoch 56, train_loss 1278.3697509765625\n",
      "epoch 56, val_loss 455.7328186035156\n",
      "epoch 57, train_loss 1278.36962890625\n",
      "epoch 57, val_loss 455.7326354980469\n",
      "epoch 58, train_loss 1278.3692626953125\n",
      "epoch 58, val_loss 455.7325134277344\n",
      "epoch 59, train_loss 1278.369140625\n",
      "epoch 59, val_loss 455.7323303222656\n",
      "epoch 60, train_loss 1278.3687744140625\n",
      "epoch 60, val_loss 455.7322082519531\n",
      "epoch 61, train_loss 1278.3685302734375\n",
      "epoch 61, val_loss 455.73199462890625\n",
      "epoch 62, train_loss 1278.3682861328125\n",
      "epoch 62, val_loss 455.7318420410156\n",
      "epoch 63, train_loss 1278.367919921875\n",
      "epoch 63, val_loss 455.7316589355469\n",
      "epoch 64, train_loss 1278.3677978515625\n",
      "epoch 64, val_loss 455.7315368652344\n",
      "epoch 65, train_loss 1278.36767578125\n",
      "epoch 65, val_loss 455.73138427734375\n",
      "epoch 66, train_loss 1278.3673095703125\n",
      "epoch 66, val_loss 455.73126220703125\n",
      "epoch 67, train_loss 1278.3668212890625\n",
      "epoch 67, val_loss 455.7310485839844\n",
      "epoch 68, train_loss 1278.3665771484375\n",
      "epoch 68, val_loss 455.7309265136719\n",
      "epoch 69, train_loss 1278.3662109375\n",
      "epoch 69, val_loss 455.7308349609375\n",
      "epoch 70, train_loss 1278.3662109375\n",
      "epoch 70, val_loss 455.7305908203125\n",
      "epoch 71, train_loss 1278.365966796875\n",
      "epoch 71, val_loss 455.7304992675781\n",
      "epoch 72, train_loss 1278.36572265625\n",
      "epoch 72, val_loss 455.7303466796875\n",
      "epoch 73, train_loss 1278.365478515625\n",
      "epoch 73, val_loss 455.73016357421875\n",
      "epoch 74, train_loss 1278.3651123046875\n",
      "epoch 74, val_loss 455.73004150390625\n",
      "epoch 75, train_loss 1278.36474609375\n",
      "epoch 75, val_loss 455.7298278808594\n",
      "epoch 76, train_loss 1278.364501953125\n",
      "epoch 76, val_loss 455.72967529296875\n",
      "epoch 77, train_loss 1278.3642578125\n",
      "epoch 77, val_loss 455.72955322265625\n",
      "epoch 78, train_loss 1278.364013671875\n",
      "epoch 78, val_loss 455.7293701171875\n",
      "epoch 79, train_loss 1278.3636474609375\n",
      "epoch 79, val_loss 455.729248046875\n",
      "epoch 80, train_loss 1278.3634033203125\n",
      "epoch 80, val_loss 455.7290344238281\n",
      "epoch 81, train_loss 1278.363037109375\n",
      "epoch 81, val_loss 455.7289123535156\n",
      "epoch 82, train_loss 1278.3629150390625\n",
      "epoch 82, val_loss 455.728759765625\n",
      "epoch 83, train_loss 1278.3626708984375\n",
      "epoch 83, val_loss 455.7286376953125\n",
      "epoch 84, train_loss 1278.362548828125\n",
      "epoch 84, val_loss 455.72845458984375\n",
      "epoch 85, train_loss 1278.3621826171875\n",
      "epoch 85, val_loss 455.7283020019531\n",
      "epoch 86, train_loss 1278.36181640625\n",
      "epoch 86, val_loss 455.7281188964844\n",
      "epoch 87, train_loss 1278.3614501953125\n",
      "epoch 87, val_loss 455.7279968261719\n",
      "epoch 88, train_loss 1278.3614501953125\n",
      "epoch 88, val_loss 455.72784423828125\n",
      "epoch 89, train_loss 1278.3609619140625\n",
      "epoch 89, val_loss 455.72772216796875\n",
      "epoch 90, train_loss 1278.36083984375\n",
      "epoch 90, val_loss 455.7275390625\n",
      "epoch 91, train_loss 1278.3604736328125\n",
      "epoch 91, val_loss 455.7273864746094\n",
      "epoch 92, train_loss 1278.360107421875\n",
      "epoch 92, val_loss 455.7272033691406\n",
      "epoch 93, train_loss 1278.3599853515625\n",
      "epoch 93, val_loss 455.72705078125\n",
      "epoch 94, train_loss 1278.3597412109375\n",
      "epoch 94, val_loss 455.7268371582031\n",
      "epoch 95, train_loss 1278.359619140625\n",
      "epoch 95, val_loss 455.72674560546875\n",
      "epoch 96, train_loss 1278.3592529296875\n",
      "epoch 96, val_loss 455.72662353515625\n",
      "epoch 97, train_loss 1278.35888671875\n",
      "epoch 97, val_loss 455.7264099121094\n",
      "epoch 98, train_loss 1278.358642578125\n",
      "epoch 98, val_loss 455.72625732421875\n",
      "epoch 99, train_loss 1278.3582763671875\n",
      "epoch 99, val_loss 455.72607421875\n",
      "Parameter containing:\n",
      "tensor([1.4275e-06], requires_grad=True)\n",
      "iter 27, train_loss_regularization 1.0387636423110962\n",
      "iter 27, val_loss_regularization 1.0387636423110962\n",
      "epoch 0, train_loss 1278.3582763671875\n",
      "epoch 0, val_loss 455.7259521484375\n",
      "epoch 1, train_loss 1278.35791015625\n",
      "epoch 1, val_loss 455.7257995605469\n",
      "epoch 2, train_loss 1278.3577880859375\n",
      "epoch 2, val_loss 455.7257080078125\n",
      "epoch 3, train_loss 1278.3572998046875\n",
      "epoch 3, val_loss 455.7255554199219\n",
      "epoch 4, train_loss 1278.3570556640625\n",
      "epoch 4, val_loss 455.72540283203125\n",
      "epoch 5, train_loss 1278.35693359375\n",
      "epoch 5, val_loss 455.72515869140625\n",
      "epoch 6, train_loss 1278.3565673828125\n",
      "epoch 6, val_loss 455.72503662109375\n",
      "epoch 7, train_loss 1278.3564453125\n",
      "epoch 7, val_loss 455.72491455078125\n",
      "epoch 8, train_loss 1278.35595703125\n",
      "epoch 8, val_loss 455.7247619628906\n",
      "epoch 9, train_loss 1278.3558349609375\n",
      "epoch 9, val_loss 455.7245788574219\n",
      "epoch 10, train_loss 1278.35546875\n",
      "epoch 10, val_loss 455.72442626953125\n",
      "epoch 11, train_loss 1278.355224609375\n",
      "epoch 11, val_loss 455.7242431640625\n",
      "epoch 12, train_loss 1278.3548583984375\n",
      "epoch 12, val_loss 455.7240905761719\n",
      "epoch 13, train_loss 1278.3546142578125\n",
      "epoch 13, val_loss 455.7239074707031\n",
      "epoch 14, train_loss 1278.3544921875\n",
      "epoch 14, val_loss 455.723876953125\n",
      "epoch 15, train_loss 1278.3541259765625\n",
      "epoch 15, val_loss 455.7236328125\n",
      "epoch 16, train_loss 1278.35400390625\n",
      "epoch 16, val_loss 455.72344970703125\n",
      "epoch 17, train_loss 1278.3536376953125\n",
      "epoch 17, val_loss 455.72332763671875\n",
      "epoch 18, train_loss 1278.3533935546875\n",
      "epoch 18, val_loss 455.7231750488281\n",
      "epoch 19, train_loss 1278.353271484375\n",
      "epoch 19, val_loss 455.7229919433594\n",
      "epoch 20, train_loss 1278.3529052734375\n",
      "epoch 20, val_loss 455.7229309082031\n",
      "epoch 21, train_loss 1278.3525390625\n",
      "epoch 21, val_loss 455.72271728515625\n",
      "epoch 22, train_loss 1278.3521728515625\n",
      "epoch 22, val_loss 455.7225341796875\n",
      "epoch 23, train_loss 1278.35205078125\n",
      "epoch 23, val_loss 455.722412109375\n",
      "epoch 24, train_loss 1278.351806640625\n",
      "epoch 24, val_loss 455.7222595214844\n",
      "epoch 25, train_loss 1278.3514404296875\n",
      "epoch 25, val_loss 455.72216796875\n",
      "epoch 26, train_loss 1278.3514404296875\n",
      "epoch 26, val_loss 455.7219543457031\n",
      "epoch 27, train_loss 1278.35107421875\n",
      "epoch 27, val_loss 455.7218322753906\n",
      "epoch 28, train_loss 1278.3507080078125\n",
      "epoch 28, val_loss 455.7216796875\n",
      "epoch 29, train_loss 1278.3504638671875\n",
      "epoch 29, val_loss 455.72149658203125\n",
      "epoch 30, train_loss 1278.3502197265625\n",
      "epoch 30, val_loss 455.72137451171875\n",
      "epoch 31, train_loss 1278.3499755859375\n",
      "epoch 31, val_loss 455.7211608886719\n",
      "epoch 32, train_loss 1278.349609375\n",
      "epoch 32, val_loss 455.7210388183594\n",
      "epoch 33, train_loss 1278.3494873046875\n",
      "epoch 33, val_loss 455.72088623046875\n",
      "epoch 34, train_loss 1278.34912109375\n",
      "epoch 34, val_loss 455.72076416015625\n",
      "epoch 35, train_loss 1278.3489990234375\n",
      "epoch 35, val_loss 455.7205810546875\n",
      "epoch 36, train_loss 1278.3486328125\n",
      "epoch 36, val_loss 455.720458984375\n",
      "epoch 37, train_loss 1278.348388671875\n",
      "epoch 37, val_loss 455.7202453613281\n",
      "epoch 38, train_loss 1278.3480224609375\n",
      "epoch 38, val_loss 455.7200927734375\n",
      "epoch 39, train_loss 1278.3477783203125\n",
      "epoch 39, val_loss 455.7200012207031\n",
      "epoch 40, train_loss 1278.34765625\n",
      "epoch 40, val_loss 455.7198181152344\n",
      "epoch 41, train_loss 1278.3472900390625\n",
      "epoch 41, val_loss 455.71966552734375\n",
      "epoch 42, train_loss 1278.34716796875\n",
      "epoch 42, val_loss 455.7195129394531\n",
      "epoch 43, train_loss 1278.3468017578125\n",
      "epoch 43, val_loss 455.7193298339844\n",
      "epoch 44, train_loss 1278.3465576171875\n",
      "epoch 44, val_loss 455.71917724609375\n",
      "epoch 45, train_loss 1278.34619140625\n",
      "epoch 45, val_loss 455.718994140625\n",
      "epoch 46, train_loss 1278.345947265625\n",
      "epoch 46, val_loss 455.7189025878906\n",
      "epoch 47, train_loss 1278.3458251953125\n",
      "epoch 47, val_loss 455.71875\n",
      "epoch 48, train_loss 1278.345458984375\n",
      "epoch 48, val_loss 455.7185974121094\n",
      "epoch 49, train_loss 1278.3453369140625\n",
      "epoch 49, val_loss 455.71844482421875\n",
      "epoch 50, train_loss 1278.344970703125\n",
      "epoch 50, val_loss 455.71826171875\n",
      "epoch 51, train_loss 1278.3448486328125\n",
      "epoch 51, val_loss 455.7180480957031\n",
      "epoch 52, train_loss 1278.3443603515625\n",
      "epoch 52, val_loss 455.7179870605469\n",
      "epoch 53, train_loss 1278.3441162109375\n",
      "epoch 53, val_loss 455.71783447265625\n",
      "epoch 54, train_loss 1278.343994140625\n",
      "epoch 54, val_loss 455.71771240234375\n",
      "epoch 55, train_loss 1278.34375\n",
      "epoch 55, val_loss 455.7174987792969\n",
      "epoch 56, train_loss 1278.343505859375\n",
      "epoch 56, val_loss 455.71734619140625\n",
      "epoch 57, train_loss 1278.34326171875\n",
      "epoch 57, val_loss 455.7171936035156\n",
      "epoch 58, train_loss 1278.343017578125\n",
      "epoch 58, val_loss 455.717041015625\n",
      "epoch 59, train_loss 1278.3426513671875\n",
      "epoch 59, val_loss 455.7168884277344\n",
      "epoch 60, train_loss 1278.3424072265625\n",
      "epoch 60, val_loss 455.716796875\n",
      "epoch 61, train_loss 1278.3421630859375\n",
      "epoch 61, val_loss 455.71661376953125\n",
      "epoch 62, train_loss 1278.342041015625\n",
      "epoch 62, val_loss 455.71649169921875\n",
      "epoch 63, train_loss 1278.3416748046875\n",
      "epoch 63, val_loss 455.7163391113281\n",
      "epoch 64, train_loss 1278.3414306640625\n",
      "epoch 64, val_loss 455.71612548828125\n",
      "epoch 65, train_loss 1278.3411865234375\n",
      "epoch 65, val_loss 455.7159729003906\n",
      "epoch 66, train_loss 1278.3406982421875\n",
      "epoch 66, val_loss 455.7158203125\n",
      "epoch 67, train_loss 1278.3406982421875\n",
      "epoch 67, val_loss 455.71563720703125\n",
      "epoch 68, train_loss 1278.3402099609375\n",
      "epoch 68, val_loss 455.715576171875\n",
      "epoch 69, train_loss 1278.3402099609375\n",
      "epoch 69, val_loss 455.7154235839844\n",
      "epoch 70, train_loss 1278.3397216796875\n",
      "epoch 70, val_loss 455.7152404785156\n",
      "epoch 71, train_loss 1278.3394775390625\n",
      "epoch 71, val_loss 455.71502685546875\n",
      "epoch 72, train_loss 1278.33935546875\n",
      "epoch 72, val_loss 455.7148742675781\n",
      "epoch 73, train_loss 1278.339111328125\n",
      "epoch 73, val_loss 455.7147216796875\n",
      "epoch 74, train_loss 1278.3388671875\n",
      "epoch 74, val_loss 455.7146301269531\n",
      "epoch 75, train_loss 1278.338623046875\n",
      "epoch 75, val_loss 455.7145080566406\n",
      "epoch 76, train_loss 1278.3382568359375\n",
      "epoch 76, val_loss 455.7143859863281\n",
      "epoch 77, train_loss 1278.337890625\n",
      "epoch 77, val_loss 455.71417236328125\n",
      "epoch 78, train_loss 1278.3377685546875\n",
      "epoch 78, val_loss 455.7139587402344\n",
      "epoch 79, train_loss 1278.3375244140625\n",
      "epoch 79, val_loss 455.7138671875\n",
      "epoch 80, train_loss 1278.3372802734375\n",
      "epoch 80, val_loss 455.7136535644531\n",
      "epoch 81, train_loss 1278.3370361328125\n",
      "epoch 81, val_loss 455.7135314941406\n",
      "epoch 82, train_loss 1278.3367919921875\n",
      "epoch 82, val_loss 455.7134094238281\n",
      "epoch 83, train_loss 1278.3363037109375\n",
      "epoch 83, val_loss 455.7132873535156\n",
      "epoch 84, train_loss 1278.3363037109375\n",
      "epoch 84, val_loss 455.713134765625\n",
      "epoch 85, train_loss 1278.3359375\n",
      "epoch 85, val_loss 455.71295166015625\n",
      "epoch 86, train_loss 1278.3358154296875\n",
      "epoch 86, val_loss 455.7127380371094\n",
      "epoch 87, train_loss 1278.33544921875\n",
      "epoch 87, val_loss 455.7126159667969\n",
      "epoch 88, train_loss 1278.335205078125\n",
      "epoch 88, val_loss 455.71246337890625\n",
      "epoch 89, train_loss 1278.3349609375\n",
      "epoch 89, val_loss 455.71234130859375\n",
      "epoch 90, train_loss 1278.3348388671875\n",
      "epoch 90, val_loss 455.71221923828125\n",
      "epoch 91, train_loss 1278.33447265625\n",
      "epoch 91, val_loss 455.7120361328125\n",
      "epoch 92, train_loss 1278.334228515625\n",
      "epoch 92, val_loss 455.7118835449219\n",
      "epoch 93, train_loss 1278.3338623046875\n",
      "epoch 93, val_loss 455.7117004394531\n",
      "epoch 94, train_loss 1278.3336181640625\n",
      "epoch 94, val_loss 455.71148681640625\n",
      "epoch 95, train_loss 1278.3333740234375\n",
      "epoch 95, val_loss 455.71142578125\n",
      "epoch 96, train_loss 1278.3330078125\n",
      "epoch 96, val_loss 455.7113342285156\n",
      "epoch 97, train_loss 1278.3328857421875\n",
      "epoch 97, val_loss 455.71112060546875\n",
      "epoch 98, train_loss 1278.33251953125\n",
      "epoch 98, val_loss 455.7109680175781\n",
      "epoch 99, train_loss 1278.332275390625\n",
      "epoch 99, val_loss 455.7107849121094\n",
      "Parameter containing:\n",
      "tensor([9.0065e-07], requires_grad=True)\n",
      "iter 28, train_loss_regularization 1.0319108963012695\n",
      "iter 28, val_loss_regularization 1.0319108963012695\n",
      "epoch 0, train_loss 1278.3321533203125\n",
      "epoch 0, val_loss 455.7106628417969\n",
      "epoch 1, train_loss 1278.3319091796875\n",
      "epoch 1, val_loss 455.71044921875\n",
      "epoch 2, train_loss 1278.33154296875\n",
      "epoch 2, val_loss 455.71038818359375\n",
      "epoch 3, train_loss 1278.33154296875\n",
      "epoch 3, val_loss 455.710205078125\n",
      "epoch 4, train_loss 1278.3311767578125\n",
      "epoch 4, val_loss 455.7100524902344\n",
      "epoch 5, train_loss 1278.330810546875\n",
      "epoch 5, val_loss 455.7099304199219\n",
      "epoch 6, train_loss 1278.3304443359375\n",
      "epoch 6, val_loss 455.7097473144531\n",
      "epoch 7, train_loss 1278.330322265625\n",
      "epoch 7, val_loss 455.7095947265625\n",
      "epoch 8, train_loss 1278.330078125\n",
      "epoch 8, val_loss 455.7095031738281\n",
      "epoch 9, train_loss 1278.3299560546875\n",
      "epoch 9, val_loss 455.70928955078125\n",
      "epoch 10, train_loss 1278.3297119140625\n",
      "epoch 10, val_loss 455.7091369628906\n",
      "epoch 11, train_loss 1278.329345703125\n",
      "epoch 11, val_loss 455.7090148925781\n",
      "epoch 12, train_loss 1278.3292236328125\n",
      "epoch 12, val_loss 455.7088317871094\n",
      "epoch 13, train_loss 1278.328857421875\n",
      "epoch 13, val_loss 455.70867919921875\n",
      "epoch 14, train_loss 1278.3284912109375\n",
      "epoch 14, val_loss 455.7085876464844\n",
      "epoch 15, train_loss 1278.328369140625\n",
      "epoch 15, val_loss 455.7084045410156\n",
      "epoch 16, train_loss 1278.328125\n",
      "epoch 16, val_loss 455.708251953125\n",
      "epoch 17, train_loss 1278.3277587890625\n",
      "epoch 17, val_loss 455.7081298828125\n",
      "epoch 18, train_loss 1278.3275146484375\n",
      "epoch 18, val_loss 455.7079162597656\n",
      "epoch 19, train_loss 1278.327392578125\n",
      "epoch 19, val_loss 455.707763671875\n",
      "epoch 20, train_loss 1278.3271484375\n",
      "epoch 20, val_loss 455.7076110839844\n",
      "epoch 21, train_loss 1278.3267822265625\n",
      "epoch 21, val_loss 455.7075500488281\n",
      "epoch 22, train_loss 1278.3265380859375\n",
      "epoch 22, val_loss 455.7073059082031\n",
      "epoch 23, train_loss 1278.3262939453125\n",
      "epoch 23, val_loss 455.70721435546875\n",
      "epoch 24, train_loss 1278.3260498046875\n",
      "epoch 24, val_loss 455.70703125\n",
      "epoch 25, train_loss 1278.3258056640625\n",
      "epoch 25, val_loss 455.70697021484375\n",
      "epoch 26, train_loss 1278.325439453125\n",
      "epoch 26, val_loss 455.706787109375\n",
      "epoch 27, train_loss 1278.3251953125\n",
      "epoch 27, val_loss 455.7065734863281\n",
      "epoch 28, train_loss 1278.324951171875\n",
      "epoch 28, val_loss 455.7064514160156\n",
      "epoch 29, train_loss 1278.32470703125\n",
      "epoch 29, val_loss 455.70623779296875\n",
      "epoch 30, train_loss 1278.3245849609375\n",
      "epoch 30, val_loss 455.7061767578125\n",
      "epoch 31, train_loss 1278.32421875\n",
      "epoch 31, val_loss 455.70599365234375\n",
      "epoch 32, train_loss 1278.323974609375\n",
      "epoch 32, val_loss 455.7058410644531\n",
      "epoch 33, train_loss 1278.323974609375\n",
      "epoch 33, val_loss 455.7057189941406\n",
      "epoch 34, train_loss 1278.323486328125\n",
      "epoch 34, val_loss 455.7055358886719\n",
      "epoch 35, train_loss 1278.3231201171875\n",
      "epoch 35, val_loss 455.70538330078125\n",
      "epoch 36, train_loss 1278.3231201171875\n",
      "epoch 36, val_loss 455.70526123046875\n",
      "epoch 37, train_loss 1278.3228759765625\n",
      "epoch 37, val_loss 455.70513916015625\n",
      "epoch 38, train_loss 1278.322509765625\n",
      "epoch 38, val_loss 455.7049560546875\n",
      "epoch 39, train_loss 1278.3223876953125\n",
      "epoch 39, val_loss 455.7047424316406\n",
      "epoch 40, train_loss 1278.3221435546875\n",
      "epoch 40, val_loss 455.7046203613281\n",
      "epoch 41, train_loss 1278.32177734375\n",
      "epoch 41, val_loss 455.70440673828125\n",
      "epoch 42, train_loss 1278.3212890625\n",
      "epoch 42, val_loss 455.70440673828125\n",
      "epoch 43, train_loss 1278.3211669921875\n",
      "epoch 43, val_loss 455.7041931152344\n",
      "epoch 44, train_loss 1278.3209228515625\n",
      "epoch 44, val_loss 455.7040100097656\n",
      "epoch 45, train_loss 1278.3206787109375\n",
      "epoch 45, val_loss 455.7038269042969\n",
      "epoch 46, train_loss 1278.3203125\n",
      "epoch 46, val_loss 455.7037353515625\n",
      "epoch 47, train_loss 1278.3203125\n",
      "epoch 47, val_loss 455.70361328125\n",
      "epoch 48, train_loss 1278.3199462890625\n",
      "epoch 48, val_loss 455.703369140625\n",
      "epoch 49, train_loss 1278.3199462890625\n",
      "epoch 49, val_loss 455.7032470703125\n",
      "epoch 50, train_loss 1278.3194580078125\n",
      "epoch 50, val_loss 455.7030944824219\n",
      "epoch 51, train_loss 1278.3192138671875\n",
      "epoch 51, val_loss 455.7030029296875\n",
      "epoch 52, train_loss 1278.319091796875\n",
      "epoch 52, val_loss 455.70281982421875\n",
      "epoch 53, train_loss 1278.31884765625\n",
      "epoch 53, val_loss 455.70269775390625\n",
      "epoch 54, train_loss 1278.318359375\n",
      "epoch 54, val_loss 455.7025146484375\n",
      "epoch 55, train_loss 1278.318115234375\n",
      "epoch 55, val_loss 455.7023620605469\n",
      "epoch 56, train_loss 1278.3179931640625\n",
      "epoch 56, val_loss 455.7022399902344\n",
      "epoch 57, train_loss 1278.31787109375\n",
      "epoch 57, val_loss 455.70208740234375\n",
      "epoch 58, train_loss 1278.3173828125\n",
      "epoch 58, val_loss 455.7018737792969\n",
      "epoch 59, train_loss 1278.317138671875\n",
      "epoch 59, val_loss 455.7017517089844\n",
      "epoch 60, train_loss 1278.31689453125\n",
      "epoch 60, val_loss 455.7015686035156\n",
      "epoch 61, train_loss 1278.316650390625\n",
      "epoch 61, val_loss 455.7015075683594\n",
      "epoch 62, train_loss 1278.31640625\n",
      "epoch 62, val_loss 455.7012939453125\n",
      "epoch 63, train_loss 1278.3162841796875\n",
      "epoch 63, val_loss 455.70111083984375\n",
      "epoch 64, train_loss 1278.3160400390625\n",
      "epoch 64, val_loss 455.70098876953125\n",
      "epoch 65, train_loss 1278.3157958984375\n",
      "epoch 65, val_loss 455.700927734375\n",
      "epoch 66, train_loss 1278.3154296875\n",
      "epoch 66, val_loss 455.7008056640625\n",
      "epoch 67, train_loss 1278.3153076171875\n",
      "epoch 67, val_loss 455.7005920410156\n",
      "epoch 68, train_loss 1278.31494140625\n",
      "epoch 68, val_loss 455.7004089355469\n",
      "epoch 69, train_loss 1278.314697265625\n",
      "epoch 69, val_loss 455.70025634765625\n",
      "epoch 70, train_loss 1278.314453125\n",
      "epoch 70, val_loss 455.7000732421875\n",
      "epoch 71, train_loss 1278.314208984375\n",
      "epoch 71, val_loss 455.70001220703125\n",
      "epoch 72, train_loss 1278.3140869140625\n",
      "epoch 72, val_loss 455.6998291015625\n",
      "epoch 73, train_loss 1278.313720703125\n",
      "epoch 73, val_loss 455.6996765136719\n",
      "epoch 74, train_loss 1278.3134765625\n",
      "epoch 74, val_loss 455.6995544433594\n",
      "epoch 75, train_loss 1278.3131103515625\n",
      "epoch 75, val_loss 455.69940185546875\n",
      "epoch 76, train_loss 1278.3128662109375\n",
      "epoch 76, val_loss 455.69927978515625\n",
      "epoch 77, train_loss 1278.3126220703125\n",
      "epoch 77, val_loss 455.6990966796875\n",
      "epoch 78, train_loss 1278.3123779296875\n",
      "epoch 78, val_loss 455.6989440917969\n",
      "epoch 79, train_loss 1278.3123779296875\n",
      "epoch 79, val_loss 455.6987609863281\n",
      "epoch 80, train_loss 1278.3121337890625\n",
      "epoch 80, val_loss 455.69866943359375\n",
      "epoch 81, train_loss 1278.311767578125\n",
      "epoch 81, val_loss 455.698486328125\n",
      "epoch 82, train_loss 1278.3115234375\n",
      "epoch 82, val_loss 455.6983337402344\n",
      "epoch 83, train_loss 1278.311279296875\n",
      "epoch 83, val_loss 455.69818115234375\n",
      "epoch 84, train_loss 1278.31103515625\n",
      "epoch 84, val_loss 455.697998046875\n",
      "epoch 85, train_loss 1278.3106689453125\n",
      "epoch 85, val_loss 455.6978759765625\n",
      "epoch 86, train_loss 1278.310546875\n",
      "epoch 86, val_loss 455.6977233886719\n",
      "epoch 87, train_loss 1278.310302734375\n",
      "epoch 87, val_loss 455.69757080078125\n",
      "epoch 88, train_loss 1278.3099365234375\n",
      "epoch 88, val_loss 455.69744873046875\n",
      "epoch 89, train_loss 1278.31005859375\n",
      "epoch 89, val_loss 455.6972961425781\n",
      "epoch 90, train_loss 1278.3095703125\n",
      "epoch 90, val_loss 455.6971740722656\n",
      "epoch 91, train_loss 1278.3094482421875\n",
      "epoch 91, val_loss 455.6969909667969\n",
      "epoch 92, train_loss 1278.30908203125\n",
      "epoch 92, val_loss 455.69683837890625\n",
      "epoch 93, train_loss 1278.3087158203125\n",
      "epoch 93, val_loss 455.69671630859375\n",
      "epoch 94, train_loss 1278.3084716796875\n",
      "epoch 94, val_loss 455.6965026855469\n",
      "epoch 95, train_loss 1278.308349609375\n",
      "epoch 95, val_loss 455.6964111328125\n",
      "epoch 96, train_loss 1278.307861328125\n",
      "epoch 96, val_loss 455.6962585449219\n",
      "epoch 97, train_loss 1278.3077392578125\n",
      "epoch 97, val_loss 455.6960754394531\n",
      "epoch 98, train_loss 1278.3074951171875\n",
      "epoch 98, val_loss 455.6959228515625\n",
      "epoch 99, train_loss 1278.30712890625\n",
      "epoch 99, val_loss 455.6958312988281\n",
      "Parameter containing:\n",
      "tensor([5.6891e-07], requires_grad=True)\n",
      "iter 29, train_loss_regularization 1.0251907110214233\n",
      "iter 29, val_loss_regularization 1.0251907110214233\n",
      "epoch 0, train_loss 1278.3070068359375\n",
      "epoch 0, val_loss 455.6956787109375\n",
      "epoch 1, train_loss 1278.306884765625\n",
      "epoch 1, val_loss 455.695556640625\n",
      "epoch 2, train_loss 1278.306396484375\n",
      "epoch 2, val_loss 455.69537353515625\n",
      "epoch 3, train_loss 1278.3062744140625\n",
      "epoch 3, val_loss 455.6952209472656\n",
      "epoch 4, train_loss 1278.3062744140625\n",
      "epoch 4, val_loss 455.6950378417969\n",
      "epoch 5, train_loss 1278.3057861328125\n",
      "epoch 5, val_loss 455.69488525390625\n",
      "epoch 6, train_loss 1278.3055419921875\n",
      "epoch 6, val_loss 455.6947937011719\n",
      "epoch 7, train_loss 1278.3052978515625\n",
      "epoch 7, val_loss 455.694580078125\n",
      "epoch 8, train_loss 1278.30517578125\n",
      "epoch 8, val_loss 455.6944580078125\n",
      "epoch 9, train_loss 1278.3048095703125\n",
      "epoch 9, val_loss 455.6943054199219\n",
      "epoch 10, train_loss 1278.3045654296875\n",
      "epoch 10, val_loss 455.69415283203125\n",
      "epoch 11, train_loss 1278.3043212890625\n",
      "epoch 11, val_loss 455.6940002441406\n",
      "epoch 12, train_loss 1278.30419921875\n",
      "epoch 12, val_loss 455.6938781738281\n",
      "epoch 13, train_loss 1278.3038330078125\n",
      "epoch 13, val_loss 455.6936340332031\n",
      "epoch 14, train_loss 1278.303466796875\n",
      "epoch 14, val_loss 455.6935729980469\n",
      "epoch 15, train_loss 1278.303466796875\n",
      "epoch 15, val_loss 455.6934509277344\n",
      "epoch 16, train_loss 1278.3031005859375\n",
      "epoch 16, val_loss 455.6933288574219\n",
      "epoch 17, train_loss 1278.302734375\n",
      "epoch 17, val_loss 455.6930847167969\n",
      "epoch 18, train_loss 1278.302490234375\n",
      "epoch 18, val_loss 455.6929931640625\n",
      "epoch 19, train_loss 1278.3023681640625\n",
      "epoch 19, val_loss 455.6928405761719\n",
      "epoch 20, train_loss 1278.302001953125\n",
      "epoch 20, val_loss 455.6927185058594\n",
      "epoch 21, train_loss 1278.3018798828125\n",
      "epoch 21, val_loss 455.6925354003906\n",
      "epoch 22, train_loss 1278.3016357421875\n",
      "epoch 22, val_loss 455.6924133300781\n",
      "epoch 23, train_loss 1278.3013916015625\n",
      "epoch 23, val_loss 455.6922607421875\n",
      "epoch 24, train_loss 1278.301025390625\n",
      "epoch 24, val_loss 455.692138671875\n",
      "epoch 25, train_loss 1278.3009033203125\n",
      "epoch 25, val_loss 455.6919250488281\n",
      "epoch 26, train_loss 1278.30078125\n",
      "epoch 26, val_loss 455.6918029785156\n",
      "epoch 27, train_loss 1278.3004150390625\n",
      "epoch 27, val_loss 455.6916198730469\n",
      "epoch 28, train_loss 1278.30029296875\n",
      "epoch 28, val_loss 455.6914978027344\n",
      "epoch 29, train_loss 1278.2999267578125\n",
      "epoch 29, val_loss 455.69134521484375\n",
      "epoch 30, train_loss 1278.299560546875\n",
      "epoch 30, val_loss 455.691162109375\n",
      "epoch 31, train_loss 1278.2994384765625\n",
      "epoch 31, val_loss 455.6910400390625\n",
      "epoch 32, train_loss 1278.2991943359375\n",
      "epoch 32, val_loss 455.6909484863281\n",
      "epoch 33, train_loss 1278.299072265625\n",
      "epoch 33, val_loss 455.6907958984375\n",
      "epoch 34, train_loss 1278.2987060546875\n",
      "epoch 34, val_loss 455.690673828125\n",
      "epoch 35, train_loss 1278.2984619140625\n",
      "epoch 35, val_loss 455.69049072265625\n",
      "epoch 36, train_loss 1278.2982177734375\n",
      "epoch 36, val_loss 455.6902770996094\n",
      "epoch 37, train_loss 1278.2979736328125\n",
      "epoch 37, val_loss 455.6901550292969\n",
      "epoch 38, train_loss 1278.297607421875\n",
      "epoch 38, val_loss 455.69000244140625\n",
      "epoch 39, train_loss 1278.2974853515625\n",
      "epoch 39, val_loss 455.68988037109375\n",
      "epoch 40, train_loss 1278.2972412109375\n",
      "epoch 40, val_loss 455.689697265625\n",
      "epoch 41, train_loss 1278.296875\n",
      "epoch 41, val_loss 455.6895446777344\n",
      "epoch 42, train_loss 1278.2967529296875\n",
      "epoch 42, val_loss 455.689453125\n",
      "epoch 43, train_loss 1278.2965087890625\n",
      "epoch 43, val_loss 455.6892395019531\n",
      "epoch 44, train_loss 1278.2962646484375\n",
      "epoch 44, val_loss 455.6890869140625\n",
      "epoch 45, train_loss 1278.2960205078125\n",
      "epoch 45, val_loss 455.68890380859375\n",
      "epoch 46, train_loss 1278.2957763671875\n",
      "epoch 46, val_loss 455.6888732910156\n",
      "epoch 47, train_loss 1278.2955322265625\n",
      "epoch 47, val_loss 455.688720703125\n",
      "epoch 48, train_loss 1278.2952880859375\n",
      "epoch 48, val_loss 455.6885070800781\n",
      "epoch 49, train_loss 1278.2950439453125\n",
      "epoch 49, val_loss 455.68841552734375\n",
      "epoch 50, train_loss 1278.2947998046875\n",
      "epoch 50, val_loss 455.6882629394531\n",
      "epoch 51, train_loss 1278.294677734375\n",
      "epoch 51, val_loss 455.6881103515625\n",
      "epoch 52, train_loss 1278.29443359375\n",
      "epoch 52, val_loss 455.68798828125\n",
      "epoch 53, train_loss 1278.2940673828125\n",
      "epoch 53, val_loss 455.6878356933594\n",
      "epoch 54, train_loss 1278.293701171875\n",
      "epoch 54, val_loss 455.6876525878906\n",
      "epoch 55, train_loss 1278.293701171875\n",
      "epoch 55, val_loss 455.6875305175781\n",
      "epoch 56, train_loss 1278.29345703125\n",
      "epoch 56, val_loss 455.6873474121094\n",
      "epoch 57, train_loss 1278.2930908203125\n",
      "epoch 57, val_loss 455.687255859375\n",
      "epoch 58, train_loss 1278.2928466796875\n",
      "epoch 58, val_loss 455.6870422363281\n",
      "epoch 59, train_loss 1278.292724609375\n",
      "epoch 59, val_loss 455.6868896484375\n",
      "epoch 60, train_loss 1278.29248046875\n",
      "epoch 60, val_loss 455.6867370605469\n",
      "epoch 61, train_loss 1278.2921142578125\n",
      "epoch 61, val_loss 455.6866149902344\n",
      "epoch 62, train_loss 1278.2918701171875\n",
      "epoch 62, val_loss 455.6864929199219\n",
      "epoch 63, train_loss 1278.2916259765625\n",
      "epoch 63, val_loss 455.686279296875\n",
      "epoch 64, train_loss 1278.2913818359375\n",
      "epoch 64, val_loss 455.68621826171875\n",
      "epoch 65, train_loss 1278.2911376953125\n",
      "epoch 65, val_loss 455.68603515625\n",
      "epoch 66, train_loss 1278.2908935546875\n",
      "epoch 66, val_loss 455.6858825683594\n",
      "epoch 67, train_loss 1278.290771484375\n",
      "epoch 67, val_loss 455.6856994628906\n",
      "epoch 68, train_loss 1278.290283203125\n",
      "epoch 68, val_loss 455.685546875\n",
      "epoch 69, train_loss 1278.2901611328125\n",
      "epoch 69, val_loss 455.6854553222656\n",
      "epoch 70, train_loss 1278.2899169921875\n",
      "epoch 70, val_loss 455.6853332519531\n",
      "epoch 71, train_loss 1278.2896728515625\n",
      "epoch 71, val_loss 455.6852111816406\n",
      "epoch 72, train_loss 1278.28955078125\n",
      "epoch 72, val_loss 455.68499755859375\n",
      "epoch 73, train_loss 1278.2891845703125\n",
      "epoch 73, val_loss 455.68487548828125\n",
      "epoch 74, train_loss 1278.2889404296875\n",
      "epoch 74, val_loss 455.6846618652344\n",
      "epoch 75, train_loss 1278.2886962890625\n",
      "epoch 75, val_loss 455.6845703125\n",
      "epoch 76, train_loss 1278.2884521484375\n",
      "epoch 76, val_loss 455.6844177246094\n",
      "epoch 77, train_loss 1278.2884521484375\n",
      "epoch 77, val_loss 455.6842956542969\n",
      "epoch 78, train_loss 1278.2882080078125\n",
      "epoch 78, val_loss 455.6841125488281\n",
      "epoch 79, train_loss 1278.287841796875\n",
      "epoch 79, val_loss 455.6839599609375\n",
      "epoch 80, train_loss 1278.2874755859375\n",
      "epoch 80, val_loss 455.683837890625\n",
      "epoch 81, train_loss 1278.287109375\n",
      "epoch 81, val_loss 455.6836242675781\n",
      "epoch 82, train_loss 1278.287109375\n",
      "epoch 82, val_loss 455.68359375\n",
      "epoch 83, train_loss 1278.2867431640625\n",
      "epoch 83, val_loss 455.68341064453125\n",
      "epoch 84, train_loss 1278.2864990234375\n",
      "epoch 84, val_loss 455.6831970214844\n",
      "epoch 85, train_loss 1278.2862548828125\n",
      "epoch 85, val_loss 455.6830749511719\n",
      "epoch 86, train_loss 1278.2861328125\n",
      "epoch 86, val_loss 455.6829528808594\n",
      "epoch 87, train_loss 1278.285888671875\n",
      "epoch 87, val_loss 455.68280029296875\n",
      "epoch 88, train_loss 1278.2855224609375\n",
      "epoch 88, val_loss 455.6826171875\n",
      "epoch 89, train_loss 1278.2852783203125\n",
      "epoch 89, val_loss 455.6824645996094\n",
      "epoch 90, train_loss 1278.2850341796875\n",
      "epoch 90, val_loss 455.6823425292969\n",
      "epoch 91, train_loss 1278.284912109375\n",
      "epoch 91, val_loss 455.6822204589844\n",
      "epoch 92, train_loss 1278.28466796875\n",
      "epoch 92, val_loss 455.6820983886719\n",
      "epoch 93, train_loss 1278.284423828125\n",
      "epoch 93, val_loss 455.6819152832031\n",
      "epoch 94, train_loss 1278.2840576171875\n",
      "epoch 94, val_loss 455.6817932128906\n",
      "epoch 95, train_loss 1278.2838134765625\n",
      "epoch 95, val_loss 455.68170166015625\n",
      "epoch 96, train_loss 1278.28369140625\n",
      "epoch 96, val_loss 455.6814880371094\n",
      "epoch 97, train_loss 1278.283447265625\n",
      "epoch 97, val_loss 455.68133544921875\n",
      "epoch 98, train_loss 1278.283203125\n",
      "epoch 98, val_loss 455.68115234375\n",
      "epoch 99, train_loss 1278.2828369140625\n",
      "epoch 99, val_loss 455.68109130859375\n",
      "Parameter containing:\n",
      "tensor([3.5982e-07], requires_grad=True)\n",
      "iter 30, train_loss_regularization 1.0185976028442383\n",
      "iter 30, val_loss_regularization 1.0185976028442383\n",
      "epoch 0, train_loss 1278.28271484375\n",
      "epoch 0, val_loss 455.680908203125\n",
      "epoch 1, train_loss 1278.2825927734375\n",
      "epoch 1, val_loss 455.6807556152344\n",
      "epoch 2, train_loss 1278.2822265625\n",
      "epoch 2, val_loss 455.6806335449219\n",
      "epoch 3, train_loss 1278.281982421875\n",
      "epoch 3, val_loss 455.6804504394531\n",
      "epoch 4, train_loss 1278.2816162109375\n",
      "epoch 4, val_loss 455.6803283691406\n",
      "epoch 5, train_loss 1278.281494140625\n",
      "epoch 5, val_loss 455.68017578125\n",
      "epoch 6, train_loss 1278.28125\n",
      "epoch 6, val_loss 455.6800537109375\n",
      "epoch 7, train_loss 1278.2811279296875\n",
      "epoch 7, val_loss 455.6798400878906\n",
      "epoch 8, train_loss 1278.2808837890625\n",
      "epoch 8, val_loss 455.67974853515625\n",
      "epoch 9, train_loss 1278.2806396484375\n",
      "epoch 9, val_loss 455.67962646484375\n",
      "epoch 10, train_loss 1278.2802734375\n",
      "epoch 10, val_loss 455.67950439453125\n",
      "epoch 11, train_loss 1278.2801513671875\n",
      "epoch 11, val_loss 455.67926025390625\n",
      "epoch 12, train_loss 1278.27978515625\n",
      "epoch 12, val_loss 455.67913818359375\n",
      "epoch 13, train_loss 1278.27978515625\n",
      "epoch 13, val_loss 455.6789855957031\n",
      "epoch 14, train_loss 1278.279296875\n",
      "epoch 14, val_loss 455.6788330078125\n",
      "epoch 15, train_loss 1278.279052734375\n",
      "epoch 15, val_loss 455.6787109375\n",
      "epoch 16, train_loss 1278.27880859375\n",
      "epoch 16, val_loss 455.67852783203125\n",
      "epoch 17, train_loss 1278.2786865234375\n",
      "epoch 17, val_loss 455.678466796875\n",
      "epoch 18, train_loss 1278.2784423828125\n",
      "epoch 18, val_loss 455.6782531738281\n",
      "epoch 19, train_loss 1278.2779541015625\n",
      "epoch 19, val_loss 455.6781311035156\n",
      "epoch 20, train_loss 1278.27783203125\n",
      "epoch 20, val_loss 455.67791748046875\n",
      "epoch 21, train_loss 1278.2777099609375\n",
      "epoch 21, val_loss 455.67779541015625\n",
      "epoch 22, train_loss 1278.27734375\n",
      "epoch 22, val_loss 455.6777038574219\n",
      "epoch 23, train_loss 1278.2772216796875\n",
      "epoch 23, val_loss 455.67755126953125\n",
      "epoch 24, train_loss 1278.27685546875\n",
      "epoch 24, val_loss 455.6773681640625\n",
      "epoch 25, train_loss 1278.276611328125\n",
      "epoch 25, val_loss 455.6772155761719\n",
      "epoch 26, train_loss 1278.276611328125\n",
      "epoch 26, val_loss 455.6770935058594\n",
      "epoch 27, train_loss 1278.2762451171875\n",
      "epoch 27, val_loss 455.6769104003906\n",
      "epoch 28, train_loss 1278.276123046875\n",
      "epoch 28, val_loss 455.6767883300781\n",
      "epoch 29, train_loss 1278.27587890625\n",
      "epoch 29, val_loss 455.6766662597656\n",
      "epoch 30, train_loss 1278.275634765625\n",
      "epoch 30, val_loss 455.676513671875\n",
      "epoch 31, train_loss 1278.275390625\n",
      "epoch 31, val_loss 455.6763610839844\n",
      "epoch 32, train_loss 1278.275146484375\n",
      "epoch 32, val_loss 455.6761779785156\n",
      "epoch 33, train_loss 1278.27490234375\n",
      "epoch 33, val_loss 455.6759948730469\n",
      "epoch 34, train_loss 1278.274658203125\n",
      "epoch 34, val_loss 455.6759948730469\n",
      "epoch 35, train_loss 1278.2742919921875\n",
      "epoch 35, val_loss 455.67578125\n",
      "epoch 36, train_loss 1278.2742919921875\n",
      "epoch 36, val_loss 455.6756286621094\n",
      "epoch 37, train_loss 1278.27392578125\n",
      "epoch 37, val_loss 455.6755065917969\n",
      "epoch 38, train_loss 1278.2738037109375\n",
      "epoch 38, val_loss 455.67529296875\n",
      "epoch 39, train_loss 1278.2735595703125\n",
      "epoch 39, val_loss 455.6752624511719\n",
      "epoch 40, train_loss 1278.2730712890625\n",
      "epoch 40, val_loss 455.6750793457031\n",
      "epoch 41, train_loss 1278.27294921875\n",
      "epoch 41, val_loss 455.6749572753906\n",
      "epoch 42, train_loss 1278.272705078125\n",
      "epoch 42, val_loss 455.67474365234375\n",
      "epoch 43, train_loss 1278.2725830078125\n",
      "epoch 43, val_loss 455.67462158203125\n",
      "epoch 44, train_loss 1278.272216796875\n",
      "epoch 44, val_loss 455.67449951171875\n",
      "epoch 45, train_loss 1278.27197265625\n",
      "epoch 45, val_loss 455.6743469238281\n",
      "epoch 46, train_loss 1278.2716064453125\n",
      "epoch 46, val_loss 455.67425537109375\n",
      "epoch 47, train_loss 1278.2716064453125\n",
      "epoch 47, val_loss 455.6740417480469\n",
      "epoch 48, train_loss 1278.271240234375\n",
      "epoch 48, val_loss 455.6739196777344\n",
      "epoch 49, train_loss 1278.2711181640625\n",
      "epoch 49, val_loss 455.6737365722656\n",
      "epoch 50, train_loss 1278.2708740234375\n",
      "epoch 50, val_loss 455.6736755371094\n",
      "epoch 51, train_loss 1278.2706298828125\n",
      "epoch 51, val_loss 455.6734313964844\n",
      "epoch 52, train_loss 1278.2703857421875\n",
      "epoch 52, val_loss 455.67327880859375\n",
      "epoch 53, train_loss 1278.2701416015625\n",
      "epoch 53, val_loss 455.6732177734375\n",
      "epoch 54, train_loss 1278.269775390625\n",
      "epoch 54, val_loss 455.6730041503906\n",
      "epoch 55, train_loss 1278.26953125\n",
      "epoch 55, val_loss 455.6728820800781\n",
      "epoch 56, train_loss 1278.2694091796875\n",
      "epoch 56, val_loss 455.67279052734375\n",
      "epoch 57, train_loss 1278.2691650390625\n",
      "epoch 57, val_loss 455.6725769042969\n",
      "epoch 58, train_loss 1278.268798828125\n",
      "epoch 58, val_loss 455.6724548339844\n",
      "epoch 59, train_loss 1278.2686767578125\n",
      "epoch 59, val_loss 455.67230224609375\n",
      "epoch 60, train_loss 1278.2685546875\n",
      "epoch 60, val_loss 455.6722106933594\n",
      "epoch 61, train_loss 1278.268310546875\n",
      "epoch 61, val_loss 455.6719970703125\n",
      "epoch 62, train_loss 1278.26806640625\n",
      "epoch 62, val_loss 455.671875\n",
      "epoch 63, train_loss 1278.2677001953125\n",
      "epoch 63, val_loss 455.6717224121094\n",
      "epoch 64, train_loss 1278.2674560546875\n",
      "epoch 64, val_loss 455.671630859375\n",
      "epoch 65, train_loss 1278.2674560546875\n",
      "epoch 65, val_loss 455.67144775390625\n",
      "epoch 66, train_loss 1278.26708984375\n",
      "epoch 66, val_loss 455.6712951660156\n",
      "epoch 67, train_loss 1278.266845703125\n",
      "epoch 67, val_loss 455.6711730957031\n",
      "epoch 68, train_loss 1278.2666015625\n",
      "epoch 68, val_loss 455.6709899902344\n",
      "epoch 69, train_loss 1278.2662353515625\n",
      "epoch 69, val_loss 455.6708679199219\n",
      "epoch 70, train_loss 1278.2659912109375\n",
      "epoch 70, val_loss 455.6707458496094\n",
      "epoch 71, train_loss 1278.2659912109375\n",
      "epoch 71, val_loss 455.6705322265625\n",
      "epoch 72, train_loss 1278.265625\n",
      "epoch 72, val_loss 455.67041015625\n",
      "epoch 73, train_loss 1278.2655029296875\n",
      "epoch 73, val_loss 455.6703186035156\n",
      "epoch 74, train_loss 1278.26513671875\n",
      "epoch 74, val_loss 455.6701354980469\n",
      "epoch 75, train_loss 1278.264892578125\n",
      "epoch 75, val_loss 455.6699523925781\n",
      "epoch 76, train_loss 1278.264892578125\n",
      "epoch 76, val_loss 455.6697998046875\n",
      "epoch 77, train_loss 1278.264404296875\n",
      "epoch 77, val_loss 455.6697082519531\n",
      "epoch 78, train_loss 1278.26416015625\n",
      "epoch 78, val_loss 455.6695556640625\n",
      "epoch 79, train_loss 1278.2637939453125\n",
      "epoch 79, val_loss 455.6694030761719\n",
      "epoch 80, train_loss 1278.2637939453125\n",
      "epoch 80, val_loss 455.66925048828125\n",
      "epoch 81, train_loss 1278.2635498046875\n",
      "epoch 81, val_loss 455.6690979003906\n",
      "epoch 82, train_loss 1278.2633056640625\n",
      "epoch 82, val_loss 455.66900634765625\n",
      "epoch 83, train_loss 1278.2630615234375\n",
      "epoch 83, val_loss 455.6688232421875\n",
      "epoch 84, train_loss 1278.2628173828125\n",
      "epoch 84, val_loss 455.6686706542969\n",
      "epoch 85, train_loss 1278.2626953125\n",
      "epoch 85, val_loss 455.6685791015625\n",
      "epoch 86, train_loss 1278.262451171875\n",
      "epoch 86, val_loss 455.6684265136719\n",
      "epoch 87, train_loss 1278.2620849609375\n",
      "epoch 87, val_loss 455.6682434082031\n",
      "epoch 88, train_loss 1278.261962890625\n",
      "epoch 88, val_loss 455.6681213378906\n",
      "epoch 89, train_loss 1278.26171875\n",
      "epoch 89, val_loss 455.66796875\n",
      "epoch 90, train_loss 1278.261474609375\n",
      "epoch 90, val_loss 455.6678466796875\n",
      "epoch 91, train_loss 1278.26123046875\n",
      "epoch 91, val_loss 455.66766357421875\n",
      "epoch 92, train_loss 1278.2611083984375\n",
      "epoch 92, val_loss 455.6675109863281\n",
      "epoch 93, train_loss 1278.2607421875\n",
      "epoch 93, val_loss 455.66741943359375\n",
      "epoch 94, train_loss 1278.2603759765625\n",
      "epoch 94, val_loss 455.667236328125\n",
      "epoch 95, train_loss 1278.26025390625\n",
      "epoch 95, val_loss 455.6671142578125\n",
      "epoch 96, train_loss 1278.2598876953125\n",
      "epoch 96, val_loss 455.66693115234375\n",
      "epoch 97, train_loss 1278.259765625\n",
      "epoch 97, val_loss 455.6668395996094\n",
      "epoch 98, train_loss 1278.259521484375\n",
      "epoch 98, val_loss 455.6666564941406\n",
      "epoch 99, train_loss 1278.25927734375\n",
      "epoch 99, val_loss 455.6665344238281\n",
      "Parameter containing:\n",
      "tensor([2.2789e-07], requires_grad=True)\n",
      "iter 31, train_loss_regularization 1.0121279954910278\n",
      "iter 31, val_loss_regularization 1.0121279954910278\n",
      "epoch 0, train_loss 1278.259033203125\n",
      "epoch 0, val_loss 455.66632080078125\n",
      "epoch 1, train_loss 1278.2589111328125\n",
      "epoch 1, val_loss 455.6661682128906\n",
      "epoch 2, train_loss 1278.258544921875\n",
      "epoch 2, val_loss 455.66607666015625\n",
      "epoch 3, train_loss 1278.2584228515625\n",
      "epoch 3, val_loss 455.66595458984375\n",
      "epoch 4, train_loss 1278.258056640625\n",
      "epoch 4, val_loss 455.6657409667969\n",
      "epoch 5, train_loss 1278.2579345703125\n",
      "epoch 5, val_loss 455.6656799316406\n",
      "epoch 6, train_loss 1278.2576904296875\n",
      "epoch 6, val_loss 455.6654968261719\n",
      "epoch 7, train_loss 1278.2574462890625\n",
      "epoch 7, val_loss 455.6654052734375\n",
      "epoch 8, train_loss 1278.2572021484375\n",
      "epoch 8, val_loss 455.66522216796875\n",
      "epoch 9, train_loss 1278.257080078125\n",
      "epoch 9, val_loss 455.6650390625\n",
      "epoch 10, train_loss 1278.2567138671875\n",
      "epoch 10, val_loss 455.6649475097656\n",
      "epoch 11, train_loss 1278.2564697265625\n",
      "epoch 11, val_loss 455.664794921875\n",
      "epoch 12, train_loss 1278.2564697265625\n",
      "epoch 12, val_loss 455.66461181640625\n",
      "epoch 13, train_loss 1278.2559814453125\n",
      "epoch 13, val_loss 455.66448974609375\n",
      "epoch 14, train_loss 1278.255859375\n",
      "epoch 14, val_loss 455.6643371582031\n",
      "epoch 15, train_loss 1278.255615234375\n",
      "epoch 15, val_loss 455.6642150878906\n",
      "epoch 16, train_loss 1278.25537109375\n",
      "epoch 16, val_loss 455.6640319824219\n",
      "epoch 17, train_loss 1278.255126953125\n",
      "epoch 17, val_loss 455.6639099121094\n",
      "epoch 18, train_loss 1278.2547607421875\n",
      "epoch 18, val_loss 455.663818359375\n",
      "epoch 19, train_loss 1278.254638671875\n",
      "epoch 19, val_loss 455.6636657714844\n",
      "epoch 20, train_loss 1278.2545166015625\n",
      "epoch 20, val_loss 455.66351318359375\n",
      "epoch 21, train_loss 1278.2540283203125\n",
      "epoch 21, val_loss 455.6633605957031\n",
      "epoch 22, train_loss 1278.2540283203125\n",
      "epoch 22, val_loss 455.6632080078125\n",
      "epoch 23, train_loss 1278.25390625\n",
      "epoch 23, val_loss 455.6630554199219\n",
      "epoch 24, train_loss 1278.2535400390625\n",
      "epoch 24, val_loss 455.66290283203125\n",
      "epoch 25, train_loss 1278.2532958984375\n",
      "epoch 25, val_loss 455.6627502441406\n",
      "epoch 26, train_loss 1278.2530517578125\n",
      "epoch 26, val_loss 455.6627197265625\n",
      "epoch 27, train_loss 1278.2528076171875\n",
      "epoch 27, val_loss 455.6625061035156\n",
      "epoch 28, train_loss 1278.252685546875\n",
      "epoch 28, val_loss 455.6623229980469\n",
      "epoch 29, train_loss 1278.25244140625\n",
      "epoch 29, val_loss 455.6622009277344\n",
      "epoch 30, train_loss 1278.252197265625\n",
      "epoch 30, val_loss 455.6620788574219\n",
      "epoch 31, train_loss 1278.2518310546875\n",
      "epoch 31, val_loss 455.66192626953125\n",
      "epoch 32, train_loss 1278.251708984375\n",
      "epoch 32, val_loss 455.66180419921875\n",
      "epoch 33, train_loss 1278.25146484375\n",
      "epoch 33, val_loss 455.6615905761719\n",
      "epoch 34, train_loss 1278.251220703125\n",
      "epoch 34, val_loss 455.6614990234375\n",
      "epoch 35, train_loss 1278.2509765625\n",
      "epoch 35, val_loss 455.6613464355469\n",
      "epoch 36, train_loss 1278.250732421875\n",
      "epoch 36, val_loss 455.66119384765625\n",
      "epoch 37, train_loss 1278.2506103515625\n",
      "epoch 37, val_loss 455.6610412597656\n",
      "epoch 38, train_loss 1278.250244140625\n",
      "epoch 38, val_loss 455.6609191894531\n",
      "epoch 39, train_loss 1278.2501220703125\n",
      "epoch 39, val_loss 455.6607971191406\n",
      "epoch 40, train_loss 1278.2498779296875\n",
      "epoch 40, val_loss 455.66058349609375\n",
      "epoch 41, train_loss 1278.2496337890625\n",
      "epoch 41, val_loss 455.6604309082031\n",
      "epoch 42, train_loss 1278.2493896484375\n",
      "epoch 42, val_loss 455.66033935546875\n",
      "epoch 43, train_loss 1278.2491455078125\n",
      "epoch 43, val_loss 455.66015625\n",
      "epoch 44, train_loss 1278.2489013671875\n",
      "epoch 44, val_loss 455.6600341796875\n",
      "epoch 45, train_loss 1278.248779296875\n",
      "epoch 45, val_loss 455.6598815917969\n",
      "epoch 46, train_loss 1278.24853515625\n",
      "epoch 46, val_loss 455.6597900390625\n",
      "epoch 47, train_loss 1278.2481689453125\n",
      "epoch 47, val_loss 455.6595764160156\n",
      "epoch 48, train_loss 1278.2479248046875\n",
      "epoch 48, val_loss 455.6594543457031\n",
      "epoch 49, train_loss 1278.2476806640625\n",
      "epoch 49, val_loss 455.6593017578125\n",
      "epoch 50, train_loss 1278.24755859375\n",
      "epoch 50, val_loss 455.65924072265625\n",
      "epoch 51, train_loss 1278.247314453125\n",
      "epoch 51, val_loss 455.6590270996094\n",
      "epoch 52, train_loss 1278.2471923828125\n",
      "epoch 52, val_loss 455.65887451171875\n",
      "epoch 53, train_loss 1278.2467041015625\n",
      "epoch 53, val_loss 455.65875244140625\n",
      "epoch 54, train_loss 1278.24658203125\n",
      "epoch 54, val_loss 455.6585693359375\n",
      "epoch 55, train_loss 1278.2464599609375\n",
      "epoch 55, val_loss 455.65850830078125\n",
      "epoch 56, train_loss 1278.2462158203125\n",
      "epoch 56, val_loss 455.6583251953125\n",
      "epoch 57, train_loss 1278.2459716796875\n",
      "epoch 57, val_loss 455.6581726074219\n",
      "epoch 58, train_loss 1278.2457275390625\n",
      "epoch 58, val_loss 455.6580810546875\n",
      "epoch 59, train_loss 1278.245361328125\n",
      "epoch 59, val_loss 455.6579284667969\n",
      "epoch 60, train_loss 1278.2452392578125\n",
      "epoch 60, val_loss 455.6577453613281\n",
      "epoch 61, train_loss 1278.2451171875\n",
      "epoch 61, val_loss 455.6576232910156\n",
      "epoch 62, train_loss 1278.24462890625\n",
      "epoch 62, val_loss 455.657470703125\n",
      "epoch 63, train_loss 1278.24462890625\n",
      "epoch 63, val_loss 455.6573181152344\n",
      "epoch 64, train_loss 1278.244384765625\n",
      "epoch 64, val_loss 455.6571960449219\n",
      "epoch 65, train_loss 1278.2440185546875\n",
      "epoch 65, val_loss 455.6569519042969\n",
      "epoch 66, train_loss 1278.2437744140625\n",
      "epoch 66, val_loss 455.6569519042969\n",
      "epoch 67, train_loss 1278.243408203125\n",
      "epoch 67, val_loss 455.65673828125\n",
      "epoch 68, train_loss 1278.243408203125\n",
      "epoch 68, val_loss 455.6566162109375\n",
      "epoch 69, train_loss 1278.2432861328125\n",
      "epoch 69, val_loss 455.6564636230469\n",
      "epoch 70, train_loss 1278.2430419921875\n",
      "epoch 70, val_loss 455.6563415527344\n",
      "epoch 71, train_loss 1278.24267578125\n",
      "epoch 71, val_loss 455.6562194824219\n",
      "epoch 72, train_loss 1278.2425537109375\n",
      "epoch 72, val_loss 455.6560974121094\n",
      "epoch 73, train_loss 1278.2423095703125\n",
      "epoch 73, val_loss 455.6559143066406\n",
      "epoch 74, train_loss 1278.241943359375\n",
      "epoch 74, val_loss 455.6557922363281\n",
      "epoch 75, train_loss 1278.24169921875\n",
      "epoch 75, val_loss 455.6556396484375\n",
      "epoch 76, train_loss 1278.24169921875\n",
      "epoch 76, val_loss 455.6554870605469\n",
      "epoch 77, train_loss 1278.2413330078125\n",
      "epoch 77, val_loss 455.6552429199219\n",
      "epoch 78, train_loss 1278.2410888671875\n",
      "epoch 78, val_loss 455.6553039550781\n",
      "epoch 79, train_loss 1278.240966796875\n",
      "epoch 79, val_loss 455.655029296875\n",
      "epoch 80, train_loss 1278.2406005859375\n",
      "epoch 80, val_loss 455.6549072265625\n",
      "epoch 81, train_loss 1278.2403564453125\n",
      "epoch 81, val_loss 455.6547546386719\n",
      "epoch 82, train_loss 1278.240234375\n",
      "epoch 82, val_loss 455.6545715332031\n",
      "epoch 83, train_loss 1278.2398681640625\n",
      "epoch 83, val_loss 455.6545104980469\n",
      "epoch 84, train_loss 1278.2396240234375\n",
      "epoch 84, val_loss 455.654296875\n",
      "epoch 85, train_loss 1278.239501953125\n",
      "epoch 85, val_loss 455.65411376953125\n",
      "epoch 86, train_loss 1278.2391357421875\n",
      "epoch 86, val_loss 455.654052734375\n",
      "epoch 87, train_loss 1278.2391357421875\n",
      "epoch 87, val_loss 455.65386962890625\n",
      "epoch 88, train_loss 1278.2388916015625\n",
      "epoch 88, val_loss 455.65374755859375\n",
      "epoch 89, train_loss 1278.238525390625\n",
      "epoch 89, val_loss 455.65362548828125\n",
      "epoch 90, train_loss 1278.2384033203125\n",
      "epoch 90, val_loss 455.65350341796875\n",
      "epoch 91, train_loss 1278.2381591796875\n",
      "epoch 91, val_loss 455.65338134765625\n",
      "epoch 92, train_loss 1278.23779296875\n",
      "epoch 92, val_loss 455.65313720703125\n",
      "epoch 93, train_loss 1278.23779296875\n",
      "epoch 93, val_loss 455.653076171875\n",
      "epoch 94, train_loss 1278.237548828125\n",
      "epoch 94, val_loss 455.6529235839844\n",
      "epoch 95, train_loss 1278.2371826171875\n",
      "epoch 95, val_loss 455.6527404785156\n",
      "epoch 96, train_loss 1278.237060546875\n",
      "epoch 96, val_loss 455.652587890625\n",
      "epoch 97, train_loss 1278.23681640625\n",
      "epoch 97, val_loss 455.6524963378906\n",
      "epoch 98, train_loss 1278.2366943359375\n",
      "epoch 98, val_loss 455.6523742675781\n",
      "epoch 99, train_loss 1278.2362060546875\n",
      "epoch 99, val_loss 455.6522216796875\n",
      "Parameter containing:\n",
      "tensor([1.4453e-07], requires_grad=True)\n",
      "iter 32, train_loss_regularization 1.0057759284973145\n",
      "iter 32, val_loss_regularization 1.0057759284973145\n",
      "epoch 0, train_loss 1278.236083984375\n",
      "epoch 0, val_loss 455.6519470214844\n",
      "epoch 1, train_loss 1278.23583984375\n",
      "epoch 1, val_loss 455.65191650390625\n",
      "epoch 2, train_loss 1278.235595703125\n",
      "epoch 2, val_loss 455.6517639160156\n",
      "epoch 3, train_loss 1278.2353515625\n",
      "epoch 3, val_loss 455.65167236328125\n",
      "epoch 4, train_loss 1278.2352294921875\n",
      "epoch 4, val_loss 455.6514587402344\n",
      "epoch 5, train_loss 1278.235107421875\n",
      "epoch 5, val_loss 455.65142822265625\n",
      "epoch 6, train_loss 1278.23486328125\n",
      "epoch 6, val_loss 455.6512145996094\n",
      "epoch 7, train_loss 1278.2344970703125\n",
      "epoch 7, val_loss 455.6510925292969\n",
      "epoch 8, train_loss 1278.234375\n",
      "epoch 8, val_loss 455.6509094238281\n",
      "epoch 9, train_loss 1278.234130859375\n",
      "epoch 9, val_loss 455.6507568359375\n",
      "epoch 10, train_loss 1278.23388671875\n",
      "epoch 10, val_loss 455.650634765625\n",
      "epoch 11, train_loss 1278.233642578125\n",
      "epoch 11, val_loss 455.65045166015625\n",
      "epoch 12, train_loss 1278.2332763671875\n",
      "epoch 12, val_loss 455.6502990722656\n",
      "epoch 13, train_loss 1278.233154296875\n",
      "epoch 13, val_loss 455.65020751953125\n",
      "epoch 14, train_loss 1278.23291015625\n",
      "epoch 14, val_loss 455.6500549316406\n",
      "epoch 15, train_loss 1278.2325439453125\n",
      "epoch 15, val_loss 455.64996337890625\n",
      "epoch 16, train_loss 1278.2325439453125\n",
      "epoch 16, val_loss 455.6497497558594\n",
      "epoch 17, train_loss 1278.232177734375\n",
      "epoch 17, val_loss 455.6496276855469\n",
      "epoch 18, train_loss 1278.232177734375\n",
      "epoch 18, val_loss 455.6495056152344\n",
      "epoch 19, train_loss 1278.2318115234375\n",
      "epoch 19, val_loss 455.6493225097656\n",
      "epoch 20, train_loss 1278.2314453125\n",
      "epoch 20, val_loss 455.6492614746094\n",
      "epoch 21, train_loss 1278.231201171875\n",
      "epoch 21, val_loss 455.6490783691406\n",
      "epoch 22, train_loss 1278.231201171875\n",
      "epoch 22, val_loss 455.6489562988281\n",
      "epoch 23, train_loss 1278.2308349609375\n",
      "epoch 23, val_loss 455.6488037109375\n",
      "epoch 24, train_loss 1278.2305908203125\n",
      "epoch 24, val_loss 455.648681640625\n",
      "epoch 25, train_loss 1278.2303466796875\n",
      "epoch 25, val_loss 455.64849853515625\n",
      "epoch 26, train_loss 1278.2303466796875\n",
      "epoch 26, val_loss 455.64837646484375\n",
      "epoch 27, train_loss 1278.2298583984375\n",
      "epoch 27, val_loss 455.648193359375\n",
      "epoch 28, train_loss 1278.229736328125\n",
      "epoch 28, val_loss 455.6480712890625\n",
      "epoch 29, train_loss 1278.2296142578125\n",
      "epoch 29, val_loss 455.6479187011719\n",
      "epoch 30, train_loss 1278.2294921875\n",
      "epoch 30, val_loss 455.6477966308594\n",
      "epoch 31, train_loss 1278.22900390625\n",
      "epoch 31, val_loss 455.6476745605469\n",
      "epoch 32, train_loss 1278.2288818359375\n",
      "epoch 32, val_loss 455.6475830078125\n",
      "epoch 33, train_loss 1278.2286376953125\n",
      "epoch 33, val_loss 455.6473693847656\n",
      "epoch 34, train_loss 1278.2283935546875\n",
      "epoch 34, val_loss 455.6472473144531\n",
      "epoch 35, train_loss 1278.2281494140625\n",
      "epoch 35, val_loss 455.6470031738281\n",
      "epoch 36, train_loss 1278.22802734375\n",
      "epoch 36, val_loss 455.64691162109375\n",
      "epoch 37, train_loss 1278.2276611328125\n",
      "epoch 37, val_loss 455.6468200683594\n",
      "epoch 38, train_loss 1278.2275390625\n",
      "epoch 38, val_loss 455.6466369628906\n",
      "epoch 39, train_loss 1278.227294921875\n",
      "epoch 39, val_loss 455.6465148925781\n",
      "epoch 40, train_loss 1278.22705078125\n",
      "epoch 40, val_loss 455.6463623046875\n",
      "epoch 41, train_loss 1278.226806640625\n",
      "epoch 41, val_loss 455.64630126953125\n",
      "epoch 42, train_loss 1278.2266845703125\n",
      "epoch 42, val_loss 455.6461181640625\n",
      "epoch 43, train_loss 1278.226318359375\n",
      "epoch 43, val_loss 455.64599609375\n",
      "epoch 44, train_loss 1278.2261962890625\n",
      "epoch 44, val_loss 455.6458435058594\n",
      "epoch 45, train_loss 1278.2259521484375\n",
      "epoch 45, val_loss 455.6456604003906\n",
      "epoch 46, train_loss 1278.2257080078125\n",
      "epoch 46, val_loss 455.6455383300781\n",
      "epoch 47, train_loss 1278.2254638671875\n",
      "epoch 47, val_loss 455.6454162597656\n",
      "epoch 48, train_loss 1278.2252197265625\n",
      "epoch 48, val_loss 455.645263671875\n",
      "epoch 49, train_loss 1278.22509765625\n",
      "epoch 49, val_loss 455.64508056640625\n",
      "epoch 50, train_loss 1278.224853515625\n",
      "epoch 50, val_loss 455.6449890136719\n",
      "epoch 51, train_loss 1278.2244873046875\n",
      "epoch 51, val_loss 455.6448669433594\n",
      "epoch 52, train_loss 1278.224365234375\n",
      "epoch 52, val_loss 455.64471435546875\n",
      "epoch 53, train_loss 1278.22412109375\n",
      "epoch 53, val_loss 455.64459228515625\n",
      "epoch 54, train_loss 1278.2239990234375\n",
      "epoch 54, val_loss 455.6443786621094\n",
      "epoch 55, train_loss 1278.2236328125\n",
      "epoch 55, val_loss 455.644287109375\n",
      "epoch 56, train_loss 1278.223388671875\n",
      "epoch 56, val_loss 455.6441345214844\n",
      "epoch 57, train_loss 1278.2232666015625\n",
      "epoch 57, val_loss 455.6439514160156\n",
      "epoch 58, train_loss 1278.2230224609375\n",
      "epoch 58, val_loss 455.64385986328125\n",
      "epoch 59, train_loss 1278.22265625\n",
      "epoch 59, val_loss 455.6437072753906\n",
      "epoch 60, train_loss 1278.222412109375\n",
      "epoch 60, val_loss 455.6435546875\n",
      "epoch 61, train_loss 1278.2222900390625\n",
      "epoch 61, val_loss 455.64337158203125\n",
      "epoch 62, train_loss 1278.22216796875\n",
      "epoch 62, val_loss 455.6432800292969\n",
      "epoch 63, train_loss 1278.2218017578125\n",
      "epoch 63, val_loss 455.6431579589844\n",
      "epoch 64, train_loss 1278.2218017578125\n",
      "epoch 64, val_loss 455.6430358886719\n",
      "epoch 65, train_loss 1278.2215576171875\n",
      "epoch 65, val_loss 455.642822265625\n",
      "epoch 66, train_loss 1278.2213134765625\n",
      "epoch 66, val_loss 455.6427001953125\n",
      "epoch 67, train_loss 1278.220947265625\n",
      "epoch 67, val_loss 455.642578125\n",
      "epoch 68, train_loss 1278.2208251953125\n",
      "epoch 68, val_loss 455.6424560546875\n",
      "epoch 69, train_loss 1278.2205810546875\n",
      "epoch 69, val_loss 455.6423034667969\n",
      "epoch 70, train_loss 1278.22021484375\n",
      "epoch 70, val_loss 455.6422119140625\n",
      "epoch 71, train_loss 1278.219970703125\n",
      "epoch 71, val_loss 455.6419982910156\n",
      "epoch 72, train_loss 1278.219970703125\n",
      "epoch 72, val_loss 455.6418762207031\n",
      "epoch 73, train_loss 1278.2197265625\n",
      "epoch 73, val_loss 455.6416931152344\n",
      "epoch 74, train_loss 1278.219482421875\n",
      "epoch 74, val_loss 455.6415710449219\n",
      "epoch 75, train_loss 1278.2191162109375\n",
      "epoch 75, val_loss 455.64141845703125\n",
      "epoch 76, train_loss 1278.218994140625\n",
      "epoch 76, val_loss 455.64129638671875\n",
      "epoch 77, train_loss 1278.21875\n",
      "epoch 77, val_loss 455.64117431640625\n",
      "epoch 78, train_loss 1278.218505859375\n",
      "epoch 78, val_loss 455.64105224609375\n",
      "epoch 79, train_loss 1278.2183837890625\n",
      "epoch 79, val_loss 455.640869140625\n",
      "epoch 80, train_loss 1278.2181396484375\n",
      "epoch 80, val_loss 455.6407470703125\n",
      "epoch 81, train_loss 1278.2178955078125\n",
      "epoch 81, val_loss 455.6406555175781\n",
      "epoch 82, train_loss 1278.2176513671875\n",
      "epoch 82, val_loss 455.6405029296875\n",
      "epoch 83, train_loss 1278.2174072265625\n",
      "epoch 83, val_loss 455.64031982421875\n",
      "epoch 84, train_loss 1278.21728515625\n",
      "epoch 84, val_loss 455.6401672363281\n",
      "epoch 85, train_loss 1278.217041015625\n",
      "epoch 85, val_loss 455.6400451660156\n",
      "epoch 86, train_loss 1278.2166748046875\n",
      "epoch 86, val_loss 455.6399230957031\n",
      "epoch 87, train_loss 1278.216552734375\n",
      "epoch 87, val_loss 455.6397399902344\n",
      "epoch 88, train_loss 1278.21630859375\n",
      "epoch 88, val_loss 455.6396789550781\n",
      "epoch 89, train_loss 1278.2161865234375\n",
      "epoch 89, val_loss 455.6394958496094\n",
      "epoch 90, train_loss 1278.2156982421875\n",
      "epoch 90, val_loss 455.63934326171875\n",
      "epoch 91, train_loss 1278.2156982421875\n",
      "epoch 91, val_loss 455.63916015625\n",
      "epoch 92, train_loss 1278.2154541015625\n",
      "epoch 92, val_loss 455.6390686035156\n",
      "epoch 93, train_loss 1278.2152099609375\n",
      "epoch 93, val_loss 455.638916015625\n",
      "epoch 94, train_loss 1278.2149658203125\n",
      "epoch 94, val_loss 455.6387939453125\n",
      "epoch 95, train_loss 1278.2147216796875\n",
      "epoch 95, val_loss 455.63861083984375\n",
      "epoch 96, train_loss 1278.214599609375\n",
      "epoch 96, val_loss 455.6385498046875\n",
      "epoch 97, train_loss 1278.21435546875\n",
      "epoch 97, val_loss 455.6383361816406\n",
      "epoch 98, train_loss 1278.2139892578125\n",
      "epoch 98, val_loss 455.6382141113281\n",
      "epoch 99, train_loss 1278.2138671875\n",
      "epoch 99, val_loss 455.6380310058594\n",
      "Parameter containing:\n",
      "tensor([9.1798e-08], requires_grad=True)\n",
      "iter 33, train_loss_regularization 0.9995390176773071\n",
      "iter 33, val_loss_regularization 0.9995390176773071\n",
      "epoch 0, train_loss 1278.213623046875\n",
      "epoch 0, val_loss 455.63800048828125\n",
      "epoch 1, train_loss 1278.21337890625\n",
      "epoch 1, val_loss 455.6377868652344\n",
      "epoch 2, train_loss 1278.2132568359375\n",
      "epoch 2, val_loss 455.6376647949219\n",
      "epoch 3, train_loss 1278.2130126953125\n",
      "epoch 3, val_loss 455.63751220703125\n",
      "epoch 4, train_loss 1278.212646484375\n",
      "epoch 4, val_loss 455.6374206542969\n",
      "epoch 5, train_loss 1278.21240234375\n",
      "epoch 5, val_loss 455.63720703125\n",
      "epoch 6, train_loss 1278.2122802734375\n",
      "epoch 6, val_loss 455.6370544433594\n",
      "epoch 7, train_loss 1278.2120361328125\n",
      "epoch 7, val_loss 455.636962890625\n",
      "epoch 8, train_loss 1278.2117919921875\n",
      "epoch 8, val_loss 455.63677978515625\n",
      "epoch 9, train_loss 1278.211669921875\n",
      "epoch 9, val_loss 455.63665771484375\n",
      "epoch 10, train_loss 1278.21142578125\n",
      "epoch 10, val_loss 455.6365051269531\n",
      "epoch 11, train_loss 1278.2113037109375\n",
      "epoch 11, val_loss 455.6363830566406\n",
      "epoch 12, train_loss 1278.2108154296875\n",
      "epoch 12, val_loss 455.6362609863281\n",
      "epoch 13, train_loss 1278.2108154296875\n",
      "epoch 13, val_loss 455.6361389160156\n",
      "epoch 14, train_loss 1278.2105712890625\n",
      "epoch 14, val_loss 455.635986328125\n",
      "epoch 15, train_loss 1278.210205078125\n",
      "epoch 15, val_loss 455.6358337402344\n",
      "epoch 16, train_loss 1278.210205078125\n",
      "epoch 16, val_loss 455.6357116699219\n",
      "epoch 17, train_loss 1278.2099609375\n",
      "epoch 17, val_loss 455.6355285644531\n",
      "epoch 18, train_loss 1278.209716796875\n",
      "epoch 18, val_loss 455.6354675292969\n",
      "epoch 19, train_loss 1278.20947265625\n",
      "epoch 19, val_loss 455.6352233886719\n",
      "epoch 20, train_loss 1278.2091064453125\n",
      "epoch 20, val_loss 455.6351318359375\n",
      "epoch 21, train_loss 1278.208984375\n",
      "epoch 21, val_loss 455.63494873046875\n",
      "epoch 22, train_loss 1278.208740234375\n",
      "epoch 22, val_loss 455.6348876953125\n",
      "epoch 23, train_loss 1278.20849609375\n",
      "epoch 23, val_loss 455.63470458984375\n",
      "epoch 24, train_loss 1278.2083740234375\n",
      "epoch 24, val_loss 455.63458251953125\n",
      "epoch 25, train_loss 1278.2081298828125\n",
      "epoch 25, val_loss 455.6344299316406\n",
      "epoch 26, train_loss 1278.2078857421875\n",
      "epoch 26, val_loss 455.63433837890625\n",
      "epoch 27, train_loss 1278.2076416015625\n",
      "epoch 27, val_loss 455.6341552734375\n",
      "epoch 28, train_loss 1278.2073974609375\n",
      "epoch 28, val_loss 455.634033203125\n",
      "epoch 29, train_loss 1278.2071533203125\n",
      "epoch 29, val_loss 455.6338806152344\n",
      "epoch 30, train_loss 1278.20703125\n",
      "epoch 30, val_loss 455.6337585449219\n",
      "epoch 31, train_loss 1278.206787109375\n",
      "epoch 31, val_loss 455.6335754394531\n",
      "epoch 32, train_loss 1278.20654296875\n",
      "epoch 32, val_loss 455.6334533691406\n",
      "epoch 33, train_loss 1278.206298828125\n",
      "epoch 33, val_loss 455.6333312988281\n",
      "epoch 34, train_loss 1278.2060546875\n",
      "epoch 34, val_loss 455.6332092285156\n",
      "epoch 35, train_loss 1278.2060546875\n",
      "epoch 35, val_loss 455.6329650878906\n",
      "epoch 36, train_loss 1278.205810546875\n",
      "epoch 36, val_loss 455.6329040527344\n",
      "epoch 37, train_loss 1278.2054443359375\n",
      "epoch 37, val_loss 455.63275146484375\n",
      "epoch 38, train_loss 1278.205322265625\n",
      "epoch 38, val_loss 455.63262939453125\n",
      "epoch 39, train_loss 1278.2049560546875\n",
      "epoch 39, val_loss 455.6324462890625\n",
      "epoch 40, train_loss 1278.204833984375\n",
      "epoch 40, val_loss 455.63238525390625\n",
      "epoch 41, train_loss 1278.2044677734375\n",
      "epoch 41, val_loss 455.6322021484375\n",
      "epoch 42, train_loss 1278.2044677734375\n",
      "epoch 42, val_loss 455.632080078125\n",
      "epoch 43, train_loss 1278.2041015625\n",
      "epoch 43, val_loss 455.6319580078125\n",
      "epoch 44, train_loss 1278.203857421875\n",
      "epoch 44, val_loss 455.6318359375\n",
      "epoch 45, train_loss 1278.20361328125\n",
      "epoch 45, val_loss 455.6316223144531\n",
      "epoch 46, train_loss 1278.2034912109375\n",
      "epoch 46, val_loss 455.6315002441406\n",
      "epoch 47, train_loss 1278.2032470703125\n",
      "epoch 47, val_loss 455.6313781738281\n",
      "epoch 48, train_loss 1278.202880859375\n",
      "epoch 48, val_loss 455.6311950683594\n",
      "epoch 49, train_loss 1278.202880859375\n",
      "epoch 49, val_loss 455.63104248046875\n",
      "epoch 50, train_loss 1278.2025146484375\n",
      "epoch 50, val_loss 455.6310119628906\n",
      "epoch 51, train_loss 1278.2022705078125\n",
      "epoch 51, val_loss 455.6307373046875\n",
      "epoch 52, train_loss 1278.2020263671875\n",
      "epoch 52, val_loss 455.63067626953125\n",
      "epoch 53, train_loss 1278.2020263671875\n",
      "epoch 53, val_loss 455.6304931640625\n",
      "epoch 54, train_loss 1278.2017822265625\n",
      "epoch 54, val_loss 455.6304626464844\n",
      "epoch 55, train_loss 1278.2015380859375\n",
      "epoch 55, val_loss 455.6302795410156\n",
      "epoch 56, train_loss 1278.2012939453125\n",
      "epoch 56, val_loss 455.630126953125\n",
      "epoch 57, train_loss 1278.2010498046875\n",
      "epoch 57, val_loss 455.6299133300781\n",
      "epoch 58, train_loss 1278.2008056640625\n",
      "epoch 58, val_loss 455.6298828125\n",
      "epoch 59, train_loss 1278.2005615234375\n",
      "epoch 59, val_loss 455.6296691894531\n",
      "epoch 60, train_loss 1278.200439453125\n",
      "epoch 60, val_loss 455.62957763671875\n",
      "epoch 61, train_loss 1278.2003173828125\n",
      "epoch 61, val_loss 455.6293640136719\n",
      "epoch 62, train_loss 1278.2000732421875\n",
      "epoch 62, val_loss 455.6293029785156\n",
      "epoch 63, train_loss 1278.1998291015625\n",
      "epoch 63, val_loss 455.6291198730469\n",
      "epoch 64, train_loss 1278.1995849609375\n",
      "epoch 64, val_loss 455.62896728515625\n",
      "epoch 65, train_loss 1278.1993408203125\n",
      "epoch 65, val_loss 455.6288757324219\n",
      "epoch 66, train_loss 1278.1990966796875\n",
      "epoch 66, val_loss 455.6287841796875\n",
      "epoch 67, train_loss 1278.198974609375\n",
      "epoch 67, val_loss 455.6285705566406\n",
      "epoch 68, train_loss 1278.1986083984375\n",
      "epoch 68, val_loss 455.6285095214844\n",
      "epoch 69, train_loss 1278.198486328125\n",
      "epoch 69, val_loss 455.6283264160156\n",
      "epoch 70, train_loss 1278.1983642578125\n",
      "epoch 70, val_loss 455.628173828125\n",
      "epoch 71, train_loss 1278.1981201171875\n",
      "epoch 71, val_loss 455.62799072265625\n",
      "epoch 72, train_loss 1278.19775390625\n",
      "epoch 72, val_loss 455.6279296875\n",
      "epoch 73, train_loss 1278.1976318359375\n",
      "epoch 73, val_loss 455.6277160644531\n",
      "epoch 74, train_loss 1278.1973876953125\n",
      "epoch 74, val_loss 455.6275939941406\n",
      "epoch 75, train_loss 1278.1971435546875\n",
      "epoch 75, val_loss 455.6274108886719\n",
      "epoch 76, train_loss 1278.1968994140625\n",
      "epoch 76, val_loss 455.6273193359375\n",
      "epoch 77, train_loss 1278.19677734375\n",
      "epoch 77, val_loss 455.62725830078125\n",
      "epoch 78, train_loss 1278.196533203125\n",
      "epoch 78, val_loss 455.62701416015625\n",
      "epoch 79, train_loss 1278.1961669921875\n",
      "epoch 79, val_loss 455.626953125\n",
      "epoch 80, train_loss 1278.196044921875\n",
      "epoch 80, val_loss 455.6268005371094\n",
      "epoch 81, train_loss 1278.19580078125\n",
      "epoch 81, val_loss 455.6266174316406\n",
      "epoch 82, train_loss 1278.1956787109375\n",
      "epoch 82, val_loss 455.62646484375\n",
      "epoch 83, train_loss 1278.1953125\n",
      "epoch 83, val_loss 455.62640380859375\n",
      "epoch 84, train_loss 1278.1951904296875\n",
      "epoch 84, val_loss 455.626220703125\n",
      "epoch 85, train_loss 1278.1949462890625\n",
      "epoch 85, val_loss 455.62603759765625\n",
      "epoch 86, train_loss 1278.194580078125\n",
      "epoch 86, val_loss 455.6259460449219\n",
      "epoch 87, train_loss 1278.1944580078125\n",
      "epoch 87, val_loss 455.6258239746094\n",
      "epoch 88, train_loss 1278.1942138671875\n",
      "epoch 88, val_loss 455.62567138671875\n",
      "epoch 89, train_loss 1278.1942138671875\n",
      "epoch 89, val_loss 455.62548828125\n",
      "epoch 90, train_loss 1278.1937255859375\n",
      "epoch 90, val_loss 455.6253662109375\n",
      "epoch 91, train_loss 1278.1937255859375\n",
      "epoch 91, val_loss 455.62530517578125\n",
      "epoch 92, train_loss 1278.1934814453125\n",
      "epoch 92, val_loss 455.6250915527344\n",
      "epoch 93, train_loss 1278.1932373046875\n",
      "epoch 93, val_loss 455.625\n",
      "epoch 94, train_loss 1278.19287109375\n",
      "epoch 94, val_loss 455.6248474121094\n",
      "epoch 95, train_loss 1278.1927490234375\n",
      "epoch 95, val_loss 455.62469482421875\n",
      "epoch 96, train_loss 1278.192626953125\n",
      "epoch 96, val_loss 455.6245422363281\n",
      "epoch 97, train_loss 1278.1923828125\n",
      "epoch 97, val_loss 455.62445068359375\n",
      "epoch 98, train_loss 1278.1920166015625\n",
      "epoch 98, val_loss 455.6242980957031\n",
      "epoch 99, train_loss 1278.1920166015625\n",
      "epoch 99, val_loss 455.6241149902344\n",
      "Parameter containing:\n",
      "tensor([5.8391e-08], requires_grad=True)\n",
      "iter 34, train_loss_regularization 0.9934133291244507\n",
      "iter 34, val_loss_regularization 0.9934133291244507\n",
      "epoch 0, train_loss 1278.191650390625\n",
      "epoch 0, val_loss 455.6239929199219\n",
      "epoch 1, train_loss 1278.1915283203125\n",
      "epoch 1, val_loss 455.62384033203125\n",
      "epoch 2, train_loss 1278.1912841796875\n",
      "epoch 2, val_loss 455.6236572265625\n",
      "epoch 3, train_loss 1278.1910400390625\n",
      "epoch 3, val_loss 455.62359619140625\n",
      "epoch 4, train_loss 1278.1907958984375\n",
      "epoch 4, val_loss 455.6234436035156\n",
      "epoch 5, train_loss 1278.190673828125\n",
      "epoch 5, val_loss 455.6233215332031\n",
      "epoch 6, train_loss 1278.1903076171875\n",
      "epoch 6, val_loss 455.6231689453125\n",
      "epoch 7, train_loss 1278.1903076171875\n",
      "epoch 7, val_loss 455.6230773925781\n",
      "epoch 8, train_loss 1278.18994140625\n",
      "epoch 8, val_loss 455.6229248046875\n",
      "epoch 9, train_loss 1278.189697265625\n",
      "epoch 9, val_loss 455.62274169921875\n",
      "epoch 10, train_loss 1278.189453125\n",
      "epoch 10, val_loss 455.6225891113281\n",
      "epoch 11, train_loss 1278.189453125\n",
      "epoch 11, val_loss 455.62249755859375\n",
      "epoch 12, train_loss 1278.189208984375\n",
      "epoch 12, val_loss 455.6223449707031\n",
      "epoch 13, train_loss 1278.18896484375\n",
      "epoch 13, val_loss 455.6222229003906\n",
      "epoch 14, train_loss 1278.188720703125\n",
      "epoch 14, val_loss 455.6220703125\n",
      "epoch 15, train_loss 1278.1884765625\n",
      "epoch 15, val_loss 455.6219177246094\n",
      "epoch 16, train_loss 1278.1881103515625\n",
      "epoch 16, val_loss 455.6217956542969\n",
      "epoch 17, train_loss 1278.1878662109375\n",
      "epoch 17, val_loss 455.6216735839844\n",
      "epoch 18, train_loss 1278.187744140625\n",
      "epoch 18, val_loss 455.6214904785156\n",
      "epoch 19, train_loss 1278.1873779296875\n",
      "epoch 19, val_loss 455.6213684082031\n",
      "epoch 20, train_loss 1278.187255859375\n",
      "epoch 20, val_loss 455.6212463378906\n",
      "epoch 21, train_loss 1278.1871337890625\n",
      "epoch 21, val_loss 455.62109375\n",
      "epoch 22, train_loss 1278.1868896484375\n",
      "epoch 22, val_loss 455.6209716796875\n",
      "epoch 23, train_loss 1278.1865234375\n",
      "epoch 23, val_loss 455.6208190917969\n",
      "epoch 24, train_loss 1278.1865234375\n",
      "epoch 24, val_loss 455.6206359863281\n",
      "epoch 25, train_loss 1278.186279296875\n",
      "epoch 25, val_loss 455.62054443359375\n",
      "epoch 26, train_loss 1278.18603515625\n",
      "epoch 26, val_loss 455.62042236328125\n",
      "epoch 27, train_loss 1278.1859130859375\n",
      "epoch 27, val_loss 455.62030029296875\n",
      "epoch 28, train_loss 1278.185791015625\n",
      "epoch 28, val_loss 455.6200866699219\n",
      "epoch 29, train_loss 1278.185546875\n",
      "epoch 29, val_loss 455.6199951171875\n",
      "epoch 30, train_loss 1278.1851806640625\n",
      "epoch 30, val_loss 455.6198425292969\n",
      "epoch 31, train_loss 1278.18505859375\n",
      "epoch 31, val_loss 455.6197204589844\n",
      "epoch 32, train_loss 1278.1846923828125\n",
      "epoch 32, val_loss 455.6195373535156\n",
      "epoch 33, train_loss 1278.1844482421875\n",
      "epoch 33, val_loss 455.61944580078125\n",
      "epoch 34, train_loss 1278.184326171875\n",
      "epoch 34, val_loss 455.6192932128906\n",
      "epoch 35, train_loss 1278.1842041015625\n",
      "epoch 35, val_loss 455.61920166015625\n",
      "epoch 36, train_loss 1278.183837890625\n",
      "epoch 36, val_loss 455.6190490722656\n",
      "epoch 37, train_loss 1278.1837158203125\n",
      "epoch 37, val_loss 455.6189270019531\n",
      "epoch 38, train_loss 1278.1834716796875\n",
      "epoch 38, val_loss 455.61871337890625\n",
      "epoch 39, train_loss 1278.1832275390625\n",
      "epoch 39, val_loss 455.61865234375\n",
      "epoch 40, train_loss 1278.1829833984375\n",
      "epoch 40, val_loss 455.61846923828125\n",
      "epoch 41, train_loss 1278.182861328125\n",
      "epoch 41, val_loss 455.61834716796875\n",
      "epoch 42, train_loss 1278.1826171875\n",
      "epoch 42, val_loss 455.6181945800781\n",
      "epoch 43, train_loss 1278.182373046875\n",
      "epoch 43, val_loss 455.6180725097656\n",
      "epoch 44, train_loss 1278.1822509765625\n",
      "epoch 44, val_loss 455.6178894042969\n",
      "epoch 45, train_loss 1278.1820068359375\n",
      "epoch 45, val_loss 455.6177978515625\n",
      "epoch 46, train_loss 1278.1817626953125\n",
      "epoch 46, val_loss 455.61767578125\n",
      "epoch 47, train_loss 1278.1815185546875\n",
      "epoch 47, val_loss 455.6175537109375\n",
      "epoch 48, train_loss 1278.1812744140625\n",
      "epoch 48, val_loss 455.61737060546875\n",
      "epoch 49, train_loss 1278.18115234375\n",
      "epoch 49, val_loss 455.6172790527344\n",
      "epoch 50, train_loss 1278.1807861328125\n",
      "epoch 50, val_loss 455.6170959472656\n",
      "epoch 51, train_loss 1278.1806640625\n",
      "epoch 51, val_loss 455.616943359375\n",
      "epoch 52, train_loss 1278.1805419921875\n",
      "epoch 52, val_loss 455.61688232421875\n",
      "epoch 53, train_loss 1278.1802978515625\n",
      "epoch 53, val_loss 455.6166687011719\n",
      "epoch 54, train_loss 1278.1800537109375\n",
      "epoch 54, val_loss 455.6164855957031\n",
      "epoch 55, train_loss 1278.1796875\n",
      "epoch 55, val_loss 455.6164245605469\n",
      "epoch 56, train_loss 1278.1796875\n",
      "epoch 56, val_loss 455.6162414550781\n",
      "epoch 57, train_loss 1278.1793212890625\n",
      "epoch 57, val_loss 455.6161804199219\n",
      "epoch 58, train_loss 1278.17919921875\n",
      "epoch 58, val_loss 455.6159973144531\n",
      "epoch 59, train_loss 1278.1788330078125\n",
      "epoch 59, val_loss 455.6158447265625\n",
      "epoch 60, train_loss 1278.1788330078125\n",
      "epoch 60, val_loss 455.61572265625\n",
      "epoch 61, train_loss 1278.178466796875\n",
      "epoch 61, val_loss 455.6155700683594\n",
      "epoch 62, train_loss 1278.178466796875\n",
      "epoch 62, val_loss 455.61541748046875\n",
      "epoch 63, train_loss 1278.1778564453125\n",
      "epoch 63, val_loss 455.6153259277344\n",
      "epoch 64, train_loss 1278.1778564453125\n",
      "epoch 64, val_loss 455.61517333984375\n",
      "epoch 65, train_loss 1278.1776123046875\n",
      "epoch 65, val_loss 455.614990234375\n",
      "epoch 66, train_loss 1278.1776123046875\n",
      "epoch 66, val_loss 455.61492919921875\n",
      "epoch 67, train_loss 1278.1771240234375\n",
      "epoch 67, val_loss 455.61474609375\n",
      "epoch 68, train_loss 1278.177001953125\n",
      "epoch 68, val_loss 455.6146240234375\n",
      "epoch 69, train_loss 1278.1768798828125\n",
      "epoch 69, val_loss 455.614501953125\n",
      "epoch 70, train_loss 1278.1766357421875\n",
      "epoch 70, val_loss 455.6143798828125\n",
      "epoch 71, train_loss 1278.1763916015625\n",
      "epoch 71, val_loss 455.61419677734375\n",
      "epoch 72, train_loss 1278.176025390625\n",
      "epoch 72, val_loss 455.61407470703125\n",
      "epoch 73, train_loss 1278.176025390625\n",
      "epoch 73, val_loss 455.614013671875\n",
      "epoch 74, train_loss 1278.17578125\n",
      "epoch 74, val_loss 455.61383056640625\n",
      "epoch 75, train_loss 1278.175537109375\n",
      "epoch 75, val_loss 455.6136779785156\n",
      "epoch 76, train_loss 1278.17529296875\n",
      "epoch 76, val_loss 455.61358642578125\n",
      "epoch 77, train_loss 1278.175048828125\n",
      "epoch 77, val_loss 455.6133728027344\n",
      "epoch 78, train_loss 1278.1748046875\n",
      "epoch 78, val_loss 455.6132507324219\n",
      "epoch 79, train_loss 1278.1748046875\n",
      "epoch 79, val_loss 455.61309814453125\n",
      "epoch 80, train_loss 1278.174560546875\n",
      "epoch 80, val_loss 455.6129455566406\n",
      "epoch 81, train_loss 1278.174072265625\n",
      "epoch 81, val_loss 455.6128234863281\n",
      "epoch 82, train_loss 1278.1741943359375\n",
      "epoch 82, val_loss 455.6126708984375\n",
      "epoch 83, train_loss 1278.1737060546875\n",
      "epoch 83, val_loss 455.612548828125\n",
      "epoch 84, train_loss 1278.1737060546875\n",
      "epoch 84, val_loss 455.6124572753906\n",
      "epoch 85, train_loss 1278.1734619140625\n",
      "epoch 85, val_loss 455.6123352050781\n",
      "epoch 86, train_loss 1278.1732177734375\n",
      "epoch 86, val_loss 455.61212158203125\n",
      "epoch 87, train_loss 1278.1729736328125\n",
      "epoch 87, val_loss 455.6120910644531\n",
      "epoch 88, train_loss 1278.172607421875\n",
      "epoch 88, val_loss 455.6119079589844\n",
      "epoch 89, train_loss 1278.172607421875\n",
      "epoch 89, val_loss 455.61175537109375\n",
      "epoch 90, train_loss 1278.17236328125\n",
      "epoch 90, val_loss 455.61163330078125\n",
      "epoch 91, train_loss 1278.172119140625\n",
      "epoch 91, val_loss 455.61151123046875\n",
      "epoch 92, train_loss 1278.171875\n",
      "epoch 92, val_loss 455.611328125\n",
      "epoch 93, train_loss 1278.1717529296875\n",
      "epoch 93, val_loss 455.6111755371094\n",
      "epoch 94, train_loss 1278.17138671875\n",
      "epoch 94, val_loss 455.6110534667969\n",
      "epoch 95, train_loss 1278.17138671875\n",
      "epoch 95, val_loss 455.6109313964844\n",
      "epoch 96, train_loss 1278.171142578125\n",
      "epoch 96, val_loss 455.61077880859375\n",
      "epoch 97, train_loss 1278.1710205078125\n",
      "epoch 97, val_loss 455.61065673828125\n",
      "epoch 98, train_loss 1278.1707763671875\n",
      "epoch 98, val_loss 455.6104431152344\n",
      "epoch 99, train_loss 1278.1702880859375\n",
      "epoch 99, val_loss 455.6103820800781\n",
      "Parameter containing:\n",
      "tensor([3.7197e-08], requires_grad=True)\n",
      "iter 35, train_loss_regularization 0.9873956441879272\n",
      "iter 35, val_loss_regularization 0.9873956441879272\n",
      "epoch 0, train_loss 1278.170166015625\n",
      "epoch 0, val_loss 455.6101989746094\n",
      "epoch 1, train_loss 1278.1700439453125\n",
      "epoch 1, val_loss 455.6100769042969\n",
      "epoch 2, train_loss 1278.1697998046875\n",
      "epoch 2, val_loss 455.6099548339844\n",
      "epoch 3, train_loss 1278.1695556640625\n",
      "epoch 3, val_loss 455.6098327636719\n",
      "epoch 4, train_loss 1278.1695556640625\n",
      "epoch 4, val_loss 455.6097106933594\n",
      "epoch 5, train_loss 1278.1690673828125\n",
      "epoch 5, val_loss 455.6095886230469\n",
      "epoch 6, train_loss 1278.1689453125\n",
      "epoch 6, val_loss 455.609375\n",
      "epoch 7, train_loss 1278.168701171875\n",
      "epoch 7, val_loss 455.6092834472656\n",
      "epoch 8, train_loss 1278.16845703125\n",
      "epoch 8, val_loss 455.6091613769531\n",
      "epoch 9, train_loss 1278.1683349609375\n",
      "epoch 9, val_loss 455.60894775390625\n",
      "epoch 10, train_loss 1278.1680908203125\n",
      "epoch 10, val_loss 455.60882568359375\n",
      "epoch 11, train_loss 1278.16796875\n",
      "epoch 11, val_loss 455.60870361328125\n",
      "epoch 12, train_loss 1278.1676025390625\n",
      "epoch 12, val_loss 455.6086120605469\n",
      "epoch 13, train_loss 1278.1673583984375\n",
      "epoch 13, val_loss 455.60845947265625\n",
      "epoch 14, train_loss 1278.167236328125\n",
      "epoch 14, val_loss 455.60833740234375\n",
      "epoch 15, train_loss 1278.1671142578125\n",
      "epoch 15, val_loss 455.60821533203125\n",
      "epoch 16, train_loss 1278.166748046875\n",
      "epoch 16, val_loss 455.6080322265625\n",
      "epoch 17, train_loss 1278.1666259765625\n",
      "epoch 17, val_loss 455.6078796386719\n",
      "epoch 18, train_loss 1278.1663818359375\n",
      "epoch 18, val_loss 455.6077880859375\n",
      "epoch 19, train_loss 1278.1661376953125\n",
      "epoch 19, val_loss 455.6075744628906\n",
      "epoch 20, train_loss 1278.1658935546875\n",
      "epoch 20, val_loss 455.6074523925781\n",
      "epoch 21, train_loss 1278.1656494140625\n",
      "epoch 21, val_loss 455.607421875\n",
      "epoch 22, train_loss 1278.165283203125\n",
      "epoch 22, val_loss 455.60723876953125\n",
      "epoch 23, train_loss 1278.165283203125\n",
      "epoch 23, val_loss 455.6070861816406\n",
      "epoch 24, train_loss 1278.1651611328125\n",
      "epoch 24, val_loss 455.60699462890625\n",
      "epoch 25, train_loss 1278.1649169921875\n",
      "epoch 25, val_loss 455.60687255859375\n",
      "epoch 26, train_loss 1278.164794921875\n",
      "epoch 26, val_loss 455.6066589355469\n",
      "epoch 27, train_loss 1278.1644287109375\n",
      "epoch 27, val_loss 455.6065979003906\n",
      "epoch 28, train_loss 1278.164306640625\n",
      "epoch 28, val_loss 455.6064147949219\n",
      "epoch 29, train_loss 1278.1640625\n",
      "epoch 29, val_loss 455.6062927246094\n",
      "epoch 30, train_loss 1278.163818359375\n",
      "epoch 30, val_loss 455.60614013671875\n",
      "epoch 31, train_loss 1278.1636962890625\n",
      "epoch 31, val_loss 455.6060791015625\n",
      "epoch 32, train_loss 1278.1634521484375\n",
      "epoch 32, val_loss 455.6058654785156\n",
      "epoch 33, train_loss 1278.1632080078125\n",
      "epoch 33, val_loss 455.6057434082031\n",
      "epoch 34, train_loss 1278.1629638671875\n",
      "epoch 34, val_loss 455.6056213378906\n",
      "epoch 35, train_loss 1278.162841796875\n",
      "epoch 35, val_loss 455.60546875\n",
      "epoch 36, train_loss 1278.1627197265625\n",
      "epoch 36, val_loss 455.6053466796875\n",
      "epoch 37, train_loss 1278.1624755859375\n",
      "epoch 37, val_loss 455.6051940917969\n",
      "epoch 38, train_loss 1278.162109375\n",
      "epoch 38, val_loss 455.6050720214844\n",
      "epoch 39, train_loss 1278.161865234375\n",
      "epoch 39, val_loss 455.60491943359375\n",
      "epoch 40, train_loss 1278.16162109375\n",
      "epoch 40, val_loss 455.60479736328125\n",
      "epoch 41, train_loss 1278.16162109375\n",
      "epoch 41, val_loss 455.60467529296875\n",
      "epoch 42, train_loss 1278.161376953125\n",
      "epoch 42, val_loss 455.6044921875\n",
      "epoch 43, train_loss 1278.1611328125\n",
      "epoch 43, val_loss 455.6042785644531\n",
      "epoch 44, train_loss 1278.160888671875\n",
      "epoch 44, val_loss 455.604248046875\n",
      "epoch 45, train_loss 1278.1607666015625\n",
      "epoch 45, val_loss 455.6040954589844\n",
      "epoch 46, train_loss 1278.160400390625\n",
      "epoch 46, val_loss 455.6039733886719\n",
      "epoch 47, train_loss 1278.1602783203125\n",
      "epoch 47, val_loss 455.6038818359375\n",
      "epoch 48, train_loss 1278.16015625\n",
      "epoch 48, val_loss 455.60369873046875\n",
      "epoch 49, train_loss 1278.15966796875\n",
      "epoch 49, val_loss 455.60357666015625\n",
      "epoch 50, train_loss 1278.1595458984375\n",
      "epoch 50, val_loss 455.6034240722656\n",
      "epoch 51, train_loss 1278.1595458984375\n",
      "epoch 51, val_loss 455.6033020019531\n",
      "epoch 52, train_loss 1278.1593017578125\n",
      "epoch 52, val_loss 455.6031188964844\n",
      "epoch 53, train_loss 1278.158935546875\n",
      "epoch 53, val_loss 455.6029968261719\n",
      "epoch 54, train_loss 1278.158935546875\n",
      "epoch 54, val_loss 455.6028747558594\n",
      "epoch 55, train_loss 1278.1585693359375\n",
      "epoch 55, val_loss 455.6027526855469\n",
      "epoch 56, train_loss 1278.1583251953125\n",
      "epoch 56, val_loss 455.6025695800781\n",
      "epoch 57, train_loss 1278.158203125\n",
      "epoch 57, val_loss 455.6024475097656\n",
      "epoch 58, train_loss 1278.157958984375\n",
      "epoch 58, val_loss 455.6023254394531\n",
      "epoch 59, train_loss 1278.1578369140625\n",
      "epoch 59, val_loss 455.6022033691406\n",
      "epoch 60, train_loss 1278.157470703125\n",
      "epoch 60, val_loss 455.60205078125\n",
      "epoch 61, train_loss 1278.1573486328125\n",
      "epoch 61, val_loss 455.6019287109375\n",
      "epoch 62, train_loss 1278.1571044921875\n",
      "epoch 62, val_loss 455.601806640625\n",
      "epoch 63, train_loss 1278.1568603515625\n",
      "epoch 63, val_loss 455.6016540527344\n",
      "epoch 64, train_loss 1278.1566162109375\n",
      "epoch 64, val_loss 455.6015319824219\n",
      "epoch 65, train_loss 1278.156494140625\n",
      "epoch 65, val_loss 455.6014099121094\n",
      "epoch 66, train_loss 1278.15625\n",
      "epoch 66, val_loss 455.60125732421875\n",
      "epoch 67, train_loss 1278.156005859375\n",
      "epoch 67, val_loss 455.6011657714844\n",
      "epoch 68, train_loss 1278.1558837890625\n",
      "epoch 68, val_loss 455.6009521484375\n",
      "epoch 69, train_loss 1278.1556396484375\n",
      "epoch 69, val_loss 455.600830078125\n",
      "epoch 70, train_loss 1278.155517578125\n",
      "epoch 70, val_loss 455.6006774902344\n",
      "epoch 71, train_loss 1278.1552734375\n",
      "epoch 71, val_loss 455.6005859375\n",
      "epoch 72, train_loss 1278.155029296875\n",
      "epoch 72, val_loss 455.60040283203125\n",
      "epoch 73, train_loss 1278.15478515625\n",
      "epoch 73, val_loss 455.60028076171875\n",
      "epoch 74, train_loss 1278.154541015625\n",
      "epoch 74, val_loss 455.60015869140625\n",
      "epoch 75, train_loss 1278.154541015625\n",
      "epoch 75, val_loss 455.60009765625\n",
      "epoch 76, train_loss 1278.154052734375\n",
      "epoch 76, val_loss 455.5998229980469\n",
      "epoch 77, train_loss 1278.154052734375\n",
      "epoch 77, val_loss 455.5997619628906\n",
      "epoch 78, train_loss 1278.15380859375\n",
      "epoch 78, val_loss 455.5996398925781\n",
      "epoch 79, train_loss 1278.1534423828125\n",
      "epoch 79, val_loss 455.5994567871094\n",
      "epoch 80, train_loss 1278.1533203125\n",
      "epoch 80, val_loss 455.5993347167969\n",
      "epoch 81, train_loss 1278.153076171875\n",
      "epoch 81, val_loss 455.5991516113281\n",
      "epoch 82, train_loss 1278.1529541015625\n",
      "epoch 82, val_loss 455.5990905761719\n",
      "epoch 83, train_loss 1278.1527099609375\n",
      "epoch 83, val_loss 455.5989074707031\n",
      "epoch 84, train_loss 1278.152587890625\n",
      "epoch 84, val_loss 455.5987854003906\n",
      "epoch 85, train_loss 1278.15234375\n",
      "epoch 85, val_loss 455.5986633300781\n",
      "epoch 86, train_loss 1278.152099609375\n",
      "epoch 86, val_loss 455.5985107421875\n",
      "epoch 87, train_loss 1278.1517333984375\n",
      "epoch 87, val_loss 455.5984191894531\n",
      "epoch 88, train_loss 1278.151611328125\n",
      "epoch 88, val_loss 455.5982971191406\n",
      "epoch 89, train_loss 1278.1513671875\n",
      "epoch 89, val_loss 455.5981140136719\n",
      "epoch 90, train_loss 1278.1512451171875\n",
      "epoch 90, val_loss 455.5979919433594\n",
      "epoch 91, train_loss 1278.15087890625\n",
      "epoch 91, val_loss 455.5978698730469\n",
      "epoch 92, train_loss 1278.15087890625\n",
      "epoch 92, val_loss 455.59771728515625\n",
      "epoch 93, train_loss 1278.150634765625\n",
      "epoch 93, val_loss 455.5975341796875\n",
      "epoch 94, train_loss 1278.150390625\n",
      "epoch 94, val_loss 455.597412109375\n",
      "epoch 95, train_loss 1278.1502685546875\n",
      "epoch 95, val_loss 455.5973205566406\n",
      "epoch 96, train_loss 1278.14990234375\n",
      "epoch 96, val_loss 455.5971984863281\n",
      "epoch 97, train_loss 1278.149658203125\n",
      "epoch 97, val_loss 455.5970764160156\n",
      "epoch 98, train_loss 1278.149658203125\n",
      "epoch 98, val_loss 455.5969543457031\n",
      "epoch 99, train_loss 1278.1492919921875\n",
      "epoch 99, val_loss 455.5968017578125\n",
      "Parameter containing:\n",
      "tensor([2.3732e-08], requires_grad=True)\n",
      "iter 36, train_loss_regularization 0.981483519077301\n",
      "iter 36, val_loss_regularization 0.981483519077301\n",
      "epoch 0, train_loss 1278.1490478515625\n",
      "epoch 0, val_loss 455.5966796875\n",
      "epoch 1, train_loss 1278.1490478515625\n",
      "epoch 1, val_loss 455.59649658203125\n",
      "epoch 2, train_loss 1278.148681640625\n",
      "epoch 2, val_loss 455.5964050292969\n",
      "epoch 3, train_loss 1278.1485595703125\n",
      "epoch 3, val_loss 455.59625244140625\n",
      "epoch 4, train_loss 1278.1484375\n",
      "epoch 4, val_loss 455.59613037109375\n",
      "epoch 5, train_loss 1278.1483154296875\n",
      "epoch 5, val_loss 455.595947265625\n",
      "epoch 6, train_loss 1278.1478271484375\n",
      "epoch 6, val_loss 455.5958251953125\n",
      "epoch 7, train_loss 1278.147705078125\n",
      "epoch 7, val_loss 455.5956726074219\n",
      "epoch 8, train_loss 1278.1474609375\n",
      "epoch 8, val_loss 455.5955505371094\n",
      "epoch 9, train_loss 1278.1473388671875\n",
      "epoch 9, val_loss 455.5954284667969\n",
      "epoch 10, train_loss 1278.1470947265625\n",
      "epoch 10, val_loss 455.5953369140625\n",
      "epoch 11, train_loss 1278.14697265625\n",
      "epoch 11, val_loss 455.5951232910156\n",
      "epoch 12, train_loss 1278.1466064453125\n",
      "epoch 12, val_loss 455.59503173828125\n",
      "epoch 13, train_loss 1278.146484375\n",
      "epoch 13, val_loss 455.59490966796875\n",
      "epoch 14, train_loss 1278.1461181640625\n",
      "epoch 14, val_loss 455.59478759765625\n",
      "epoch 15, train_loss 1278.1461181640625\n",
      "epoch 15, val_loss 455.5945739746094\n",
      "epoch 16, train_loss 1278.1458740234375\n",
      "epoch 16, val_loss 455.5944519042969\n",
      "epoch 17, train_loss 1278.1456298828125\n",
      "epoch 17, val_loss 455.5943298339844\n",
      "epoch 18, train_loss 1278.1453857421875\n",
      "epoch 18, val_loss 455.59417724609375\n",
      "epoch 19, train_loss 1278.1453857421875\n",
      "epoch 19, val_loss 455.5940856933594\n",
      "epoch 20, train_loss 1278.14501953125\n",
      "epoch 20, val_loss 455.5939636230469\n",
      "epoch 21, train_loss 1278.1448974609375\n",
      "epoch 21, val_loss 455.5938415527344\n",
      "epoch 22, train_loss 1278.14453125\n",
      "epoch 22, val_loss 455.5936279296875\n",
      "epoch 23, train_loss 1278.144287109375\n",
      "epoch 23, val_loss 455.5935363769531\n",
      "epoch 24, train_loss 1278.144287109375\n",
      "epoch 24, val_loss 455.59332275390625\n",
      "epoch 25, train_loss 1278.1439208984375\n",
      "epoch 25, val_loss 455.59326171875\n",
      "epoch 26, train_loss 1278.1436767578125\n",
      "epoch 26, val_loss 455.5931701660156\n",
      "epoch 27, train_loss 1278.1435546875\n",
      "epoch 27, val_loss 455.5929870605469\n",
      "epoch 28, train_loss 1278.143310546875\n",
      "epoch 28, val_loss 455.59283447265625\n",
      "epoch 29, train_loss 1278.1429443359375\n",
      "epoch 29, val_loss 455.5927429199219\n",
      "epoch 30, train_loss 1278.14306640625\n",
      "epoch 30, val_loss 455.5926208496094\n",
      "epoch 31, train_loss 1278.142822265625\n",
      "epoch 31, val_loss 455.59246826171875\n",
      "epoch 32, train_loss 1278.1424560546875\n",
      "epoch 32, val_loss 455.59228515625\n",
      "epoch 33, train_loss 1278.142333984375\n",
      "epoch 33, val_loss 455.5921936035156\n",
      "epoch 34, train_loss 1278.14208984375\n",
      "epoch 34, val_loss 455.592041015625\n",
      "epoch 35, train_loss 1278.141845703125\n",
      "epoch 35, val_loss 455.5919494628906\n",
      "epoch 36, train_loss 1278.1417236328125\n",
      "epoch 36, val_loss 455.591796875\n",
      "epoch 37, train_loss 1278.141357421875\n",
      "epoch 37, val_loss 455.5917053222656\n",
      "epoch 38, train_loss 1278.1412353515625\n",
      "epoch 38, val_loss 455.59149169921875\n",
      "epoch 39, train_loss 1278.14111328125\n",
      "epoch 39, val_loss 455.5914306640625\n",
      "epoch 40, train_loss 1278.140869140625\n",
      "epoch 40, val_loss 455.59124755859375\n",
      "epoch 41, train_loss 1278.140625\n",
      "epoch 41, val_loss 455.59112548828125\n",
      "epoch 42, train_loss 1278.140380859375\n",
      "epoch 42, val_loss 455.5910339355469\n",
      "epoch 43, train_loss 1278.1402587890625\n",
      "epoch 43, val_loss 455.5908203125\n",
      "epoch 44, train_loss 1278.14013671875\n",
      "epoch 44, val_loss 455.59075927734375\n",
      "epoch 45, train_loss 1278.139892578125\n",
      "epoch 45, val_loss 455.590576171875\n",
      "epoch 46, train_loss 1278.1397705078125\n",
      "epoch 46, val_loss 455.5904235839844\n",
      "epoch 47, train_loss 1278.1395263671875\n",
      "epoch 47, val_loss 455.5902404785156\n",
      "epoch 48, train_loss 1278.13916015625\n",
      "epoch 48, val_loss 455.5901794433594\n",
      "epoch 49, train_loss 1278.138916015625\n",
      "epoch 49, val_loss 455.59002685546875\n",
      "epoch 50, train_loss 1278.1387939453125\n",
      "epoch 50, val_loss 455.58990478515625\n",
      "epoch 51, train_loss 1278.1385498046875\n",
      "epoch 51, val_loss 455.58978271484375\n",
      "epoch 52, train_loss 1278.13818359375\n",
      "epoch 52, val_loss 455.58966064453125\n",
      "epoch 53, train_loss 1278.13818359375\n",
      "epoch 53, val_loss 455.5895080566406\n",
      "epoch 54, train_loss 1278.1376953125\n",
      "epoch 54, val_loss 455.5893859863281\n",
      "epoch 55, train_loss 1278.1376953125\n",
      "epoch 55, val_loss 455.5892639160156\n",
      "epoch 56, train_loss 1278.1375732421875\n",
      "epoch 56, val_loss 455.589111328125\n",
      "epoch 57, train_loss 1278.13720703125\n",
      "epoch 57, val_loss 455.5889587402344\n",
      "epoch 58, train_loss 1278.136962890625\n",
      "epoch 58, val_loss 455.58880615234375\n",
      "epoch 59, train_loss 1278.136962890625\n",
      "epoch 59, val_loss 455.5887145996094\n",
      "epoch 60, train_loss 1278.1368408203125\n",
      "epoch 60, val_loss 455.5885314941406\n",
      "epoch 61, train_loss 1278.1365966796875\n",
      "epoch 61, val_loss 455.5884094238281\n",
      "epoch 62, train_loss 1278.136474609375\n",
      "epoch 62, val_loss 455.5882873535156\n",
      "epoch 63, train_loss 1278.1361083984375\n",
      "epoch 63, val_loss 455.5881652832031\n",
      "epoch 64, train_loss 1278.1358642578125\n",
      "epoch 64, val_loss 455.5880126953125\n",
      "epoch 65, train_loss 1278.1358642578125\n",
      "epoch 65, val_loss 455.58795166015625\n",
      "epoch 66, train_loss 1278.135498046875\n",
      "epoch 66, val_loss 455.5877380371094\n",
      "epoch 67, train_loss 1278.1353759765625\n",
      "epoch 67, val_loss 455.5876159667969\n",
      "epoch 68, train_loss 1278.1351318359375\n",
      "epoch 68, val_loss 455.5874938964844\n",
      "epoch 69, train_loss 1278.1348876953125\n",
      "epoch 69, val_loss 455.5873718261719\n",
      "epoch 70, train_loss 1278.1346435546875\n",
      "epoch 70, val_loss 455.58721923828125\n",
      "epoch 71, train_loss 1278.1343994140625\n",
      "epoch 71, val_loss 455.58709716796875\n",
      "epoch 72, train_loss 1278.1341552734375\n",
      "epoch 72, val_loss 455.5870056152344\n",
      "epoch 73, train_loss 1278.1341552734375\n",
      "epoch 73, val_loss 455.5868835449219\n",
      "epoch 74, train_loss 1278.1337890625\n",
      "epoch 74, val_loss 455.5867004394531\n",
      "epoch 75, train_loss 1278.1336669921875\n",
      "epoch 75, val_loss 455.5865783691406\n",
      "epoch 76, train_loss 1278.1334228515625\n",
      "epoch 76, val_loss 455.58642578125\n",
      "epoch 77, train_loss 1278.13330078125\n",
      "epoch 77, val_loss 455.5863037109375\n",
      "epoch 78, train_loss 1278.133056640625\n",
      "epoch 78, val_loss 455.586181640625\n",
      "epoch 79, train_loss 1278.1328125\n",
      "epoch 79, val_loss 455.5860290527344\n",
      "epoch 80, train_loss 1278.132568359375\n",
      "epoch 80, val_loss 455.5859069824219\n",
      "epoch 81, train_loss 1278.1324462890625\n",
      "epoch 81, val_loss 455.5857849121094\n",
      "epoch 82, train_loss 1278.1322021484375\n",
      "epoch 82, val_loss 455.58563232421875\n",
      "epoch 83, train_loss 1278.1318359375\n",
      "epoch 83, val_loss 455.58544921875\n",
      "epoch 84, train_loss 1278.1318359375\n",
      "epoch 84, val_loss 455.58538818359375\n",
      "epoch 85, train_loss 1278.1314697265625\n",
      "epoch 85, val_loss 455.5852355957031\n",
      "epoch 86, train_loss 1278.1312255859375\n",
      "epoch 86, val_loss 455.5850830078125\n",
      "epoch 87, train_loss 1278.1312255859375\n",
      "epoch 87, val_loss 455.5849914550781\n",
      "epoch 88, train_loss 1278.1309814453125\n",
      "epoch 88, val_loss 455.58477783203125\n",
      "epoch 89, train_loss 1278.130859375\n",
      "epoch 89, val_loss 455.58465576171875\n",
      "epoch 90, train_loss 1278.130615234375\n",
      "epoch 90, val_loss 455.58453369140625\n",
      "epoch 91, train_loss 1278.1302490234375\n",
      "epoch 91, val_loss 455.58441162109375\n",
      "epoch 92, train_loss 1278.130126953125\n",
      "epoch 92, val_loss 455.58428955078125\n",
      "epoch 93, train_loss 1278.1298828125\n",
      "epoch 93, val_loss 455.58416748046875\n",
      "epoch 94, train_loss 1278.1298828125\n",
      "epoch 94, val_loss 455.5840759277344\n",
      "epoch 95, train_loss 1278.129638671875\n",
      "epoch 95, val_loss 455.5838623046875\n",
      "epoch 96, train_loss 1278.1292724609375\n",
      "epoch 96, val_loss 455.583740234375\n",
      "epoch 97, train_loss 1278.129150390625\n",
      "epoch 97, val_loss 455.5836181640625\n",
      "epoch 98, train_loss 1278.1290283203125\n",
      "epoch 98, val_loss 455.58349609375\n",
      "epoch 99, train_loss 1278.128662109375\n",
      "epoch 99, val_loss 455.5833435058594\n",
      "Parameter containing:\n",
      "tensor([1.5165e-08], requires_grad=True)\n",
      "iter 37, train_loss_regularization 0.9756749868392944\n",
      "iter 37, val_loss_regularization 0.9756749868392944\n",
      "epoch 0, train_loss 1278.1285400390625\n",
      "epoch 0, val_loss 455.5832214355469\n",
      "epoch 1, train_loss 1278.1282958984375\n",
      "epoch 1, val_loss 455.5831298828125\n",
      "epoch 2, train_loss 1278.1280517578125\n",
      "epoch 2, val_loss 455.5830078125\n",
      "epoch 3, train_loss 1278.1278076171875\n",
      "epoch 3, val_loss 455.5828857421875\n",
      "epoch 4, train_loss 1278.1278076171875\n",
      "epoch 4, val_loss 455.5826721191406\n",
      "epoch 5, train_loss 1278.1273193359375\n",
      "epoch 5, val_loss 455.5824890136719\n",
      "epoch 6, train_loss 1278.1273193359375\n",
      "epoch 6, val_loss 455.58245849609375\n",
      "epoch 7, train_loss 1278.1270751953125\n",
      "epoch 7, val_loss 455.5822448730469\n",
      "epoch 8, train_loss 1278.1268310546875\n",
      "epoch 8, val_loss 455.5821228027344\n",
      "epoch 9, train_loss 1278.126708984375\n",
      "epoch 9, val_loss 455.5820007324219\n",
      "epoch 10, train_loss 1278.12646484375\n",
      "epoch 10, val_loss 455.5819091796875\n",
      "epoch 11, train_loss 1278.126220703125\n",
      "epoch 11, val_loss 455.5816955566406\n",
      "epoch 12, train_loss 1278.1260986328125\n",
      "epoch 12, val_loss 455.5816345214844\n",
      "epoch 13, train_loss 1278.125732421875\n",
      "epoch 13, val_loss 455.5814514160156\n",
      "epoch 14, train_loss 1278.1256103515625\n",
      "epoch 14, val_loss 455.58135986328125\n",
      "epoch 15, train_loss 1278.1253662109375\n",
      "epoch 15, val_loss 455.5811767578125\n",
      "epoch 16, train_loss 1278.1251220703125\n",
      "epoch 16, val_loss 455.58111572265625\n",
      "epoch 17, train_loss 1278.1251220703125\n",
      "epoch 17, val_loss 455.5809631347656\n",
      "epoch 18, train_loss 1278.1248779296875\n",
      "epoch 18, val_loss 455.5807800292969\n",
      "epoch 19, train_loss 1278.1246337890625\n",
      "epoch 19, val_loss 455.5807189941406\n",
      "epoch 20, train_loss 1278.1243896484375\n",
      "epoch 20, val_loss 455.5805358886719\n",
      "epoch 21, train_loss 1278.1241455078125\n",
      "epoch 21, val_loss 455.5804138183594\n",
      "epoch 22, train_loss 1278.1240234375\n",
      "epoch 22, val_loss 455.5802917480469\n",
      "epoch 23, train_loss 1278.1240234375\n",
      "epoch 23, val_loss 455.5801696777344\n",
      "epoch 24, train_loss 1278.1236572265625\n",
      "epoch 24, val_loss 455.5799865722656\n",
      "epoch 25, train_loss 1278.123291015625\n",
      "epoch 25, val_loss 455.5798645019531\n",
      "epoch 26, train_loss 1278.1231689453125\n",
      "epoch 26, val_loss 455.5797424316406\n",
      "epoch 27, train_loss 1278.1229248046875\n",
      "epoch 27, val_loss 455.57958984375\n",
      "epoch 28, train_loss 1278.122802734375\n",
      "epoch 28, val_loss 455.5794982910156\n",
      "epoch 29, train_loss 1278.1226806640625\n",
      "epoch 29, val_loss 455.5793762207031\n",
      "epoch 30, train_loss 1278.122314453125\n",
      "epoch 30, val_loss 455.5792541503906\n",
      "epoch 31, train_loss 1278.1221923828125\n",
      "epoch 31, val_loss 455.5790710449219\n",
      "epoch 32, train_loss 1278.1219482421875\n",
      "epoch 32, val_loss 455.57891845703125\n",
      "epoch 33, train_loss 1278.1217041015625\n",
      "epoch 33, val_loss 455.5788269042969\n",
      "epoch 34, train_loss 1278.1217041015625\n",
      "epoch 34, val_loss 455.5787353515625\n",
      "epoch 35, train_loss 1278.1214599609375\n",
      "epoch 35, val_loss 455.5785827636719\n",
      "epoch 36, train_loss 1278.1212158203125\n",
      "epoch 36, val_loss 455.57843017578125\n",
      "epoch 37, train_loss 1278.120849609375\n",
      "epoch 37, val_loss 455.5782775878906\n",
      "epoch 38, train_loss 1278.120849609375\n",
      "epoch 38, val_loss 455.5781555175781\n",
      "epoch 39, train_loss 1278.12060546875\n",
      "epoch 39, val_loss 455.5780334472656\n",
      "epoch 40, train_loss 1278.120361328125\n",
      "epoch 40, val_loss 455.57781982421875\n",
      "epoch 41, train_loss 1278.1201171875\n",
      "epoch 41, val_loss 455.5777893066406\n",
      "epoch 42, train_loss 1278.1199951171875\n",
      "epoch 42, val_loss 455.57763671875\n",
      "epoch 43, train_loss 1278.11962890625\n",
      "epoch 43, val_loss 455.5775146484375\n",
      "epoch 44, train_loss 1278.11962890625\n",
      "epoch 44, val_loss 455.57733154296875\n",
      "epoch 45, train_loss 1278.1195068359375\n",
      "epoch 45, val_loss 455.57720947265625\n",
      "epoch 46, train_loss 1278.119140625\n",
      "epoch 46, val_loss 455.57708740234375\n",
      "epoch 47, train_loss 1278.1190185546875\n",
      "epoch 47, val_loss 455.57696533203125\n",
      "epoch 48, train_loss 1278.1187744140625\n",
      "epoch 48, val_loss 455.57684326171875\n",
      "epoch 49, train_loss 1278.1185302734375\n",
      "epoch 49, val_loss 455.5767517089844\n",
      "epoch 50, train_loss 1278.118408203125\n",
      "epoch 50, val_loss 455.5765686035156\n",
      "epoch 51, train_loss 1278.1181640625\n",
      "epoch 51, val_loss 455.576416015625\n",
      "epoch 52, train_loss 1278.117919921875\n",
      "epoch 52, val_loss 455.5762634277344\n",
      "epoch 53, train_loss 1278.11767578125\n",
      "epoch 53, val_loss 455.5762023925781\n",
      "epoch 54, train_loss 1278.1175537109375\n",
      "epoch 54, val_loss 455.5760803222656\n",
      "epoch 55, train_loss 1278.1171875\n",
      "epoch 55, val_loss 455.57586669921875\n",
      "epoch 56, train_loss 1278.1170654296875\n",
      "epoch 56, val_loss 455.5758056640625\n",
      "epoch 57, train_loss 1278.1168212890625\n",
      "epoch 57, val_loss 455.57562255859375\n",
      "epoch 58, train_loss 1278.1168212890625\n",
      "epoch 58, val_loss 455.5755310058594\n",
      "epoch 59, train_loss 1278.1165771484375\n",
      "epoch 59, val_loss 455.5753479003906\n",
      "epoch 60, train_loss 1278.1163330078125\n",
      "epoch 60, val_loss 455.57525634765625\n",
      "epoch 61, train_loss 1278.115966796875\n",
      "epoch 61, val_loss 455.5750732421875\n",
      "epoch 62, train_loss 1278.115966796875\n",
      "epoch 62, val_loss 455.57501220703125\n",
      "epoch 63, train_loss 1278.11572265625\n",
      "epoch 63, val_loss 455.57489013671875\n",
      "epoch 64, train_loss 1278.1153564453125\n",
      "epoch 64, val_loss 455.5746765136719\n",
      "epoch 65, train_loss 1278.1151123046875\n",
      "epoch 65, val_loss 455.5745544433594\n",
      "epoch 66, train_loss 1278.1148681640625\n",
      "epoch 66, val_loss 455.574462890625\n",
      "epoch 67, train_loss 1278.1148681640625\n",
      "epoch 67, val_loss 455.57427978515625\n",
      "epoch 68, train_loss 1278.1146240234375\n",
      "epoch 68, val_loss 455.57415771484375\n",
      "epoch 69, train_loss 1278.114501953125\n",
      "epoch 69, val_loss 455.5740966796875\n",
      "epoch 70, train_loss 1278.1143798828125\n",
      "epoch 70, val_loss 455.5739440917969\n",
      "epoch 71, train_loss 1278.1141357421875\n",
      "epoch 71, val_loss 455.5737609863281\n",
      "epoch 72, train_loss 1278.114013671875\n",
      "epoch 72, val_loss 455.5735778808594\n",
      "epoch 73, train_loss 1278.1138916015625\n",
      "epoch 73, val_loss 455.57354736328125\n",
      "epoch 74, train_loss 1278.113525390625\n",
      "epoch 74, val_loss 455.57342529296875\n",
      "epoch 75, train_loss 1278.11328125\n",
      "epoch 75, val_loss 455.5732116699219\n",
      "epoch 76, train_loss 1278.11328125\n",
      "epoch 76, val_loss 455.5731201171875\n",
      "epoch 77, train_loss 1278.1129150390625\n",
      "epoch 77, val_loss 455.572998046875\n",
      "epoch 78, train_loss 1278.1126708984375\n",
      "epoch 78, val_loss 455.5728454589844\n",
      "epoch 79, train_loss 1278.1123046875\n",
      "epoch 79, val_loss 455.5727233886719\n",
      "epoch 80, train_loss 1278.1123046875\n",
      "epoch 80, val_loss 455.5725402832031\n",
      "epoch 81, train_loss 1278.1121826171875\n",
      "epoch 81, val_loss 455.572509765625\n",
      "epoch 82, train_loss 1278.112060546875\n",
      "epoch 82, val_loss 455.57232666015625\n",
      "epoch 83, train_loss 1278.111572265625\n",
      "epoch 83, val_loss 455.57220458984375\n",
      "epoch 84, train_loss 1278.1114501953125\n",
      "epoch 84, val_loss 455.5721130371094\n",
      "epoch 85, train_loss 1278.1112060546875\n",
      "epoch 85, val_loss 455.5719299316406\n",
      "epoch 86, train_loss 1278.1112060546875\n",
      "epoch 86, val_loss 455.57177734375\n",
      "epoch 87, train_loss 1278.1107177734375\n",
      "epoch 87, val_loss 455.5717468261719\n",
      "epoch 88, train_loss 1278.110595703125\n",
      "epoch 88, val_loss 455.571533203125\n",
      "epoch 89, train_loss 1278.1104736328125\n",
      "epoch 89, val_loss 455.57147216796875\n",
      "epoch 90, train_loss 1278.1102294921875\n",
      "epoch 90, val_loss 455.5712890625\n",
      "epoch 91, train_loss 1278.1099853515625\n",
      "epoch 91, val_loss 455.5711364746094\n",
      "epoch 92, train_loss 1278.10986328125\n",
      "epoch 92, val_loss 455.5710144042969\n",
      "epoch 93, train_loss 1278.10986328125\n",
      "epoch 93, val_loss 455.5709228515625\n",
      "epoch 94, train_loss 1278.1094970703125\n",
      "epoch 94, val_loss 455.57073974609375\n",
      "epoch 95, train_loss 1278.109375\n",
      "epoch 95, val_loss 455.5705871582031\n",
      "epoch 96, train_loss 1278.1090087890625\n",
      "epoch 96, val_loss 455.57049560546875\n",
      "epoch 97, train_loss 1278.10888671875\n",
      "epoch 97, val_loss 455.5704040527344\n",
      "epoch 98, train_loss 1278.1087646484375\n",
      "epoch 98, val_loss 455.5702209472656\n",
      "epoch 99, train_loss 1278.1085205078125\n",
      "epoch 99, val_loss 455.5700378417969\n",
      "Parameter containing:\n",
      "tensor([9.7053e-09], requires_grad=True)\n",
      "iter 38, train_loss_regularization 0.969965934753418\n",
      "iter 38, val_loss_regularization 0.969965934753418\n",
      "epoch 0, train_loss 1278.108154296875\n",
      "epoch 0, val_loss 455.5699462890625\n",
      "epoch 1, train_loss 1278.108154296875\n",
      "epoch 1, val_loss 455.56982421875\n",
      "epoch 2, train_loss 1278.1077880859375\n",
      "epoch 2, val_loss 455.5697021484375\n",
      "epoch 3, train_loss 1278.107666015625\n",
      "epoch 3, val_loss 455.5695495605469\n",
      "epoch 4, train_loss 1278.107421875\n",
      "epoch 4, val_loss 455.5693664550781\n",
      "epoch 5, train_loss 1278.1072998046875\n",
      "epoch 5, val_loss 455.5692443847656\n",
      "epoch 6, train_loss 1278.1070556640625\n",
      "epoch 6, val_loss 455.56915283203125\n",
      "epoch 7, train_loss 1278.1070556640625\n",
      "epoch 7, val_loss 455.56903076171875\n",
      "epoch 8, train_loss 1278.1065673828125\n",
      "epoch 8, val_loss 455.5688781738281\n",
      "epoch 9, train_loss 1278.1064453125\n",
      "epoch 9, val_loss 455.56878662109375\n",
      "epoch 10, train_loss 1278.106201171875\n",
      "epoch 10, val_loss 455.56866455078125\n",
      "epoch 11, train_loss 1278.10595703125\n",
      "epoch 11, val_loss 455.5684509277344\n",
      "epoch 12, train_loss 1278.1058349609375\n",
      "epoch 12, val_loss 455.56842041015625\n",
      "epoch 13, train_loss 1278.1055908203125\n",
      "epoch 13, val_loss 455.5682067871094\n",
      "epoch 14, train_loss 1278.10546875\n",
      "epoch 14, val_loss 455.5680847167969\n",
      "epoch 15, train_loss 1278.105224609375\n",
      "epoch 15, val_loss 455.5679931640625\n",
      "epoch 16, train_loss 1278.1051025390625\n",
      "epoch 16, val_loss 455.5678405761719\n",
      "epoch 17, train_loss 1278.1048583984375\n",
      "epoch 17, val_loss 455.5677185058594\n",
      "epoch 18, train_loss 1278.1046142578125\n",
      "epoch 18, val_loss 455.5675964355469\n",
      "epoch 19, train_loss 1278.1044921875\n",
      "epoch 19, val_loss 455.56744384765625\n",
      "epoch 20, train_loss 1278.1043701171875\n",
      "epoch 20, val_loss 455.56732177734375\n",
      "epoch 21, train_loss 1278.1041259765625\n",
      "epoch 21, val_loss 455.5671691894531\n",
      "epoch 22, train_loss 1278.103759765625\n",
      "epoch 22, val_loss 455.5669860839844\n",
      "epoch 23, train_loss 1278.1036376953125\n",
      "epoch 23, val_loss 455.5669250488281\n",
      "epoch 24, train_loss 1278.1033935546875\n",
      "epoch 24, val_loss 455.5668029785156\n",
      "epoch 25, train_loss 1278.1033935546875\n",
      "epoch 25, val_loss 455.5666809082031\n",
      "epoch 26, train_loss 1278.10302734375\n",
      "epoch 26, val_loss 455.5664978027344\n",
      "epoch 27, train_loss 1278.1029052734375\n",
      "epoch 27, val_loss 455.56640625\n",
      "epoch 28, train_loss 1278.1025390625\n",
      "epoch 28, val_loss 455.5662841796875\n",
      "epoch 29, train_loss 1278.1024169921875\n",
      "epoch 29, val_loss 455.5660705566406\n",
      "epoch 30, train_loss 1278.102294921875\n",
      "epoch 30, val_loss 455.5660095214844\n",
      "epoch 31, train_loss 1278.10205078125\n",
      "epoch 31, val_loss 455.56591796875\n",
      "epoch 32, train_loss 1278.101806640625\n",
      "epoch 32, val_loss 455.5657043457031\n",
      "epoch 33, train_loss 1278.1015625\n",
      "epoch 33, val_loss 455.56561279296875\n",
      "epoch 34, train_loss 1278.1014404296875\n",
      "epoch 34, val_loss 455.5654602050781\n",
      "epoch 35, train_loss 1278.1011962890625\n",
      "epoch 35, val_loss 455.5653381347656\n",
      "epoch 36, train_loss 1278.1009521484375\n",
      "epoch 36, val_loss 455.5652160644531\n",
      "epoch 37, train_loss 1278.100830078125\n",
      "epoch 37, val_loss 455.5650329589844\n",
      "epoch 38, train_loss 1278.100830078125\n",
      "epoch 38, val_loss 455.5649719238281\n",
      "epoch 39, train_loss 1278.1004638671875\n",
      "epoch 39, val_loss 455.56488037109375\n",
      "epoch 40, train_loss 1278.1004638671875\n",
      "epoch 40, val_loss 455.5646667480469\n",
      "epoch 41, train_loss 1278.10009765625\n",
      "epoch 41, val_loss 455.5645446777344\n",
      "epoch 42, train_loss 1278.099853515625\n",
      "epoch 42, val_loss 455.5644226074219\n",
      "epoch 43, train_loss 1278.099609375\n",
      "epoch 43, val_loss 455.5643005371094\n",
      "epoch 44, train_loss 1278.0994873046875\n",
      "epoch 44, val_loss 455.5641784667969\n",
      "epoch 45, train_loss 1278.09912109375\n",
      "epoch 45, val_loss 455.5640563964844\n",
      "epoch 46, train_loss 1278.09912109375\n",
      "epoch 46, val_loss 455.56390380859375\n",
      "epoch 47, train_loss 1278.098876953125\n",
      "epoch 47, val_loss 455.5638427734375\n",
      "epoch 48, train_loss 1278.0987548828125\n",
      "epoch 48, val_loss 455.56365966796875\n",
      "epoch 49, train_loss 1278.098388671875\n",
      "epoch 49, val_loss 455.5635070800781\n",
      "epoch 50, train_loss 1278.098388671875\n",
      "epoch 50, val_loss 455.5633850097656\n",
      "epoch 51, train_loss 1278.0980224609375\n",
      "epoch 51, val_loss 455.5632629394531\n",
      "epoch 52, train_loss 1278.0977783203125\n",
      "epoch 52, val_loss 455.56317138671875\n",
      "epoch 53, train_loss 1278.0975341796875\n",
      "epoch 53, val_loss 455.56292724609375\n",
      "epoch 54, train_loss 1278.0975341796875\n",
      "epoch 54, val_loss 455.56292724609375\n",
      "epoch 55, train_loss 1278.09716796875\n",
      "epoch 55, val_loss 455.562744140625\n",
      "epoch 56, train_loss 1278.0970458984375\n",
      "epoch 56, val_loss 455.5625915527344\n",
      "epoch 57, train_loss 1278.0968017578125\n",
      "epoch 57, val_loss 455.5624694824219\n",
      "epoch 58, train_loss 1278.0965576171875\n",
      "epoch 58, val_loss 455.5623474121094\n",
      "epoch 59, train_loss 1278.0966796875\n",
      "epoch 59, val_loss 455.56219482421875\n",
      "epoch 60, train_loss 1278.0963134765625\n",
      "epoch 60, val_loss 455.5620422363281\n",
      "epoch 61, train_loss 1278.0960693359375\n",
      "epoch 61, val_loss 455.5619201660156\n",
      "epoch 62, train_loss 1278.095947265625\n",
      "epoch 62, val_loss 455.5617980957031\n",
      "epoch 63, train_loss 1278.0955810546875\n",
      "epoch 63, val_loss 455.56170654296875\n",
      "epoch 64, train_loss 1278.095458984375\n",
      "epoch 64, val_loss 455.5615539550781\n",
      "epoch 65, train_loss 1278.0953369140625\n",
      "epoch 65, val_loss 455.5614318847656\n",
      "epoch 66, train_loss 1278.0950927734375\n",
      "epoch 66, val_loss 455.561279296875\n",
      "epoch 67, train_loss 1278.094970703125\n",
      "epoch 67, val_loss 455.5611267089844\n",
      "epoch 68, train_loss 1278.0947265625\n",
      "epoch 68, val_loss 455.56103515625\n",
      "epoch 69, train_loss 1278.094482421875\n",
      "epoch 69, val_loss 455.5609130859375\n",
      "epoch 70, train_loss 1278.0943603515625\n",
      "epoch 70, val_loss 455.560791015625\n",
      "epoch 71, train_loss 1278.09423828125\n",
      "epoch 71, val_loss 455.5606689453125\n",
      "epoch 72, train_loss 1278.0938720703125\n",
      "epoch 72, val_loss 455.5604248046875\n",
      "epoch 73, train_loss 1278.0936279296875\n",
      "epoch 73, val_loss 455.56036376953125\n",
      "epoch 74, train_loss 1278.093505859375\n",
      "epoch 74, val_loss 455.5601806640625\n",
      "epoch 75, train_loss 1278.09326171875\n",
      "epoch 75, val_loss 455.56011962890625\n",
      "epoch 76, train_loss 1278.0931396484375\n",
      "epoch 76, val_loss 455.55999755859375\n",
      "epoch 77, train_loss 1278.0928955078125\n",
      "epoch 77, val_loss 455.5598449707031\n",
      "epoch 78, train_loss 1278.092529296875\n",
      "epoch 78, val_loss 455.5597229003906\n",
      "epoch 79, train_loss 1278.092529296875\n",
      "epoch 79, val_loss 455.5595703125\n",
      "epoch 80, train_loss 1278.09228515625\n",
      "epoch 80, val_loss 455.5594177246094\n",
      "epoch 81, train_loss 1278.092041015625\n",
      "epoch 81, val_loss 455.559326171875\n",
      "epoch 82, train_loss 1278.091796875\n",
      "epoch 82, val_loss 455.5591735839844\n",
      "epoch 83, train_loss 1278.0916748046875\n",
      "epoch 83, val_loss 455.55908203125\n",
      "epoch 84, train_loss 1278.091552734375\n",
      "epoch 84, val_loss 455.5589599609375\n",
      "epoch 85, train_loss 1278.09130859375\n",
      "epoch 85, val_loss 455.5587463378906\n",
      "epoch 86, train_loss 1278.091064453125\n",
      "epoch 86, val_loss 455.5587158203125\n",
      "epoch 87, train_loss 1278.091064453125\n",
      "epoch 87, val_loss 455.55853271484375\n",
      "epoch 88, train_loss 1278.0906982421875\n",
      "epoch 88, val_loss 455.55841064453125\n",
      "epoch 89, train_loss 1278.090576171875\n",
      "epoch 89, val_loss 455.5582580566406\n",
      "epoch 90, train_loss 1278.09033203125\n",
      "epoch 90, val_loss 455.5580749511719\n",
      "epoch 91, train_loss 1278.090087890625\n",
      "epoch 91, val_loss 455.5580139160156\n",
      "epoch 92, train_loss 1278.0899658203125\n",
      "epoch 92, val_loss 455.557861328125\n",
      "epoch 93, train_loss 1278.0897216796875\n",
      "epoch 93, val_loss 455.5577087402344\n",
      "epoch 94, train_loss 1278.0894775390625\n",
      "epoch 94, val_loss 455.5576171875\n",
      "epoch 95, train_loss 1278.08935546875\n",
      "epoch 95, val_loss 455.5574951171875\n",
      "epoch 96, train_loss 1278.089111328125\n",
      "epoch 96, val_loss 455.557373046875\n",
      "epoch 97, train_loss 1278.0889892578125\n",
      "epoch 97, val_loss 455.5572204589844\n",
      "epoch 98, train_loss 1278.0887451171875\n",
      "epoch 98, val_loss 455.55712890625\n",
      "epoch 99, train_loss 1278.088623046875\n",
      "epoch 99, val_loss 455.5570068359375\n",
      "Parameter containing:\n",
      "tensor([6.2209e-09], requires_grad=True)\n",
      "iter 39, train_loss_regularization 0.9643551111221313\n",
      "iter 39, val_loss_regularization 0.9643551111221313\n",
      "epoch 0, train_loss 1278.08837890625\n",
      "epoch 0, val_loss 455.556884765625\n",
      "epoch 1, train_loss 1278.088134765625\n",
      "epoch 1, val_loss 455.5567626953125\n",
      "epoch 2, train_loss 1278.087890625\n",
      "epoch 2, val_loss 455.55657958984375\n",
      "epoch 3, train_loss 1278.0877685546875\n",
      "epoch 3, val_loss 455.55645751953125\n",
      "epoch 4, train_loss 1278.0875244140625\n",
      "epoch 4, val_loss 455.5562438964844\n",
      "epoch 5, train_loss 1278.08740234375\n",
      "epoch 5, val_loss 455.55621337890625\n",
      "epoch 6, train_loss 1278.087158203125\n",
      "epoch 6, val_loss 455.55609130859375\n",
      "epoch 7, train_loss 1278.0870361328125\n",
      "epoch 7, val_loss 455.55596923828125\n",
      "epoch 8, train_loss 1278.0869140625\n",
      "epoch 8, val_loss 455.5557861328125\n",
      "epoch 9, train_loss 1278.0865478515625\n",
      "epoch 9, val_loss 455.5556640625\n",
      "epoch 10, train_loss 1278.0863037109375\n",
      "epoch 10, val_loss 455.5555114746094\n",
      "epoch 11, train_loss 1278.0860595703125\n",
      "epoch 11, val_loss 455.555419921875\n",
      "epoch 12, train_loss 1278.0860595703125\n",
      "epoch 12, val_loss 455.55523681640625\n",
      "epoch 13, train_loss 1278.0859375\n",
      "epoch 13, val_loss 455.5550842285156\n",
      "epoch 14, train_loss 1278.08544921875\n",
      "epoch 14, val_loss 455.55499267578125\n",
      "epoch 15, train_loss 1278.08544921875\n",
      "epoch 15, val_loss 455.5548400878906\n",
      "epoch 16, train_loss 1278.085205078125\n",
      "epoch 16, val_loss 455.5547180175781\n",
      "epoch 17, train_loss 1278.0850830078125\n",
      "epoch 17, val_loss 455.5546569824219\n",
      "epoch 18, train_loss 1278.084716796875\n",
      "epoch 18, val_loss 455.55450439453125\n",
      "epoch 19, train_loss 1278.08447265625\n",
      "epoch 19, val_loss 455.55438232421875\n",
      "epoch 20, train_loss 1278.08447265625\n",
      "epoch 20, val_loss 455.55426025390625\n",
      "epoch 21, train_loss 1278.084228515625\n",
      "epoch 21, val_loss 455.5540771484375\n",
      "epoch 22, train_loss 1278.083984375\n",
      "epoch 22, val_loss 455.5539855957031\n",
      "epoch 23, train_loss 1278.0838623046875\n",
      "epoch 23, val_loss 455.5538330078125\n",
      "epoch 24, train_loss 1278.0836181640625\n",
      "epoch 24, val_loss 455.5537414550781\n",
      "epoch 25, train_loss 1278.0836181640625\n",
      "epoch 25, val_loss 455.5536193847656\n",
      "epoch 26, train_loss 1278.0833740234375\n",
      "epoch 26, val_loss 455.55340576171875\n",
      "epoch 27, train_loss 1278.0831298828125\n",
      "epoch 27, val_loss 455.55328369140625\n",
      "epoch 28, train_loss 1278.0828857421875\n",
      "epoch 28, val_loss 455.55322265625\n",
      "epoch 29, train_loss 1278.0826416015625\n",
      "epoch 29, val_loss 455.5530700683594\n",
      "epoch 30, train_loss 1278.08251953125\n",
      "epoch 30, val_loss 455.5529479980469\n",
      "epoch 31, train_loss 1278.082275390625\n",
      "epoch 31, val_loss 455.5528259277344\n",
      "epoch 32, train_loss 1278.08203125\n",
      "epoch 32, val_loss 455.55267333984375\n",
      "epoch 33, train_loss 1278.0819091796875\n",
      "epoch 33, val_loss 455.55255126953125\n",
      "epoch 34, train_loss 1278.081787109375\n",
      "epoch 34, val_loss 455.55242919921875\n",
      "epoch 35, train_loss 1278.08154296875\n",
      "epoch 35, val_loss 455.5522766113281\n",
      "epoch 36, train_loss 1278.081298828125\n",
      "epoch 36, val_loss 455.5521545410156\n",
      "epoch 37, train_loss 1278.0811767578125\n",
      "epoch 37, val_loss 455.552001953125\n",
      "epoch 38, train_loss 1278.0810546875\n",
      "epoch 38, val_loss 455.5518798828125\n",
      "epoch 39, train_loss 1278.08056640625\n",
      "epoch 39, val_loss 455.5517883300781\n",
      "epoch 40, train_loss 1278.08056640625\n",
      "epoch 40, val_loss 455.5516662597656\n",
      "epoch 41, train_loss 1278.080322265625\n",
      "epoch 41, val_loss 455.5515441894531\n",
      "epoch 42, train_loss 1278.0802001953125\n",
      "epoch 42, val_loss 455.5514221191406\n",
      "epoch 43, train_loss 1278.0799560546875\n",
      "epoch 43, val_loss 455.5513000488281\n",
      "epoch 44, train_loss 1278.0797119140625\n",
      "epoch 44, val_loss 455.5511169433594\n",
      "epoch 45, train_loss 1278.0794677734375\n",
      "epoch 45, val_loss 455.5509948730469\n",
      "epoch 46, train_loss 1278.0794677734375\n",
      "epoch 46, val_loss 455.5508728027344\n",
      "epoch 47, train_loss 1278.0792236328125\n",
      "epoch 47, val_loss 455.5507507324219\n",
      "epoch 48, train_loss 1278.078857421875\n",
      "epoch 48, val_loss 455.5506286621094\n",
      "epoch 49, train_loss 1278.078857421875\n",
      "epoch 49, val_loss 455.5504455566406\n",
      "epoch 50, train_loss 1278.07861328125\n",
      "epoch 50, val_loss 455.5503845214844\n",
      "epoch 51, train_loss 1278.078369140625\n",
      "epoch 51, val_loss 455.5502624511719\n",
      "epoch 52, train_loss 1278.0782470703125\n",
      "epoch 52, val_loss 455.5500793457031\n",
      "epoch 53, train_loss 1278.0780029296875\n",
      "epoch 53, val_loss 455.5499572753906\n",
      "epoch 54, train_loss 1278.07763671875\n",
      "epoch 54, val_loss 455.54986572265625\n",
      "epoch 55, train_loss 1278.07763671875\n",
      "epoch 55, val_loss 455.54962158203125\n",
      "epoch 56, train_loss 1278.077392578125\n",
      "epoch 56, val_loss 455.5495910644531\n",
      "epoch 57, train_loss 1278.0772705078125\n",
      "epoch 57, val_loss 455.5494079589844\n",
      "epoch 58, train_loss 1278.0770263671875\n",
      "epoch 58, val_loss 455.5492858886719\n",
      "epoch 59, train_loss 1278.0767822265625\n",
      "epoch 59, val_loss 455.5491943359375\n",
      "epoch 60, train_loss 1278.0765380859375\n",
      "epoch 60, val_loss 455.549072265625\n",
      "epoch 61, train_loss 1278.0762939453125\n",
      "epoch 61, val_loss 455.5489196777344\n",
      "epoch 62, train_loss 1278.076171875\n",
      "epoch 62, val_loss 455.548828125\n",
      "epoch 63, train_loss 1278.075927734375\n",
      "epoch 63, val_loss 455.5486755371094\n",
      "epoch 64, train_loss 1278.0758056640625\n",
      "epoch 64, val_loss 455.548583984375\n",
      "epoch 65, train_loss 1278.0755615234375\n",
      "epoch 65, val_loss 455.5484619140625\n",
      "epoch 66, train_loss 1278.075439453125\n",
      "epoch 66, val_loss 455.54827880859375\n",
      "epoch 67, train_loss 1278.0753173828125\n",
      "epoch 67, val_loss 455.5481262207031\n",
      "epoch 68, train_loss 1278.0751953125\n",
      "epoch 68, val_loss 455.5480041503906\n",
      "epoch 69, train_loss 1278.07470703125\n",
      "epoch 69, val_loss 455.5478820800781\n",
      "epoch 70, train_loss 1278.07470703125\n",
      "epoch 70, val_loss 455.5476989746094\n",
      "epoch 71, train_loss 1278.0743408203125\n",
      "epoch 71, val_loss 455.54766845703125\n",
      "epoch 72, train_loss 1278.07421875\n",
      "epoch 72, val_loss 455.5474548339844\n",
      "epoch 73, train_loss 1278.073974609375\n",
      "epoch 73, val_loss 455.54736328125\n",
      "epoch 74, train_loss 1278.073974609375\n",
      "epoch 74, val_loss 455.5472106933594\n",
      "epoch 75, train_loss 1278.0736083984375\n",
      "epoch 75, val_loss 455.547119140625\n",
      "epoch 76, train_loss 1278.0733642578125\n",
      "epoch 76, val_loss 455.5469970703125\n",
      "epoch 77, train_loss 1278.0733642578125\n",
      "epoch 77, val_loss 455.546875\n",
      "epoch 78, train_loss 1278.072998046875\n",
      "epoch 78, val_loss 455.5466613769531\n",
      "epoch 79, train_loss 1278.0728759765625\n",
      "epoch 79, val_loss 455.546630859375\n",
      "epoch 80, train_loss 1278.07275390625\n",
      "epoch 80, val_loss 455.54644775390625\n",
      "epoch 81, train_loss 1278.0726318359375\n",
      "epoch 81, val_loss 455.54638671875\n",
      "epoch 82, train_loss 1278.0723876953125\n",
      "epoch 82, val_loss 455.5462646484375\n",
      "epoch 83, train_loss 1278.072265625\n",
      "epoch 83, val_loss 455.5460510253906\n",
      "epoch 84, train_loss 1278.07177734375\n",
      "epoch 84, val_loss 455.5459289550781\n",
      "epoch 85, train_loss 1278.07177734375\n",
      "epoch 85, val_loss 455.54583740234375\n",
      "epoch 86, train_loss 1278.071533203125\n",
      "epoch 86, val_loss 455.545654296875\n",
      "epoch 87, train_loss 1278.0712890625\n",
      "epoch 87, val_loss 455.5455322265625\n",
      "epoch 88, train_loss 1278.0711669921875\n",
      "epoch 88, val_loss 455.5455017089844\n",
      "epoch 89, train_loss 1278.071044921875\n",
      "epoch 89, val_loss 455.5452880859375\n",
      "epoch 90, train_loss 1278.07080078125\n",
      "epoch 90, val_loss 455.545166015625\n",
      "epoch 91, train_loss 1278.070556640625\n",
      "epoch 91, val_loss 455.5450744628906\n",
      "epoch 92, train_loss 1278.0704345703125\n",
      "epoch 92, val_loss 455.544921875\n",
      "epoch 93, train_loss 1278.0701904296875\n",
      "epoch 93, val_loss 455.54473876953125\n",
      "epoch 94, train_loss 1278.0699462890625\n",
      "epoch 94, val_loss 455.544677734375\n",
      "epoch 95, train_loss 1278.0697021484375\n",
      "epoch 95, val_loss 455.5445556640625\n",
      "epoch 96, train_loss 1278.069580078125\n",
      "epoch 96, val_loss 455.54437255859375\n",
      "epoch 97, train_loss 1278.0694580078125\n",
      "epoch 97, val_loss 455.54425048828125\n",
      "epoch 98, train_loss 1278.0693359375\n",
      "epoch 98, val_loss 455.54412841796875\n",
      "epoch 99, train_loss 1278.0689697265625\n",
      "epoch 99, val_loss 455.54400634765625\n",
      "Parameter containing:\n",
      "tensor([3.9936e-09], requires_grad=True)\n",
      "iter 40, train_loss_regularization 0.95884108543396\n",
      "iter 40, val_loss_regularization 0.95884108543396\n",
      "epoch 0, train_loss 1278.06884765625\n",
      "epoch 0, val_loss 455.5437927246094\n",
      "epoch 1, train_loss 1278.068359375\n",
      "epoch 1, val_loss 455.54376220703125\n",
      "epoch 2, train_loss 1278.0684814453125\n",
      "epoch 2, val_loss 455.54364013671875\n",
      "epoch 3, train_loss 1278.068359375\n",
      "epoch 3, val_loss 455.5434875488281\n",
      "epoch 4, train_loss 1278.068115234375\n",
      "epoch 4, val_loss 455.5433654785156\n",
      "epoch 5, train_loss 1278.0679931640625\n",
      "epoch 5, val_loss 455.5433044433594\n",
      "epoch 6, train_loss 1278.0677490234375\n",
      "epoch 6, val_loss 455.5431213378906\n",
      "epoch 7, train_loss 1278.0675048828125\n",
      "epoch 7, val_loss 455.5429992675781\n",
      "epoch 8, train_loss 1278.0672607421875\n",
      "epoch 8, val_loss 455.5428771972656\n",
      "epoch 9, train_loss 1278.0673828125\n",
      "epoch 9, val_loss 455.5427551269531\n",
      "epoch 10, train_loss 1278.06689453125\n",
      "epoch 10, val_loss 455.5426330566406\n",
      "epoch 11, train_loss 1278.0667724609375\n",
      "epoch 11, val_loss 455.5425109863281\n",
      "epoch 12, train_loss 1278.0665283203125\n",
      "epoch 12, val_loss 455.5423278808594\n",
      "epoch 13, train_loss 1278.06640625\n",
      "epoch 13, val_loss 455.54217529296875\n",
      "epoch 14, train_loss 1278.066162109375\n",
      "epoch 14, val_loss 455.5421142578125\n",
      "epoch 15, train_loss 1278.06591796875\n",
      "epoch 15, val_loss 455.5419616699219\n",
      "epoch 16, train_loss 1278.0657958984375\n",
      "epoch 16, val_loss 455.5418395996094\n",
      "epoch 17, train_loss 1278.0655517578125\n",
      "epoch 17, val_loss 455.5417175292969\n",
      "epoch 18, train_loss 1278.0653076171875\n",
      "epoch 18, val_loss 455.54150390625\n",
      "epoch 19, train_loss 1278.065185546875\n",
      "epoch 19, val_loss 455.5414123535156\n",
      "epoch 20, train_loss 1278.06494140625\n",
      "epoch 20, val_loss 455.54132080078125\n",
      "epoch 21, train_loss 1278.0648193359375\n",
      "epoch 21, val_loss 455.5411682128906\n",
      "epoch 22, train_loss 1278.064453125\n",
      "epoch 22, val_loss 455.54107666015625\n",
      "epoch 23, train_loss 1278.0643310546875\n",
      "epoch 23, val_loss 455.5409851074219\n",
      "epoch 24, train_loss 1278.0643310546875\n",
      "epoch 24, val_loss 455.5408020019531\n",
      "epoch 25, train_loss 1278.06396484375\n",
      "epoch 25, val_loss 455.5406188964844\n",
      "epoch 26, train_loss 1278.0638427734375\n",
      "epoch 26, val_loss 455.54058837890625\n",
      "epoch 27, train_loss 1278.063720703125\n",
      "epoch 27, val_loss 455.5404052734375\n",
      "epoch 28, train_loss 1278.0634765625\n",
      "epoch 28, val_loss 455.540283203125\n",
      "epoch 29, train_loss 1278.0634765625\n",
      "epoch 29, val_loss 455.5401306152344\n",
      "epoch 30, train_loss 1278.0631103515625\n",
      "epoch 30, val_loss 455.5400085449219\n",
      "epoch 31, train_loss 1278.0628662109375\n",
      "epoch 31, val_loss 455.5399169921875\n",
      "epoch 32, train_loss 1278.0626220703125\n",
      "epoch 32, val_loss 455.5398254394531\n",
      "epoch 33, train_loss 1278.0626220703125\n",
      "epoch 33, val_loss 455.53961181640625\n",
      "epoch 34, train_loss 1278.0621337890625\n",
      "epoch 34, val_loss 455.53948974609375\n",
      "epoch 35, train_loss 1278.0621337890625\n",
      "epoch 35, val_loss 455.5394287109375\n",
      "epoch 36, train_loss 1278.0618896484375\n",
      "epoch 36, val_loss 455.53924560546875\n",
      "epoch 37, train_loss 1278.061767578125\n",
      "epoch 37, val_loss 455.5391540527344\n",
      "epoch 38, train_loss 1278.0615234375\n",
      "epoch 38, val_loss 455.5390930175781\n",
      "epoch 39, train_loss 1278.061279296875\n",
      "epoch 39, val_loss 455.53887939453125\n",
      "epoch 40, train_loss 1278.06103515625\n",
      "epoch 40, val_loss 455.53875732421875\n",
      "epoch 41, train_loss 1278.06103515625\n",
      "epoch 41, val_loss 455.53863525390625\n",
      "epoch 42, train_loss 1278.0606689453125\n",
      "epoch 42, val_loss 455.5384521484375\n",
      "epoch 43, train_loss 1278.060546875\n",
      "epoch 43, val_loss 455.5384216308594\n",
      "epoch 44, train_loss 1278.060546875\n",
      "epoch 44, val_loss 455.5382385253906\n",
      "epoch 45, train_loss 1278.060302734375\n",
      "epoch 45, val_loss 455.5380859375\n",
      "epoch 46, train_loss 1278.06005859375\n",
      "epoch 46, val_loss 455.5379943847656\n",
      "epoch 47, train_loss 1278.0596923828125\n",
      "epoch 47, val_loss 455.537841796875\n",
      "epoch 48, train_loss 1278.0596923828125\n",
      "epoch 48, val_loss 455.5377197265625\n",
      "epoch 49, train_loss 1278.059326171875\n",
      "epoch 49, val_loss 455.5376281738281\n",
      "epoch 50, train_loss 1278.0592041015625\n",
      "epoch 50, val_loss 455.5374450683594\n",
      "epoch 51, train_loss 1278.0589599609375\n",
      "epoch 51, val_loss 455.53729248046875\n",
      "epoch 52, train_loss 1278.058837890625\n",
      "epoch 52, val_loss 455.5372009277344\n",
      "epoch 53, train_loss 1278.0587158203125\n",
      "epoch 53, val_loss 455.53704833984375\n",
      "epoch 54, train_loss 1278.0584716796875\n",
      "epoch 54, val_loss 455.5369567871094\n",
      "epoch 55, train_loss 1278.0582275390625\n",
      "epoch 55, val_loss 455.536865234375\n",
      "epoch 56, train_loss 1278.0579833984375\n",
      "epoch 56, val_loss 455.5367126464844\n",
      "epoch 57, train_loss 1278.057861328125\n",
      "epoch 57, val_loss 455.5365295410156\n",
      "epoch 58, train_loss 1278.0574951171875\n",
      "epoch 58, val_loss 455.5364990234375\n",
      "epoch 59, train_loss 1278.057373046875\n",
      "epoch 59, val_loss 455.5362854003906\n",
      "epoch 60, train_loss 1278.057373046875\n",
      "epoch 60, val_loss 455.5361633300781\n",
      "epoch 61, train_loss 1278.05712890625\n",
      "epoch 61, val_loss 455.5361328125\n",
      "epoch 62, train_loss 1278.0570068359375\n",
      "epoch 62, val_loss 455.5359191894531\n",
      "epoch 63, train_loss 1278.0567626953125\n",
      "epoch 63, val_loss 455.53582763671875\n",
      "epoch 64, train_loss 1278.0565185546875\n",
      "epoch 64, val_loss 455.5356750488281\n",
      "epoch 65, train_loss 1278.0562744140625\n",
      "epoch 65, val_loss 455.5355529785156\n",
      "epoch 66, train_loss 1278.0560302734375\n",
      "epoch 66, val_loss 455.53546142578125\n",
      "epoch 67, train_loss 1278.055908203125\n",
      "epoch 67, val_loss 455.5352783203125\n",
      "epoch 68, train_loss 1278.0557861328125\n",
      "epoch 68, val_loss 455.5351257324219\n",
      "epoch 69, train_loss 1278.0555419921875\n",
      "epoch 69, val_loss 455.53509521484375\n",
      "epoch 70, train_loss 1278.0552978515625\n",
      "epoch 70, val_loss 455.534912109375\n",
      "epoch 71, train_loss 1278.0552978515625\n",
      "epoch 71, val_loss 455.5347595214844\n",
      "epoch 72, train_loss 1278.0550537109375\n",
      "epoch 72, val_loss 455.5346984863281\n",
      "epoch 73, train_loss 1278.0546875\n",
      "epoch 73, val_loss 455.53448486328125\n",
      "epoch 74, train_loss 1278.0546875\n",
      "epoch 74, val_loss 455.5343017578125\n",
      "epoch 75, train_loss 1278.0543212890625\n",
      "epoch 75, val_loss 455.53424072265625\n",
      "epoch 76, train_loss 1278.05419921875\n",
      "epoch 76, val_loss 455.5341796875\n",
      "epoch 77, train_loss 1278.0540771484375\n",
      "epoch 77, val_loss 455.5340270996094\n",
      "epoch 78, train_loss 1278.0538330078125\n",
      "epoch 78, val_loss 455.53387451171875\n",
      "epoch 79, train_loss 1278.0537109375\n",
      "epoch 79, val_loss 455.5337219238281\n",
      "epoch 80, train_loss 1278.053466796875\n",
      "epoch 80, val_loss 455.53363037109375\n",
      "epoch 81, train_loss 1278.05322265625\n",
      "epoch 81, val_loss 455.5335388183594\n",
      "epoch 82, train_loss 1278.052978515625\n",
      "epoch 82, val_loss 455.53338623046875\n",
      "epoch 83, train_loss 1278.0528564453125\n",
      "epoch 83, val_loss 455.53326416015625\n",
      "epoch 84, train_loss 1278.0526123046875\n",
      "epoch 84, val_loss 455.5331115722656\n",
      "epoch 85, train_loss 1278.052490234375\n",
      "epoch 85, val_loss 455.532958984375\n",
      "epoch 86, train_loss 1278.0523681640625\n",
      "epoch 86, val_loss 455.5328674316406\n",
      "epoch 87, train_loss 1278.0521240234375\n",
      "epoch 87, val_loss 455.5327453613281\n",
      "epoch 88, train_loss 1278.0518798828125\n",
      "epoch 88, val_loss 455.5326232910156\n",
      "epoch 89, train_loss 1278.0516357421875\n",
      "epoch 89, val_loss 455.532470703125\n",
      "epoch 90, train_loss 1278.051513671875\n",
      "epoch 90, val_loss 455.53228759765625\n",
      "epoch 91, train_loss 1278.0513916015625\n",
      "epoch 91, val_loss 455.5321960449219\n",
      "epoch 92, train_loss 1278.0511474609375\n",
      "epoch 92, val_loss 455.5320739746094\n",
      "epoch 93, train_loss 1278.051025390625\n",
      "epoch 93, val_loss 455.5320129394531\n",
      "epoch 94, train_loss 1278.05078125\n",
      "epoch 94, val_loss 455.5318298339844\n",
      "epoch 95, train_loss 1278.0506591796875\n",
      "epoch 95, val_loss 455.53167724609375\n",
      "epoch 96, train_loss 1278.05029296875\n",
      "epoch 96, val_loss 455.53155517578125\n",
      "epoch 97, train_loss 1278.05029296875\n",
      "epoch 97, val_loss 455.5314636230469\n",
      "epoch 98, train_loss 1278.05029296875\n",
      "epoch 98, val_loss 455.5313720703125\n",
      "epoch 99, train_loss 1278.0499267578125\n",
      "epoch 99, val_loss 455.5312194824219\n",
      "Parameter containing:\n",
      "tensor([2.5678e-09], requires_grad=True)\n",
      "iter 41, train_loss_regularization 0.9534199833869934\n",
      "iter 41, val_loss_regularization 0.9534199833869934\n",
      "epoch 0, train_loss 1278.049560546875\n",
      "epoch 0, val_loss 455.5310363769531\n",
      "epoch 1, train_loss 1278.049560546875\n",
      "epoch 1, val_loss 455.531005859375\n",
      "epoch 2, train_loss 1278.0491943359375\n",
      "epoch 2, val_loss 455.5307922363281\n",
      "epoch 3, train_loss 1278.0491943359375\n",
      "epoch 3, val_loss 455.53070068359375\n",
      "epoch 4, train_loss 1278.0489501953125\n",
      "epoch 4, val_loss 455.53057861328125\n",
      "epoch 5, train_loss 1278.0487060546875\n",
      "epoch 5, val_loss 455.5304260253906\n",
      "epoch 6, train_loss 1278.048583984375\n",
      "epoch 6, val_loss 455.53033447265625\n",
      "epoch 7, train_loss 1278.04833984375\n",
      "epoch 7, val_loss 455.5301818847656\n",
      "epoch 8, train_loss 1278.048095703125\n",
      "epoch 8, val_loss 455.530029296875\n",
      "epoch 9, train_loss 1278.0478515625\n",
      "epoch 9, val_loss 455.52996826171875\n",
      "epoch 10, train_loss 1278.0477294921875\n",
      "epoch 10, val_loss 455.52984619140625\n",
      "epoch 11, train_loss 1278.0474853515625\n",
      "epoch 11, val_loss 455.5296325683594\n",
      "epoch 12, train_loss 1278.04736328125\n",
      "epoch 12, val_loss 455.5295715332031\n",
      "epoch 13, train_loss 1278.047119140625\n",
      "epoch 13, val_loss 455.5294189453125\n",
      "epoch 14, train_loss 1278.0469970703125\n",
      "epoch 14, val_loss 455.5293273925781\n",
      "epoch 15, train_loss 1278.0467529296875\n",
      "epoch 15, val_loss 455.5291748046875\n",
      "epoch 16, train_loss 1278.0465087890625\n",
      "epoch 16, val_loss 455.529052734375\n",
      "epoch 17, train_loss 1278.0465087890625\n",
      "epoch 17, val_loss 455.5289306640625\n",
      "epoch 18, train_loss 1278.04638671875\n",
      "epoch 18, val_loss 455.5287780761719\n",
      "epoch 19, train_loss 1278.0460205078125\n",
      "epoch 19, val_loss 455.5287170410156\n",
      "epoch 20, train_loss 1278.0458984375\n",
      "epoch 20, val_loss 455.52850341796875\n",
      "epoch 21, train_loss 1278.0457763671875\n",
      "epoch 21, val_loss 455.52838134765625\n",
      "epoch 22, train_loss 1278.0455322265625\n",
      "epoch 22, val_loss 455.52825927734375\n",
      "epoch 23, train_loss 1278.0452880859375\n",
      "epoch 23, val_loss 455.5281677246094\n",
      "epoch 24, train_loss 1278.045166015625\n",
      "epoch 24, val_loss 455.52801513671875\n",
      "epoch 25, train_loss 1278.0447998046875\n",
      "epoch 25, val_loss 455.5279235839844\n",
      "epoch 26, train_loss 1278.0447998046875\n",
      "epoch 26, val_loss 455.5278015136719\n",
      "epoch 27, train_loss 1278.0445556640625\n",
      "epoch 27, val_loss 455.5276184082031\n",
      "epoch 28, train_loss 1278.0443115234375\n",
      "epoch 28, val_loss 455.5274963378906\n",
      "epoch 29, train_loss 1278.0440673828125\n",
      "epoch 29, val_loss 455.5273742675781\n",
      "epoch 30, train_loss 1278.0439453125\n",
      "epoch 30, val_loss 455.5272521972656\n",
      "epoch 31, train_loss 1278.043701171875\n",
      "epoch 31, val_loss 455.52716064453125\n",
      "epoch 32, train_loss 1278.0435791015625\n",
      "epoch 32, val_loss 455.5270080566406\n",
      "epoch 33, train_loss 1278.043701171875\n",
      "epoch 33, val_loss 455.5268859863281\n",
      "epoch 34, train_loss 1278.0433349609375\n",
      "epoch 34, val_loss 455.52679443359375\n",
      "epoch 35, train_loss 1278.0430908203125\n",
      "epoch 35, val_loss 455.526611328125\n",
      "epoch 36, train_loss 1278.0428466796875\n",
      "epoch 36, val_loss 455.5264892578125\n",
      "epoch 37, train_loss 1278.0426025390625\n",
      "epoch 37, val_loss 455.52642822265625\n",
      "epoch 38, train_loss 1278.0423583984375\n",
      "epoch 38, val_loss 455.52630615234375\n",
      "epoch 39, train_loss 1278.0423583984375\n",
      "epoch 39, val_loss 455.5260925292969\n",
      "epoch 40, train_loss 1278.0419921875\n",
      "epoch 40, val_loss 455.5260314941406\n",
      "epoch 41, train_loss 1278.0419921875\n",
      "epoch 41, val_loss 455.5259094238281\n",
      "epoch 42, train_loss 1278.0418701171875\n",
      "epoch 42, val_loss 455.5257568359375\n",
      "epoch 43, train_loss 1278.0413818359375\n",
      "epoch 43, val_loss 455.525634765625\n",
      "epoch 44, train_loss 1278.041259765625\n",
      "epoch 44, val_loss 455.5255126953125\n",
      "epoch 45, train_loss 1278.0411376953125\n",
      "epoch 45, val_loss 455.52532958984375\n",
      "epoch 46, train_loss 1278.0408935546875\n",
      "epoch 46, val_loss 455.5252990722656\n",
      "epoch 47, train_loss 1278.0408935546875\n",
      "epoch 47, val_loss 455.5251770019531\n",
      "epoch 48, train_loss 1278.0406494140625\n",
      "epoch 48, val_loss 455.5249938964844\n",
      "epoch 49, train_loss 1278.040283203125\n",
      "epoch 49, val_loss 455.52484130859375\n",
      "epoch 50, train_loss 1278.040283203125\n",
      "epoch 50, val_loss 455.5247497558594\n",
      "epoch 51, train_loss 1278.0400390625\n",
      "epoch 51, val_loss 455.52459716796875\n",
      "epoch 52, train_loss 1278.0399169921875\n",
      "epoch 52, val_loss 455.5244445800781\n",
      "epoch 53, train_loss 1278.0396728515625\n",
      "epoch 53, val_loss 455.5243835449219\n",
      "epoch 54, train_loss 1278.03955078125\n",
      "epoch 54, val_loss 455.5242004394531\n",
      "epoch 55, train_loss 1278.039306640625\n",
      "epoch 55, val_loss 455.5241394042969\n",
      "epoch 56, train_loss 1278.0390625\n",
      "epoch 56, val_loss 455.5239562988281\n",
      "epoch 57, train_loss 1278.0389404296875\n",
      "epoch 57, val_loss 455.5238342285156\n",
      "epoch 58, train_loss 1278.0386962890625\n",
      "epoch 58, val_loss 455.5237121582031\n",
      "epoch 59, train_loss 1278.0384521484375\n",
      "epoch 59, val_loss 455.5235900878906\n",
      "epoch 60, train_loss 1278.038330078125\n",
      "epoch 60, val_loss 455.5234680175781\n",
      "epoch 61, train_loss 1278.0379638671875\n",
      "epoch 61, val_loss 455.5233459472656\n",
      "epoch 62, train_loss 1278.0379638671875\n",
      "epoch 62, val_loss 455.52325439453125\n",
      "epoch 63, train_loss 1278.0377197265625\n",
      "epoch 63, val_loss 455.52313232421875\n",
      "epoch 64, train_loss 1278.0374755859375\n",
      "epoch 64, val_loss 455.52294921875\n",
      "epoch 65, train_loss 1278.0372314453125\n",
      "epoch 65, val_loss 455.52288818359375\n",
      "epoch 66, train_loss 1278.0372314453125\n",
      "epoch 66, val_loss 455.5227355957031\n",
      "epoch 67, train_loss 1278.037109375\n",
      "epoch 67, val_loss 455.5225524902344\n",
      "epoch 68, train_loss 1278.036865234375\n",
      "epoch 68, val_loss 455.5224914550781\n",
      "epoch 69, train_loss 1278.03662109375\n",
      "epoch 69, val_loss 455.5223388671875\n",
      "epoch 70, train_loss 1278.03662109375\n",
      "epoch 70, val_loss 455.5221252441406\n",
      "epoch 71, train_loss 1278.036376953125\n",
      "epoch 71, val_loss 455.5221252441406\n",
      "epoch 72, train_loss 1278.035888671875\n",
      "epoch 72, val_loss 455.52197265625\n",
      "epoch 73, train_loss 1278.035888671875\n",
      "epoch 73, val_loss 455.5218200683594\n",
      "epoch 74, train_loss 1278.03564453125\n",
      "epoch 74, val_loss 455.52166748046875\n",
      "epoch 75, train_loss 1278.035400390625\n",
      "epoch 75, val_loss 455.5215759277344\n",
      "epoch 76, train_loss 1278.0352783203125\n",
      "epoch 76, val_loss 455.52142333984375\n",
      "epoch 77, train_loss 1278.0350341796875\n",
      "epoch 77, val_loss 455.5213317871094\n",
      "epoch 78, train_loss 1278.0347900390625\n",
      "epoch 78, val_loss 455.5212097167969\n",
      "epoch 79, train_loss 1278.03466796875\n",
      "epoch 79, val_loss 455.5210876464844\n",
      "epoch 80, train_loss 1278.0345458984375\n",
      "epoch 80, val_loss 455.5209045410156\n",
      "epoch 81, train_loss 1278.0343017578125\n",
      "epoch 81, val_loss 455.5207824707031\n",
      "epoch 82, train_loss 1278.0343017578125\n",
      "epoch 82, val_loss 455.5206604003906\n",
      "epoch 83, train_loss 1278.033935546875\n",
      "epoch 83, val_loss 455.5205383300781\n",
      "epoch 84, train_loss 1278.03369140625\n",
      "epoch 84, val_loss 455.52044677734375\n",
      "epoch 85, train_loss 1278.0335693359375\n",
      "epoch 85, val_loss 455.52032470703125\n",
      "epoch 86, train_loss 1278.033447265625\n",
      "epoch 86, val_loss 455.52020263671875\n",
      "epoch 87, train_loss 1278.0333251953125\n",
      "epoch 87, val_loss 455.5200500488281\n",
      "epoch 88, train_loss 1278.033203125\n",
      "epoch 88, val_loss 455.51995849609375\n",
      "epoch 89, train_loss 1278.032958984375\n",
      "epoch 89, val_loss 455.5198059082031\n",
      "epoch 90, train_loss 1278.03271484375\n",
      "epoch 90, val_loss 455.5196533203125\n",
      "epoch 91, train_loss 1278.032470703125\n",
      "epoch 91, val_loss 455.51953125\n",
      "epoch 92, train_loss 1278.0322265625\n",
      "epoch 92, val_loss 455.5194091796875\n",
      "epoch 93, train_loss 1278.0321044921875\n",
      "epoch 93, val_loss 455.51934814453125\n",
      "epoch 94, train_loss 1278.031982421875\n",
      "epoch 94, val_loss 455.5191650390625\n",
      "epoch 95, train_loss 1278.0316162109375\n",
      "epoch 95, val_loss 455.5190124511719\n",
      "epoch 96, train_loss 1278.0316162109375\n",
      "epoch 96, val_loss 455.5189514160156\n",
      "epoch 97, train_loss 1278.0313720703125\n",
      "epoch 97, val_loss 455.518798828125\n",
      "epoch 98, train_loss 1278.0311279296875\n",
      "epoch 98, val_loss 455.5186767578125\n",
      "epoch 99, train_loss 1278.031005859375\n",
      "epoch 99, val_loss 455.5185546875\n",
      "Parameter containing:\n",
      "tensor([1.6535e-09], requires_grad=True)\n",
      "iter 42, train_loss_regularization 0.9480916261672974\n",
      "iter 42, val_loss_regularization 0.9480916261672974\n",
      "epoch 0, train_loss 1278.0308837890625\n",
      "epoch 0, val_loss 455.5184020996094\n",
      "epoch 1, train_loss 1278.0306396484375\n",
      "epoch 1, val_loss 455.51824951171875\n",
      "epoch 2, train_loss 1278.0303955078125\n",
      "epoch 2, val_loss 455.5181579589844\n",
      "epoch 3, train_loss 1278.0302734375\n",
      "epoch 3, val_loss 455.5180358886719\n",
      "epoch 4, train_loss 1278.030029296875\n",
      "epoch 4, val_loss 455.5179443359375\n",
      "epoch 5, train_loss 1278.02978515625\n",
      "epoch 5, val_loss 455.517822265625\n",
      "epoch 6, train_loss 1278.0296630859375\n",
      "epoch 6, val_loss 455.51763916015625\n",
      "epoch 7, train_loss 1278.029541015625\n",
      "epoch 7, val_loss 455.517578125\n",
      "epoch 8, train_loss 1278.0294189453125\n",
      "epoch 8, val_loss 455.5174560546875\n",
      "epoch 9, train_loss 1278.0291748046875\n",
      "epoch 9, val_loss 455.5173034667969\n",
      "epoch 10, train_loss 1278.0289306640625\n",
      "epoch 10, val_loss 455.5172119140625\n",
      "epoch 11, train_loss 1278.02880859375\n",
      "epoch 11, val_loss 455.5169982910156\n",
      "epoch 12, train_loss 1278.0284423828125\n",
      "epoch 12, val_loss 455.5168762207031\n",
      "epoch 13, train_loss 1278.0283203125\n",
      "epoch 13, val_loss 455.51678466796875\n",
      "epoch 14, train_loss 1278.0281982421875\n",
      "epoch 14, val_loss 455.51666259765625\n",
      "epoch 15, train_loss 1278.028076171875\n",
      "epoch 15, val_loss 455.51654052734375\n",
      "epoch 16, train_loss 1278.0277099609375\n",
      "epoch 16, val_loss 455.5164489746094\n",
      "epoch 17, train_loss 1278.027587890625\n",
      "epoch 17, val_loss 455.5162353515625\n",
      "epoch 18, train_loss 1278.0274658203125\n",
      "epoch 18, val_loss 455.51617431640625\n",
      "epoch 19, train_loss 1278.02734375\n",
      "epoch 19, val_loss 455.5159912109375\n",
      "epoch 20, train_loss 1278.027099609375\n",
      "epoch 20, val_loss 455.51593017578125\n",
      "epoch 21, train_loss 1278.02685546875\n",
      "epoch 21, val_loss 455.5157775878906\n",
      "epoch 22, train_loss 1278.0267333984375\n",
      "epoch 22, val_loss 455.515625\n",
      "epoch 23, train_loss 1278.0263671875\n",
      "epoch 23, val_loss 455.5155334472656\n",
      "epoch 24, train_loss 1278.0262451171875\n",
      "epoch 24, val_loss 455.5154113769531\n",
      "epoch 25, train_loss 1278.026123046875\n",
      "epoch 25, val_loss 455.51531982421875\n",
      "epoch 26, train_loss 1278.0260009765625\n",
      "epoch 26, val_loss 455.5151672363281\n",
      "epoch 27, train_loss 1278.0257568359375\n",
      "epoch 27, val_loss 455.51507568359375\n",
      "epoch 28, train_loss 1278.025634765625\n",
      "epoch 28, val_loss 455.5149230957031\n",
      "epoch 29, train_loss 1278.0255126953125\n",
      "epoch 29, val_loss 455.5147399902344\n",
      "epoch 30, train_loss 1278.0250244140625\n",
      "epoch 30, val_loss 455.5146789550781\n",
      "epoch 31, train_loss 1278.02490234375\n",
      "epoch 31, val_loss 455.5144958496094\n",
      "epoch 32, train_loss 1278.0247802734375\n",
      "epoch 32, val_loss 455.5143737792969\n",
      "epoch 33, train_loss 1278.024658203125\n",
      "epoch 33, val_loss 455.5142822265625\n",
      "epoch 34, train_loss 1278.0244140625\n",
      "epoch 34, val_loss 455.5141296386719\n",
      "epoch 35, train_loss 1278.0244140625\n",
      "epoch 35, val_loss 455.5140075683594\n",
      "epoch 36, train_loss 1278.024169921875\n",
      "epoch 36, val_loss 455.5139465332031\n",
      "epoch 37, train_loss 1278.02392578125\n",
      "epoch 37, val_loss 455.5137023925781\n",
      "epoch 38, train_loss 1278.0238037109375\n",
      "epoch 38, val_loss 455.513671875\n",
      "epoch 39, train_loss 1278.0235595703125\n",
      "epoch 39, val_loss 455.51348876953125\n",
      "epoch 40, train_loss 1278.0233154296875\n",
      "epoch 40, val_loss 455.513427734375\n",
      "epoch 41, train_loss 1278.023193359375\n",
      "epoch 41, val_loss 455.5133056640625\n",
      "epoch 42, train_loss 1278.02294921875\n",
      "epoch 42, val_loss 455.51312255859375\n",
      "epoch 43, train_loss 1278.022705078125\n",
      "epoch 43, val_loss 455.5130920410156\n",
      "epoch 44, train_loss 1278.022705078125\n",
      "epoch 44, val_loss 455.5129089355469\n",
      "epoch 45, train_loss 1278.0224609375\n",
      "epoch 45, val_loss 455.5127868652344\n",
      "epoch 46, train_loss 1278.022216796875\n",
      "epoch 46, val_loss 455.5126953125\n",
      "epoch 47, train_loss 1278.0220947265625\n",
      "epoch 47, val_loss 455.51251220703125\n",
      "epoch 48, train_loss 1278.02197265625\n",
      "epoch 48, val_loss 455.5123291015625\n",
      "epoch 49, train_loss 1278.0216064453125\n",
      "epoch 49, val_loss 455.5123291015625\n",
      "epoch 50, train_loss 1278.0213623046875\n",
      "epoch 50, val_loss 455.5121765136719\n",
      "epoch 51, train_loss 1278.0213623046875\n",
      "epoch 51, val_loss 455.5120544433594\n",
      "epoch 52, train_loss 1278.0211181640625\n",
      "epoch 52, val_loss 455.5118713378906\n",
      "epoch 53, train_loss 1278.0208740234375\n",
      "epoch 53, val_loss 455.51177978515625\n",
      "epoch 54, train_loss 1278.020751953125\n",
      "epoch 54, val_loss 455.51165771484375\n",
      "epoch 55, train_loss 1278.0206298828125\n",
      "epoch 55, val_loss 455.51153564453125\n",
      "epoch 56, train_loss 1278.0203857421875\n",
      "epoch 56, val_loss 455.51141357421875\n",
      "epoch 57, train_loss 1278.020263671875\n",
      "epoch 57, val_loss 455.51129150390625\n",
      "epoch 58, train_loss 1278.0201416015625\n",
      "epoch 58, val_loss 455.51116943359375\n",
      "epoch 59, train_loss 1278.019775390625\n",
      "epoch 59, val_loss 455.51104736328125\n",
      "epoch 60, train_loss 1278.0196533203125\n",
      "epoch 60, val_loss 455.51092529296875\n",
      "epoch 61, train_loss 1278.0194091796875\n",
      "epoch 61, val_loss 455.51080322265625\n",
      "epoch 62, train_loss 1278.01904296875\n",
      "epoch 62, val_loss 455.5107116699219\n",
      "epoch 63, train_loss 1278.01904296875\n",
      "epoch 63, val_loss 455.510498046875\n",
      "epoch 64, train_loss 1278.0189208984375\n",
      "epoch 64, val_loss 455.5104064941406\n",
      "epoch 65, train_loss 1278.0186767578125\n",
      "epoch 65, val_loss 455.5103454589844\n",
      "epoch 66, train_loss 1278.0185546875\n",
      "epoch 66, val_loss 455.5102233886719\n",
      "epoch 67, train_loss 1278.0184326171875\n",
      "epoch 67, val_loss 455.5100402832031\n",
      "epoch 68, train_loss 1278.01806640625\n",
      "epoch 68, val_loss 455.5099182128906\n",
      "epoch 69, train_loss 1278.0181884765625\n",
      "epoch 69, val_loss 455.5097961425781\n",
      "epoch 70, train_loss 1278.0179443359375\n",
      "epoch 70, val_loss 455.5096130371094\n",
      "epoch 71, train_loss 1278.017578125\n",
      "epoch 71, val_loss 455.5095520019531\n",
      "epoch 72, train_loss 1278.0174560546875\n",
      "epoch 72, val_loss 455.5093688964844\n",
      "epoch 73, train_loss 1278.0172119140625\n",
      "epoch 73, val_loss 455.50927734375\n",
      "epoch 74, train_loss 1278.01708984375\n",
      "epoch 74, val_loss 455.5091552734375\n",
      "epoch 75, train_loss 1278.016845703125\n",
      "epoch 75, val_loss 455.509033203125\n",
      "epoch 76, train_loss 1278.0167236328125\n",
      "epoch 76, val_loss 455.5089111328125\n",
      "epoch 77, train_loss 1278.0164794921875\n",
      "epoch 77, val_loss 455.5088195800781\n",
      "epoch 78, train_loss 1278.0162353515625\n",
      "epoch 78, val_loss 455.5086364746094\n",
      "epoch 79, train_loss 1278.01611328125\n",
      "epoch 79, val_loss 455.508544921875\n",
      "epoch 80, train_loss 1278.0159912109375\n",
      "epoch 80, val_loss 455.50836181640625\n",
      "epoch 81, train_loss 1278.0157470703125\n",
      "epoch 81, val_loss 455.5083312988281\n",
      "epoch 82, train_loss 1278.0155029296875\n",
      "epoch 82, val_loss 455.5082092285156\n",
      "epoch 83, train_loss 1278.0155029296875\n",
      "epoch 83, val_loss 455.50799560546875\n",
      "epoch 84, train_loss 1278.01513671875\n",
      "epoch 84, val_loss 455.5079040527344\n",
      "epoch 85, train_loss 1278.0150146484375\n",
      "epoch 85, val_loss 455.5077819824219\n",
      "epoch 86, train_loss 1278.014892578125\n",
      "epoch 86, val_loss 455.5076599121094\n",
      "epoch 87, train_loss 1278.0147705078125\n",
      "epoch 87, val_loss 455.5075378417969\n",
      "epoch 88, train_loss 1278.0145263671875\n",
      "epoch 88, val_loss 455.5074157714844\n",
      "epoch 89, train_loss 1278.0142822265625\n",
      "epoch 89, val_loss 455.5072937011719\n",
      "epoch 90, train_loss 1278.0142822265625\n",
      "epoch 90, val_loss 455.5071716308594\n",
      "epoch 91, train_loss 1278.013916015625\n",
      "epoch 91, val_loss 455.5069885253906\n",
      "epoch 92, train_loss 1278.0137939453125\n",
      "epoch 92, val_loss 455.5069274902344\n",
      "epoch 93, train_loss 1278.0135498046875\n",
      "epoch 93, val_loss 455.5067443847656\n",
      "epoch 94, train_loss 1278.0133056640625\n",
      "epoch 94, val_loss 455.50665283203125\n",
      "epoch 95, train_loss 1278.01318359375\n",
      "epoch 95, val_loss 455.50653076171875\n",
      "epoch 96, train_loss 1278.012939453125\n",
      "epoch 96, val_loss 455.5064697265625\n",
      "epoch 97, train_loss 1278.0126953125\n",
      "epoch 97, val_loss 455.50628662109375\n",
      "epoch 98, train_loss 1278.0126953125\n",
      "epoch 98, val_loss 455.50616455078125\n",
      "epoch 99, train_loss 1278.0125732421875\n",
      "epoch 99, val_loss 455.50604248046875\n",
      "Parameter containing:\n",
      "tensor([1.0665e-09], requires_grad=True)\n",
      "iter 43, train_loss_regularization 0.942855954170227\n",
      "iter 43, val_loss_regularization 0.942855954170227\n",
      "epoch 0, train_loss 1278.01220703125\n",
      "epoch 0, val_loss 455.50592041015625\n",
      "epoch 1, train_loss 1278.0120849609375\n",
      "epoch 1, val_loss 455.50579833984375\n",
      "epoch 2, train_loss 1278.011962890625\n",
      "epoch 2, val_loss 455.5057067871094\n",
      "epoch 3, train_loss 1278.0118408203125\n",
      "epoch 3, val_loss 455.50555419921875\n",
      "epoch 4, train_loss 1278.011474609375\n",
      "epoch 4, val_loss 455.5054016113281\n",
      "epoch 5, train_loss 1278.0113525390625\n",
      "epoch 5, val_loss 455.5052795410156\n",
      "epoch 6, train_loss 1278.0111083984375\n",
      "epoch 6, val_loss 455.5052185058594\n",
      "epoch 7, train_loss 1278.010986328125\n",
      "epoch 7, val_loss 455.5050048828125\n",
      "epoch 8, train_loss 1278.0107421875\n",
      "epoch 8, val_loss 455.5049133300781\n",
      "epoch 9, train_loss 1278.0106201171875\n",
      "epoch 9, val_loss 455.50482177734375\n",
      "epoch 10, train_loss 1278.0106201171875\n",
      "epoch 10, val_loss 455.504638671875\n",
      "epoch 11, train_loss 1278.01025390625\n",
      "epoch 11, val_loss 455.5045471191406\n",
      "epoch 12, train_loss 1278.0101318359375\n",
      "epoch 12, val_loss 455.50445556640625\n",
      "epoch 13, train_loss 1278.0098876953125\n",
      "epoch 13, val_loss 455.5042419433594\n",
      "epoch 14, train_loss 1278.0096435546875\n",
      "epoch 14, val_loss 455.50421142578125\n",
      "epoch 15, train_loss 1278.009521484375\n",
      "epoch 15, val_loss 455.50408935546875\n",
      "epoch 16, train_loss 1278.00927734375\n",
      "epoch 16, val_loss 455.50396728515625\n",
      "epoch 17, train_loss 1278.00927734375\n",
      "epoch 17, val_loss 455.50384521484375\n",
      "epoch 18, train_loss 1278.0089111328125\n",
      "epoch 18, val_loss 455.503662109375\n",
      "epoch 19, train_loss 1278.0087890625\n",
      "epoch 19, val_loss 455.5035400390625\n",
      "epoch 20, train_loss 1278.008544921875\n",
      "epoch 20, val_loss 455.5035095214844\n",
      "epoch 21, train_loss 1278.008544921875\n",
      "epoch 21, val_loss 455.5033264160156\n",
      "epoch 22, train_loss 1278.00830078125\n",
      "epoch 22, val_loss 455.5032043457031\n",
      "epoch 23, train_loss 1278.0081787109375\n",
      "epoch 23, val_loss 455.5030822753906\n",
      "epoch 24, train_loss 1278.0079345703125\n",
      "epoch 24, val_loss 455.5029296875\n",
      "epoch 25, train_loss 1278.0079345703125\n",
      "epoch 25, val_loss 455.5028381347656\n",
      "epoch 26, train_loss 1278.0074462890625\n",
      "epoch 26, val_loss 455.5027160644531\n",
      "epoch 27, train_loss 1278.0072021484375\n",
      "epoch 27, val_loss 455.5025329589844\n",
      "epoch 28, train_loss 1278.0072021484375\n",
      "epoch 28, val_loss 455.5024108886719\n",
      "epoch 29, train_loss 1278.0068359375\n",
      "epoch 29, val_loss 455.50238037109375\n",
      "epoch 30, train_loss 1278.0067138671875\n",
      "epoch 30, val_loss 455.50225830078125\n",
      "epoch 31, train_loss 1278.0064697265625\n",
      "epoch 31, val_loss 455.5020446777344\n",
      "epoch 32, train_loss 1278.0064697265625\n",
      "epoch 32, val_loss 455.5019226074219\n",
      "epoch 33, train_loss 1278.0062255859375\n",
      "epoch 33, val_loss 455.5018310546875\n",
      "epoch 34, train_loss 1278.0062255859375\n",
      "epoch 34, val_loss 455.501708984375\n",
      "epoch 35, train_loss 1278.0059814453125\n",
      "epoch 35, val_loss 455.5015869140625\n",
      "epoch 36, train_loss 1278.0054931640625\n",
      "epoch 36, val_loss 455.5014953613281\n",
      "epoch 37, train_loss 1278.0054931640625\n",
      "epoch 37, val_loss 455.5013427734375\n",
      "epoch 38, train_loss 1278.0052490234375\n",
      "epoch 38, val_loss 455.501220703125\n",
      "epoch 39, train_loss 1278.0052490234375\n",
      "epoch 39, val_loss 455.5010681152344\n",
      "epoch 40, train_loss 1278.0048828125\n",
      "epoch 40, val_loss 455.50103759765625\n",
      "epoch 41, train_loss 1278.0048828125\n",
      "epoch 41, val_loss 455.5008239746094\n",
      "epoch 42, train_loss 1278.0045166015625\n",
      "epoch 42, val_loss 455.5007629394531\n",
      "epoch 43, train_loss 1278.00439453125\n",
      "epoch 43, val_loss 455.5006103515625\n",
      "epoch 44, train_loss 1278.0042724609375\n",
      "epoch 44, val_loss 455.50042724609375\n",
      "epoch 45, train_loss 1278.00390625\n",
      "epoch 45, val_loss 455.5003356933594\n",
      "epoch 46, train_loss 1278.0037841796875\n",
      "epoch 46, val_loss 455.5002136230469\n",
      "epoch 47, train_loss 1278.003662109375\n",
      "epoch 47, val_loss 455.5000915527344\n",
      "epoch 48, train_loss 1278.0035400390625\n",
      "epoch 48, val_loss 455.4999694824219\n",
      "epoch 49, train_loss 1278.0032958984375\n",
      "epoch 49, val_loss 455.4998779296875\n",
      "epoch 50, train_loss 1278.0030517578125\n",
      "epoch 50, val_loss 455.499755859375\n",
      "epoch 51, train_loss 1278.0029296875\n",
      "epoch 51, val_loss 455.49957275390625\n",
      "epoch 52, train_loss 1278.0028076171875\n",
      "epoch 52, val_loss 455.49951171875\n",
      "epoch 53, train_loss 1278.002685546875\n",
      "epoch 53, val_loss 455.49932861328125\n",
      "epoch 54, train_loss 1278.00244140625\n",
      "epoch 54, val_loss 455.4992370605469\n",
      "epoch 55, train_loss 1278.002197265625\n",
      "epoch 55, val_loss 455.4991149902344\n",
      "epoch 56, train_loss 1278.0020751953125\n",
      "epoch 56, val_loss 455.49896240234375\n",
      "epoch 57, train_loss 1278.001708984375\n",
      "epoch 57, val_loss 455.4988708496094\n",
      "epoch 58, train_loss 1278.001708984375\n",
      "epoch 58, val_loss 455.4986572265625\n",
      "epoch 59, train_loss 1278.0015869140625\n",
      "epoch 59, val_loss 455.49859619140625\n",
      "epoch 60, train_loss 1278.001220703125\n",
      "epoch 60, val_loss 455.49853515625\n",
      "epoch 61, train_loss 1278.0009765625\n",
      "epoch 61, val_loss 455.4983825683594\n",
      "epoch 62, train_loss 1278.0009765625\n",
      "epoch 62, val_loss 455.4982604980469\n",
      "epoch 63, train_loss 1278.0008544921875\n",
      "epoch 63, val_loss 455.4981384277344\n",
      "epoch 64, train_loss 1278.0006103515625\n",
      "epoch 64, val_loss 455.4979553222656\n",
      "epoch 65, train_loss 1278.00048828125\n",
      "epoch 65, val_loss 455.4978332519531\n",
      "epoch 66, train_loss 1278.000244140625\n",
      "epoch 66, val_loss 455.49774169921875\n",
      "epoch 67, train_loss 1278.0\n",
      "epoch 67, val_loss 455.49761962890625\n",
      "epoch 68, train_loss 1277.9998779296875\n",
      "epoch 68, val_loss 455.4974670410156\n",
      "epoch 69, train_loss 1277.9996337890625\n",
      "epoch 69, val_loss 455.49737548828125\n",
      "epoch 70, train_loss 1277.9996337890625\n",
      "epoch 70, val_loss 455.49725341796875\n",
      "epoch 71, train_loss 1277.9993896484375\n",
      "epoch 71, val_loss 455.4971618652344\n",
      "epoch 72, train_loss 1277.9991455078125\n",
      "epoch 72, val_loss 455.49700927734375\n",
      "epoch 73, train_loss 1277.998779296875\n",
      "epoch 73, val_loss 455.496826171875\n",
      "epoch 74, train_loss 1277.998779296875\n",
      "epoch 74, val_loss 455.4967956542969\n",
      "epoch 75, train_loss 1277.99853515625\n",
      "epoch 75, val_loss 455.4966125488281\n",
      "epoch 76, train_loss 1277.9984130859375\n",
      "epoch 76, val_loss 455.4965515136719\n",
      "epoch 77, train_loss 1277.9981689453125\n",
      "epoch 77, val_loss 455.4964294433594\n",
      "epoch 78, train_loss 1277.9979248046875\n",
      "epoch 78, val_loss 455.4962463378906\n",
      "epoch 79, train_loss 1277.997802734375\n",
      "epoch 79, val_loss 455.4961242675781\n",
      "epoch 80, train_loss 1277.9976806640625\n",
      "epoch 80, val_loss 455.49603271484375\n",
      "epoch 81, train_loss 1277.99755859375\n",
      "epoch 81, val_loss 455.4958801269531\n",
      "epoch 82, train_loss 1277.99755859375\n",
      "epoch 82, val_loss 455.49578857421875\n",
      "epoch 83, train_loss 1277.9970703125\n",
      "epoch 83, val_loss 455.4956359863281\n",
      "epoch 84, train_loss 1277.9970703125\n",
      "epoch 84, val_loss 455.49554443359375\n",
      "epoch 85, train_loss 1277.996826171875\n",
      "epoch 85, val_loss 455.49542236328125\n",
      "epoch 86, train_loss 1277.9964599609375\n",
      "epoch 86, val_loss 455.49530029296875\n",
      "epoch 87, train_loss 1277.9964599609375\n",
      "epoch 87, val_loss 455.49517822265625\n",
      "epoch 88, train_loss 1277.9962158203125\n",
      "epoch 88, val_loss 455.49505615234375\n",
      "epoch 89, train_loss 1277.9959716796875\n",
      "epoch 89, val_loss 455.4949035644531\n",
      "epoch 90, train_loss 1277.995849609375\n",
      "epoch 90, val_loss 455.4947814941406\n",
      "epoch 91, train_loss 1277.99560546875\n",
      "epoch 91, val_loss 455.4946594238281\n",
      "epoch 92, train_loss 1277.9954833984375\n",
      "epoch 92, val_loss 455.4945068359375\n",
      "epoch 93, train_loss 1277.995361328125\n",
      "epoch 93, val_loss 455.4944152832031\n",
      "epoch 94, train_loss 1277.9952392578125\n",
      "epoch 94, val_loss 455.4942932128906\n",
      "epoch 95, train_loss 1277.994873046875\n",
      "epoch 95, val_loss 455.49420166015625\n",
      "epoch 96, train_loss 1277.994873046875\n",
      "epoch 96, val_loss 455.4940490722656\n",
      "epoch 97, train_loss 1277.9945068359375\n",
      "epoch 97, val_loss 455.4939270019531\n",
      "epoch 98, train_loss 1277.994384765625\n",
      "epoch 98, val_loss 455.49383544921875\n",
      "epoch 99, train_loss 1277.994384765625\n",
      "epoch 99, val_loss 455.49365234375\n",
      "Parameter containing:\n",
      "tensor([6.8886e-10], requires_grad=True)\n",
      "iter 44, train_loss_regularization 0.9377044439315796\n",
      "iter 44, val_loss_regularization 0.9377044439315796\n",
      "epoch 0, train_loss 1277.9940185546875\n",
      "epoch 0, val_loss 455.4935302734375\n",
      "epoch 1, train_loss 1277.993896484375\n",
      "epoch 1, val_loss 455.493408203125\n",
      "epoch 2, train_loss 1277.9935302734375\n",
      "epoch 2, val_loss 455.49334716796875\n",
      "epoch 3, train_loss 1277.9935302734375\n",
      "epoch 3, val_loss 455.4931640625\n",
      "epoch 4, train_loss 1277.9932861328125\n",
      "epoch 4, val_loss 455.4930725097656\n",
      "epoch 5, train_loss 1277.9932861328125\n",
      "epoch 5, val_loss 455.492919921875\n",
      "epoch 6, train_loss 1277.9930419921875\n",
      "epoch 6, val_loss 455.4928894042969\n",
      "epoch 7, train_loss 1277.9927978515625\n",
      "epoch 7, val_loss 455.49267578125\n",
      "epoch 8, train_loss 1277.9925537109375\n",
      "epoch 8, val_loss 455.4925537109375\n",
      "epoch 9, train_loss 1277.9923095703125\n",
      "epoch 9, val_loss 455.4924621582031\n",
      "epoch 10, train_loss 1277.9923095703125\n",
      "epoch 10, val_loss 455.4923400878906\n",
      "epoch 11, train_loss 1277.9921875\n",
      "epoch 11, val_loss 455.49224853515625\n",
      "epoch 12, train_loss 1277.991943359375\n",
      "epoch 12, val_loss 455.49212646484375\n",
      "epoch 13, train_loss 1277.9918212890625\n",
      "epoch 13, val_loss 455.491943359375\n",
      "epoch 14, train_loss 1277.991455078125\n",
      "epoch 14, val_loss 455.4918212890625\n",
      "epoch 15, train_loss 1277.9913330078125\n",
      "epoch 15, val_loss 455.49169921875\n",
      "epoch 16, train_loss 1277.9912109375\n",
      "epoch 16, val_loss 455.4916687011719\n",
      "epoch 17, train_loss 1277.990966796875\n",
      "epoch 17, val_loss 455.4914855957031\n",
      "epoch 18, train_loss 1277.9908447265625\n",
      "epoch 18, val_loss 455.4913330078125\n",
      "epoch 19, train_loss 1277.99072265625\n",
      "epoch 19, val_loss 455.4912109375\n",
      "epoch 20, train_loss 1277.9903564453125\n",
      "epoch 20, val_loss 455.4910888671875\n",
      "epoch 21, train_loss 1277.990234375\n",
      "epoch 21, val_loss 455.490966796875\n",
      "epoch 22, train_loss 1277.9901123046875\n",
      "epoch 22, val_loss 455.4908447265625\n",
      "epoch 23, train_loss 1277.9898681640625\n",
      "epoch 23, val_loss 455.4907531738281\n",
      "epoch 24, train_loss 1277.98974609375\n",
      "epoch 24, val_loss 455.49066162109375\n",
      "epoch 25, train_loss 1277.9896240234375\n",
      "epoch 25, val_loss 455.4905090332031\n",
      "epoch 26, train_loss 1277.9893798828125\n",
      "epoch 26, val_loss 455.49041748046875\n",
      "epoch 27, train_loss 1277.9892578125\n",
      "epoch 27, val_loss 455.4902648925781\n",
      "epoch 28, train_loss 1277.9891357421875\n",
      "epoch 28, val_loss 455.4900817871094\n",
      "epoch 29, train_loss 1277.9888916015625\n",
      "epoch 29, val_loss 455.49005126953125\n",
      "epoch 30, train_loss 1277.98876953125\n",
      "epoch 30, val_loss 455.48992919921875\n",
      "epoch 31, train_loss 1277.988525390625\n",
      "epoch 31, val_loss 455.4897155761719\n",
      "epoch 32, train_loss 1277.98828125\n",
      "epoch 32, val_loss 455.4896545410156\n",
      "epoch 33, train_loss 1277.988037109375\n",
      "epoch 33, val_loss 455.4895324707031\n",
      "epoch 34, train_loss 1277.988037109375\n",
      "epoch 34, val_loss 455.4894104003906\n",
      "epoch 35, train_loss 1277.98779296875\n",
      "epoch 35, val_loss 455.4892578125\n",
      "epoch 36, train_loss 1277.987548828125\n",
      "epoch 36, val_loss 455.4891662597656\n",
      "epoch 37, train_loss 1277.9874267578125\n",
      "epoch 37, val_loss 455.4890441894531\n",
      "epoch 38, train_loss 1277.9871826171875\n",
      "epoch 38, val_loss 455.4889221191406\n",
      "epoch 39, train_loss 1277.987060546875\n",
      "epoch 39, val_loss 455.4887390136719\n",
      "epoch 40, train_loss 1277.98681640625\n",
      "epoch 40, val_loss 455.48870849609375\n",
      "epoch 41, train_loss 1277.9866943359375\n",
      "epoch 41, val_loss 455.4885559082031\n",
      "epoch 42, train_loss 1277.986572265625\n",
      "epoch 42, val_loss 455.4883728027344\n",
      "epoch 43, train_loss 1277.986328125\n",
      "epoch 43, val_loss 455.4882507324219\n",
      "epoch 44, train_loss 1277.9862060546875\n",
      "epoch 44, val_loss 455.48822021484375\n",
      "epoch 45, train_loss 1277.9859619140625\n",
      "epoch 45, val_loss 455.48809814453125\n",
      "epoch 46, train_loss 1277.98583984375\n",
      "epoch 46, val_loss 455.4879455566406\n",
      "epoch 47, train_loss 1277.9857177734375\n",
      "epoch 47, val_loss 455.48779296875\n",
      "epoch 48, train_loss 1277.9854736328125\n",
      "epoch 48, val_loss 455.4877014160156\n",
      "epoch 49, train_loss 1277.9852294921875\n",
      "epoch 49, val_loss 455.487548828125\n",
      "epoch 50, train_loss 1277.9849853515625\n",
      "epoch 50, val_loss 455.4874267578125\n",
      "epoch 51, train_loss 1277.9849853515625\n",
      "epoch 51, val_loss 455.4873046875\n",
      "epoch 52, train_loss 1277.98486328125\n",
      "epoch 52, val_loss 455.4871520996094\n",
      "epoch 53, train_loss 1277.9844970703125\n",
      "epoch 53, val_loss 455.4870910644531\n",
      "epoch 54, train_loss 1277.9842529296875\n",
      "epoch 54, val_loss 455.4869689941406\n",
      "epoch 55, train_loss 1277.984130859375\n",
      "epoch 55, val_loss 455.4868469238281\n",
      "epoch 56, train_loss 1277.984130859375\n",
      "epoch 56, val_loss 455.4866638183594\n",
      "epoch 57, train_loss 1277.98388671875\n",
      "epoch 57, val_loss 455.486572265625\n",
      "epoch 58, train_loss 1277.983642578125\n",
      "epoch 58, val_loss 455.4864501953125\n",
      "epoch 59, train_loss 1277.983642578125\n",
      "epoch 59, val_loss 455.48638916015625\n",
      "epoch 60, train_loss 1277.9832763671875\n",
      "epoch 60, val_loss 455.4862060546875\n",
      "epoch 61, train_loss 1277.9830322265625\n",
      "epoch 61, val_loss 455.486083984375\n",
      "epoch 62, train_loss 1277.98291015625\n",
      "epoch 62, val_loss 455.4859924316406\n",
      "epoch 63, train_loss 1277.9827880859375\n",
      "epoch 63, val_loss 455.4858703613281\n",
      "epoch 64, train_loss 1277.9825439453125\n",
      "epoch 64, val_loss 455.4857482910156\n",
      "epoch 65, train_loss 1277.982421875\n",
      "epoch 65, val_loss 455.4856262207031\n",
      "epoch 66, train_loss 1277.9822998046875\n",
      "epoch 66, val_loss 455.4854431152344\n",
      "epoch 67, train_loss 1277.98193359375\n",
      "epoch 67, val_loss 455.4853820800781\n",
      "epoch 68, train_loss 1277.98193359375\n",
      "epoch 68, val_loss 455.4851989746094\n",
      "epoch 69, train_loss 1277.981689453125\n",
      "epoch 69, val_loss 455.48516845703125\n",
      "epoch 70, train_loss 1277.981689453125\n",
      "epoch 70, val_loss 455.4850769042969\n",
      "epoch 71, train_loss 1277.9814453125\n",
      "epoch 71, val_loss 455.4848327636719\n",
      "epoch 72, train_loss 1277.981201171875\n",
      "epoch 72, val_loss 455.48480224609375\n",
      "epoch 73, train_loss 1277.98095703125\n",
      "epoch 73, val_loss 455.484619140625\n",
      "epoch 74, train_loss 1277.980712890625\n",
      "epoch 74, val_loss 455.4844970703125\n",
      "epoch 75, train_loss 1277.9805908203125\n",
      "epoch 75, val_loss 455.484375\n",
      "epoch 76, train_loss 1277.98046875\n",
      "epoch 76, val_loss 455.4842224121094\n",
      "epoch 77, train_loss 1277.980224609375\n",
      "epoch 77, val_loss 455.4841613769531\n",
      "epoch 78, train_loss 1277.97998046875\n",
      "epoch 78, val_loss 455.4840393066406\n",
      "epoch 79, train_loss 1277.9798583984375\n",
      "epoch 79, val_loss 455.48394775390625\n",
      "epoch 80, train_loss 1277.979736328125\n",
      "epoch 80, val_loss 455.48382568359375\n",
      "epoch 81, train_loss 1277.9796142578125\n",
      "epoch 81, val_loss 455.4836730957031\n",
      "epoch 82, train_loss 1277.9793701171875\n",
      "epoch 82, val_loss 455.4835510253906\n",
      "epoch 83, train_loss 1277.9791259765625\n",
      "epoch 83, val_loss 455.48345947265625\n",
      "epoch 84, train_loss 1277.9791259765625\n",
      "epoch 84, val_loss 455.4833068847656\n",
      "epoch 85, train_loss 1277.978759765625\n",
      "epoch 85, val_loss 455.483154296875\n",
      "epoch 86, train_loss 1277.978515625\n",
      "epoch 86, val_loss 455.4830322265625\n",
      "epoch 87, train_loss 1277.9786376953125\n",
      "epoch 87, val_loss 455.48297119140625\n",
      "epoch 88, train_loss 1277.9783935546875\n",
      "epoch 88, val_loss 455.4828186035156\n",
      "epoch 89, train_loss 1277.97802734375\n",
      "epoch 89, val_loss 455.482666015625\n",
      "epoch 90, train_loss 1277.97802734375\n",
      "epoch 90, val_loss 455.4825744628906\n",
      "epoch 91, train_loss 1277.977783203125\n",
      "epoch 91, val_loss 455.4824523925781\n",
      "epoch 92, train_loss 1277.9775390625\n",
      "epoch 92, val_loss 455.4822998046875\n",
      "epoch 93, train_loss 1277.9774169921875\n",
      "epoch 93, val_loss 455.4822082519531\n",
      "epoch 94, train_loss 1277.977294921875\n",
      "epoch 94, val_loss 455.48211669921875\n",
      "epoch 95, train_loss 1277.97705078125\n",
      "epoch 95, val_loss 455.4819641113281\n",
      "epoch 96, train_loss 1277.976806640625\n",
      "epoch 96, val_loss 455.4818420410156\n",
      "epoch 97, train_loss 1277.976806640625\n",
      "epoch 97, val_loss 455.4817199707031\n",
      "epoch 98, train_loss 1277.9766845703125\n",
      "epoch 98, val_loss 455.48162841796875\n",
      "epoch 99, train_loss 1277.9761962890625\n",
      "epoch 99, val_loss 455.4814453125\n",
      "Parameter containing:\n",
      "tensor([4.4563e-10], requires_grad=True)\n",
      "iter 45, train_loss_regularization 0.9326423406600952\n",
      "iter 45, val_loss_regularization 0.9326423406600952\n",
      "epoch 0, train_loss 1277.97607421875\n",
      "epoch 0, val_loss 455.4812927246094\n",
      "epoch 1, train_loss 1277.9759521484375\n",
      "epoch 1, val_loss 455.481201171875\n",
      "epoch 2, train_loss 1277.9759521484375\n",
      "epoch 2, val_loss 455.4810791015625\n",
      "epoch 3, train_loss 1277.9754638671875\n",
      "epoch 3, val_loss 455.4809875488281\n",
      "epoch 4, train_loss 1277.9754638671875\n",
      "epoch 4, val_loss 455.4808654785156\n",
      "epoch 5, train_loss 1277.9754638671875\n",
      "epoch 5, val_loss 455.48065185546875\n",
      "epoch 6, train_loss 1277.9749755859375\n",
      "epoch 6, val_loss 455.4806213378906\n",
      "epoch 7, train_loss 1277.9749755859375\n",
      "epoch 7, val_loss 455.48046875\n",
      "epoch 8, train_loss 1277.9747314453125\n",
      "epoch 8, val_loss 455.4803771972656\n",
      "epoch 9, train_loss 1277.974609375\n",
      "epoch 9, val_loss 455.4802551269531\n",
      "epoch 10, train_loss 1277.9744873046875\n",
      "epoch 10, val_loss 455.4801330566406\n",
      "epoch 11, train_loss 1277.974365234375\n",
      "epoch 11, val_loss 455.48004150390625\n",
      "epoch 12, train_loss 1277.973876953125\n",
      "epoch 12, val_loss 455.4798889160156\n",
      "epoch 13, train_loss 1277.9737548828125\n",
      "epoch 13, val_loss 455.47979736328125\n",
      "epoch 14, train_loss 1277.9736328125\n",
      "epoch 14, val_loss 455.47967529296875\n",
      "epoch 15, train_loss 1277.9736328125\n",
      "epoch 15, val_loss 455.4794921875\n",
      "epoch 16, train_loss 1277.973388671875\n",
      "epoch 16, val_loss 455.47943115234375\n",
      "epoch 17, train_loss 1277.9730224609375\n",
      "epoch 17, val_loss 455.4792785644531\n",
      "epoch 18, train_loss 1277.972900390625\n",
      "epoch 18, val_loss 455.4791564941406\n",
      "epoch 19, train_loss 1277.9727783203125\n",
      "epoch 19, val_loss 455.4790344238281\n",
      "epoch 20, train_loss 1277.9725341796875\n",
      "epoch 20, val_loss 455.4789123535156\n",
      "epoch 21, train_loss 1277.972412109375\n",
      "epoch 21, val_loss 455.4787902832031\n",
      "epoch 22, train_loss 1277.9722900390625\n",
      "epoch 22, val_loss 455.4786682128906\n",
      "epoch 23, train_loss 1277.9720458984375\n",
      "epoch 23, val_loss 455.47857666015625\n",
      "epoch 24, train_loss 1277.971923828125\n",
      "epoch 24, val_loss 455.4784851074219\n",
      "epoch 25, train_loss 1277.9718017578125\n",
      "epoch 25, val_loss 455.47833251953125\n",
      "epoch 26, train_loss 1277.9715576171875\n",
      "epoch 26, val_loss 455.4781799316406\n",
      "epoch 27, train_loss 1277.971435546875\n",
      "epoch 27, val_loss 455.47808837890625\n",
      "epoch 28, train_loss 1277.97119140625\n",
      "epoch 28, val_loss 455.4779968261719\n",
      "epoch 29, train_loss 1277.970947265625\n",
      "epoch 29, val_loss 455.47784423828125\n",
      "epoch 30, train_loss 1277.9708251953125\n",
      "epoch 30, val_loss 455.47772216796875\n",
      "epoch 31, train_loss 1277.970703125\n",
      "epoch 31, val_loss 455.4775695800781\n",
      "epoch 32, train_loss 1277.9705810546875\n",
      "epoch 32, val_loss 455.4775390625\n",
      "epoch 33, train_loss 1277.97021484375\n",
      "epoch 33, val_loss 455.4773254394531\n",
      "epoch 34, train_loss 1277.97021484375\n",
      "epoch 34, val_loss 455.4772033691406\n",
      "epoch 35, train_loss 1277.969970703125\n",
      "epoch 35, val_loss 455.47711181640625\n",
      "epoch 36, train_loss 1277.9698486328125\n",
      "epoch 36, val_loss 455.4769592285156\n",
      "epoch 37, train_loss 1277.9696044921875\n",
      "epoch 37, val_loss 455.4769287109375\n",
      "epoch 38, train_loss 1277.9693603515625\n",
      "epoch 38, val_loss 455.47674560546875\n",
      "epoch 39, train_loss 1277.9693603515625\n",
      "epoch 39, val_loss 455.4765930175781\n",
      "epoch 40, train_loss 1277.968994140625\n",
      "epoch 40, val_loss 455.4765319824219\n",
      "epoch 41, train_loss 1277.9688720703125\n",
      "epoch 41, val_loss 455.47637939453125\n",
      "epoch 42, train_loss 1277.96875\n",
      "epoch 42, val_loss 455.476318359375\n",
      "epoch 43, train_loss 1277.9686279296875\n",
      "epoch 43, val_loss 455.47613525390625\n",
      "epoch 44, train_loss 1277.9683837890625\n",
      "epoch 44, val_loss 455.47601318359375\n",
      "epoch 45, train_loss 1277.9683837890625\n",
      "epoch 45, val_loss 455.4759216308594\n",
      "epoch 46, train_loss 1277.968017578125\n",
      "epoch 46, val_loss 455.4757995605469\n",
      "epoch 47, train_loss 1277.9678955078125\n",
      "epoch 47, val_loss 455.4756774902344\n",
      "epoch 48, train_loss 1277.9676513671875\n",
      "epoch 48, val_loss 455.4754638671875\n",
      "epoch 49, train_loss 1277.967529296875\n",
      "epoch 49, val_loss 455.4753723144531\n",
      "epoch 50, train_loss 1277.9674072265625\n",
      "epoch 50, val_loss 455.475341796875\n",
      "epoch 51, train_loss 1277.9671630859375\n",
      "epoch 51, val_loss 455.47515869140625\n",
      "epoch 52, train_loss 1277.966796875\n",
      "epoch 52, val_loss 455.4750061035156\n",
      "epoch 53, train_loss 1277.966796875\n",
      "epoch 53, val_loss 455.47491455078125\n",
      "epoch 54, train_loss 1277.966552734375\n",
      "epoch 54, val_loss 455.47479248046875\n",
      "epoch 55, train_loss 1277.96630859375\n",
      "epoch 55, val_loss 455.4747619628906\n",
      "epoch 56, train_loss 1277.9661865234375\n",
      "epoch 56, val_loss 455.47454833984375\n",
      "epoch 57, train_loss 1277.9659423828125\n",
      "epoch 57, val_loss 455.474365234375\n",
      "epoch 58, train_loss 1277.9659423828125\n",
      "epoch 58, val_loss 455.4743347167969\n",
      "epoch 59, train_loss 1277.9658203125\n",
      "epoch 59, val_loss 455.4742126464844\n",
      "epoch 60, train_loss 1277.965576171875\n",
      "epoch 60, val_loss 455.4740905761719\n",
      "epoch 61, train_loss 1277.96533203125\n",
      "epoch 61, val_loss 455.4739685058594\n",
      "epoch 62, train_loss 1277.9652099609375\n",
      "epoch 62, val_loss 455.4738464355469\n",
      "epoch 63, train_loss 1277.9649658203125\n",
      "epoch 63, val_loss 455.4737548828125\n",
      "epoch 64, train_loss 1277.9649658203125\n",
      "epoch 64, val_loss 455.4736328125\n",
      "epoch 65, train_loss 1277.9647216796875\n",
      "epoch 65, val_loss 455.47344970703125\n",
      "epoch 66, train_loss 1277.9644775390625\n",
      "epoch 66, val_loss 455.47332763671875\n",
      "epoch 67, train_loss 1277.96435546875\n",
      "epoch 67, val_loss 455.47320556640625\n",
      "epoch 68, train_loss 1277.964111328125\n",
      "epoch 68, val_loss 455.4731140136719\n",
      "epoch 69, train_loss 1277.9638671875\n",
      "epoch 69, val_loss 455.4729919433594\n",
      "epoch 70, train_loss 1277.9637451171875\n",
      "epoch 70, val_loss 455.4729309082031\n",
      "epoch 71, train_loss 1277.963623046875\n",
      "epoch 71, val_loss 455.4727783203125\n",
      "epoch 72, train_loss 1277.9632568359375\n",
      "epoch 72, val_loss 455.4726257324219\n",
      "epoch 73, train_loss 1277.963134765625\n",
      "epoch 73, val_loss 455.4725341796875\n",
      "epoch 74, train_loss 1277.963134765625\n",
      "epoch 74, val_loss 455.4723815917969\n",
      "epoch 75, train_loss 1277.962890625\n",
      "epoch 75, val_loss 455.4722595214844\n",
      "epoch 76, train_loss 1277.9627685546875\n",
      "epoch 76, val_loss 455.4721374511719\n",
      "epoch 77, train_loss 1277.9625244140625\n",
      "epoch 77, val_loss 455.4720458984375\n",
      "epoch 78, train_loss 1277.96240234375\n",
      "epoch 78, val_loss 455.47186279296875\n",
      "epoch 79, train_loss 1277.9622802734375\n",
      "epoch 79, val_loss 455.4718017578125\n",
      "epoch 80, train_loss 1277.9620361328125\n",
      "epoch 80, val_loss 455.4716796875\n",
      "epoch 81, train_loss 1277.9620361328125\n",
      "epoch 81, val_loss 455.4715270996094\n",
      "epoch 82, train_loss 1277.961669921875\n",
      "epoch 82, val_loss 455.47137451171875\n",
      "epoch 83, train_loss 1277.96142578125\n",
      "epoch 83, val_loss 455.4712829589844\n",
      "epoch 84, train_loss 1277.96142578125\n",
      "epoch 84, val_loss 455.4712219238281\n",
      "epoch 85, train_loss 1277.9613037109375\n",
      "epoch 85, val_loss 455.4710693359375\n",
      "epoch 86, train_loss 1277.9609375\n",
      "epoch 86, val_loss 455.470947265625\n",
      "epoch 87, train_loss 1277.9608154296875\n",
      "epoch 87, val_loss 455.4708251953125\n",
      "epoch 88, train_loss 1277.960693359375\n",
      "epoch 88, val_loss 455.470703125\n",
      "epoch 89, train_loss 1277.9603271484375\n",
      "epoch 89, val_loss 455.4706115722656\n",
      "epoch 90, train_loss 1277.9603271484375\n",
      "epoch 90, val_loss 455.4704895019531\n",
      "epoch 91, train_loss 1277.960205078125\n",
      "epoch 91, val_loss 455.4703674316406\n",
      "epoch 92, train_loss 1277.9599609375\n",
      "epoch 92, val_loss 455.47021484375\n",
      "epoch 93, train_loss 1277.959716796875\n",
      "epoch 93, val_loss 455.4700927734375\n",
      "epoch 94, train_loss 1277.959716796875\n",
      "epoch 94, val_loss 455.4700012207031\n",
      "epoch 95, train_loss 1277.95947265625\n",
      "epoch 95, val_loss 455.4698791503906\n",
      "epoch 96, train_loss 1277.959228515625\n",
      "epoch 96, val_loss 455.4697570800781\n",
      "epoch 97, train_loss 1277.9588623046875\n",
      "epoch 97, val_loss 455.4696350097656\n",
      "epoch 98, train_loss 1277.9588623046875\n",
      "epoch 98, val_loss 455.4694519042969\n",
      "epoch 99, train_loss 1277.9586181640625\n",
      "epoch 99, val_loss 455.46942138671875\n",
      "Parameter containing:\n",
      "tensor([2.8871e-10], requires_grad=True)\n",
      "iter 46, train_loss_regularization 0.9276678562164307\n",
      "iter 46, val_loss_regularization 0.9276678562164307\n",
      "epoch 0, train_loss 1277.9583740234375\n",
      "epoch 0, val_loss 455.4693298339844\n",
      "epoch 1, train_loss 1277.9583740234375\n",
      "epoch 1, val_loss 455.4690856933594\n",
      "epoch 2, train_loss 1277.9581298828125\n",
      "epoch 2, val_loss 455.468994140625\n",
      "epoch 3, train_loss 1277.9578857421875\n",
      "epoch 3, val_loss 455.4689025878906\n",
      "epoch 4, train_loss 1277.9578857421875\n",
      "epoch 4, val_loss 455.4688415527344\n",
      "epoch 5, train_loss 1277.9576416015625\n",
      "epoch 5, val_loss 455.4686279296875\n",
      "epoch 6, train_loss 1277.95751953125\n",
      "epoch 6, val_loss 455.4685363769531\n",
      "epoch 7, train_loss 1277.957275390625\n",
      "epoch 7, val_loss 455.46844482421875\n",
      "epoch 8, train_loss 1277.95703125\n",
      "epoch 8, val_loss 455.46832275390625\n",
      "epoch 9, train_loss 1277.95703125\n",
      "epoch 9, val_loss 455.46820068359375\n",
      "epoch 10, train_loss 1277.956787109375\n",
      "epoch 10, val_loss 455.4680480957031\n",
      "epoch 11, train_loss 1277.95654296875\n",
      "epoch 11, val_loss 455.4679870605469\n",
      "epoch 12, train_loss 1277.9564208984375\n",
      "epoch 12, val_loss 455.46783447265625\n",
      "epoch 13, train_loss 1277.956298828125\n",
      "epoch 13, val_loss 455.4677429199219\n",
      "epoch 14, train_loss 1277.9560546875\n",
      "epoch 14, val_loss 455.46759033203125\n",
      "epoch 15, train_loss 1277.955810546875\n",
      "epoch 15, val_loss 455.4674987792969\n",
      "epoch 16, train_loss 1277.9556884765625\n",
      "epoch 16, val_loss 455.46734619140625\n",
      "epoch 17, train_loss 1277.9554443359375\n",
      "epoch 17, val_loss 455.4672546386719\n",
      "epoch 18, train_loss 1277.9554443359375\n",
      "epoch 18, val_loss 455.4671325683594\n",
      "epoch 19, train_loss 1277.955322265625\n",
      "epoch 19, val_loss 455.4670104980469\n",
      "epoch 20, train_loss 1277.9549560546875\n",
      "epoch 20, val_loss 455.4668884277344\n",
      "epoch 21, train_loss 1277.9549560546875\n",
      "epoch 21, val_loss 455.46673583984375\n",
      "epoch 22, train_loss 1277.9547119140625\n",
      "epoch 22, val_loss 455.4666748046875\n",
      "epoch 23, train_loss 1277.95458984375\n",
      "epoch 23, val_loss 455.46649169921875\n",
      "epoch 24, train_loss 1277.954345703125\n",
      "epoch 24, val_loss 455.46636962890625\n",
      "epoch 25, train_loss 1277.954345703125\n",
      "epoch 25, val_loss 455.4662780761719\n",
      "epoch 26, train_loss 1277.9541015625\n",
      "epoch 26, val_loss 455.4662170410156\n",
      "epoch 27, train_loss 1277.9537353515625\n",
      "epoch 27, val_loss 455.46600341796875\n",
      "epoch 28, train_loss 1277.95361328125\n",
      "epoch 28, val_loss 455.4659729003906\n",
      "epoch 29, train_loss 1277.953369140625\n",
      "epoch 29, val_loss 455.46575927734375\n",
      "epoch 30, train_loss 1277.953125\n",
      "epoch 30, val_loss 455.4656982421875\n",
      "epoch 31, train_loss 1277.953125\n",
      "epoch 31, val_loss 455.4655456542969\n",
      "epoch 32, train_loss 1277.9530029296875\n",
      "epoch 32, val_loss 455.4654235839844\n",
      "epoch 33, train_loss 1277.952880859375\n",
      "epoch 33, val_loss 455.4653015136719\n",
      "epoch 34, train_loss 1277.95263671875\n",
      "epoch 34, val_loss 455.4652099609375\n",
      "epoch 35, train_loss 1277.952392578125\n",
      "epoch 35, val_loss 455.465087890625\n",
      "epoch 36, train_loss 1277.952392578125\n",
      "epoch 36, val_loss 455.4649658203125\n",
      "epoch 37, train_loss 1277.9521484375\n",
      "epoch 37, val_loss 455.46484375\n",
      "epoch 38, train_loss 1277.9517822265625\n",
      "epoch 38, val_loss 455.4647216796875\n",
      "epoch 39, train_loss 1277.9517822265625\n",
      "epoch 39, val_loss 455.46453857421875\n",
      "epoch 40, train_loss 1277.9515380859375\n",
      "epoch 40, val_loss 455.4645080566406\n",
      "epoch 41, train_loss 1277.951416015625\n",
      "epoch 41, val_loss 455.4643859863281\n",
      "epoch 42, train_loss 1277.9512939453125\n",
      "epoch 42, val_loss 455.4642639160156\n",
      "epoch 43, train_loss 1277.9510498046875\n",
      "epoch 43, val_loss 455.464111328125\n",
      "epoch 44, train_loss 1277.950927734375\n",
      "epoch 44, val_loss 455.4639892578125\n",
      "epoch 45, train_loss 1277.9508056640625\n",
      "epoch 45, val_loss 455.46392822265625\n",
      "epoch 46, train_loss 1277.950439453125\n",
      "epoch 46, val_loss 455.4637451171875\n",
      "epoch 47, train_loss 1277.950439453125\n",
      "epoch 47, val_loss 455.463623046875\n",
      "epoch 48, train_loss 1277.9501953125\n",
      "epoch 48, val_loss 455.4635009765625\n",
      "epoch 49, train_loss 1277.9500732421875\n",
      "epoch 49, val_loss 455.4634094238281\n",
      "epoch 50, train_loss 1277.9498291015625\n",
      "epoch 50, val_loss 455.4632873535156\n",
      "epoch 51, train_loss 1277.94970703125\n",
      "epoch 51, val_loss 455.4631652832031\n",
      "epoch 52, train_loss 1277.949462890625\n",
      "epoch 52, val_loss 455.4630432128906\n",
      "epoch 53, train_loss 1277.949462890625\n",
      "epoch 53, val_loss 455.46295166015625\n",
      "epoch 54, train_loss 1277.9490966796875\n",
      "epoch 54, val_loss 455.4627990722656\n",
      "epoch 55, train_loss 1277.948974609375\n",
      "epoch 55, val_loss 455.46270751953125\n",
      "epoch 56, train_loss 1277.9488525390625\n",
      "epoch 56, val_loss 455.46258544921875\n",
      "epoch 57, train_loss 1277.9486083984375\n",
      "epoch 57, val_loss 455.46240234375\n",
      "epoch 58, train_loss 1277.948486328125\n",
      "epoch 58, val_loss 455.46234130859375\n",
      "epoch 59, train_loss 1277.9483642578125\n",
      "epoch 59, val_loss 455.46221923828125\n",
      "epoch 60, train_loss 1277.9481201171875\n",
      "epoch 60, val_loss 455.4621276855469\n",
      "epoch 61, train_loss 1277.947998046875\n",
      "epoch 61, val_loss 455.4620056152344\n",
      "epoch 62, train_loss 1277.9476318359375\n",
      "epoch 62, val_loss 455.4618225097656\n",
      "epoch 63, train_loss 1277.9476318359375\n",
      "epoch 63, val_loss 455.4617614746094\n",
      "epoch 64, train_loss 1277.947509765625\n",
      "epoch 64, val_loss 455.4616394042969\n",
      "epoch 65, train_loss 1277.9473876953125\n",
      "epoch 65, val_loss 455.46148681640625\n",
      "epoch 66, train_loss 1277.947021484375\n",
      "epoch 66, val_loss 455.4613342285156\n",
      "epoch 67, train_loss 1277.9468994140625\n",
      "epoch 67, val_loss 455.46124267578125\n",
      "epoch 68, train_loss 1277.94677734375\n",
      "epoch 68, val_loss 455.461181640625\n",
      "epoch 69, train_loss 1277.946533203125\n",
      "epoch 69, val_loss 455.4610290527344\n",
      "epoch 70, train_loss 1277.9462890625\n",
      "epoch 70, val_loss 455.4609069824219\n",
      "epoch 71, train_loss 1277.9462890625\n",
      "epoch 71, val_loss 455.4607849121094\n",
      "epoch 72, train_loss 1277.946044921875\n",
      "epoch 72, val_loss 455.46063232421875\n",
      "epoch 73, train_loss 1277.946044921875\n",
      "epoch 73, val_loss 455.4605712890625\n",
      "epoch 74, train_loss 1277.94580078125\n",
      "epoch 74, val_loss 455.46044921875\n",
      "epoch 75, train_loss 1277.945556640625\n",
      "epoch 75, val_loss 455.4602966308594\n",
      "epoch 76, train_loss 1277.9454345703125\n",
      "epoch 76, val_loss 455.460205078125\n",
      "epoch 77, train_loss 1277.9451904296875\n",
      "epoch 77, val_loss 455.4599914550781\n",
      "epoch 78, train_loss 1277.9449462890625\n",
      "epoch 78, val_loss 455.4599304199219\n",
      "epoch 79, train_loss 1277.94482421875\n",
      "epoch 79, val_loss 455.4598388671875\n",
      "epoch 80, train_loss 1277.944580078125\n",
      "epoch 80, val_loss 455.4597473144531\n",
      "epoch 81, train_loss 1277.9444580078125\n",
      "epoch 81, val_loss 455.4595947265625\n",
      "epoch 82, train_loss 1277.9443359375\n",
      "epoch 82, val_loss 455.4595031738281\n",
      "epoch 83, train_loss 1277.944091796875\n",
      "epoch 83, val_loss 455.45941162109375\n",
      "epoch 84, train_loss 1277.9439697265625\n",
      "epoch 84, val_loss 455.4593200683594\n",
      "epoch 85, train_loss 1277.9437255859375\n",
      "epoch 85, val_loss 455.4590759277344\n",
      "epoch 86, train_loss 1277.9437255859375\n",
      "epoch 86, val_loss 455.45904541015625\n",
      "epoch 87, train_loss 1277.9434814453125\n",
      "epoch 87, val_loss 455.45892333984375\n",
      "epoch 88, train_loss 1277.943359375\n",
      "epoch 88, val_loss 455.458740234375\n",
      "epoch 89, train_loss 1277.943115234375\n",
      "epoch 89, val_loss 455.45867919921875\n",
      "epoch 90, train_loss 1277.943115234375\n",
      "epoch 90, val_loss 455.45849609375\n",
      "epoch 91, train_loss 1277.94287109375\n",
      "epoch 91, val_loss 455.4584045410156\n",
      "epoch 92, train_loss 1277.942626953125\n",
      "epoch 92, val_loss 455.4583435058594\n",
      "epoch 93, train_loss 1277.9423828125\n",
      "epoch 93, val_loss 455.4581298828125\n",
      "epoch 94, train_loss 1277.9422607421875\n",
      "epoch 94, val_loss 455.45806884765625\n",
      "epoch 95, train_loss 1277.9420166015625\n",
      "epoch 95, val_loss 455.4579162597656\n",
      "epoch 96, train_loss 1277.9420166015625\n",
      "epoch 96, val_loss 455.45782470703125\n",
      "epoch 97, train_loss 1277.941650390625\n",
      "epoch 97, val_loss 455.4576721191406\n",
      "epoch 98, train_loss 1277.9415283203125\n",
      "epoch 98, val_loss 455.4575500488281\n",
      "epoch 99, train_loss 1277.9412841796875\n",
      "epoch 99, val_loss 455.4574279785156\n",
      "Parameter containing:\n",
      "tensor([1.8733e-10], requires_grad=True)\n",
      "iter 47, train_loss_regularization 0.9227750301361084\n",
      "iter 47, val_loss_regularization 0.9227750301361084\n",
      "epoch 0, train_loss 1277.941162109375\n",
      "epoch 0, val_loss 455.45733642578125\n",
      "epoch 1, train_loss 1277.9410400390625\n",
      "epoch 1, val_loss 455.45721435546875\n",
      "epoch 2, train_loss 1277.9410400390625\n",
      "epoch 2, val_loss 455.45709228515625\n",
      "epoch 3, train_loss 1277.9405517578125\n",
      "epoch 3, val_loss 455.45697021484375\n",
      "epoch 4, train_loss 1277.9405517578125\n",
      "epoch 4, val_loss 455.45684814453125\n",
      "epoch 5, train_loss 1277.9404296875\n",
      "epoch 5, val_loss 455.4567565917969\n",
      "epoch 6, train_loss 1277.940185546875\n",
      "epoch 6, val_loss 455.4566345214844\n",
      "epoch 7, train_loss 1277.9400634765625\n",
      "epoch 7, val_loss 455.45654296875\n",
      "epoch 8, train_loss 1277.93994140625\n",
      "epoch 8, val_loss 455.45635986328125\n",
      "epoch 9, train_loss 1277.939697265625\n",
      "epoch 9, val_loss 455.456298828125\n",
      "epoch 10, train_loss 1277.939697265625\n",
      "epoch 10, val_loss 455.45611572265625\n",
      "epoch 11, train_loss 1277.9393310546875\n",
      "epoch 11, val_loss 455.4560546875\n",
      "epoch 12, train_loss 1277.939208984375\n",
      "epoch 12, val_loss 455.4559020996094\n",
      "epoch 13, train_loss 1277.9388427734375\n",
      "epoch 13, val_loss 455.4557800292969\n",
      "epoch 14, train_loss 1277.938720703125\n",
      "epoch 14, val_loss 455.4557189941406\n",
      "epoch 15, train_loss 1277.938720703125\n",
      "epoch 15, val_loss 455.4555969238281\n",
      "epoch 16, train_loss 1277.9385986328125\n",
      "epoch 16, val_loss 455.4555358886719\n",
      "epoch 17, train_loss 1277.9383544921875\n",
      "epoch 17, val_loss 455.455322265625\n",
      "epoch 18, train_loss 1277.9381103515625\n",
      "epoch 18, val_loss 455.4552001953125\n",
      "epoch 19, train_loss 1277.9381103515625\n",
      "epoch 19, val_loss 455.455078125\n",
      "epoch 20, train_loss 1277.9378662109375\n",
      "epoch 20, val_loss 455.4550476074219\n",
      "epoch 21, train_loss 1277.9376220703125\n",
      "epoch 21, val_loss 455.454833984375\n",
      "epoch 22, train_loss 1277.9376220703125\n",
      "epoch 22, val_loss 455.4547424316406\n",
      "epoch 23, train_loss 1277.9371337890625\n",
      "epoch 23, val_loss 455.45458984375\n",
      "epoch 24, train_loss 1277.9371337890625\n",
      "epoch 24, val_loss 455.45452880859375\n",
      "epoch 25, train_loss 1277.9371337890625\n",
      "epoch 25, val_loss 455.4543762207031\n",
      "epoch 26, train_loss 1277.936767578125\n",
      "epoch 26, val_loss 455.4542541503906\n",
      "epoch 27, train_loss 1277.9365234375\n",
      "epoch 27, val_loss 455.45416259765625\n",
      "epoch 28, train_loss 1277.9364013671875\n",
      "epoch 28, val_loss 455.45404052734375\n",
      "epoch 29, train_loss 1277.9361572265625\n",
      "epoch 29, val_loss 455.4539489746094\n",
      "epoch 30, train_loss 1277.93603515625\n",
      "epoch 30, val_loss 455.45379638671875\n",
      "epoch 31, train_loss 1277.93603515625\n",
      "epoch 31, val_loss 455.45367431640625\n",
      "epoch 32, train_loss 1277.935791015625\n",
      "epoch 32, val_loss 455.4535827636719\n",
      "epoch 33, train_loss 1277.935546875\n",
      "epoch 33, val_loss 455.45343017578125\n",
      "epoch 34, train_loss 1277.9354248046875\n",
      "epoch 34, val_loss 455.4533386230469\n",
      "epoch 35, train_loss 1277.935302734375\n",
      "epoch 35, val_loss 455.4532165527344\n",
      "epoch 36, train_loss 1277.93505859375\n",
      "epoch 36, val_loss 455.4530334472656\n",
      "epoch 37, train_loss 1277.934814453125\n",
      "epoch 37, val_loss 455.4529724121094\n",
      "epoch 38, train_loss 1277.9346923828125\n",
      "epoch 38, val_loss 455.45281982421875\n",
      "epoch 39, train_loss 1277.9346923828125\n",
      "epoch 39, val_loss 455.4527587890625\n",
      "epoch 40, train_loss 1277.934326171875\n",
      "epoch 40, val_loss 455.45263671875\n",
      "epoch 41, train_loss 1277.9342041015625\n",
      "epoch 41, val_loss 455.4525146484375\n",
      "epoch 42, train_loss 1277.93408203125\n",
      "epoch 42, val_loss 455.4523620605469\n",
      "epoch 43, train_loss 1277.93408203125\n",
      "epoch 43, val_loss 455.4523010253906\n",
      "epoch 44, train_loss 1277.9337158203125\n",
      "epoch 44, val_loss 455.4521179199219\n",
      "epoch 45, train_loss 1277.9337158203125\n",
      "epoch 45, val_loss 455.4520568847656\n",
      "epoch 46, train_loss 1277.933349609375\n",
      "epoch 46, val_loss 455.45184326171875\n",
      "epoch 47, train_loss 1277.9332275390625\n",
      "epoch 47, val_loss 455.4517822265625\n",
      "epoch 48, train_loss 1277.9329833984375\n",
      "epoch 48, val_loss 455.45172119140625\n",
      "epoch 49, train_loss 1277.932861328125\n",
      "epoch 49, val_loss 455.4515380859375\n",
      "epoch 50, train_loss 1277.9326171875\n",
      "epoch 50, val_loss 455.4514465332031\n",
      "epoch 51, train_loss 1277.9324951171875\n",
      "epoch 51, val_loss 455.4512939453125\n",
      "epoch 52, train_loss 1277.932373046875\n",
      "epoch 52, val_loss 455.451171875\n",
      "epoch 53, train_loss 1277.932373046875\n",
      "epoch 53, val_loss 455.4510498046875\n",
      "epoch 54, train_loss 1277.931884765625\n",
      "epoch 54, val_loss 455.45098876953125\n",
      "epoch 55, train_loss 1277.931884765625\n",
      "epoch 55, val_loss 455.4508361816406\n",
      "epoch 56, train_loss 1277.931640625\n",
      "epoch 56, val_loss 455.4507141113281\n",
      "epoch 57, train_loss 1277.931640625\n",
      "epoch 57, val_loss 455.4505920410156\n",
      "epoch 58, train_loss 1277.931396484375\n",
      "epoch 58, val_loss 455.4504699707031\n",
      "epoch 59, train_loss 1277.9312744140625\n",
      "epoch 59, val_loss 455.4504089355469\n",
      "epoch 60, train_loss 1277.9310302734375\n",
      "epoch 60, val_loss 455.45025634765625\n",
      "epoch 61, train_loss 1277.9307861328125\n",
      "epoch 61, val_loss 455.4500732421875\n",
      "epoch 62, train_loss 1277.9307861328125\n",
      "epoch 62, val_loss 455.45001220703125\n",
      "epoch 63, train_loss 1277.9305419921875\n",
      "epoch 63, val_loss 455.4499206542969\n",
      "epoch 64, train_loss 1277.9302978515625\n",
      "epoch 64, val_loss 455.4498291015625\n",
      "epoch 65, train_loss 1277.9302978515625\n",
      "epoch 65, val_loss 455.4496154785156\n",
      "epoch 66, train_loss 1277.929931640625\n",
      "epoch 66, val_loss 455.4495544433594\n",
      "epoch 67, train_loss 1277.9296875\n",
      "epoch 67, val_loss 455.449462890625\n",
      "epoch 68, train_loss 1277.9296875\n",
      "epoch 68, val_loss 455.4493408203125\n",
      "epoch 69, train_loss 1277.929443359375\n",
      "epoch 69, val_loss 455.44915771484375\n",
      "epoch 70, train_loss 1277.9293212890625\n",
      "epoch 70, val_loss 455.4490966796875\n",
      "epoch 71, train_loss 1277.9290771484375\n",
      "epoch 71, val_loss 455.44891357421875\n",
      "epoch 72, train_loss 1277.928955078125\n",
      "epoch 72, val_loss 455.4488830566406\n",
      "epoch 73, train_loss 1277.9288330078125\n",
      "epoch 73, val_loss 455.4486999511719\n",
      "epoch 74, train_loss 1277.9287109375\n",
      "epoch 74, val_loss 455.4485778808594\n",
      "epoch 75, train_loss 1277.928466796875\n",
      "epoch 75, val_loss 455.448486328125\n",
      "epoch 76, train_loss 1277.928466796875\n",
      "epoch 76, val_loss 455.4483337402344\n",
      "epoch 77, train_loss 1277.927978515625\n",
      "epoch 77, val_loss 455.4482421875\n",
      "epoch 78, train_loss 1277.9278564453125\n",
      "epoch 78, val_loss 455.4480895996094\n",
      "epoch 79, train_loss 1277.9278564453125\n",
      "epoch 79, val_loss 455.447998046875\n",
      "epoch 80, train_loss 1277.9276123046875\n",
      "epoch 80, val_loss 455.4479064941406\n",
      "epoch 81, train_loss 1277.9273681640625\n",
      "epoch 81, val_loss 455.44775390625\n",
      "epoch 82, train_loss 1277.92724609375\n",
      "epoch 82, val_loss 455.4476623535156\n",
      "epoch 83, train_loss 1277.927001953125\n",
      "epoch 83, val_loss 455.44757080078125\n",
      "epoch 84, train_loss 1277.9268798828125\n",
      "epoch 84, val_loss 455.44744873046875\n",
      "epoch 85, train_loss 1277.9267578125\n",
      "epoch 85, val_loss 455.4472961425781\n",
      "epoch 86, train_loss 1277.9266357421875\n",
      "epoch 86, val_loss 455.4471740722656\n",
      "epoch 87, train_loss 1277.9263916015625\n",
      "epoch 87, val_loss 455.44708251953125\n",
      "epoch 88, train_loss 1277.92626953125\n",
      "epoch 88, val_loss 455.4469909667969\n",
      "epoch 89, train_loss 1277.926025390625\n",
      "epoch 89, val_loss 455.44683837890625\n",
      "epoch 90, train_loss 1277.9259033203125\n",
      "epoch 90, val_loss 455.44671630859375\n",
      "epoch 91, train_loss 1277.92578125\n",
      "epoch 91, val_loss 455.44659423828125\n",
      "epoch 92, train_loss 1277.925537109375\n",
      "epoch 92, val_loss 455.446533203125\n",
      "epoch 93, train_loss 1277.9254150390625\n",
      "epoch 93, val_loss 455.4463195800781\n",
      "epoch 94, train_loss 1277.9254150390625\n",
      "epoch 94, val_loss 455.4462585449219\n",
      "epoch 95, train_loss 1277.925048828125\n",
      "epoch 95, val_loss 455.4461669921875\n",
      "epoch 96, train_loss 1277.9249267578125\n",
      "epoch 96, val_loss 455.4460144042969\n",
      "epoch 97, train_loss 1277.9246826171875\n",
      "epoch 97, val_loss 455.44586181640625\n",
      "epoch 98, train_loss 1277.924560546875\n",
      "epoch 98, val_loss 455.44580078125\n",
      "epoch 99, train_loss 1277.92431640625\n",
      "epoch 99, val_loss 455.4456787109375\n",
      "Parameter containing:\n",
      "tensor([1.2172e-10], requires_grad=True)\n",
      "iter 48, train_loss_regularization 0.9179626703262329\n",
      "iter 48, val_loss_regularization 0.9179626703262329\n",
      "epoch 0, train_loss 1277.9241943359375\n",
      "epoch 0, val_loss 455.4455871582031\n",
      "epoch 1, train_loss 1277.924072265625\n",
      "epoch 1, val_loss 455.4454650878906\n",
      "epoch 2, train_loss 1277.9239501953125\n",
      "epoch 2, val_loss 455.4452819824219\n",
      "epoch 3, train_loss 1277.9237060546875\n",
      "epoch 3, val_loss 455.4452209472656\n",
      "epoch 4, train_loss 1277.923583984375\n",
      "epoch 4, val_loss 455.445068359375\n",
      "epoch 5, train_loss 1277.92333984375\n",
      "epoch 5, val_loss 455.4449462890625\n",
      "epoch 6, train_loss 1277.9232177734375\n",
      "epoch 6, val_loss 455.44488525390625\n",
      "epoch 7, train_loss 1277.923095703125\n",
      "epoch 7, val_loss 455.44476318359375\n",
      "epoch 8, train_loss 1277.9228515625\n",
      "epoch 8, val_loss 455.4446105957031\n",
      "epoch 9, train_loss 1277.922607421875\n",
      "epoch 9, val_loss 455.4444885253906\n",
      "epoch 10, train_loss 1277.9224853515625\n",
      "epoch 10, val_loss 455.4443664550781\n",
      "epoch 11, train_loss 1277.92236328125\n",
      "epoch 11, val_loss 455.4442443847656\n",
      "epoch 12, train_loss 1277.9222412109375\n",
      "epoch 12, val_loss 455.4441223144531\n",
      "epoch 13, train_loss 1277.9219970703125\n",
      "epoch 13, val_loss 455.444091796875\n",
      "epoch 14, train_loss 1277.9219970703125\n",
      "epoch 14, val_loss 455.44390869140625\n",
      "epoch 15, train_loss 1277.921630859375\n",
      "epoch 15, val_loss 455.44384765625\n",
      "epoch 16, train_loss 1277.921630859375\n",
      "epoch 16, val_loss 455.4436950683594\n",
      "epoch 17, train_loss 1277.9215087890625\n",
      "epoch 17, val_loss 455.4435119628906\n",
      "epoch 18, train_loss 1277.921142578125\n",
      "epoch 18, val_loss 455.44342041015625\n",
      "epoch 19, train_loss 1277.9210205078125\n",
      "epoch 19, val_loss 455.4433288574219\n",
      "epoch 20, train_loss 1277.9207763671875\n",
      "epoch 20, val_loss 455.4432373046875\n",
      "epoch 21, train_loss 1277.920654296875\n",
      "epoch 21, val_loss 455.4430847167969\n",
      "epoch 22, train_loss 1277.9205322265625\n",
      "epoch 22, val_loss 455.4429931640625\n",
      "epoch 23, train_loss 1277.9202880859375\n",
      "epoch 23, val_loss 455.44287109375\n",
      "epoch 24, train_loss 1277.9200439453125\n",
      "epoch 24, val_loss 455.4427490234375\n",
      "epoch 25, train_loss 1277.9200439453125\n",
      "epoch 25, val_loss 455.442626953125\n",
      "epoch 26, train_loss 1277.9197998046875\n",
      "epoch 26, val_loss 455.4425048828125\n",
      "epoch 27, train_loss 1277.919677734375\n",
      "epoch 27, val_loss 455.4423828125\n",
      "epoch 28, train_loss 1277.919677734375\n",
      "epoch 28, val_loss 455.4422607421875\n",
      "epoch 29, train_loss 1277.919189453125\n",
      "epoch 29, val_loss 455.4421691894531\n",
      "epoch 30, train_loss 1277.919189453125\n",
      "epoch 30, val_loss 455.4420471191406\n",
      "epoch 31, train_loss 1277.9190673828125\n",
      "epoch 31, val_loss 455.44195556640625\n",
      "epoch 32, train_loss 1277.918701171875\n",
      "epoch 32, val_loss 455.44183349609375\n",
      "epoch 33, train_loss 1277.918701171875\n",
      "epoch 33, val_loss 455.44171142578125\n",
      "epoch 34, train_loss 1277.9185791015625\n",
      "epoch 34, val_loss 455.44158935546875\n",
      "epoch 35, train_loss 1277.9183349609375\n",
      "epoch 35, val_loss 455.44140625\n",
      "epoch 36, train_loss 1277.918212890625\n",
      "epoch 36, val_loss 455.4413757324219\n",
      "epoch 37, train_loss 1277.91796875\n",
      "epoch 37, val_loss 455.44122314453125\n",
      "epoch 38, train_loss 1277.9178466796875\n",
      "epoch 38, val_loss 455.4410705566406\n",
      "epoch 39, train_loss 1277.9176025390625\n",
      "epoch 39, val_loss 455.4410400390625\n",
      "epoch 40, train_loss 1277.91748046875\n",
      "epoch 40, val_loss 455.4408874511719\n",
      "epoch 41, train_loss 1277.9173583984375\n",
      "epoch 41, val_loss 455.4407958984375\n",
      "epoch 42, train_loss 1277.917236328125\n",
      "epoch 42, val_loss 455.440673828125\n",
      "epoch 43, train_loss 1277.9169921875\n",
      "epoch 43, val_loss 455.44049072265625\n",
      "epoch 44, train_loss 1277.9168701171875\n",
      "epoch 44, val_loss 455.4404296875\n",
      "epoch 45, train_loss 1277.9166259765625\n",
      "epoch 45, val_loss 455.4402770996094\n",
      "epoch 46, train_loss 1277.91650390625\n",
      "epoch 46, val_loss 455.4401550292969\n",
      "epoch 47, train_loss 1277.916259765625\n",
      "epoch 47, val_loss 455.4400939941406\n",
      "epoch 48, train_loss 1277.9161376953125\n",
      "epoch 48, val_loss 455.4399719238281\n",
      "epoch 49, train_loss 1277.9158935546875\n",
      "epoch 49, val_loss 455.4398193359375\n",
      "epoch 50, train_loss 1277.9158935546875\n",
      "epoch 50, val_loss 455.4396667480469\n",
      "epoch 51, train_loss 1277.91552734375\n",
      "epoch 51, val_loss 455.4395751953125\n",
      "epoch 52, train_loss 1277.9154052734375\n",
      "epoch 52, val_loss 455.43951416015625\n",
      "epoch 53, train_loss 1277.9154052734375\n",
      "epoch 53, val_loss 455.4393310546875\n",
      "epoch 54, train_loss 1277.915283203125\n",
      "epoch 54, val_loss 455.4392395019531\n",
      "epoch 55, train_loss 1277.9150390625\n",
      "epoch 55, val_loss 455.4391174316406\n",
      "epoch 56, train_loss 1277.914794921875\n",
      "epoch 56, val_loss 455.4390563964844\n",
      "epoch 57, train_loss 1277.9146728515625\n",
      "epoch 57, val_loss 455.43890380859375\n",
      "epoch 58, train_loss 1277.9144287109375\n",
      "epoch 58, val_loss 455.43878173828125\n",
      "epoch 59, train_loss 1277.9144287109375\n",
      "epoch 59, val_loss 455.43865966796875\n",
      "epoch 60, train_loss 1277.9139404296875\n",
      "epoch 60, val_loss 455.43853759765625\n",
      "epoch 61, train_loss 1277.9139404296875\n",
      "epoch 61, val_loss 455.4384460449219\n",
      "epoch 62, train_loss 1277.913818359375\n",
      "epoch 62, val_loss 455.4383239746094\n",
      "epoch 63, train_loss 1277.91357421875\n",
      "epoch 63, val_loss 455.4382019042969\n",
      "epoch 64, train_loss 1277.9134521484375\n",
      "epoch 64, val_loss 455.4380798339844\n",
      "epoch 65, train_loss 1277.913330078125\n",
      "epoch 65, val_loss 455.4379577636719\n",
      "epoch 66, train_loss 1277.9129638671875\n",
      "epoch 66, val_loss 455.4378662109375\n",
      "epoch 67, train_loss 1277.9129638671875\n",
      "epoch 67, val_loss 455.4377136230469\n",
      "epoch 68, train_loss 1277.912841796875\n",
      "epoch 68, val_loss 455.4376220703125\n",
      "epoch 69, train_loss 1277.9127197265625\n",
      "epoch 69, val_loss 455.4375305175781\n",
      "epoch 70, train_loss 1277.9124755859375\n",
      "epoch 70, val_loss 455.4374084472656\n",
      "epoch 71, train_loss 1277.912353515625\n",
      "epoch 71, val_loss 455.4372863769531\n",
      "epoch 72, train_loss 1277.912109375\n",
      "epoch 72, val_loss 455.4371643066406\n",
      "epoch 73, train_loss 1277.9119873046875\n",
      "epoch 73, val_loss 455.4370422363281\n",
      "epoch 74, train_loss 1277.911865234375\n",
      "epoch 74, val_loss 455.4369201660156\n",
      "epoch 75, train_loss 1277.9117431640625\n",
      "epoch 75, val_loss 455.43682861328125\n",
      "epoch 76, train_loss 1277.9114990234375\n",
      "epoch 76, val_loss 455.4366760253906\n",
      "epoch 77, train_loss 1277.911376953125\n",
      "epoch 77, val_loss 455.43658447265625\n",
      "epoch 78, train_loss 1277.9111328125\n",
      "epoch 78, val_loss 455.43646240234375\n",
      "epoch 79, train_loss 1277.9110107421875\n",
      "epoch 79, val_loss 455.43634033203125\n",
      "epoch 80, train_loss 1277.9107666015625\n",
      "epoch 80, val_loss 455.43621826171875\n",
      "epoch 81, train_loss 1277.9105224609375\n",
      "epoch 81, val_loss 455.4361267089844\n",
      "epoch 82, train_loss 1277.9105224609375\n",
      "epoch 82, val_loss 455.4360046386719\n",
      "epoch 83, train_loss 1277.9102783203125\n",
      "epoch 83, val_loss 455.4359130859375\n",
      "epoch 84, train_loss 1277.9100341796875\n",
      "epoch 84, val_loss 455.4357604980469\n",
      "epoch 85, train_loss 1277.9100341796875\n",
      "epoch 85, val_loss 455.4356384277344\n",
      "epoch 86, train_loss 1277.90966796875\n",
      "epoch 86, val_loss 455.4355773925781\n",
      "epoch 87, train_loss 1277.9095458984375\n",
      "epoch 87, val_loss 455.4354248046875\n",
      "epoch 88, train_loss 1277.9095458984375\n",
      "epoch 88, val_loss 455.435302734375\n",
      "epoch 89, train_loss 1277.9093017578125\n",
      "epoch 89, val_loss 455.4352111816406\n",
      "epoch 90, train_loss 1277.9090576171875\n",
      "epoch 90, val_loss 455.43499755859375\n",
      "epoch 91, train_loss 1277.9090576171875\n",
      "epoch 91, val_loss 455.4349670410156\n",
      "epoch 92, train_loss 1277.90869140625\n",
      "epoch 92, val_loss 455.4347839355469\n",
      "epoch 93, train_loss 1277.90869140625\n",
      "epoch 93, val_loss 455.43475341796875\n",
      "epoch 94, train_loss 1277.908447265625\n",
      "epoch 94, val_loss 455.43463134765625\n",
      "epoch 95, train_loss 1277.908447265625\n",
      "epoch 95, val_loss 455.43450927734375\n",
      "epoch 96, train_loss 1277.908203125\n",
      "epoch 96, val_loss 455.43438720703125\n",
      "epoch 97, train_loss 1277.907958984375\n",
      "epoch 97, val_loss 455.4342041015625\n",
      "epoch 98, train_loss 1277.90771484375\n",
      "epoch 98, val_loss 455.4341735839844\n",
      "epoch 99, train_loss 1277.90771484375\n",
      "epoch 99, val_loss 455.4340515136719\n",
      "Parameter containing:\n",
      "tensor([7.9209e-11], requires_grad=True)\n",
      "iter 49, train_loss_regularization 0.9132330417633057\n",
      "iter 49, val_loss_regularization 0.9132330417633057\n",
      "epoch 0, train_loss 1277.907470703125\n",
      "epoch 0, val_loss 455.4339599609375\n",
      "epoch 1, train_loss 1277.9072265625\n",
      "epoch 1, val_loss 455.43377685546875\n",
      "epoch 2, train_loss 1277.9072265625\n",
      "epoch 2, val_loss 455.43365478515625\n",
      "epoch 3, train_loss 1277.9068603515625\n",
      "epoch 3, val_loss 455.43359375\n",
      "epoch 4, train_loss 1277.90673828125\n",
      "epoch 4, val_loss 455.4334716796875\n",
      "epoch 5, train_loss 1277.9066162109375\n",
      "epoch 5, val_loss 455.4333190917969\n",
      "epoch 6, train_loss 1277.9063720703125\n",
      "epoch 6, val_loss 455.43316650390625\n",
      "epoch 7, train_loss 1277.9063720703125\n",
      "epoch 7, val_loss 455.4331359863281\n",
      "epoch 8, train_loss 1277.90625\n",
      "epoch 8, val_loss 455.4330139160156\n",
      "epoch 9, train_loss 1277.906005859375\n",
      "epoch 9, val_loss 455.4328308105469\n",
      "epoch 10, train_loss 1277.90576171875\n",
      "epoch 10, val_loss 455.4327087402344\n",
      "epoch 11, train_loss 1277.90576171875\n",
      "epoch 11, val_loss 455.43267822265625\n",
      "epoch 12, train_loss 1277.905517578125\n",
      "epoch 12, val_loss 455.4325866699219\n",
      "epoch 13, train_loss 1277.9053955078125\n",
      "epoch 13, val_loss 455.4324035644531\n",
      "epoch 14, train_loss 1277.905029296875\n",
      "epoch 14, val_loss 455.4322814941406\n",
      "epoch 15, train_loss 1277.905029296875\n",
      "epoch 15, val_loss 455.4321594238281\n",
      "epoch 16, train_loss 1277.90478515625\n",
      "epoch 16, val_loss 455.4320983886719\n",
      "epoch 17, train_loss 1277.9046630859375\n",
      "epoch 17, val_loss 455.4320068359375\n",
      "epoch 18, train_loss 1277.9044189453125\n",
      "epoch 18, val_loss 455.43182373046875\n",
      "epoch 19, train_loss 1277.904296875\n",
      "epoch 19, val_loss 455.4316711425781\n",
      "epoch 20, train_loss 1277.9041748046875\n",
      "epoch 20, val_loss 455.4316101074219\n",
      "epoch 21, train_loss 1277.9039306640625\n",
      "epoch 21, val_loss 455.4314880371094\n",
      "epoch 22, train_loss 1277.90380859375\n",
      "epoch 22, val_loss 455.4313659667969\n",
      "epoch 23, train_loss 1277.903564453125\n",
      "epoch 23, val_loss 455.43121337890625\n",
      "epoch 24, train_loss 1277.9034423828125\n",
      "epoch 24, val_loss 455.43109130859375\n",
      "epoch 25, train_loss 1277.9034423828125\n",
      "epoch 25, val_loss 455.4310302734375\n",
      "epoch 26, train_loss 1277.903076171875\n",
      "epoch 26, val_loss 455.4308776855469\n",
      "epoch 27, train_loss 1277.9029541015625\n",
      "epoch 27, val_loss 455.43084716796875\n",
      "epoch 28, train_loss 1277.90283203125\n",
      "epoch 28, val_loss 455.4306640625\n",
      "epoch 29, train_loss 1277.9027099609375\n",
      "epoch 29, val_loss 455.4305725097656\n",
      "epoch 30, train_loss 1277.9024658203125\n",
      "epoch 30, val_loss 455.4304504394531\n",
      "epoch 31, train_loss 1277.90234375\n",
      "epoch 31, val_loss 455.4302978515625\n",
      "epoch 32, train_loss 1277.9019775390625\n",
      "epoch 32, val_loss 455.4302062988281\n",
      "epoch 33, train_loss 1277.902099609375\n",
      "epoch 33, val_loss 455.4300842285156\n",
      "epoch 34, train_loss 1277.9019775390625\n",
      "epoch 34, val_loss 455.42999267578125\n",
      "epoch 35, train_loss 1277.901611328125\n",
      "epoch 35, val_loss 455.4298400878906\n",
      "epoch 36, train_loss 1277.9014892578125\n",
      "epoch 36, val_loss 455.4297180175781\n",
      "epoch 37, train_loss 1277.9013671875\n",
      "epoch 37, val_loss 455.4296569824219\n",
      "epoch 38, train_loss 1277.9012451171875\n",
      "epoch 38, val_loss 455.42950439453125\n",
      "epoch 39, train_loss 1277.9010009765625\n",
      "epoch 39, val_loss 455.4294128417969\n",
      "epoch 40, train_loss 1277.9010009765625\n",
      "epoch 40, val_loss 455.4292907714844\n",
      "epoch 41, train_loss 1277.9007568359375\n",
      "epoch 41, val_loss 455.4291687011719\n",
      "epoch 42, train_loss 1277.900390625\n",
      "epoch 42, val_loss 455.4290771484375\n",
      "epoch 43, train_loss 1277.9002685546875\n",
      "epoch 43, val_loss 455.4289245605469\n",
      "epoch 44, train_loss 1277.9000244140625\n",
      "epoch 44, val_loss 455.4288330078125\n",
      "epoch 45, train_loss 1277.89990234375\n",
      "epoch 45, val_loss 455.4287414550781\n",
      "epoch 46, train_loss 1277.8997802734375\n",
      "epoch 46, val_loss 455.42852783203125\n",
      "epoch 47, train_loss 1277.8995361328125\n",
      "epoch 47, val_loss 455.428466796875\n",
      "epoch 48, train_loss 1277.8995361328125\n",
      "epoch 48, val_loss 455.4283752441406\n",
      "epoch 49, train_loss 1277.8992919921875\n",
      "epoch 49, val_loss 455.42828369140625\n",
      "epoch 50, train_loss 1277.899169921875\n",
      "epoch 50, val_loss 455.4281311035156\n",
      "epoch 51, train_loss 1277.899169921875\n",
      "epoch 51, val_loss 455.4280090332031\n",
      "epoch 52, train_loss 1277.8988037109375\n",
      "epoch 52, val_loss 455.4278869628906\n",
      "epoch 53, train_loss 1277.8988037109375\n",
      "epoch 53, val_loss 455.4277648925781\n",
      "epoch 54, train_loss 1277.898681640625\n",
      "epoch 54, val_loss 455.4277038574219\n",
      "epoch 55, train_loss 1277.8983154296875\n",
      "epoch 55, val_loss 455.4275817871094\n",
      "epoch 56, train_loss 1277.898193359375\n",
      "epoch 56, val_loss 455.42742919921875\n",
      "epoch 57, train_loss 1277.89794921875\n",
      "epoch 57, val_loss 455.4273376464844\n",
      "epoch 58, train_loss 1277.8978271484375\n",
      "epoch 58, val_loss 455.4272155761719\n",
      "epoch 59, train_loss 1277.897705078125\n",
      "epoch 59, val_loss 455.4271240234375\n",
      "epoch 60, train_loss 1277.897705078125\n",
      "epoch 60, val_loss 455.4269104003906\n",
      "epoch 61, train_loss 1277.8974609375\n",
      "epoch 61, val_loss 455.4269104003906\n",
      "epoch 62, train_loss 1277.8973388671875\n",
      "epoch 62, val_loss 455.4267578125\n",
      "epoch 63, train_loss 1277.8970947265625\n",
      "epoch 63, val_loss 455.4266662597656\n",
      "epoch 64, train_loss 1277.89697265625\n",
      "epoch 64, val_loss 455.42657470703125\n",
      "epoch 65, train_loss 1277.896728515625\n",
      "epoch 65, val_loss 455.42633056640625\n",
      "epoch 66, train_loss 1277.8966064453125\n",
      "epoch 66, val_loss 455.4263000488281\n",
      "epoch 67, train_loss 1277.896484375\n",
      "epoch 67, val_loss 455.42620849609375\n",
      "epoch 68, train_loss 1277.8961181640625\n",
      "epoch 68, val_loss 455.42608642578125\n",
      "epoch 69, train_loss 1277.8961181640625\n",
      "epoch 69, val_loss 455.4258728027344\n",
      "epoch 70, train_loss 1277.8958740234375\n",
      "epoch 70, val_loss 455.42584228515625\n",
      "epoch 71, train_loss 1277.895751953125\n",
      "epoch 71, val_loss 455.4257507324219\n",
      "epoch 72, train_loss 1277.8955078125\n",
      "epoch 72, val_loss 455.42559814453125\n",
      "epoch 73, train_loss 1277.8955078125\n",
      "epoch 73, val_loss 455.4254455566406\n",
      "epoch 74, train_loss 1277.895263671875\n",
      "epoch 74, val_loss 455.4253234863281\n",
      "epoch 75, train_loss 1277.8951416015625\n",
      "epoch 75, val_loss 455.4252624511719\n",
      "epoch 76, train_loss 1277.89501953125\n",
      "epoch 76, val_loss 455.4251708984375\n",
      "epoch 77, train_loss 1277.894775390625\n",
      "epoch 77, val_loss 455.425048828125\n",
      "epoch 78, train_loss 1277.89453125\n",
      "epoch 78, val_loss 455.4249267578125\n",
      "epoch 79, train_loss 1277.894287109375\n",
      "epoch 79, val_loss 455.4248046875\n",
      "epoch 80, train_loss 1277.894287109375\n",
      "epoch 80, val_loss 455.42474365234375\n",
      "epoch 81, train_loss 1277.8941650390625\n",
      "epoch 81, val_loss 455.4245910644531\n",
      "epoch 82, train_loss 1277.89404296875\n",
      "epoch 82, val_loss 455.42449951171875\n",
      "epoch 83, train_loss 1277.893798828125\n",
      "epoch 83, val_loss 455.4243469238281\n",
      "epoch 84, train_loss 1277.8936767578125\n",
      "epoch 84, val_loss 455.42425537109375\n",
      "epoch 85, train_loss 1277.8934326171875\n",
      "epoch 85, val_loss 455.4241638183594\n",
      "epoch 86, train_loss 1277.8934326171875\n",
      "epoch 86, val_loss 455.4239501953125\n",
      "epoch 87, train_loss 1277.8931884765625\n",
      "epoch 87, val_loss 455.423828125\n",
      "epoch 88, train_loss 1277.89306640625\n",
      "epoch 88, val_loss 455.4237365722656\n",
      "epoch 89, train_loss 1277.8927001953125\n",
      "epoch 89, val_loss 455.4236755371094\n",
      "epoch 90, train_loss 1277.8927001953125\n",
      "epoch 90, val_loss 455.4235534667969\n",
      "epoch 91, train_loss 1277.8924560546875\n",
      "epoch 91, val_loss 455.4234619140625\n",
      "epoch 92, train_loss 1277.892333984375\n",
      "epoch 92, val_loss 455.4232482910156\n",
      "epoch 93, train_loss 1277.8922119140625\n",
      "epoch 93, val_loss 455.4232177734375\n",
      "epoch 94, train_loss 1277.8919677734375\n",
      "epoch 94, val_loss 455.423095703125\n",
      "epoch 95, train_loss 1277.8917236328125\n",
      "epoch 95, val_loss 455.4229431152344\n",
      "epoch 96, train_loss 1277.8917236328125\n",
      "epoch 96, val_loss 455.4228820800781\n",
      "epoch 97, train_loss 1277.8914794921875\n",
      "epoch 97, val_loss 455.4227600097656\n",
      "epoch 98, train_loss 1277.89111328125\n",
      "epoch 98, val_loss 455.42266845703125\n",
      "epoch 99, train_loss 1277.89111328125\n",
      "epoch 99, val_loss 455.4224548339844\n",
      "Parameter containing:\n",
      "tensor([5.1617e-11], requires_grad=True)\n",
      "iter 50, train_loss_regularization 0.9085842967033386\n",
      "iter 50, val_loss_regularization 0.9085842967033386\n",
      "epoch 0, train_loss 1277.89111328125\n",
      "epoch 0, val_loss 455.4224548339844\n",
      "epoch 1, train_loss 1277.890869140625\n",
      "epoch 1, val_loss 455.42230224609375\n",
      "epoch 2, train_loss 1277.8907470703125\n",
      "epoch 2, val_loss 455.4222106933594\n",
      "epoch 3, train_loss 1277.8905029296875\n",
      "epoch 3, val_loss 455.4220275878906\n",
      "epoch 4, train_loss 1277.890380859375\n",
      "epoch 4, val_loss 455.4219055175781\n",
      "epoch 5, train_loss 1277.8902587890625\n",
      "epoch 5, val_loss 455.4218444824219\n",
      "epoch 6, train_loss 1277.89013671875\n",
      "epoch 6, val_loss 455.4217224121094\n",
      "epoch 7, train_loss 1277.889892578125\n",
      "epoch 7, val_loss 455.421630859375\n",
      "epoch 8, train_loss 1277.8896484375\n",
      "epoch 8, val_loss 455.42144775390625\n",
      "epoch 9, train_loss 1277.889404296875\n",
      "epoch 9, val_loss 455.42138671875\n",
      "epoch 10, train_loss 1277.889404296875\n",
      "epoch 10, val_loss 455.4212646484375\n",
      "epoch 11, train_loss 1277.8892822265625\n",
      "epoch 11, val_loss 455.4211120605469\n",
      "epoch 12, train_loss 1277.8890380859375\n",
      "epoch 12, val_loss 455.4209899902344\n",
      "epoch 13, train_loss 1277.8887939453125\n",
      "epoch 13, val_loss 455.4209289550781\n",
      "epoch 14, train_loss 1277.888671875\n",
      "epoch 14, val_loss 455.42083740234375\n",
      "epoch 15, train_loss 1277.888671875\n",
      "epoch 15, val_loss 455.42071533203125\n",
      "epoch 16, train_loss 1277.888427734375\n",
      "epoch 16, val_loss 455.4205322265625\n",
      "epoch 17, train_loss 1277.8883056640625\n",
      "epoch 17, val_loss 455.42047119140625\n",
      "epoch 18, train_loss 1277.8880615234375\n",
      "epoch 18, val_loss 455.4203186035156\n",
      "epoch 19, train_loss 1277.8878173828125\n",
      "epoch 19, val_loss 455.4201965332031\n",
      "epoch 20, train_loss 1277.8876953125\n",
      "epoch 20, val_loss 455.4200744628906\n",
      "epoch 21, train_loss 1277.8876953125\n",
      "epoch 21, val_loss 455.4199523925781\n",
      "epoch 22, train_loss 1277.887451171875\n",
      "epoch 22, val_loss 455.41986083984375\n",
      "epoch 23, train_loss 1277.88720703125\n",
      "epoch 23, val_loss 455.4197998046875\n",
      "epoch 24, train_loss 1277.8870849609375\n",
      "epoch 24, val_loss 455.41973876953125\n",
      "epoch 25, train_loss 1277.886962890625\n",
      "epoch 25, val_loss 455.41949462890625\n",
      "epoch 26, train_loss 1277.88671875\n",
      "epoch 26, val_loss 455.4194030761719\n",
      "epoch 27, train_loss 1277.88671875\n",
      "epoch 27, val_loss 455.4193420410156\n",
      "epoch 28, train_loss 1277.8865966796875\n",
      "epoch 28, val_loss 455.4192199707031\n",
      "epoch 29, train_loss 1277.886474609375\n",
      "epoch 29, val_loss 455.4190979003906\n",
      "epoch 30, train_loss 1277.8861083984375\n",
      "epoch 30, val_loss 455.41900634765625\n",
      "epoch 31, train_loss 1277.8861083984375\n",
      "epoch 31, val_loss 455.41888427734375\n",
      "epoch 32, train_loss 1277.8858642578125\n",
      "epoch 32, val_loss 455.41876220703125\n",
      "epoch 33, train_loss 1277.8856201171875\n",
      "epoch 33, val_loss 455.41864013671875\n",
      "epoch 34, train_loss 1277.8853759765625\n",
      "epoch 34, val_loss 455.41845703125\n",
      "epoch 35, train_loss 1277.88525390625\n",
      "epoch 35, val_loss 455.4184265136719\n",
      "epoch 36, train_loss 1277.8851318359375\n",
      "epoch 36, val_loss 455.4183044433594\n",
      "epoch 37, train_loss 1277.885009765625\n",
      "epoch 37, val_loss 455.418212890625\n",
      "epoch 38, train_loss 1277.884765625\n",
      "epoch 38, val_loss 455.41802978515625\n",
      "epoch 39, train_loss 1277.8846435546875\n",
      "epoch 39, val_loss 455.41790771484375\n",
      "epoch 40, train_loss 1277.8846435546875\n",
      "epoch 40, val_loss 455.4178466796875\n",
      "epoch 41, train_loss 1277.88427734375\n",
      "epoch 41, val_loss 455.4176940917969\n",
      "epoch 42, train_loss 1277.8841552734375\n",
      "epoch 42, val_loss 455.4175720214844\n",
      "epoch 43, train_loss 1277.884033203125\n",
      "epoch 43, val_loss 455.4175109863281\n",
      "epoch 44, train_loss 1277.8837890625\n",
      "epoch 44, val_loss 455.4173889160156\n",
      "epoch 45, train_loss 1277.8837890625\n",
      "epoch 45, val_loss 455.41729736328125\n",
      "epoch 46, train_loss 1277.8834228515625\n",
      "epoch 46, val_loss 455.4172058105469\n",
      "epoch 47, train_loss 1277.8834228515625\n",
      "epoch 47, val_loss 455.4169921875\n",
      "epoch 48, train_loss 1277.88330078125\n",
      "epoch 48, val_loss 455.41693115234375\n",
      "epoch 49, train_loss 1277.883056640625\n",
      "epoch 49, val_loss 455.4168395996094\n",
      "epoch 50, train_loss 1277.8828125\n",
      "epoch 50, val_loss 455.416748046875\n",
      "epoch 51, train_loss 1277.8826904296875\n",
      "epoch 51, val_loss 455.4165954589844\n",
      "epoch 52, train_loss 1277.8824462890625\n",
      "epoch 52, val_loss 455.4164123535156\n",
      "epoch 53, train_loss 1277.8824462890625\n",
      "epoch 53, val_loss 455.4163818359375\n",
      "epoch 54, train_loss 1277.8822021484375\n",
      "epoch 54, val_loss 455.416259765625\n",
      "epoch 55, train_loss 1277.8822021484375\n",
      "epoch 55, val_loss 455.41607666015625\n",
      "epoch 56, train_loss 1277.8818359375\n",
      "epoch 56, val_loss 455.4159851074219\n",
      "epoch 57, train_loss 1277.8818359375\n",
      "epoch 57, val_loss 455.4158630371094\n",
      "epoch 58, train_loss 1277.8814697265625\n",
      "epoch 58, val_loss 455.4157409667969\n",
      "epoch 59, train_loss 1277.88134765625\n",
      "epoch 59, val_loss 455.4156799316406\n",
      "epoch 60, train_loss 1277.881103515625\n",
      "epoch 60, val_loss 455.41558837890625\n",
      "epoch 61, train_loss 1277.881103515625\n",
      "epoch 61, val_loss 455.4154052734375\n",
      "epoch 62, train_loss 1277.8809814453125\n",
      "epoch 62, val_loss 455.41534423828125\n",
      "epoch 63, train_loss 1277.880615234375\n",
      "epoch 63, val_loss 455.4152526855469\n",
      "epoch 64, train_loss 1277.880615234375\n",
      "epoch 64, val_loss 455.4150390625\n",
      "epoch 65, train_loss 1277.88037109375\n",
      "epoch 65, val_loss 455.4150085449219\n",
      "epoch 66, train_loss 1277.8802490234375\n",
      "epoch 66, val_loss 455.4148254394531\n",
      "epoch 67, train_loss 1277.880126953125\n",
      "epoch 67, val_loss 455.4147644042969\n",
      "epoch 68, train_loss 1277.880126953125\n",
      "epoch 68, val_loss 455.4146728515625\n",
      "epoch 69, train_loss 1277.8798828125\n",
      "epoch 69, val_loss 455.41448974609375\n",
      "epoch 70, train_loss 1277.879638671875\n",
      "epoch 70, val_loss 455.41436767578125\n",
      "epoch 71, train_loss 1277.87939453125\n",
      "epoch 71, val_loss 455.4143371582031\n",
      "epoch 72, train_loss 1277.87939453125\n",
      "epoch 72, val_loss 455.4142150878906\n",
      "epoch 73, train_loss 1277.8790283203125\n",
      "epoch 73, val_loss 455.4140930175781\n",
      "epoch 74, train_loss 1277.8790283203125\n",
      "epoch 74, val_loss 455.41400146484375\n",
      "epoch 75, train_loss 1277.8787841796875\n",
      "epoch 75, val_loss 455.413818359375\n",
      "epoch 76, train_loss 1277.8785400390625\n",
      "epoch 76, val_loss 455.41375732421875\n",
      "epoch 77, train_loss 1277.8785400390625\n",
      "epoch 77, val_loss 455.4136657714844\n",
      "epoch 78, train_loss 1277.8782958984375\n",
      "epoch 78, val_loss 455.41351318359375\n",
      "epoch 79, train_loss 1277.8782958984375\n",
      "epoch 79, val_loss 455.4134521484375\n",
      "epoch 80, train_loss 1277.8780517578125\n",
      "epoch 80, val_loss 455.4132385253906\n",
      "epoch 81, train_loss 1277.8779296875\n",
      "epoch 81, val_loss 455.4132080078125\n",
      "epoch 82, train_loss 1277.8778076171875\n",
      "epoch 82, val_loss 455.4130554199219\n",
      "epoch 83, train_loss 1277.87744140625\n",
      "epoch 83, val_loss 455.4129638671875\n",
      "epoch 84, train_loss 1277.8773193359375\n",
      "epoch 84, val_loss 455.412841796875\n",
      "epoch 85, train_loss 1277.877197265625\n",
      "epoch 85, val_loss 455.4127197265625\n",
      "epoch 86, train_loss 1277.8770751953125\n",
      "epoch 86, val_loss 455.41259765625\n",
      "epoch 87, train_loss 1277.8768310546875\n",
      "epoch 87, val_loss 455.4125061035156\n",
      "epoch 88, train_loss 1277.876708984375\n",
      "epoch 88, val_loss 455.41241455078125\n",
      "epoch 89, train_loss 1277.87646484375\n",
      "epoch 89, val_loss 455.4122619628906\n",
      "epoch 90, train_loss 1277.876220703125\n",
      "epoch 90, val_loss 455.41217041015625\n",
      "epoch 91, train_loss 1277.876220703125\n",
      "epoch 91, val_loss 455.4119873046875\n",
      "epoch 92, train_loss 1277.8760986328125\n",
      "epoch 92, val_loss 455.41192626953125\n",
      "epoch 93, train_loss 1277.8759765625\n",
      "epoch 93, val_loss 455.4118347167969\n",
      "epoch 94, train_loss 1277.875732421875\n",
      "epoch 94, val_loss 455.4117126464844\n",
      "epoch 95, train_loss 1277.8756103515625\n",
      "epoch 95, val_loss 455.4115905761719\n",
      "epoch 96, train_loss 1277.8753662109375\n",
      "epoch 96, val_loss 455.4114685058594\n",
      "epoch 97, train_loss 1277.8753662109375\n",
      "epoch 97, val_loss 455.411376953125\n",
      "epoch 98, train_loss 1277.8751220703125\n",
      "epoch 98, val_loss 455.4112548828125\n",
      "epoch 99, train_loss 1277.875\n",
      "epoch 99, val_loss 455.4111328125\n",
      "Parameter containing:\n",
      "tensor([3.3685e-11], requires_grad=True)\n",
      "iter 51, train_loss_regularization 0.9040155410766602\n",
      "iter 51, val_loss_regularization 0.9040155410766602\n",
      "epoch 0, train_loss 1277.8746337890625\n",
      "epoch 0, val_loss 455.41107177734375\n",
      "epoch 1, train_loss 1277.874755859375\n",
      "epoch 1, val_loss 455.4109191894531\n",
      "epoch 2, train_loss 1277.87451171875\n",
      "epoch 2, val_loss 455.4107360839844\n",
      "epoch 3, train_loss 1277.8743896484375\n",
      "epoch 3, val_loss 455.4106750488281\n",
      "epoch 4, train_loss 1277.8741455078125\n",
      "epoch 4, val_loss 455.41058349609375\n",
      "epoch 5, train_loss 1277.8741455078125\n",
      "epoch 5, val_loss 455.4104309082031\n",
      "epoch 6, train_loss 1277.873779296875\n",
      "epoch 6, val_loss 455.4103698730469\n",
      "epoch 7, train_loss 1277.873779296875\n",
      "epoch 7, val_loss 455.41021728515625\n",
      "epoch 8, train_loss 1277.87353515625\n",
      "epoch 8, val_loss 455.4101257324219\n",
      "epoch 9, train_loss 1277.873291015625\n",
      "epoch 9, val_loss 455.40997314453125\n",
      "epoch 10, train_loss 1277.8731689453125\n",
      "epoch 10, val_loss 455.409912109375\n",
      "epoch 11, train_loss 1277.873046875\n",
      "epoch 11, val_loss 455.4097595214844\n",
      "epoch 12, train_loss 1277.872802734375\n",
      "epoch 12, val_loss 455.4096374511719\n",
      "epoch 13, train_loss 1277.872802734375\n",
      "epoch 13, val_loss 455.4095458984375\n",
      "epoch 14, train_loss 1277.8724365234375\n",
      "epoch 14, val_loss 455.409423828125\n",
      "epoch 15, train_loss 1277.872314453125\n",
      "epoch 15, val_loss 455.4093322753906\n",
      "epoch 16, train_loss 1277.8721923828125\n",
      "epoch 16, val_loss 455.4092102050781\n",
      "epoch 17, train_loss 1277.8719482421875\n",
      "epoch 17, val_loss 455.4090881347656\n",
      "epoch 18, train_loss 1277.8719482421875\n",
      "epoch 18, val_loss 455.4089660644531\n",
      "epoch 19, train_loss 1277.8717041015625\n",
      "epoch 19, val_loss 455.4088439941406\n",
      "epoch 20, train_loss 1277.8714599609375\n",
      "epoch 20, val_loss 455.4087829589844\n",
      "epoch 21, train_loss 1277.8714599609375\n",
      "epoch 21, val_loss 455.40863037109375\n",
      "epoch 22, train_loss 1277.871337890625\n",
      "epoch 22, val_loss 455.4085388183594\n",
      "epoch 23, train_loss 1277.8712158203125\n",
      "epoch 23, val_loss 455.4084167480469\n",
      "epoch 24, train_loss 1277.8709716796875\n",
      "epoch 24, val_loss 455.4083251953125\n",
      "epoch 25, train_loss 1277.8707275390625\n",
      "epoch 25, val_loss 455.408203125\n",
      "epoch 26, train_loss 1277.8707275390625\n",
      "epoch 26, val_loss 455.4080810546875\n",
      "epoch 27, train_loss 1277.8704833984375\n",
      "epoch 27, val_loss 455.407958984375\n",
      "epoch 28, train_loss 1277.870361328125\n",
      "epoch 28, val_loss 455.4078369140625\n",
      "epoch 29, train_loss 1277.8701171875\n",
      "epoch 29, val_loss 455.4077453613281\n",
      "epoch 30, train_loss 1277.8699951171875\n",
      "epoch 30, val_loss 455.4076232910156\n",
      "epoch 31, train_loss 1277.869873046875\n",
      "epoch 31, val_loss 455.4075012207031\n",
      "epoch 32, train_loss 1277.869873046875\n",
      "epoch 32, val_loss 455.40740966796875\n",
      "epoch 33, train_loss 1277.86962890625\n",
      "epoch 33, val_loss 455.40728759765625\n",
      "epoch 34, train_loss 1277.869384765625\n",
      "epoch 34, val_loss 455.4071960449219\n",
      "epoch 35, train_loss 1277.869384765625\n",
      "epoch 35, val_loss 455.4070739746094\n",
      "epoch 36, train_loss 1277.869140625\n",
      "epoch 36, val_loss 455.4069519042969\n",
      "epoch 37, train_loss 1277.8687744140625\n",
      "epoch 37, val_loss 455.4068298339844\n",
      "epoch 38, train_loss 1277.8687744140625\n",
      "epoch 38, val_loss 455.40673828125\n",
      "epoch 39, train_loss 1277.8685302734375\n",
      "epoch 39, val_loss 455.4066162109375\n",
      "epoch 40, train_loss 1277.868408203125\n",
      "epoch 40, val_loss 455.4064636230469\n",
      "epoch 41, train_loss 1277.8681640625\n",
      "epoch 41, val_loss 455.4063720703125\n",
      "epoch 42, train_loss 1277.8680419921875\n",
      "epoch 42, val_loss 455.4062805175781\n",
      "epoch 43, train_loss 1277.8680419921875\n",
      "epoch 43, val_loss 455.4062194824219\n",
      "epoch 44, train_loss 1277.86767578125\n",
      "epoch 44, val_loss 455.4060363769531\n",
      "epoch 45, train_loss 1277.8675537109375\n",
      "epoch 45, val_loss 455.4059143066406\n",
      "epoch 46, train_loss 1277.8675537109375\n",
      "epoch 46, val_loss 455.4058837890625\n",
      "epoch 47, train_loss 1277.8673095703125\n",
      "epoch 47, val_loss 455.40576171875\n",
      "epoch 48, train_loss 1277.8671875\n",
      "epoch 48, val_loss 455.4056701660156\n",
      "epoch 49, train_loss 1277.866943359375\n",
      "epoch 49, val_loss 455.4054260253906\n",
      "epoch 50, train_loss 1277.8668212890625\n",
      "epoch 50, val_loss 455.4054260253906\n",
      "epoch 51, train_loss 1277.86669921875\n",
      "epoch 51, val_loss 455.4053039550781\n",
      "epoch 52, train_loss 1277.866455078125\n",
      "epoch 52, val_loss 455.4051208496094\n",
      "epoch 53, train_loss 1277.866455078125\n",
      "epoch 53, val_loss 455.40509033203125\n",
      "epoch 54, train_loss 1277.8660888671875\n",
      "epoch 54, val_loss 455.4048767089844\n",
      "epoch 55, train_loss 1277.865966796875\n",
      "epoch 55, val_loss 455.40484619140625\n",
      "epoch 56, train_loss 1277.865966796875\n",
      "epoch 56, val_loss 455.4046936035156\n",
      "epoch 57, train_loss 1277.8658447265625\n",
      "epoch 57, val_loss 455.4046630859375\n",
      "epoch 58, train_loss 1277.8656005859375\n",
      "epoch 58, val_loss 455.4044494628906\n",
      "epoch 59, train_loss 1277.8656005859375\n",
      "epoch 59, val_loss 455.4043884277344\n",
      "epoch 60, train_loss 1277.8651123046875\n",
      "epoch 60, val_loss 455.40423583984375\n",
      "epoch 61, train_loss 1277.8651123046875\n",
      "epoch 61, val_loss 455.4041748046875\n",
      "epoch 62, train_loss 1277.8648681640625\n",
      "epoch 62, val_loss 455.4040832519531\n",
      "epoch 63, train_loss 1277.8646240234375\n",
      "epoch 63, val_loss 455.4039306640625\n",
      "epoch 64, train_loss 1277.8643798828125\n",
      "epoch 64, val_loss 455.4038391113281\n",
      "epoch 65, train_loss 1277.8643798828125\n",
      "epoch 65, val_loss 455.4037170410156\n",
      "epoch 66, train_loss 1277.8641357421875\n",
      "epoch 66, val_loss 455.40362548828125\n",
      "epoch 67, train_loss 1277.8641357421875\n",
      "epoch 67, val_loss 455.4034729003906\n",
      "epoch 68, train_loss 1277.8638916015625\n",
      "epoch 68, val_loss 455.4033203125\n",
      "epoch 69, train_loss 1277.8636474609375\n",
      "epoch 69, val_loss 455.4032897949219\n",
      "epoch 70, train_loss 1277.8636474609375\n",
      "epoch 70, val_loss 455.40313720703125\n",
      "epoch 71, train_loss 1277.8634033203125\n",
      "epoch 71, val_loss 455.4030456542969\n",
      "epoch 72, train_loss 1277.86328125\n",
      "epoch 72, val_loss 455.4028625488281\n",
      "epoch 73, train_loss 1277.86328125\n",
      "epoch 73, val_loss 455.4028015136719\n",
      "epoch 74, train_loss 1277.8629150390625\n",
      "epoch 74, val_loss 455.4027099609375\n",
      "epoch 75, train_loss 1277.86279296875\n",
      "epoch 75, val_loss 455.402587890625\n",
      "epoch 76, train_loss 1277.8626708984375\n",
      "epoch 76, val_loss 455.4024963378906\n",
      "epoch 77, train_loss 1277.862548828125\n",
      "epoch 77, val_loss 455.40234375\n",
      "epoch 78, train_loss 1277.8624267578125\n",
      "epoch 78, val_loss 455.4022521972656\n",
      "epoch 79, train_loss 1277.8621826171875\n",
      "epoch 79, val_loss 455.40216064453125\n",
      "epoch 80, train_loss 1277.862060546875\n",
      "epoch 80, val_loss 455.40203857421875\n",
      "epoch 81, train_loss 1277.86181640625\n",
      "epoch 81, val_loss 455.40191650390625\n",
      "epoch 82, train_loss 1277.8616943359375\n",
      "epoch 82, val_loss 455.4017639160156\n",
      "epoch 83, train_loss 1277.8614501953125\n",
      "epoch 83, val_loss 455.4017028808594\n",
      "epoch 84, train_loss 1277.8614501953125\n",
      "epoch 84, val_loss 455.4015808105469\n",
      "epoch 85, train_loss 1277.861328125\n",
      "epoch 85, val_loss 455.40142822265625\n",
      "epoch 86, train_loss 1277.861083984375\n",
      "epoch 86, val_loss 455.4013366699219\n",
      "epoch 87, train_loss 1277.861083984375\n",
      "epoch 87, val_loss 455.4012451171875\n",
      "epoch 88, train_loss 1277.86083984375\n",
      "epoch 88, val_loss 455.4011535644531\n",
      "epoch 89, train_loss 1277.860595703125\n",
      "epoch 89, val_loss 455.4010314941406\n",
      "epoch 90, train_loss 1277.860595703125\n",
      "epoch 90, val_loss 455.40087890625\n",
      "epoch 91, train_loss 1277.8602294921875\n",
      "epoch 91, val_loss 455.4007873535156\n",
      "epoch 92, train_loss 1277.860107421875\n",
      "epoch 92, val_loss 455.4007568359375\n",
      "epoch 93, train_loss 1277.8599853515625\n",
      "epoch 93, val_loss 455.400634765625\n",
      "epoch 94, train_loss 1277.8597412109375\n",
      "epoch 94, val_loss 455.4005126953125\n",
      "epoch 95, train_loss 1277.8594970703125\n",
      "epoch 95, val_loss 455.40032958984375\n",
      "epoch 96, train_loss 1277.8594970703125\n",
      "epoch 96, val_loss 455.40020751953125\n",
      "epoch 97, train_loss 1277.859375\n",
      "epoch 97, val_loss 455.4001770019531\n",
      "epoch 98, train_loss 1277.859130859375\n",
      "epoch 98, val_loss 455.4000549316406\n",
      "epoch 99, train_loss 1277.8590087890625\n",
      "epoch 99, val_loss 455.3998718261719\n",
      "Parameter containing:\n",
      "tensor([2.2012e-11], requires_grad=True)\n",
      "iter 52, train_loss_regularization 0.8995248079299927\n",
      "iter 52, val_loss_regularization 0.8995248079299927\n",
      "epoch 0, train_loss 1277.85888671875\n",
      "epoch 0, val_loss 455.3997497558594\n",
      "epoch 1, train_loss 1277.85888671875\n",
      "epoch 1, val_loss 455.3996276855469\n",
      "epoch 2, train_loss 1277.858642578125\n",
      "epoch 2, val_loss 455.39959716796875\n",
      "epoch 3, train_loss 1277.8583984375\n",
      "epoch 3, val_loss 455.3994140625\n",
      "epoch 4, train_loss 1277.8582763671875\n",
      "epoch 4, val_loss 455.3992919921875\n",
      "epoch 5, train_loss 1277.8580322265625\n",
      "epoch 5, val_loss 455.399169921875\n",
      "epoch 6, train_loss 1277.8577880859375\n",
      "epoch 6, val_loss 455.3990783691406\n",
      "epoch 7, train_loss 1277.8577880859375\n",
      "epoch 7, val_loss 455.3990478515625\n",
      "epoch 8, train_loss 1277.857666015625\n",
      "epoch 8, val_loss 455.3988342285156\n",
      "epoch 9, train_loss 1277.857421875\n",
      "epoch 9, val_loss 455.3988037109375\n",
      "epoch 10, train_loss 1277.8572998046875\n",
      "epoch 10, val_loss 455.398681640625\n",
      "epoch 11, train_loss 1277.8572998046875\n",
      "epoch 11, val_loss 455.3985900878906\n",
      "epoch 12, train_loss 1277.8570556640625\n",
      "epoch 12, val_loss 455.3984069824219\n",
      "epoch 13, train_loss 1277.8568115234375\n",
      "epoch 13, val_loss 455.3982849121094\n",
      "epoch 14, train_loss 1277.856689453125\n",
      "epoch 14, val_loss 455.398193359375\n",
      "epoch 15, train_loss 1277.8564453125\n",
      "epoch 15, val_loss 455.39813232421875\n",
      "epoch 16, train_loss 1277.8563232421875\n",
      "epoch 16, val_loss 455.39801025390625\n",
      "epoch 17, train_loss 1277.8563232421875\n",
      "epoch 17, val_loss 455.39788818359375\n",
      "epoch 18, train_loss 1277.85595703125\n",
      "epoch 18, val_loss 455.3977355957031\n",
      "epoch 19, train_loss 1277.85595703125\n",
      "epoch 19, val_loss 455.3976745605469\n",
      "epoch 20, train_loss 1277.855712890625\n",
      "epoch 20, val_loss 455.3975524902344\n",
      "epoch 21, train_loss 1277.85546875\n",
      "epoch 21, val_loss 455.3974914550781\n",
      "epoch 22, train_loss 1277.85546875\n",
      "epoch 22, val_loss 455.39727783203125\n",
      "epoch 23, train_loss 1277.8553466796875\n",
      "epoch 23, val_loss 455.397216796875\n",
      "epoch 24, train_loss 1277.8551025390625\n",
      "epoch 24, val_loss 455.3971252441406\n",
      "epoch 25, train_loss 1277.8548583984375\n",
      "epoch 25, val_loss 455.39703369140625\n",
      "epoch 26, train_loss 1277.8548583984375\n",
      "epoch 26, val_loss 455.3968811035156\n",
      "epoch 27, train_loss 1277.8546142578125\n",
      "epoch 27, val_loss 455.3967590332031\n",
      "epoch 28, train_loss 1277.8546142578125\n",
      "epoch 28, val_loss 455.39666748046875\n",
      "epoch 29, train_loss 1277.8543701171875\n",
      "epoch 29, val_loss 455.39654541015625\n",
      "epoch 30, train_loss 1277.8541259765625\n",
      "epoch 30, val_loss 455.3964538574219\n",
      "epoch 31, train_loss 1277.8538818359375\n",
      "epoch 31, val_loss 455.3963317871094\n",
      "epoch 32, train_loss 1277.8536376953125\n",
      "epoch 32, val_loss 455.39617919921875\n",
      "epoch 33, train_loss 1277.8536376953125\n",
      "epoch 33, val_loss 455.3961181640625\n",
      "epoch 34, train_loss 1277.8536376953125\n",
      "epoch 34, val_loss 455.39599609375\n",
      "epoch 35, train_loss 1277.8533935546875\n",
      "epoch 35, val_loss 455.3959045410156\n",
      "epoch 36, train_loss 1277.8531494140625\n",
      "epoch 36, val_loss 455.395751953125\n",
      "epoch 37, train_loss 1277.85302734375\n",
      "epoch 37, val_loss 455.3956604003906\n",
      "epoch 38, train_loss 1277.85302734375\n",
      "epoch 38, val_loss 455.3955383300781\n",
      "epoch 39, train_loss 1277.8526611328125\n",
      "epoch 39, val_loss 455.39544677734375\n",
      "epoch 40, train_loss 1277.8526611328125\n",
      "epoch 40, val_loss 455.3952941894531\n",
      "epoch 41, train_loss 1277.8524169921875\n",
      "epoch 41, val_loss 455.3951721191406\n",
      "epoch 42, train_loss 1277.852294921875\n",
      "epoch 42, val_loss 455.39508056640625\n",
      "epoch 43, train_loss 1277.852294921875\n",
      "epoch 43, val_loss 455.3949890136719\n",
      "epoch 44, train_loss 1277.8519287109375\n",
      "epoch 44, val_loss 455.3948669433594\n",
      "epoch 45, train_loss 1277.851806640625\n",
      "epoch 45, val_loss 455.3947448730469\n",
      "epoch 46, train_loss 1277.8516845703125\n",
      "epoch 46, val_loss 455.3946533203125\n",
      "epoch 47, train_loss 1277.8515625\n",
      "epoch 47, val_loss 455.3945007324219\n",
      "epoch 48, train_loss 1277.8514404296875\n",
      "epoch 48, val_loss 455.3944091796875\n",
      "epoch 49, train_loss 1277.8511962890625\n",
      "epoch 49, val_loss 455.39434814453125\n",
      "epoch 50, train_loss 1277.85107421875\n",
      "epoch 50, val_loss 455.3941345214844\n",
      "epoch 51, train_loss 1277.8509521484375\n",
      "epoch 51, val_loss 455.3940734863281\n",
      "epoch 52, train_loss 1277.8505859375\n",
      "epoch 52, val_loss 455.3940124511719\n",
      "epoch 53, train_loss 1277.8504638671875\n",
      "epoch 53, val_loss 455.3939514160156\n",
      "epoch 54, train_loss 1277.850341796875\n",
      "epoch 54, val_loss 455.39373779296875\n",
      "epoch 55, train_loss 1277.8502197265625\n",
      "epoch 55, val_loss 455.39361572265625\n",
      "epoch 56, train_loss 1277.8499755859375\n",
      "epoch 56, val_loss 455.3935546875\n",
      "epoch 57, train_loss 1277.849853515625\n",
      "epoch 57, val_loss 455.3934631347656\n",
      "epoch 58, train_loss 1277.849609375\n",
      "epoch 58, val_loss 455.39337158203125\n",
      "epoch 59, train_loss 1277.849609375\n",
      "epoch 59, val_loss 455.3931579589844\n",
      "epoch 60, train_loss 1277.8494873046875\n",
      "epoch 60, val_loss 455.39312744140625\n",
      "epoch 61, train_loss 1277.849365234375\n",
      "epoch 61, val_loss 455.39300537109375\n",
      "epoch 62, train_loss 1277.8492431640625\n",
      "epoch 62, val_loss 455.39288330078125\n",
      "epoch 63, train_loss 1277.848876953125\n",
      "epoch 63, val_loss 455.3927917480469\n",
      "epoch 64, train_loss 1277.848876953125\n",
      "epoch 64, val_loss 455.39263916015625\n",
      "epoch 65, train_loss 1277.8486328125\n",
      "epoch 65, val_loss 455.3925476074219\n",
      "epoch 66, train_loss 1277.8485107421875\n",
      "epoch 66, val_loss 455.3924560546875\n",
      "epoch 67, train_loss 1277.848388671875\n",
      "epoch 67, val_loss 455.392333984375\n",
      "epoch 68, train_loss 1277.848388671875\n",
      "epoch 68, val_loss 455.3921813964844\n",
      "epoch 69, train_loss 1277.8480224609375\n",
      "epoch 69, val_loss 455.39208984375\n",
      "epoch 70, train_loss 1277.847900390625\n",
      "epoch 70, val_loss 455.3919677734375\n",
      "epoch 71, train_loss 1277.84765625\n",
      "epoch 71, val_loss 455.3918762207031\n",
      "epoch 72, train_loss 1277.8477783203125\n",
      "epoch 72, val_loss 455.3917541503906\n",
      "epoch 73, train_loss 1277.847412109375\n",
      "epoch 73, val_loss 455.3916320800781\n",
      "epoch 74, train_loss 1277.8472900390625\n",
      "epoch 74, val_loss 455.39154052734375\n",
      "epoch 75, train_loss 1277.8470458984375\n",
      "epoch 75, val_loss 455.39141845703125\n",
      "epoch 76, train_loss 1277.8470458984375\n",
      "epoch 76, val_loss 455.3913269042969\n",
      "epoch 77, train_loss 1277.8468017578125\n",
      "epoch 77, val_loss 455.39117431640625\n",
      "epoch 78, train_loss 1277.8468017578125\n",
      "epoch 78, val_loss 455.3910827636719\n",
      "epoch 79, train_loss 1277.8465576171875\n",
      "epoch 79, val_loss 455.3909912109375\n",
      "epoch 80, train_loss 1277.84619140625\n",
      "epoch 80, val_loss 455.390869140625\n",
      "epoch 81, train_loss 1277.84619140625\n",
      "epoch 81, val_loss 455.3907775878906\n",
      "epoch 82, train_loss 1277.84619140625\n",
      "epoch 82, val_loss 455.3906555175781\n",
      "epoch 83, train_loss 1277.8458251953125\n",
      "epoch 83, val_loss 455.3905334472656\n",
      "epoch 84, train_loss 1277.845703125\n",
      "epoch 84, val_loss 455.3904724121094\n",
      "epoch 85, train_loss 1277.845703125\n",
      "epoch 85, val_loss 455.39031982421875\n",
      "epoch 86, train_loss 1277.845458984375\n",
      "epoch 86, val_loss 455.39019775390625\n",
      "epoch 87, train_loss 1277.84521484375\n",
      "epoch 87, val_loss 455.39013671875\n",
      "epoch 88, train_loss 1277.844970703125\n",
      "epoch 88, val_loss 455.3900146484375\n",
      "epoch 89, train_loss 1277.844970703125\n",
      "epoch 89, val_loss 455.38983154296875\n",
      "epoch 90, train_loss 1277.8447265625\n",
      "epoch 90, val_loss 455.3898010253906\n",
      "epoch 91, train_loss 1277.8446044921875\n",
      "epoch 91, val_loss 455.3896789550781\n",
      "epoch 92, train_loss 1277.844482421875\n",
      "epoch 92, val_loss 455.3895568847656\n",
      "epoch 93, train_loss 1277.8443603515625\n",
      "epoch 93, val_loss 455.38946533203125\n",
      "epoch 94, train_loss 1277.8441162109375\n",
      "epoch 94, val_loss 455.3893737792969\n",
      "epoch 95, train_loss 1277.843994140625\n",
      "epoch 95, val_loss 455.38922119140625\n",
      "epoch 96, train_loss 1277.8438720703125\n",
      "epoch 96, val_loss 455.3890686035156\n",
      "epoch 97, train_loss 1277.8436279296875\n",
      "epoch 97, val_loss 455.3890075683594\n",
      "epoch 98, train_loss 1277.8436279296875\n",
      "epoch 98, val_loss 455.388916015625\n",
      "epoch 99, train_loss 1277.843505859375\n",
      "epoch 99, val_loss 455.3887939453125\n",
      "Parameter containing:\n",
      "tensor([1.4405e-11], requires_grad=True)\n",
      "iter 53, train_loss_regularization 0.8951041102409363\n",
      "iter 53, val_loss_regularization 0.8951041102409363\n",
      "epoch 0, train_loss 1277.84326171875\n",
      "epoch 0, val_loss 455.3887023925781\n",
      "epoch 1, train_loss 1277.8431396484375\n",
      "epoch 1, val_loss 455.3885803222656\n",
      "epoch 2, train_loss 1277.843017578125\n",
      "epoch 2, val_loss 455.38848876953125\n",
      "epoch 3, train_loss 1277.8427734375\n",
      "epoch 3, val_loss 455.3883361816406\n",
      "epoch 4, train_loss 1277.842529296875\n",
      "epoch 4, val_loss 455.3883361816406\n",
      "epoch 5, train_loss 1277.842529296875\n",
      "epoch 5, val_loss 455.3881530761719\n",
      "epoch 6, train_loss 1277.84228515625\n",
      "epoch 6, val_loss 455.38800048828125\n",
      "epoch 7, train_loss 1277.8421630859375\n",
      "epoch 7, val_loss 455.3879089355469\n",
      "epoch 8, train_loss 1277.8419189453125\n",
      "epoch 8, val_loss 455.3877868652344\n",
      "epoch 9, train_loss 1277.841796875\n",
      "epoch 9, val_loss 455.3876647949219\n",
      "epoch 10, train_loss 1277.8416748046875\n",
      "epoch 10, val_loss 455.3875732421875\n",
      "epoch 11, train_loss 1277.841552734375\n",
      "epoch 11, val_loss 455.38751220703125\n",
      "epoch 12, train_loss 1277.841552734375\n",
      "epoch 12, val_loss 455.3873291015625\n",
      "epoch 13, train_loss 1277.84130859375\n",
      "epoch 13, val_loss 455.3871765136719\n",
      "epoch 14, train_loss 1277.8409423828125\n",
      "epoch 14, val_loss 455.3871765136719\n",
      "epoch 15, train_loss 1277.8409423828125\n",
      "epoch 15, val_loss 455.3869934082031\n",
      "epoch 16, train_loss 1277.8408203125\n",
      "epoch 16, val_loss 455.38690185546875\n",
      "epoch 17, train_loss 1277.8404541015625\n",
      "epoch 17, val_loss 455.38677978515625\n",
      "epoch 18, train_loss 1277.8404541015625\n",
      "epoch 18, val_loss 455.38671875\n",
      "epoch 19, train_loss 1277.8402099609375\n",
      "epoch 19, val_loss 455.3866271972656\n",
      "epoch 20, train_loss 1277.840087890625\n",
      "epoch 20, val_loss 455.3864440917969\n",
      "epoch 21, train_loss 1277.8399658203125\n",
      "epoch 21, val_loss 455.3863830566406\n",
      "epoch 22, train_loss 1277.8399658203125\n",
      "epoch 22, val_loss 455.3862609863281\n",
      "epoch 23, train_loss 1277.8397216796875\n",
      "epoch 23, val_loss 455.38616943359375\n",
      "epoch 24, train_loss 1277.8394775390625\n",
      "epoch 24, val_loss 455.385986328125\n",
      "epoch 25, train_loss 1277.8394775390625\n",
      "epoch 25, val_loss 455.38592529296875\n",
      "epoch 26, train_loss 1277.839111328125\n",
      "epoch 26, val_loss 455.38580322265625\n",
      "epoch 27, train_loss 1277.8389892578125\n",
      "epoch 27, val_loss 455.3857116699219\n",
      "epoch 28, train_loss 1277.8389892578125\n",
      "epoch 28, val_loss 455.3856201171875\n",
      "epoch 29, train_loss 1277.8387451171875\n",
      "epoch 29, val_loss 455.3854064941406\n",
      "epoch 30, train_loss 1277.838623046875\n",
      "epoch 30, val_loss 455.3853759765625\n",
      "epoch 31, train_loss 1277.83837890625\n",
      "epoch 31, val_loss 455.38525390625\n",
      "epoch 32, train_loss 1277.8382568359375\n",
      "epoch 32, val_loss 455.3851623535156\n",
      "epoch 33, train_loss 1277.838134765625\n",
      "epoch 33, val_loss 455.385009765625\n",
      "epoch 34, train_loss 1277.837890625\n",
      "epoch 34, val_loss 455.38494873046875\n",
      "epoch 35, train_loss 1277.8377685546875\n",
      "epoch 35, val_loss 455.38482666015625\n",
      "epoch 36, train_loss 1277.837646484375\n",
      "epoch 36, val_loss 455.38470458984375\n",
      "epoch 37, train_loss 1277.8375244140625\n",
      "epoch 37, val_loss 455.3846130371094\n",
      "epoch 38, train_loss 1277.83740234375\n",
      "epoch 38, val_loss 455.3844909667969\n",
      "epoch 39, train_loss 1277.8372802734375\n",
      "epoch 39, val_loss 455.3843688964844\n",
      "epoch 40, train_loss 1277.837158203125\n",
      "epoch 40, val_loss 455.3842468261719\n",
      "epoch 41, train_loss 1277.8369140625\n",
      "epoch 41, val_loss 455.3841552734375\n",
      "epoch 42, train_loss 1277.8367919921875\n",
      "epoch 42, val_loss 455.38409423828125\n",
      "epoch 43, train_loss 1277.8365478515625\n",
      "epoch 43, val_loss 455.38397216796875\n",
      "epoch 44, train_loss 1277.83642578125\n",
      "epoch 44, val_loss 455.3838806152344\n",
      "epoch 45, train_loss 1277.8363037109375\n",
      "epoch 45, val_loss 455.3836975097656\n",
      "epoch 46, train_loss 1277.8360595703125\n",
      "epoch 46, val_loss 455.3836364746094\n",
      "epoch 47, train_loss 1277.8360595703125\n",
      "epoch 47, val_loss 455.3835144042969\n",
      "epoch 48, train_loss 1277.835693359375\n",
      "epoch 48, val_loss 455.3834228515625\n",
      "epoch 49, train_loss 1277.8355712890625\n",
      "epoch 49, val_loss 455.38330078125\n",
      "epoch 50, train_loss 1277.83544921875\n",
      "epoch 50, val_loss 455.3831787109375\n",
      "epoch 51, train_loss 1277.8353271484375\n",
      "epoch 51, val_loss 455.3830871582031\n",
      "epoch 52, train_loss 1277.835205078125\n",
      "epoch 52, val_loss 455.3829650878906\n",
      "epoch 53, train_loss 1277.8350830078125\n",
      "epoch 53, val_loss 455.3827819824219\n",
      "epoch 54, train_loss 1277.834716796875\n",
      "epoch 54, val_loss 455.3827209472656\n",
      "epoch 55, train_loss 1277.834716796875\n",
      "epoch 55, val_loss 455.3826599121094\n",
      "epoch 56, train_loss 1277.83447265625\n",
      "epoch 56, val_loss 455.38250732421875\n",
      "epoch 57, train_loss 1277.83447265625\n",
      "epoch 57, val_loss 455.3824462890625\n",
      "epoch 58, train_loss 1277.8341064453125\n",
      "epoch 58, val_loss 455.38232421875\n",
      "epoch 59, train_loss 1277.8341064453125\n",
      "epoch 59, val_loss 455.3821716308594\n",
      "epoch 60, train_loss 1277.8338623046875\n",
      "epoch 60, val_loss 455.3821105957031\n",
      "epoch 61, train_loss 1277.833740234375\n",
      "epoch 61, val_loss 455.3819580078125\n",
      "epoch 62, train_loss 1277.833740234375\n",
      "epoch 62, val_loss 455.3818359375\n",
      "epoch 63, train_loss 1277.8336181640625\n",
      "epoch 63, val_loss 455.3817443847656\n",
      "epoch 64, train_loss 1277.833251953125\n",
      "epoch 64, val_loss 455.3816223144531\n",
      "epoch 65, train_loss 1277.8331298828125\n",
      "epoch 65, val_loss 455.38153076171875\n",
      "epoch 66, train_loss 1277.8331298828125\n",
      "epoch 66, val_loss 455.3814697265625\n",
      "epoch 67, train_loss 1277.8328857421875\n",
      "epoch 67, val_loss 455.38134765625\n",
      "epoch 68, train_loss 1277.8326416015625\n",
      "epoch 68, val_loss 455.38116455078125\n",
      "epoch 69, train_loss 1277.8326416015625\n",
      "epoch 69, val_loss 455.3810729980469\n",
      "epoch 70, train_loss 1277.8323974609375\n",
      "epoch 70, val_loss 455.3810119628906\n",
      "epoch 71, train_loss 1277.832275390625\n",
      "epoch 71, val_loss 455.38092041015625\n",
      "epoch 72, train_loss 1277.8321533203125\n",
      "epoch 72, val_loss 455.3807373046875\n",
      "epoch 73, train_loss 1277.83203125\n",
      "epoch 73, val_loss 455.380615234375\n",
      "epoch 74, train_loss 1277.831787109375\n",
      "epoch 74, val_loss 455.38055419921875\n",
      "epoch 75, train_loss 1277.83154296875\n",
      "epoch 75, val_loss 455.3804016113281\n",
      "epoch 76, train_loss 1277.83154296875\n",
      "epoch 76, val_loss 455.3802795410156\n",
      "epoch 77, train_loss 1277.8314208984375\n",
      "epoch 77, val_loss 455.3802185058594\n",
      "epoch 78, train_loss 1277.831298828125\n",
      "epoch 78, val_loss 455.3800964355469\n",
      "epoch 79, train_loss 1277.8310546875\n",
      "epoch 79, val_loss 455.37994384765625\n",
      "epoch 80, train_loss 1277.8310546875\n",
      "epoch 80, val_loss 455.37994384765625\n",
      "epoch 81, train_loss 1277.830810546875\n",
      "epoch 81, val_loss 455.37982177734375\n",
      "epoch 82, train_loss 1277.8304443359375\n",
      "epoch 82, val_loss 455.379638671875\n",
      "epoch 83, train_loss 1277.8304443359375\n",
      "epoch 83, val_loss 455.3795471191406\n",
      "epoch 84, train_loss 1277.8302001953125\n",
      "epoch 84, val_loss 455.3794250488281\n",
      "epoch 85, train_loss 1277.8299560546875\n",
      "epoch 85, val_loss 455.37933349609375\n",
      "epoch 86, train_loss 1277.8299560546875\n",
      "epoch 86, val_loss 455.3792419433594\n",
      "epoch 87, train_loss 1277.829833984375\n",
      "epoch 87, val_loss 455.37908935546875\n",
      "epoch 88, train_loss 1277.82958984375\n",
      "epoch 88, val_loss 455.3789978027344\n",
      "epoch 89, train_loss 1277.8294677734375\n",
      "epoch 89, val_loss 455.3788757324219\n",
      "epoch 90, train_loss 1277.8292236328125\n",
      "epoch 90, val_loss 455.3787841796875\n",
      "epoch 91, train_loss 1277.8292236328125\n",
      "epoch 91, val_loss 455.378662109375\n",
      "epoch 92, train_loss 1277.8291015625\n",
      "epoch 92, val_loss 455.3785705566406\n",
      "epoch 93, train_loss 1277.828857421875\n",
      "epoch 93, val_loss 455.3784484863281\n",
      "epoch 94, train_loss 1277.8287353515625\n",
      "epoch 94, val_loss 455.3783874511719\n",
      "epoch 95, train_loss 1277.82861328125\n",
      "epoch 95, val_loss 455.3782958984375\n",
      "epoch 96, train_loss 1277.828369140625\n",
      "epoch 96, val_loss 455.3780822753906\n",
      "epoch 97, train_loss 1277.8282470703125\n",
      "epoch 97, val_loss 455.37799072265625\n",
      "epoch 98, train_loss 1277.828125\n",
      "epoch 98, val_loss 455.3779296875\n",
      "epoch 99, train_loss 1277.827880859375\n",
      "epoch 99, val_loss 455.3778381347656\n",
      "Parameter containing:\n",
      "tensor([9.4390e-12], requires_grad=True)\n",
      "iter 54, train_loss_regularization 0.8907577991485596\n",
      "iter 54, val_loss_regularization 0.8907577991485596\n",
      "epoch 0, train_loss 1277.827880859375\n",
      "epoch 0, val_loss 455.37774658203125\n",
      "epoch 1, train_loss 1277.82763671875\n",
      "epoch 1, val_loss 455.3775939941406\n",
      "epoch 2, train_loss 1277.8275146484375\n",
      "epoch 2, val_loss 455.3774719238281\n",
      "epoch 3, train_loss 1277.8275146484375\n",
      "epoch 3, val_loss 455.3774108886719\n",
      "epoch 4, train_loss 1277.8271484375\n",
      "epoch 4, val_loss 455.37725830078125\n",
      "epoch 5, train_loss 1277.8270263671875\n",
      "epoch 5, val_loss 455.377197265625\n",
      "epoch 6, train_loss 1277.8270263671875\n",
      "epoch 6, val_loss 455.3770446777344\n",
      "epoch 7, train_loss 1277.8267822265625\n",
      "epoch 7, val_loss 455.3769226074219\n",
      "epoch 8, train_loss 1277.8265380859375\n",
      "epoch 8, val_loss 455.3768310546875\n",
      "epoch 9, train_loss 1277.826416015625\n",
      "epoch 9, val_loss 455.376708984375\n",
      "epoch 10, train_loss 1277.8262939453125\n",
      "epoch 10, val_loss 455.3765869140625\n",
      "epoch 11, train_loss 1277.826171875\n",
      "epoch 11, val_loss 455.37646484375\n",
      "epoch 12, train_loss 1277.8260498046875\n",
      "epoch 12, val_loss 455.3763732910156\n",
      "epoch 13, train_loss 1277.8258056640625\n",
      "epoch 13, val_loss 455.37628173828125\n",
      "epoch 14, train_loss 1277.82568359375\n",
      "epoch 14, val_loss 455.37615966796875\n",
      "epoch 15, train_loss 1277.8258056640625\n",
      "epoch 15, val_loss 455.3760681152344\n",
      "epoch 16, train_loss 1277.8253173828125\n",
      "epoch 16, val_loss 455.3758850097656\n",
      "epoch 17, train_loss 1277.8251953125\n",
      "epoch 17, val_loss 455.3758239746094\n",
      "epoch 18, train_loss 1277.8251953125\n",
      "epoch 18, val_loss 455.3757629394531\n",
      "epoch 19, train_loss 1277.824951171875\n",
      "epoch 19, val_loss 455.37567138671875\n",
      "epoch 20, train_loss 1277.82470703125\n",
      "epoch 20, val_loss 455.37548828125\n",
      "epoch 21, train_loss 1277.82470703125\n",
      "epoch 21, val_loss 455.37542724609375\n",
      "epoch 22, train_loss 1277.824462890625\n",
      "epoch 22, val_loss 455.37530517578125\n",
      "epoch 23, train_loss 1277.82421875\n",
      "epoch 23, val_loss 455.3752136230469\n",
      "epoch 24, train_loss 1277.82421875\n",
      "epoch 24, val_loss 455.3751220703125\n",
      "epoch 25, train_loss 1277.8240966796875\n",
      "epoch 25, val_loss 455.3749084472656\n",
      "epoch 26, train_loss 1277.8238525390625\n",
      "epoch 26, val_loss 455.3748779296875\n",
      "epoch 27, train_loss 1277.8236083984375\n",
      "epoch 27, val_loss 455.374755859375\n",
      "epoch 28, train_loss 1277.8236083984375\n",
      "epoch 28, val_loss 455.37469482421875\n",
      "epoch 29, train_loss 1277.8233642578125\n",
      "epoch 29, val_loss 455.37457275390625\n",
      "epoch 30, train_loss 1277.8232421875\n",
      "epoch 30, val_loss 455.3743896484375\n",
      "epoch 31, train_loss 1277.8231201171875\n",
      "epoch 31, val_loss 455.37432861328125\n",
      "epoch 32, train_loss 1277.8228759765625\n",
      "epoch 32, val_loss 455.37420654296875\n",
      "epoch 33, train_loss 1277.82275390625\n",
      "epoch 33, val_loss 455.37408447265625\n",
      "epoch 34, train_loss 1277.8226318359375\n",
      "epoch 34, val_loss 455.37396240234375\n",
      "epoch 35, train_loss 1277.822509765625\n",
      "epoch 35, val_loss 455.3739318847656\n",
      "epoch 36, train_loss 1277.8223876953125\n",
      "epoch 36, val_loss 455.373779296875\n",
      "epoch 37, train_loss 1277.822265625\n",
      "epoch 37, val_loss 455.3736572265625\n",
      "epoch 38, train_loss 1277.8221435546875\n",
      "epoch 38, val_loss 455.37359619140625\n",
      "epoch 39, train_loss 1277.82177734375\n",
      "epoch 39, val_loss 455.3735046386719\n",
      "epoch 40, train_loss 1277.8216552734375\n",
      "epoch 40, val_loss 455.3733825683594\n",
      "epoch 41, train_loss 1277.8216552734375\n",
      "epoch 41, val_loss 455.3732604980469\n",
      "epoch 42, train_loss 1277.821533203125\n",
      "epoch 42, val_loss 455.3731384277344\n",
      "epoch 43, train_loss 1277.8211669921875\n",
      "epoch 43, val_loss 455.373046875\n",
      "epoch 44, train_loss 1277.8211669921875\n",
      "epoch 44, val_loss 455.3729248046875\n",
      "epoch 45, train_loss 1277.821044921875\n",
      "epoch 45, val_loss 455.372802734375\n",
      "epoch 46, train_loss 1277.82080078125\n",
      "epoch 46, val_loss 455.37274169921875\n",
      "epoch 47, train_loss 1277.8206787109375\n",
      "epoch 47, val_loss 455.3725280761719\n",
      "epoch 48, train_loss 1277.820556640625\n",
      "epoch 48, val_loss 455.3724670410156\n",
      "epoch 49, train_loss 1277.820556640625\n",
      "epoch 49, val_loss 455.3724060058594\n",
      "epoch 50, train_loss 1277.8201904296875\n",
      "epoch 50, val_loss 455.37225341796875\n",
      "epoch 51, train_loss 1277.8201904296875\n",
      "epoch 51, val_loss 455.3721618652344\n",
      "epoch 52, train_loss 1277.820068359375\n",
      "epoch 52, val_loss 455.3720397949219\n",
      "epoch 53, train_loss 1277.8197021484375\n",
      "epoch 53, val_loss 455.37200927734375\n",
      "epoch 54, train_loss 1277.8197021484375\n",
      "epoch 54, val_loss 455.371826171875\n",
      "epoch 55, train_loss 1277.8194580078125\n",
      "epoch 55, val_loss 455.3717041015625\n",
      "epoch 56, train_loss 1277.8194580078125\n",
      "epoch 56, val_loss 455.37158203125\n",
      "epoch 57, train_loss 1277.8192138671875\n",
      "epoch 57, val_loss 455.3714904785156\n",
      "epoch 58, train_loss 1277.8189697265625\n",
      "epoch 58, val_loss 455.3714294433594\n",
      "epoch 59, train_loss 1277.81884765625\n",
      "epoch 59, val_loss 455.37127685546875\n",
      "epoch 60, train_loss 1277.818603515625\n",
      "epoch 60, val_loss 455.3712158203125\n",
      "epoch 61, train_loss 1277.8184814453125\n",
      "epoch 61, val_loss 455.37109375\n",
      "epoch 62, train_loss 1277.818359375\n",
      "epoch 62, val_loss 455.3709716796875\n",
      "epoch 63, train_loss 1277.8182373046875\n",
      "epoch 63, val_loss 455.3708801269531\n",
      "epoch 64, train_loss 1277.8179931640625\n",
      "epoch 64, val_loss 455.3706970214844\n",
      "epoch 65, train_loss 1277.81787109375\n",
      "epoch 65, val_loss 455.3705749511719\n",
      "epoch 66, train_loss 1277.81787109375\n",
      "epoch 66, val_loss 455.37054443359375\n",
      "epoch 67, train_loss 1277.817626953125\n",
      "epoch 67, val_loss 455.37042236328125\n",
      "epoch 68, train_loss 1277.8173828125\n",
      "epoch 68, val_loss 455.37030029296875\n",
      "epoch 69, train_loss 1277.8173828125\n",
      "epoch 69, val_loss 455.3702392578125\n",
      "epoch 70, train_loss 1277.817138671875\n",
      "epoch 70, val_loss 455.37005615234375\n",
      "epoch 71, train_loss 1277.8170166015625\n",
      "epoch 71, val_loss 455.3699951171875\n",
      "epoch 72, train_loss 1277.8170166015625\n",
      "epoch 72, val_loss 455.369873046875\n",
      "epoch 73, train_loss 1277.816650390625\n",
      "epoch 73, val_loss 455.3698425292969\n",
      "epoch 74, train_loss 1277.816650390625\n",
      "epoch 74, val_loss 455.3696594238281\n",
      "epoch 75, train_loss 1277.8165283203125\n",
      "epoch 75, val_loss 455.3695373535156\n",
      "epoch 76, train_loss 1277.8162841796875\n",
      "epoch 76, val_loss 455.3695068359375\n",
      "epoch 77, train_loss 1277.816162109375\n",
      "epoch 77, val_loss 455.369384765625\n",
      "epoch 78, train_loss 1277.8160400390625\n",
      "epoch 78, val_loss 455.3692626953125\n",
      "epoch 79, train_loss 1277.8157958984375\n",
      "epoch 79, val_loss 455.36907958984375\n",
      "epoch 80, train_loss 1277.8157958984375\n",
      "epoch 80, val_loss 455.3689880371094\n",
      "epoch 81, train_loss 1277.815673828125\n",
      "epoch 81, val_loss 455.3689270019531\n",
      "epoch 82, train_loss 1277.8153076171875\n",
      "epoch 82, val_loss 455.3688049316406\n",
      "epoch 83, train_loss 1277.8153076171875\n",
      "epoch 83, val_loss 455.3687438964844\n",
      "epoch 84, train_loss 1277.8150634765625\n",
      "epoch 84, val_loss 455.3685302734375\n",
      "epoch 85, train_loss 1277.81494140625\n",
      "epoch 85, val_loss 455.3684997558594\n",
      "epoch 86, train_loss 1277.81494140625\n",
      "epoch 86, val_loss 455.3683776855469\n",
      "epoch 87, train_loss 1277.814697265625\n",
      "epoch 87, val_loss 455.3682861328125\n",
      "epoch 88, train_loss 1277.814453125\n",
      "epoch 88, val_loss 455.3681640625\n",
      "epoch 89, train_loss 1277.814453125\n",
      "epoch 89, val_loss 455.3680419921875\n",
      "epoch 90, train_loss 1277.8143310546875\n",
      "epoch 90, val_loss 455.367919921875\n",
      "epoch 91, train_loss 1277.8140869140625\n",
      "epoch 91, val_loss 455.3678283691406\n",
      "epoch 92, train_loss 1277.81396484375\n",
      "epoch 92, val_loss 455.3677978515625\n",
      "epoch 93, train_loss 1277.813720703125\n",
      "epoch 93, val_loss 455.3675842285156\n",
      "epoch 94, train_loss 1277.8134765625\n",
      "epoch 94, val_loss 455.36749267578125\n",
      "epoch 95, train_loss 1277.8133544921875\n",
      "epoch 95, val_loss 455.36737060546875\n",
      "epoch 96, train_loss 1277.8133544921875\n",
      "epoch 96, val_loss 455.3673400878906\n",
      "epoch 97, train_loss 1277.8131103515625\n",
      "epoch 97, val_loss 455.3672180175781\n",
      "epoch 98, train_loss 1277.81298828125\n",
      "epoch 98, val_loss 455.3670959472656\n",
      "epoch 99, train_loss 1277.8128662109375\n",
      "epoch 99, val_loss 455.36700439453125\n",
      "Parameter containing:\n",
      "tensor([6.1933e-12], requires_grad=True)\n",
      "iter 55, train_loss_regularization 0.8864856362342834\n",
      "iter 55, val_loss_regularization 0.8864856362342834\n",
      "epoch 0, train_loss 1277.8126220703125\n",
      "epoch 0, val_loss 455.3668212890625\n",
      "epoch 1, train_loss 1277.8125\n",
      "epoch 1, val_loss 455.36676025390625\n",
      "epoch 2, train_loss 1277.812255859375\n",
      "epoch 2, val_loss 455.36663818359375\n",
      "epoch 3, train_loss 1277.8121337890625\n",
      "epoch 3, val_loss 455.3665771484375\n",
      "epoch 4, train_loss 1277.8121337890625\n",
      "epoch 4, val_loss 455.3664245605469\n",
      "epoch 5, train_loss 1277.8118896484375\n",
      "epoch 5, val_loss 455.3663330078125\n",
      "epoch 6, train_loss 1277.8116455078125\n",
      "epoch 6, val_loss 455.3662414550781\n",
      "epoch 7, train_loss 1277.8116455078125\n",
      "epoch 7, val_loss 455.3661193847656\n",
      "epoch 8, train_loss 1277.8114013671875\n",
      "epoch 8, val_loss 455.365966796875\n",
      "epoch 9, train_loss 1277.811279296875\n",
      "epoch 9, val_loss 455.3658752441406\n",
      "epoch 10, train_loss 1277.811279296875\n",
      "epoch 10, val_loss 455.36578369140625\n",
      "epoch 11, train_loss 1277.81103515625\n",
      "epoch 11, val_loss 455.3656311035156\n",
      "epoch 12, train_loss 1277.810791015625\n",
      "epoch 12, val_loss 455.3655700683594\n",
      "epoch 13, train_loss 1277.810791015625\n",
      "epoch 13, val_loss 455.36541748046875\n",
      "epoch 14, train_loss 1277.810546875\n",
      "epoch 14, val_loss 455.3653869628906\n",
      "epoch 15, train_loss 1277.8104248046875\n",
      "epoch 15, val_loss 455.3652648925781\n",
      "epoch 16, train_loss 1277.810302734375\n",
      "epoch 16, val_loss 455.36517333984375\n",
      "epoch 17, train_loss 1277.81005859375\n",
      "epoch 17, val_loss 455.3650817871094\n",
      "epoch 18, train_loss 1277.809814453125\n",
      "epoch 18, val_loss 455.3648376464844\n",
      "epoch 19, train_loss 1277.8099365234375\n",
      "epoch 19, val_loss 455.3648376464844\n",
      "epoch 20, train_loss 1277.8096923828125\n",
      "epoch 20, val_loss 455.3647155761719\n",
      "epoch 21, train_loss 1277.8094482421875\n",
      "epoch 21, val_loss 455.3646240234375\n",
      "epoch 22, train_loss 1277.809326171875\n",
      "epoch 22, val_loss 455.364501953125\n",
      "epoch 23, train_loss 1277.8092041015625\n",
      "epoch 23, val_loss 455.3644104003906\n",
      "epoch 24, train_loss 1277.8092041015625\n",
      "epoch 24, val_loss 455.3642578125\n",
      "epoch 25, train_loss 1277.808837890625\n",
      "epoch 25, val_loss 455.36419677734375\n",
      "epoch 26, train_loss 1277.808837890625\n",
      "epoch 26, val_loss 455.36407470703125\n",
      "epoch 27, train_loss 1277.8087158203125\n",
      "epoch 27, val_loss 455.36395263671875\n",
      "epoch 28, train_loss 1277.8084716796875\n",
      "epoch 28, val_loss 455.36383056640625\n",
      "epoch 29, train_loss 1277.8082275390625\n",
      "epoch 29, val_loss 455.36370849609375\n",
      "epoch 30, train_loss 1277.8082275390625\n",
      "epoch 30, val_loss 455.3636779785156\n",
      "epoch 31, train_loss 1277.8082275390625\n",
      "epoch 31, val_loss 455.3634948730469\n",
      "epoch 32, train_loss 1277.807861328125\n",
      "epoch 32, val_loss 455.3634033203125\n",
      "epoch 33, train_loss 1277.8077392578125\n",
      "epoch 33, val_loss 455.36328125\n",
      "epoch 34, train_loss 1277.8076171875\n",
      "epoch 34, val_loss 455.3632507324219\n",
      "epoch 35, train_loss 1277.807373046875\n",
      "epoch 35, val_loss 455.36309814453125\n",
      "epoch 36, train_loss 1277.807373046875\n",
      "epoch 36, val_loss 455.3629455566406\n",
      "epoch 37, train_loss 1277.8072509765625\n",
      "epoch 37, val_loss 455.3629150390625\n",
      "epoch 38, train_loss 1277.8070068359375\n",
      "epoch 38, val_loss 455.36279296875\n",
      "epoch 39, train_loss 1277.8070068359375\n",
      "epoch 39, val_loss 455.3627014160156\n",
      "epoch 40, train_loss 1277.8067626953125\n",
      "epoch 40, val_loss 455.362548828125\n",
      "epoch 41, train_loss 1277.806640625\n",
      "epoch 41, val_loss 455.3624572753906\n",
      "epoch 42, train_loss 1277.8062744140625\n",
      "epoch 42, val_loss 455.36236572265625\n",
      "epoch 43, train_loss 1277.8060302734375\n",
      "epoch 43, val_loss 455.36224365234375\n",
      "epoch 44, train_loss 1277.8060302734375\n",
      "epoch 44, val_loss 455.36212158203125\n",
      "epoch 45, train_loss 1277.8060302734375\n",
      "epoch 45, val_loss 455.3620300292969\n",
      "epoch 46, train_loss 1277.8057861328125\n",
      "epoch 46, val_loss 455.3619079589844\n",
      "epoch 47, train_loss 1277.8055419921875\n",
      "epoch 47, val_loss 455.3618469238281\n",
      "epoch 48, train_loss 1277.8055419921875\n",
      "epoch 48, val_loss 455.3616638183594\n",
      "epoch 49, train_loss 1277.8052978515625\n",
      "epoch 49, val_loss 455.361572265625\n",
      "epoch 50, train_loss 1277.80517578125\n",
      "epoch 50, val_loss 455.36151123046875\n",
      "epoch 51, train_loss 1277.80517578125\n",
      "epoch 51, val_loss 455.36138916015625\n",
      "epoch 52, train_loss 1277.8048095703125\n",
      "epoch 52, val_loss 455.361328125\n",
      "epoch 53, train_loss 1277.8048095703125\n",
      "epoch 53, val_loss 455.361083984375\n",
      "epoch 54, train_loss 1277.804443359375\n",
      "epoch 54, val_loss 455.3610534667969\n",
      "epoch 55, train_loss 1277.804443359375\n",
      "epoch 55, val_loss 455.3609619140625\n",
      "epoch 56, train_loss 1277.80419921875\n",
      "epoch 56, val_loss 455.36083984375\n",
      "epoch 57, train_loss 1277.80419921875\n",
      "epoch 57, val_loss 455.36077880859375\n",
      "epoch 58, train_loss 1277.803955078125\n",
      "epoch 58, val_loss 455.3606262207031\n",
      "epoch 59, train_loss 1277.8038330078125\n",
      "epoch 59, val_loss 455.3605041503906\n",
      "epoch 60, train_loss 1277.8035888671875\n",
      "epoch 60, val_loss 455.36041259765625\n",
      "epoch 61, train_loss 1277.8037109375\n",
      "epoch 61, val_loss 455.3603820800781\n",
      "epoch 62, train_loss 1277.803466796875\n",
      "epoch 62, val_loss 455.3601989746094\n",
      "epoch 63, train_loss 1277.80322265625\n",
      "epoch 63, val_loss 455.36004638671875\n",
      "epoch 64, train_loss 1277.80322265625\n",
      "epoch 64, val_loss 455.3599853515625\n",
      "epoch 65, train_loss 1277.802978515625\n",
      "epoch 65, val_loss 455.35986328125\n",
      "epoch 66, train_loss 1277.802734375\n",
      "epoch 66, val_loss 455.35980224609375\n",
      "epoch 67, train_loss 1277.8026123046875\n",
      "epoch 67, val_loss 455.35968017578125\n",
      "epoch 68, train_loss 1277.8026123046875\n",
      "epoch 68, val_loss 455.3595886230469\n",
      "epoch 69, train_loss 1277.8023681640625\n",
      "epoch 69, val_loss 455.3594055175781\n",
      "epoch 70, train_loss 1277.80224609375\n",
      "epoch 70, val_loss 455.359375\n",
      "epoch 71, train_loss 1277.8021240234375\n",
      "epoch 71, val_loss 455.3592224121094\n",
      "epoch 72, train_loss 1277.802001953125\n",
      "epoch 72, val_loss 455.3591613769531\n",
      "epoch 73, train_loss 1277.8017578125\n",
      "epoch 73, val_loss 455.3590087890625\n",
      "epoch 74, train_loss 1277.8017578125\n",
      "epoch 74, val_loss 455.3589172363281\n",
      "epoch 75, train_loss 1277.8016357421875\n",
      "epoch 75, val_loss 455.35888671875\n",
      "epoch 76, train_loss 1277.8013916015625\n",
      "epoch 76, val_loss 455.3586730957031\n",
      "epoch 77, train_loss 1277.8011474609375\n",
      "epoch 77, val_loss 455.3586120605469\n",
      "epoch 78, train_loss 1277.801025390625\n",
      "epoch 78, val_loss 455.35845947265625\n",
      "epoch 79, train_loss 1277.80078125\n",
      "epoch 79, val_loss 455.3584289550781\n",
      "epoch 80, train_loss 1277.8006591796875\n",
      "epoch 80, val_loss 455.3583068847656\n",
      "epoch 81, train_loss 1277.8006591796875\n",
      "epoch 81, val_loss 455.358154296875\n",
      "epoch 82, train_loss 1277.8004150390625\n",
      "epoch 82, val_loss 455.35809326171875\n",
      "epoch 83, train_loss 1277.8001708984375\n",
      "epoch 83, val_loss 455.35791015625\n",
      "epoch 84, train_loss 1277.800048828125\n",
      "epoch 84, val_loss 455.3578186035156\n",
      "epoch 85, train_loss 1277.7999267578125\n",
      "epoch 85, val_loss 455.3576965332031\n",
      "epoch 86, train_loss 1277.7998046875\n",
      "epoch 86, val_loss 455.3576354980469\n",
      "epoch 87, train_loss 1277.799560546875\n",
      "epoch 87, val_loss 455.3575744628906\n",
      "epoch 88, train_loss 1277.7996826171875\n",
      "epoch 88, val_loss 455.3574523925781\n",
      "epoch 89, train_loss 1277.7994384765625\n",
      "epoch 89, val_loss 455.3573303222656\n",
      "epoch 90, train_loss 1277.7991943359375\n",
      "epoch 90, val_loss 455.3572082519531\n",
      "epoch 91, train_loss 1277.7989501953125\n",
      "epoch 91, val_loss 455.3570861816406\n",
      "epoch 92, train_loss 1277.798828125\n",
      "epoch 92, val_loss 455.35699462890625\n",
      "epoch 93, train_loss 1277.7987060546875\n",
      "epoch 93, val_loss 455.3569030761719\n",
      "epoch 94, train_loss 1277.7987060546875\n",
      "epoch 94, val_loss 455.35675048828125\n",
      "epoch 95, train_loss 1277.7984619140625\n",
      "epoch 95, val_loss 455.3566589355469\n",
      "epoch 96, train_loss 1277.7984619140625\n",
      "epoch 96, val_loss 455.3565368652344\n",
      "epoch 97, train_loss 1277.7982177734375\n",
      "epoch 97, val_loss 455.35650634765625\n",
      "epoch 98, train_loss 1277.7979736328125\n",
      "epoch 98, val_loss 455.35638427734375\n",
      "epoch 99, train_loss 1277.7979736328125\n",
      "epoch 99, val_loss 455.35626220703125\n",
      "Parameter containing:\n",
      "tensor([4.0691e-12], requires_grad=True)\n",
      "iter 56, train_loss_regularization 0.882286548614502\n",
      "iter 56, val_loss_regularization 0.882286548614502\n",
      "epoch 0, train_loss 1277.7978515625\n",
      "epoch 0, val_loss 455.35614013671875\n",
      "epoch 1, train_loss 1277.797607421875\n",
      "epoch 1, val_loss 455.3559875488281\n",
      "epoch 2, train_loss 1277.79736328125\n",
      "epoch 2, val_loss 455.35595703125\n",
      "epoch 3, train_loss 1277.79736328125\n",
      "epoch 3, val_loss 455.3558044433594\n",
      "epoch 4, train_loss 1277.797119140625\n",
      "epoch 4, val_loss 455.35565185546875\n",
      "epoch 5, train_loss 1277.797119140625\n",
      "epoch 5, val_loss 455.3555908203125\n",
      "epoch 6, train_loss 1277.7969970703125\n",
      "epoch 6, val_loss 455.3555908203125\n",
      "epoch 7, train_loss 1277.796630859375\n",
      "epoch 7, val_loss 455.35540771484375\n",
      "epoch 8, train_loss 1277.7965087890625\n",
      "epoch 8, val_loss 455.3553771972656\n",
      "epoch 9, train_loss 1277.7965087890625\n",
      "epoch 9, val_loss 455.35516357421875\n",
      "epoch 10, train_loss 1277.79638671875\n",
      "epoch 10, val_loss 455.3550720214844\n",
      "epoch 11, train_loss 1277.796142578125\n",
      "epoch 11, val_loss 455.3550109863281\n",
      "epoch 12, train_loss 1277.796142578125\n",
      "epoch 12, val_loss 455.35491943359375\n",
      "epoch 13, train_loss 1277.7957763671875\n",
      "epoch 13, val_loss 455.354736328125\n",
      "epoch 14, train_loss 1277.7957763671875\n",
      "epoch 14, val_loss 455.3546142578125\n",
      "epoch 15, train_loss 1277.7955322265625\n",
      "epoch 15, val_loss 455.3545837402344\n",
      "epoch 16, train_loss 1277.79541015625\n",
      "epoch 16, val_loss 455.3544616699219\n",
      "epoch 17, train_loss 1277.7952880859375\n",
      "epoch 17, val_loss 455.3543395996094\n",
      "epoch 18, train_loss 1277.7950439453125\n",
      "epoch 18, val_loss 455.3542175292969\n",
      "epoch 19, train_loss 1277.794921875\n",
      "epoch 19, val_loss 455.3541259765625\n",
      "epoch 20, train_loss 1277.7947998046875\n",
      "epoch 20, val_loss 455.3540344238281\n",
      "epoch 21, train_loss 1277.7947998046875\n",
      "epoch 21, val_loss 455.3539123535156\n",
      "epoch 22, train_loss 1277.79443359375\n",
      "epoch 22, val_loss 455.3537902832031\n",
      "epoch 23, train_loss 1277.7945556640625\n",
      "epoch 23, val_loss 455.3536682128906\n",
      "epoch 24, train_loss 1277.7943115234375\n",
      "epoch 24, val_loss 455.35357666015625\n",
      "epoch 25, train_loss 1277.7940673828125\n",
      "epoch 25, val_loss 455.3534851074219\n",
      "epoch 26, train_loss 1277.7940673828125\n",
      "epoch 26, val_loss 455.3534240722656\n",
      "epoch 27, train_loss 1277.7938232421875\n",
      "epoch 27, val_loss 455.3532409667969\n",
      "epoch 28, train_loss 1277.7935791015625\n",
      "epoch 28, val_loss 455.3531799316406\n",
      "epoch 29, train_loss 1277.7935791015625\n",
      "epoch 29, val_loss 455.35302734375\n",
      "epoch 30, train_loss 1277.7933349609375\n",
      "epoch 30, val_loss 455.3529968261719\n",
      "epoch 31, train_loss 1277.7930908203125\n",
      "epoch 31, val_loss 455.3528747558594\n",
      "epoch 32, train_loss 1277.79296875\n",
      "epoch 32, val_loss 455.35272216796875\n",
      "epoch 33, train_loss 1277.79296875\n",
      "epoch 33, val_loss 455.3526611328125\n",
      "epoch 34, train_loss 1277.792724609375\n",
      "epoch 34, val_loss 455.3525085449219\n",
      "epoch 35, train_loss 1277.7926025390625\n",
      "epoch 35, val_loss 455.3524475097656\n",
      "epoch 36, train_loss 1277.79248046875\n",
      "epoch 36, val_loss 455.3523254394531\n",
      "epoch 37, train_loss 1277.7923583984375\n",
      "epoch 37, val_loss 455.3522644042969\n",
      "epoch 38, train_loss 1277.7921142578125\n",
      "epoch 38, val_loss 455.3520812988281\n",
      "epoch 39, train_loss 1277.7919921875\n",
      "epoch 39, val_loss 455.35198974609375\n",
      "epoch 40, train_loss 1277.7918701171875\n",
      "epoch 40, val_loss 455.3519287109375\n",
      "epoch 41, train_loss 1277.7918701171875\n",
      "epoch 41, val_loss 455.3517150878906\n",
      "epoch 42, train_loss 1277.79150390625\n",
      "epoch 42, val_loss 455.3516540527344\n",
      "epoch 43, train_loss 1277.7913818359375\n",
      "epoch 43, val_loss 455.3515319824219\n",
      "epoch 44, train_loss 1277.791259765625\n",
      "epoch 44, val_loss 455.35150146484375\n",
      "epoch 45, train_loss 1277.7911376953125\n",
      "epoch 45, val_loss 455.351318359375\n",
      "epoch 46, train_loss 1277.791015625\n",
      "epoch 46, val_loss 455.3512878417969\n",
      "epoch 47, train_loss 1277.7908935546875\n",
      "epoch 47, val_loss 455.3511657714844\n",
      "epoch 48, train_loss 1277.790771484375\n",
      "epoch 48, val_loss 455.35107421875\n",
      "epoch 49, train_loss 1277.79052734375\n",
      "epoch 49, val_loss 455.3509216308594\n",
      "epoch 50, train_loss 1277.79052734375\n",
      "epoch 50, val_loss 455.350830078125\n",
      "epoch 51, train_loss 1277.790283203125\n",
      "epoch 51, val_loss 455.3506774902344\n",
      "epoch 52, train_loss 1277.7900390625\n",
      "epoch 52, val_loss 455.3506164550781\n",
      "epoch 53, train_loss 1277.7899169921875\n",
      "epoch 53, val_loss 455.3504943847656\n",
      "epoch 54, train_loss 1277.789794921875\n",
      "epoch 54, val_loss 455.35040283203125\n",
      "epoch 55, train_loss 1277.7896728515625\n",
      "epoch 55, val_loss 455.35028076171875\n",
      "epoch 56, train_loss 1277.78955078125\n",
      "epoch 56, val_loss 455.35015869140625\n",
      "epoch 57, train_loss 1277.789306640625\n",
      "epoch 57, val_loss 455.3501281738281\n",
      "epoch 58, train_loss 1277.789306640625\n",
      "epoch 58, val_loss 455.3500061035156\n",
      "epoch 59, train_loss 1277.7891845703125\n",
      "epoch 59, val_loss 455.3498840332031\n",
      "epoch 60, train_loss 1277.7889404296875\n",
      "epoch 60, val_loss 455.3497009277344\n",
      "epoch 61, train_loss 1277.7889404296875\n",
      "epoch 61, val_loss 455.3497009277344\n",
      "epoch 62, train_loss 1277.7886962890625\n",
      "epoch 62, val_loss 455.3495788574219\n",
      "epoch 63, train_loss 1277.7884521484375\n",
      "epoch 63, val_loss 455.3494567871094\n",
      "epoch 64, train_loss 1277.7884521484375\n",
      "epoch 64, val_loss 455.349365234375\n",
      "epoch 65, train_loss 1277.7884521484375\n",
      "epoch 65, val_loss 455.3492126464844\n",
      "epoch 66, train_loss 1277.7879638671875\n",
      "epoch 66, val_loss 455.3491516113281\n",
      "epoch 67, train_loss 1277.7879638671875\n",
      "epoch 67, val_loss 455.3490295410156\n",
      "epoch 68, train_loss 1277.787841796875\n",
      "epoch 68, val_loss 455.3489685058594\n",
      "epoch 69, train_loss 1277.78759765625\n",
      "epoch 69, val_loss 455.3487548828125\n",
      "epoch 70, train_loss 1277.7874755859375\n",
      "epoch 70, val_loss 455.34869384765625\n",
      "epoch 71, train_loss 1277.787353515625\n",
      "epoch 71, val_loss 455.3486328125\n",
      "epoch 72, train_loss 1277.7872314453125\n",
      "epoch 72, val_loss 455.3485412597656\n",
      "epoch 73, train_loss 1277.786865234375\n",
      "epoch 73, val_loss 455.34844970703125\n",
      "epoch 74, train_loss 1277.786865234375\n",
      "epoch 74, val_loss 455.3482360839844\n",
      "epoch 75, train_loss 1277.786865234375\n",
      "epoch 75, val_loss 455.34820556640625\n",
      "epoch 76, train_loss 1277.78662109375\n",
      "epoch 76, val_loss 455.3480529785156\n",
      "epoch 77, train_loss 1277.786376953125\n",
      "epoch 77, val_loss 455.3480529785156\n",
      "epoch 78, train_loss 1277.786376953125\n",
      "epoch 78, val_loss 455.3478698730469\n",
      "epoch 79, train_loss 1277.7861328125\n",
      "epoch 79, val_loss 455.34771728515625\n",
      "epoch 80, train_loss 1277.7860107421875\n",
      "epoch 80, val_loss 455.34765625\n",
      "epoch 81, train_loss 1277.785888671875\n",
      "epoch 81, val_loss 455.34759521484375\n",
      "epoch 82, train_loss 1277.7855224609375\n",
      "epoch 82, val_loss 455.34747314453125\n",
      "epoch 83, train_loss 1277.7855224609375\n",
      "epoch 83, val_loss 455.347412109375\n",
      "epoch 84, train_loss 1277.7852783203125\n",
      "epoch 84, val_loss 455.3471984863281\n",
      "epoch 85, train_loss 1277.7852783203125\n",
      "epoch 85, val_loss 455.34716796875\n",
      "epoch 86, train_loss 1277.7852783203125\n",
      "epoch 86, val_loss 455.3470458984375\n",
      "epoch 87, train_loss 1277.784912109375\n",
      "epoch 87, val_loss 455.346923828125\n",
      "epoch 88, train_loss 1277.784912109375\n",
      "epoch 88, val_loss 455.3468322753906\n",
      "epoch 89, train_loss 1277.7847900390625\n",
      "epoch 89, val_loss 455.3467102050781\n",
      "epoch 90, train_loss 1277.7845458984375\n",
      "epoch 90, val_loss 455.3466796875\n",
      "epoch 91, train_loss 1277.7843017578125\n",
      "epoch 91, val_loss 455.34649658203125\n",
      "epoch 92, train_loss 1277.7843017578125\n",
      "epoch 92, val_loss 455.3464050292969\n",
      "epoch 93, train_loss 1277.7840576171875\n",
      "epoch 93, val_loss 455.3462829589844\n",
      "epoch 94, train_loss 1277.783935546875\n",
      "epoch 94, val_loss 455.3461608886719\n",
      "epoch 95, train_loss 1277.783935546875\n",
      "epoch 95, val_loss 455.3460693359375\n",
      "epoch 96, train_loss 1277.78369140625\n",
      "epoch 96, val_loss 455.345947265625\n",
      "epoch 97, train_loss 1277.7835693359375\n",
      "epoch 97, val_loss 455.34588623046875\n",
      "epoch 98, train_loss 1277.783447265625\n",
      "epoch 98, val_loss 455.3457946777344\n",
      "epoch 99, train_loss 1277.7833251953125\n",
      "epoch 99, val_loss 455.3456726074219\n",
      "Parameter containing:\n",
      "tensor([2.6769e-12], requires_grad=True)\n",
      "iter 57, train_loss_regularization 0.8781591653823853\n",
      "iter 57, val_loss_regularization 0.8781591653823853\n",
      "epoch 0, train_loss 1277.7830810546875\n",
      "epoch 0, val_loss 455.3455505371094\n",
      "epoch 1, train_loss 1277.782958984375\n",
      "epoch 1, val_loss 455.345458984375\n",
      "epoch 2, train_loss 1277.78271484375\n",
      "epoch 2, val_loss 455.3453674316406\n",
      "epoch 3, train_loss 1277.78271484375\n",
      "epoch 3, val_loss 455.3453063964844\n",
      "epoch 4, train_loss 1277.7825927734375\n",
      "epoch 4, val_loss 455.34515380859375\n",
      "epoch 5, train_loss 1277.782470703125\n",
      "epoch 5, val_loss 455.34503173828125\n",
      "epoch 6, train_loss 1277.7823486328125\n",
      "epoch 6, val_loss 455.3448791503906\n",
      "epoch 7, train_loss 1277.7821044921875\n",
      "epoch 7, val_loss 455.34478759765625\n",
      "epoch 8, train_loss 1277.781982421875\n",
      "epoch 8, val_loss 455.3447570800781\n",
      "epoch 9, train_loss 1277.7818603515625\n",
      "epoch 9, val_loss 455.3445739746094\n",
      "epoch 10, train_loss 1277.78173828125\n",
      "epoch 10, val_loss 455.3444519042969\n",
      "epoch 11, train_loss 1277.781494140625\n",
      "epoch 11, val_loss 455.3443603515625\n",
      "epoch 12, train_loss 1277.7813720703125\n",
      "epoch 12, val_loss 455.3443298339844\n",
      "epoch 13, train_loss 1277.7813720703125\n",
      "epoch 13, val_loss 455.3442077636719\n",
      "epoch 14, train_loss 1277.7811279296875\n",
      "epoch 14, val_loss 455.34417724609375\n",
      "epoch 15, train_loss 1277.781005859375\n",
      "epoch 15, val_loss 455.343994140625\n",
      "epoch 16, train_loss 1277.7808837890625\n",
      "epoch 16, val_loss 455.3438415527344\n",
      "epoch 17, train_loss 1277.78076171875\n",
      "epoch 17, val_loss 455.3437805175781\n",
      "epoch 18, train_loss 1277.7803955078125\n",
      "epoch 18, val_loss 455.3436279296875\n",
      "epoch 19, train_loss 1277.7803955078125\n",
      "epoch 19, val_loss 455.3436279296875\n",
      "epoch 20, train_loss 1277.7802734375\n",
      "epoch 20, val_loss 455.3434143066406\n",
      "epoch 21, train_loss 1277.780029296875\n",
      "epoch 21, val_loss 455.34332275390625\n",
      "epoch 22, train_loss 1277.780029296875\n",
      "epoch 22, val_loss 455.34326171875\n",
      "epoch 23, train_loss 1277.77978515625\n",
      "epoch 23, val_loss 455.3431701660156\n",
      "epoch 24, train_loss 1277.7796630859375\n",
      "epoch 24, val_loss 455.3429870605469\n",
      "epoch 25, train_loss 1277.779541015625\n",
      "epoch 25, val_loss 455.3429260253906\n",
      "epoch 26, train_loss 1277.7794189453125\n",
      "epoch 26, val_loss 455.34283447265625\n",
      "epoch 27, train_loss 1277.779296875\n",
      "epoch 27, val_loss 455.3427429199219\n",
      "epoch 28, train_loss 1277.7791748046875\n",
      "epoch 28, val_loss 455.3426208496094\n",
      "epoch 29, train_loss 1277.7789306640625\n",
      "epoch 29, val_loss 455.342529296875\n",
      "epoch 30, train_loss 1277.77880859375\n",
      "epoch 30, val_loss 455.3424072265625\n",
      "epoch 31, train_loss 1277.7786865234375\n",
      "epoch 31, val_loss 455.3422546386719\n",
      "epoch 32, train_loss 1277.7784423828125\n",
      "epoch 32, val_loss 455.3421630859375\n",
      "epoch 33, train_loss 1277.7784423828125\n",
      "epoch 33, val_loss 455.3420715332031\n",
      "epoch 34, train_loss 1277.7781982421875\n",
      "epoch 34, val_loss 455.3419494628906\n",
      "epoch 35, train_loss 1277.778076171875\n",
      "epoch 35, val_loss 455.3418273925781\n",
      "epoch 36, train_loss 1277.77783203125\n",
      "epoch 36, val_loss 455.341796875\n",
      "epoch 37, train_loss 1277.77783203125\n",
      "epoch 37, val_loss 455.3417053222656\n",
      "epoch 38, train_loss 1277.777587890625\n",
      "epoch 38, val_loss 455.3415832519531\n",
      "epoch 39, train_loss 1277.7774658203125\n",
      "epoch 39, val_loss 455.3414611816406\n",
      "epoch 40, train_loss 1277.7774658203125\n",
      "epoch 40, val_loss 455.3413391113281\n",
      "epoch 41, train_loss 1277.7772216796875\n",
      "epoch 41, val_loss 455.3412170410156\n",
      "epoch 42, train_loss 1277.7769775390625\n",
      "epoch 42, val_loss 455.34112548828125\n",
      "epoch 43, train_loss 1277.7769775390625\n",
      "epoch 43, val_loss 455.3410339355469\n",
      "epoch 44, train_loss 1277.776611328125\n",
      "epoch 44, val_loss 455.3409118652344\n",
      "epoch 45, train_loss 1277.776611328125\n",
      "epoch 45, val_loss 455.34088134765625\n",
      "epoch 46, train_loss 1277.776611328125\n",
      "epoch 46, val_loss 455.3406982421875\n",
      "epoch 47, train_loss 1277.7762451171875\n",
      "epoch 47, val_loss 455.34063720703125\n",
      "epoch 48, train_loss 1277.776123046875\n",
      "epoch 48, val_loss 455.34051513671875\n",
      "epoch 49, train_loss 1277.776123046875\n",
      "epoch 49, val_loss 455.3404541015625\n",
      "epoch 50, train_loss 1277.77587890625\n",
      "epoch 50, val_loss 455.34033203125\n",
      "epoch 51, train_loss 1277.77587890625\n",
      "epoch 51, val_loss 455.3401794433594\n",
      "epoch 52, train_loss 1277.775634765625\n",
      "epoch 52, val_loss 455.3401184082031\n",
      "epoch 53, train_loss 1277.775390625\n",
      "epoch 53, val_loss 455.3399658203125\n",
      "epoch 54, train_loss 1277.775390625\n",
      "epoch 54, val_loss 455.33990478515625\n",
      "epoch 55, train_loss 1277.7752685546875\n",
      "epoch 55, val_loss 455.33978271484375\n",
      "epoch 56, train_loss 1277.77490234375\n",
      "epoch 56, val_loss 455.3397216796875\n",
      "epoch 57, train_loss 1277.7747802734375\n",
      "epoch 57, val_loss 455.3395690917969\n",
      "epoch 58, train_loss 1277.774658203125\n",
      "epoch 58, val_loss 455.3394470214844\n",
      "epoch 59, train_loss 1277.7745361328125\n",
      "epoch 59, val_loss 455.3393859863281\n",
      "epoch 60, train_loss 1277.7745361328125\n",
      "epoch 60, val_loss 455.3393249511719\n",
      "epoch 61, train_loss 1277.7742919921875\n",
      "epoch 61, val_loss 455.33917236328125\n",
      "epoch 62, train_loss 1277.7742919921875\n",
      "epoch 62, val_loss 455.33905029296875\n",
      "epoch 63, train_loss 1277.77392578125\n",
      "epoch 63, val_loss 455.33892822265625\n",
      "epoch 64, train_loss 1277.7740478515625\n",
      "epoch 64, val_loss 455.3388671875\n",
      "epoch 65, train_loss 1277.7738037109375\n",
      "epoch 65, val_loss 455.3387451171875\n",
      "epoch 66, train_loss 1277.7735595703125\n",
      "epoch 66, val_loss 455.3385925292969\n",
      "epoch 67, train_loss 1277.7733154296875\n",
      "epoch 67, val_loss 455.3385314941406\n",
      "epoch 68, train_loss 1277.7733154296875\n",
      "epoch 68, val_loss 455.33837890625\n",
      "epoch 69, train_loss 1277.7730712890625\n",
      "epoch 69, val_loss 455.33837890625\n",
      "epoch 70, train_loss 1277.77294921875\n",
      "epoch 70, val_loss 455.3382568359375\n",
      "epoch 71, train_loss 1277.77294921875\n",
      "epoch 71, val_loss 455.338134765625\n",
      "epoch 72, train_loss 1277.772705078125\n",
      "epoch 72, val_loss 455.3380126953125\n",
      "epoch 73, train_loss 1277.772705078125\n",
      "epoch 73, val_loss 455.3379211425781\n",
      "epoch 74, train_loss 1277.7724609375\n",
      "epoch 74, val_loss 455.33782958984375\n",
      "epoch 75, train_loss 1277.772216796875\n",
      "epoch 75, val_loss 455.33770751953125\n",
      "epoch 76, train_loss 1277.772216796875\n",
      "epoch 76, val_loss 455.33758544921875\n",
      "epoch 77, train_loss 1277.77197265625\n",
      "epoch 77, val_loss 455.33746337890625\n",
      "epoch 78, train_loss 1277.7718505859375\n",
      "epoch 78, val_loss 455.3373718261719\n",
      "epoch 79, train_loss 1277.771728515625\n",
      "epoch 79, val_loss 455.3372802734375\n",
      "epoch 80, train_loss 1277.771484375\n",
      "epoch 80, val_loss 455.337158203125\n",
      "epoch 81, train_loss 1277.7713623046875\n",
      "epoch 81, val_loss 455.33709716796875\n",
      "epoch 82, train_loss 1277.7713623046875\n",
      "epoch 82, val_loss 455.3370056152344\n",
      "epoch 83, train_loss 1277.7711181640625\n",
      "epoch 83, val_loss 455.3368225097656\n",
      "epoch 84, train_loss 1277.77099609375\n",
      "epoch 84, val_loss 455.3367919921875\n",
      "epoch 85, train_loss 1277.77099609375\n",
      "epoch 85, val_loss 455.3366394042969\n",
      "epoch 86, train_loss 1277.770751953125\n",
      "epoch 86, val_loss 455.33648681640625\n",
      "epoch 87, train_loss 1277.7705078125\n",
      "epoch 87, val_loss 455.33642578125\n",
      "epoch 88, train_loss 1277.7703857421875\n",
      "epoch 88, val_loss 455.33636474609375\n",
      "epoch 89, train_loss 1277.770263671875\n",
      "epoch 89, val_loss 455.33624267578125\n",
      "epoch 90, train_loss 1277.7701416015625\n",
      "epoch 90, val_loss 455.33612060546875\n",
      "epoch 91, train_loss 1277.77001953125\n",
      "epoch 91, val_loss 455.3360900878906\n",
      "epoch 92, train_loss 1277.769775390625\n",
      "epoch 92, val_loss 455.3359069824219\n",
      "epoch 93, train_loss 1277.7696533203125\n",
      "epoch 93, val_loss 455.33587646484375\n",
      "epoch 94, train_loss 1277.76953125\n",
      "epoch 94, val_loss 455.335693359375\n",
      "epoch 95, train_loss 1277.7694091796875\n",
      "epoch 95, val_loss 455.33563232421875\n",
      "epoch 96, train_loss 1277.769287109375\n",
      "epoch 96, val_loss 455.3355407714844\n",
      "epoch 97, train_loss 1277.76904296875\n",
      "epoch 97, val_loss 455.33544921875\n",
      "epoch 98, train_loss 1277.7689208984375\n",
      "epoch 98, val_loss 455.3353271484375\n",
      "epoch 99, train_loss 1277.768798828125\n",
      "epoch 99, val_loss 455.3351745605469\n",
      "Parameter containing:\n",
      "tensor([1.7632e-12], requires_grad=True)\n",
      "iter 58, train_loss_regularization 0.8741023540496826\n",
      "iter 58, val_loss_regularization 0.8741023540496826\n",
      "epoch 0, train_loss 1277.768798828125\n",
      "epoch 0, val_loss 455.3351135253906\n",
      "epoch 1, train_loss 1277.7685546875\n",
      "epoch 1, val_loss 455.3349914550781\n",
      "epoch 2, train_loss 1277.7684326171875\n",
      "epoch 2, val_loss 455.3349304199219\n",
      "epoch 3, train_loss 1277.7681884765625\n",
      "epoch 3, val_loss 455.33477783203125\n",
      "epoch 4, train_loss 1277.76806640625\n",
      "epoch 4, val_loss 455.33465576171875\n",
      "epoch 5, train_loss 1277.7679443359375\n",
      "epoch 5, val_loss 455.3345947265625\n",
      "epoch 6, train_loss 1277.7677001953125\n",
      "epoch 6, val_loss 455.3345031738281\n",
      "epoch 7, train_loss 1277.7677001953125\n",
      "epoch 7, val_loss 455.3343811035156\n",
      "epoch 8, train_loss 1277.7674560546875\n",
      "epoch 8, val_loss 455.3343200683594\n",
      "epoch 9, train_loss 1277.7674560546875\n",
      "epoch 9, val_loss 455.33416748046875\n",
      "epoch 10, train_loss 1277.767333984375\n",
      "epoch 10, val_loss 455.3340148925781\n",
      "epoch 11, train_loss 1277.76708984375\n",
      "epoch 11, val_loss 455.3339538574219\n",
      "epoch 12, train_loss 1277.76708984375\n",
      "epoch 12, val_loss 455.3338623046875\n",
      "epoch 13, train_loss 1277.766845703125\n",
      "epoch 13, val_loss 455.333740234375\n",
      "epoch 14, train_loss 1277.766845703125\n",
      "epoch 14, val_loss 455.3335876464844\n",
      "epoch 15, train_loss 1277.7664794921875\n",
      "epoch 15, val_loss 455.3335266113281\n",
      "epoch 16, train_loss 1277.7664794921875\n",
      "epoch 16, val_loss 455.3334045410156\n",
      "epoch 17, train_loss 1277.7662353515625\n",
      "epoch 17, val_loss 455.3333740234375\n",
      "epoch 18, train_loss 1277.76611328125\n",
      "epoch 18, val_loss 455.3332214355469\n",
      "epoch 19, train_loss 1277.7659912109375\n",
      "epoch 19, val_loss 455.3331298828125\n",
      "epoch 20, train_loss 1277.765869140625\n",
      "epoch 20, val_loss 455.3330383300781\n",
      "epoch 21, train_loss 1277.765869140625\n",
      "epoch 21, val_loss 455.3329162597656\n",
      "epoch 22, train_loss 1277.7655029296875\n",
      "epoch 22, val_loss 455.33282470703125\n",
      "epoch 23, train_loss 1277.765380859375\n",
      "epoch 23, val_loss 455.33270263671875\n",
      "epoch 24, train_loss 1277.765380859375\n",
      "epoch 24, val_loss 455.33258056640625\n",
      "epoch 25, train_loss 1277.76513671875\n",
      "epoch 25, val_loss 455.33245849609375\n",
      "epoch 26, train_loss 1277.7650146484375\n",
      "epoch 26, val_loss 455.3324279785156\n",
      "epoch 27, train_loss 1277.764892578125\n",
      "epoch 27, val_loss 455.33233642578125\n",
      "epoch 28, train_loss 1277.7647705078125\n",
      "epoch 28, val_loss 455.33221435546875\n",
      "epoch 29, train_loss 1277.7645263671875\n",
      "epoch 29, val_loss 455.33209228515625\n",
      "epoch 30, train_loss 1277.7645263671875\n",
      "epoch 30, val_loss 455.3320007324219\n",
      "epoch 31, train_loss 1277.7642822265625\n",
      "epoch 31, val_loss 455.3318786621094\n",
      "epoch 32, train_loss 1277.76416015625\n",
      "epoch 32, val_loss 455.331787109375\n",
      "epoch 33, train_loss 1277.7640380859375\n",
      "epoch 33, val_loss 455.3316650390625\n",
      "epoch 34, train_loss 1277.763916015625\n",
      "epoch 34, val_loss 455.3315734863281\n",
      "epoch 35, train_loss 1277.7637939453125\n",
      "epoch 35, val_loss 455.3314514160156\n",
      "epoch 36, train_loss 1277.763671875\n",
      "epoch 36, val_loss 455.33135986328125\n",
      "epoch 37, train_loss 1277.7633056640625\n",
      "epoch 37, val_loss 455.33123779296875\n",
      "epoch 38, train_loss 1277.763427734375\n",
      "epoch 38, val_loss 455.33111572265625\n",
      "epoch 39, train_loss 1277.7630615234375\n",
      "epoch 39, val_loss 455.3310852050781\n",
      "epoch 40, train_loss 1277.762939453125\n",
      "epoch 40, val_loss 455.3309020996094\n",
      "epoch 41, train_loss 1277.762939453125\n",
      "epoch 41, val_loss 455.3308410644531\n",
      "epoch 42, train_loss 1277.7626953125\n",
      "epoch 42, val_loss 455.3307189941406\n",
      "epoch 43, train_loss 1277.7626953125\n",
      "epoch 43, val_loss 455.33062744140625\n",
      "epoch 44, train_loss 1277.762451171875\n",
      "epoch 44, val_loss 455.3305969238281\n",
      "epoch 45, train_loss 1277.7623291015625\n",
      "epoch 45, val_loss 455.3304443359375\n",
      "epoch 46, train_loss 1277.7620849609375\n",
      "epoch 46, val_loss 455.330322265625\n",
      "epoch 47, train_loss 1277.761962890625\n",
      "epoch 47, val_loss 455.33026123046875\n",
      "epoch 48, train_loss 1277.7618408203125\n",
      "epoch 48, val_loss 455.33013916015625\n",
      "epoch 49, train_loss 1277.76171875\n",
      "epoch 49, val_loss 455.3300476074219\n",
      "epoch 50, train_loss 1277.7615966796875\n",
      "epoch 50, val_loss 455.3299255371094\n",
      "epoch 51, train_loss 1277.7613525390625\n",
      "epoch 51, val_loss 455.3298034667969\n",
      "epoch 52, train_loss 1277.76123046875\n",
      "epoch 52, val_loss 455.3297424316406\n",
      "epoch 53, train_loss 1277.7611083984375\n",
      "epoch 53, val_loss 455.32958984375\n",
      "epoch 54, train_loss 1277.7608642578125\n",
      "epoch 54, val_loss 455.3294982910156\n",
      "epoch 55, train_loss 1277.7608642578125\n",
      "epoch 55, val_loss 455.329345703125\n",
      "epoch 56, train_loss 1277.7606201171875\n",
      "epoch 56, val_loss 455.3292541503906\n",
      "epoch 57, train_loss 1277.7606201171875\n",
      "epoch 57, val_loss 455.32916259765625\n",
      "epoch 58, train_loss 1277.7603759765625\n",
      "epoch 58, val_loss 455.3290710449219\n",
      "epoch 59, train_loss 1277.7603759765625\n",
      "epoch 59, val_loss 455.3290100097656\n",
      "epoch 60, train_loss 1277.7601318359375\n",
      "epoch 60, val_loss 455.32891845703125\n",
      "epoch 61, train_loss 1277.760009765625\n",
      "epoch 61, val_loss 455.3288269042969\n",
      "epoch 62, train_loss 1277.7598876953125\n",
      "epoch 62, val_loss 455.32867431640625\n",
      "epoch 63, train_loss 1277.7596435546875\n",
      "epoch 63, val_loss 455.3285827636719\n",
      "epoch 64, train_loss 1277.759521484375\n",
      "epoch 64, val_loss 455.32843017578125\n",
      "epoch 65, train_loss 1277.7593994140625\n",
      "epoch 65, val_loss 455.328369140625\n",
      "epoch 66, train_loss 1277.75927734375\n",
      "epoch 66, val_loss 455.3282470703125\n",
      "epoch 67, train_loss 1277.759033203125\n",
      "epoch 67, val_loss 455.328125\n",
      "epoch 68, train_loss 1277.759033203125\n",
      "epoch 68, val_loss 455.3280944824219\n",
      "epoch 69, train_loss 1277.7589111328125\n",
      "epoch 69, val_loss 455.3279724121094\n",
      "epoch 70, train_loss 1277.7587890625\n",
      "epoch 70, val_loss 455.32781982421875\n",
      "epoch 71, train_loss 1277.758544921875\n",
      "epoch 71, val_loss 455.3277893066406\n",
      "epoch 72, train_loss 1277.7584228515625\n",
      "epoch 72, val_loss 455.32757568359375\n",
      "epoch 73, train_loss 1277.75830078125\n",
      "epoch 73, val_loss 455.32757568359375\n",
      "epoch 74, train_loss 1277.7581787109375\n",
      "epoch 74, val_loss 455.3274230957031\n",
      "epoch 75, train_loss 1277.758056640625\n",
      "epoch 75, val_loss 455.3273620605469\n",
      "epoch 76, train_loss 1277.758056640625\n",
      "epoch 76, val_loss 455.3273010253906\n",
      "epoch 77, train_loss 1277.7576904296875\n",
      "epoch 77, val_loss 455.3271789550781\n",
      "epoch 78, train_loss 1277.757568359375\n",
      "epoch 78, val_loss 455.3270568847656\n",
      "epoch 79, train_loss 1277.7574462890625\n",
      "epoch 79, val_loss 455.3268737792969\n",
      "epoch 80, train_loss 1277.7574462890625\n",
      "epoch 80, val_loss 455.32684326171875\n",
      "epoch 81, train_loss 1277.7572021484375\n",
      "epoch 81, val_loss 455.32672119140625\n",
      "epoch 82, train_loss 1277.757080078125\n",
      "epoch 82, val_loss 455.3266296386719\n",
      "epoch 83, train_loss 1277.7568359375\n",
      "epoch 83, val_loss 455.3265075683594\n",
      "epoch 84, train_loss 1277.7567138671875\n",
      "epoch 84, val_loss 455.3263854980469\n",
      "epoch 85, train_loss 1277.7567138671875\n",
      "epoch 85, val_loss 455.3262939453125\n",
      "epoch 86, train_loss 1277.7564697265625\n",
      "epoch 86, val_loss 455.3262634277344\n",
      "epoch 87, train_loss 1277.7564697265625\n",
      "epoch 87, val_loss 455.32611083984375\n",
      "epoch 88, train_loss 1277.7562255859375\n",
      "epoch 88, val_loss 455.3259582519531\n",
      "epoch 89, train_loss 1277.756103515625\n",
      "epoch 89, val_loss 455.32586669921875\n",
      "epoch 90, train_loss 1277.755859375\n",
      "epoch 90, val_loss 455.32574462890625\n",
      "epoch 91, train_loss 1277.755859375\n",
      "epoch 91, val_loss 455.3256530761719\n",
      "epoch 92, train_loss 1277.755615234375\n",
      "epoch 92, val_loss 455.3255920410156\n",
      "epoch 93, train_loss 1277.755615234375\n",
      "epoch 93, val_loss 455.32550048828125\n",
      "epoch 94, train_loss 1277.7554931640625\n",
      "epoch 94, val_loss 455.32537841796875\n",
      "epoch 95, train_loss 1277.755126953125\n",
      "epoch 95, val_loss 455.3252868652344\n",
      "epoch 96, train_loss 1277.755126953125\n",
      "epoch 96, val_loss 455.3251953125\n",
      "epoch 97, train_loss 1277.7548828125\n",
      "epoch 97, val_loss 455.32513427734375\n",
      "epoch 98, train_loss 1277.7548828125\n",
      "epoch 98, val_loss 455.3249206542969\n",
      "epoch 99, train_loss 1277.7547607421875\n",
      "epoch 99, val_loss 455.32489013671875\n",
      "Parameter containing:\n",
      "tensor([1.1629e-12], requires_grad=True)\n",
      "iter 59, train_loss_regularization 0.8701150417327881\n",
      "iter 59, val_loss_regularization 0.8701150417327881\n",
      "epoch 0, train_loss 1277.754638671875\n",
      "epoch 0, val_loss 455.3247375488281\n",
      "epoch 1, train_loss 1277.75439453125\n",
      "epoch 1, val_loss 455.32470703125\n",
      "epoch 2, train_loss 1277.7542724609375\n",
      "epoch 2, val_loss 455.3245544433594\n",
      "epoch 3, train_loss 1277.7542724609375\n",
      "epoch 3, val_loss 455.324462890625\n",
      "epoch 4, train_loss 1277.75390625\n",
      "epoch 4, val_loss 455.3243408203125\n",
      "epoch 5, train_loss 1277.75390625\n",
      "epoch 5, val_loss 455.3242492675781\n",
      "epoch 6, train_loss 1277.753662109375\n",
      "epoch 6, val_loss 455.3240966796875\n",
      "epoch 7, train_loss 1277.7535400390625\n",
      "epoch 7, val_loss 455.32403564453125\n",
      "epoch 8, train_loss 1277.7535400390625\n",
      "epoch 8, val_loss 455.3240051269531\n",
      "epoch 9, train_loss 1277.753173828125\n",
      "epoch 9, val_loss 455.32379150390625\n",
      "epoch 10, train_loss 1277.753173828125\n",
      "epoch 10, val_loss 455.3237609863281\n",
      "epoch 11, train_loss 1277.7528076171875\n",
      "epoch 11, val_loss 455.3236389160156\n",
      "epoch 12, train_loss 1277.7528076171875\n",
      "epoch 12, val_loss 455.32354736328125\n",
      "epoch 13, train_loss 1277.7525634765625\n",
      "epoch 13, val_loss 455.32342529296875\n",
      "epoch 14, train_loss 1277.75244140625\n",
      "epoch 14, val_loss 455.3233337402344\n",
      "epoch 15, train_loss 1277.75244140625\n",
      "epoch 15, val_loss 455.3232116699219\n",
      "epoch 16, train_loss 1277.7523193359375\n",
      "epoch 16, val_loss 455.32318115234375\n",
      "epoch 17, train_loss 1277.752197265625\n",
      "epoch 17, val_loss 455.322998046875\n",
      "epoch 18, train_loss 1277.751953125\n",
      "epoch 18, val_loss 455.3229064941406\n",
      "epoch 19, train_loss 1277.7518310546875\n",
      "epoch 19, val_loss 455.3228454589844\n",
      "epoch 20, train_loss 1277.751708984375\n",
      "epoch 20, val_loss 455.32275390625\n",
      "epoch 21, train_loss 1277.7515869140625\n",
      "epoch 21, val_loss 455.32257080078125\n",
      "epoch 22, train_loss 1277.75146484375\n",
      "epoch 22, val_loss 455.322509765625\n",
      "epoch 23, train_loss 1277.751220703125\n",
      "epoch 23, val_loss 455.3224182128906\n",
      "epoch 24, train_loss 1277.7510986328125\n",
      "epoch 24, val_loss 455.32232666015625\n",
      "epoch 25, train_loss 1277.7510986328125\n",
      "epoch 25, val_loss 455.32220458984375\n",
      "epoch 26, train_loss 1277.7509765625\n",
      "epoch 26, val_loss 455.32208251953125\n",
      "epoch 27, train_loss 1277.7506103515625\n",
      "epoch 27, val_loss 455.3219909667969\n",
      "epoch 28, train_loss 1277.75048828125\n",
      "epoch 28, val_loss 455.32183837890625\n",
      "epoch 29, train_loss 1277.7503662109375\n",
      "epoch 29, val_loss 455.32183837890625\n",
      "epoch 30, train_loss 1277.750244140625\n",
      "epoch 30, val_loss 455.3217468261719\n",
      "epoch 31, train_loss 1277.7501220703125\n",
      "epoch 31, val_loss 455.32159423828125\n",
      "epoch 32, train_loss 1277.75\n",
      "epoch 32, val_loss 455.3215026855469\n",
      "epoch 33, train_loss 1277.7498779296875\n",
      "epoch 33, val_loss 455.3213806152344\n",
      "epoch 34, train_loss 1277.749755859375\n",
      "epoch 34, val_loss 455.3212890625\n",
      "epoch 35, train_loss 1277.7496337890625\n",
      "epoch 35, val_loss 455.3211669921875\n",
      "epoch 36, train_loss 1277.7493896484375\n",
      "epoch 36, val_loss 455.321044921875\n",
      "epoch 37, train_loss 1277.7493896484375\n",
      "epoch 37, val_loss 455.3209228515625\n",
      "epoch 38, train_loss 1277.7490234375\n",
      "epoch 38, val_loss 455.32086181640625\n",
      "epoch 39, train_loss 1277.7490234375\n",
      "epoch 39, val_loss 455.32073974609375\n",
      "epoch 40, train_loss 1277.7489013671875\n",
      "epoch 40, val_loss 455.32061767578125\n",
      "epoch 41, train_loss 1277.748779296875\n",
      "epoch 41, val_loss 455.3205871582031\n",
      "epoch 42, train_loss 1277.74853515625\n",
      "epoch 42, val_loss 455.32049560546875\n",
      "epoch 43, train_loss 1277.74853515625\n",
      "epoch 43, val_loss 455.32037353515625\n",
      "epoch 44, train_loss 1277.748291015625\n",
      "epoch 44, val_loss 455.3202819824219\n",
      "epoch 45, train_loss 1277.748291015625\n",
      "epoch 45, val_loss 455.3201599121094\n",
      "epoch 46, train_loss 1277.7481689453125\n",
      "epoch 46, val_loss 455.320068359375\n",
      "epoch 47, train_loss 1277.7479248046875\n",
      "epoch 47, val_loss 455.3199157714844\n",
      "epoch 48, train_loss 1277.747802734375\n",
      "epoch 48, val_loss 455.31988525390625\n",
      "epoch 49, train_loss 1277.7476806640625\n",
      "epoch 49, val_loss 455.31976318359375\n",
      "epoch 50, train_loss 1277.74755859375\n",
      "epoch 50, val_loss 455.3196716308594\n",
      "epoch 51, train_loss 1277.747314453125\n",
      "epoch 51, val_loss 455.3195495605469\n",
      "epoch 52, train_loss 1277.7471923828125\n",
      "epoch 52, val_loss 455.3194274902344\n",
      "epoch 53, train_loss 1277.7470703125\n",
      "epoch 53, val_loss 455.3193359375\n",
      "epoch 54, train_loss 1277.7470703125\n",
      "epoch 54, val_loss 455.3192138671875\n",
      "epoch 55, train_loss 1277.7469482421875\n",
      "epoch 55, val_loss 455.319091796875\n",
      "epoch 56, train_loss 1277.746826171875\n",
      "epoch 56, val_loss 455.3190002441406\n",
      "epoch 57, train_loss 1277.7464599609375\n",
      "epoch 57, val_loss 455.31890869140625\n",
      "epoch 58, train_loss 1277.7464599609375\n",
      "epoch 58, val_loss 455.3187561035156\n",
      "epoch 59, train_loss 1277.7462158203125\n",
      "epoch 59, val_loss 455.3187561035156\n",
      "epoch 60, train_loss 1277.7462158203125\n",
      "epoch 60, val_loss 455.3185729980469\n",
      "epoch 61, train_loss 1277.74609375\n",
      "epoch 61, val_loss 455.3185729980469\n",
      "epoch 62, train_loss 1277.745849609375\n",
      "epoch 62, val_loss 455.31842041015625\n",
      "epoch 63, train_loss 1277.7457275390625\n",
      "epoch 63, val_loss 455.31829833984375\n",
      "epoch 64, train_loss 1277.7454833984375\n",
      "epoch 64, val_loss 455.3182067871094\n",
      "epoch 65, train_loss 1277.7454833984375\n",
      "epoch 65, val_loss 455.3180847167969\n",
      "epoch 66, train_loss 1277.745361328125\n",
      "epoch 66, val_loss 455.31805419921875\n",
      "epoch 67, train_loss 1277.7451171875\n",
      "epoch 67, val_loss 455.3179931640625\n",
      "epoch 68, train_loss 1277.7449951171875\n",
      "epoch 68, val_loss 455.3177490234375\n",
      "epoch 69, train_loss 1277.744873046875\n",
      "epoch 69, val_loss 455.3176574707031\n",
      "epoch 70, train_loss 1277.7447509765625\n",
      "epoch 70, val_loss 455.3175964355469\n",
      "epoch 71, train_loss 1277.74462890625\n",
      "epoch 71, val_loss 455.3175048828125\n",
      "epoch 72, train_loss 1277.7445068359375\n",
      "epoch 72, val_loss 455.3174133300781\n",
      "epoch 73, train_loss 1277.744384765625\n",
      "epoch 73, val_loss 455.3172607421875\n",
      "epoch 74, train_loss 1277.744140625\n",
      "epoch 74, val_loss 455.3171691894531\n",
      "epoch 75, train_loss 1277.744140625\n",
      "epoch 75, val_loss 455.31707763671875\n",
      "epoch 76, train_loss 1277.743896484375\n",
      "epoch 76, val_loss 455.3169860839844\n",
      "epoch 77, train_loss 1277.743896484375\n",
      "epoch 77, val_loss 455.3168640136719\n",
      "epoch 78, train_loss 1277.7435302734375\n",
      "epoch 78, val_loss 455.3168029785156\n",
      "epoch 79, train_loss 1277.7435302734375\n",
      "epoch 79, val_loss 455.3166809082031\n",
      "epoch 80, train_loss 1277.743408203125\n",
      "epoch 80, val_loss 455.31658935546875\n",
      "epoch 81, train_loss 1277.7431640625\n",
      "epoch 81, val_loss 455.3164978027344\n",
      "epoch 82, train_loss 1277.7430419921875\n",
      "epoch 82, val_loss 455.31634521484375\n",
      "epoch 83, train_loss 1277.742919921875\n",
      "epoch 83, val_loss 455.3162841796875\n",
      "epoch 84, train_loss 1277.74267578125\n",
      "epoch 84, val_loss 455.316162109375\n",
      "epoch 85, train_loss 1277.7425537109375\n",
      "epoch 85, val_loss 455.3160400390625\n",
      "epoch 86, train_loss 1277.7425537109375\n",
      "epoch 86, val_loss 455.3159484863281\n",
      "epoch 87, train_loss 1277.7423095703125\n",
      "epoch 87, val_loss 455.3158264160156\n",
      "epoch 88, train_loss 1277.7421875\n",
      "epoch 88, val_loss 455.31573486328125\n",
      "epoch 89, train_loss 1277.7420654296875\n",
      "epoch 89, val_loss 455.3157043457031\n",
      "epoch 90, train_loss 1277.741943359375\n",
      "epoch 90, val_loss 455.3155517578125\n",
      "epoch 91, train_loss 1277.741943359375\n",
      "epoch 91, val_loss 455.3154602050781\n",
      "epoch 92, train_loss 1277.74169921875\n",
      "epoch 92, val_loss 455.31536865234375\n",
      "epoch 93, train_loss 1277.741455078125\n",
      "epoch 93, val_loss 455.3152770996094\n",
      "epoch 94, train_loss 1277.741455078125\n",
      "epoch 94, val_loss 455.31512451171875\n",
      "epoch 95, train_loss 1277.7412109375\n",
      "epoch 95, val_loss 455.3150939941406\n",
      "epoch 96, train_loss 1277.7412109375\n",
      "epoch 96, val_loss 455.3149719238281\n",
      "epoch 97, train_loss 1277.740966796875\n",
      "epoch 97, val_loss 455.31488037109375\n",
      "epoch 98, train_loss 1277.74072265625\n",
      "epoch 98, val_loss 455.31475830078125\n",
      "epoch 99, train_loss 1277.74072265625\n",
      "epoch 99, val_loss 455.314697265625\n",
      "Parameter containing:\n",
      "tensor([7.6789e-13], requires_grad=True)\n",
      "iter 60, train_loss_regularization 0.8661960959434509\n",
      "iter 60, val_loss_regularization 0.8661960959434509\n",
      "epoch 0, train_loss 1277.74072265625\n",
      "epoch 0, val_loss 455.3145751953125\n",
      "epoch 1, train_loss 1277.7403564453125\n",
      "epoch 1, val_loss 455.3144226074219\n",
      "epoch 2, train_loss 1277.7403564453125\n",
      "epoch 2, val_loss 455.3143615722656\n",
      "epoch 3, train_loss 1277.7401123046875\n",
      "epoch 3, val_loss 455.3142395019531\n",
      "epoch 4, train_loss 1277.739990234375\n",
      "epoch 4, val_loss 455.3141784667969\n",
      "epoch 5, train_loss 1277.7398681640625\n",
      "epoch 5, val_loss 455.3140869140625\n",
      "epoch 6, train_loss 1277.73974609375\n",
      "epoch 6, val_loss 455.31390380859375\n",
      "epoch 7, train_loss 1277.7396240234375\n",
      "epoch 7, val_loss 455.3138427734375\n",
      "epoch 8, train_loss 1277.7393798828125\n",
      "epoch 8, val_loss 455.3137512207031\n",
      "epoch 9, train_loss 1277.7392578125\n",
      "epoch 9, val_loss 455.31365966796875\n",
      "epoch 10, train_loss 1277.7391357421875\n",
      "epoch 10, val_loss 455.3135681152344\n",
      "epoch 11, train_loss 1277.739013671875\n",
      "epoch 11, val_loss 455.31341552734375\n",
      "epoch 12, train_loss 1277.7388916015625\n",
      "epoch 12, val_loss 455.3133850097656\n",
      "epoch 13, train_loss 1277.73876953125\n",
      "epoch 13, val_loss 455.3132629394531\n",
      "epoch 14, train_loss 1277.73876953125\n",
      "epoch 14, val_loss 455.3131103515625\n",
      "epoch 15, train_loss 1277.738525390625\n",
      "epoch 15, val_loss 455.31304931640625\n",
      "epoch 16, train_loss 1277.7384033203125\n",
      "epoch 16, val_loss 455.31292724609375\n",
      "epoch 17, train_loss 1277.73828125\n",
      "epoch 17, val_loss 455.3128356933594\n",
      "epoch 18, train_loss 1277.738037109375\n",
      "epoch 18, val_loss 455.3127136230469\n",
      "epoch 19, train_loss 1277.73779296875\n",
      "epoch 19, val_loss 455.3126525878906\n",
      "epoch 20, train_loss 1277.73779296875\n",
      "epoch 20, val_loss 455.3125\n",
      "epoch 21, train_loss 1277.7376708984375\n",
      "epoch 21, val_loss 455.3124084472656\n",
      "epoch 22, train_loss 1277.737548828125\n",
      "epoch 22, val_loss 455.3122863769531\n",
      "epoch 23, train_loss 1277.7373046875\n",
      "epoch 23, val_loss 455.31219482421875\n",
      "epoch 24, train_loss 1277.7373046875\n",
      "epoch 24, val_loss 455.3121337890625\n",
      "epoch 25, train_loss 1277.7371826171875\n",
      "epoch 25, val_loss 455.3120422363281\n",
      "epoch 26, train_loss 1277.737060546875\n",
      "epoch 26, val_loss 455.3119201660156\n",
      "epoch 27, train_loss 1277.737060546875\n",
      "epoch 27, val_loss 455.31182861328125\n",
      "epoch 28, train_loss 1277.7366943359375\n",
      "epoch 28, val_loss 455.3117370605469\n",
      "epoch 29, train_loss 1277.7364501953125\n",
      "epoch 29, val_loss 455.3116149902344\n",
      "epoch 30, train_loss 1277.7364501953125\n",
      "epoch 30, val_loss 455.3114929199219\n",
      "epoch 31, train_loss 1277.736328125\n",
      "epoch 31, val_loss 455.3114318847656\n",
      "epoch 32, train_loss 1277.7362060546875\n",
      "epoch 32, val_loss 455.31134033203125\n",
      "epoch 33, train_loss 1277.73583984375\n",
      "epoch 33, val_loss 455.3111572265625\n",
      "epoch 34, train_loss 1277.73583984375\n",
      "epoch 34, val_loss 455.31109619140625\n",
      "epoch 35, train_loss 1277.7357177734375\n",
      "epoch 35, val_loss 455.3110046386719\n",
      "epoch 36, train_loss 1277.735595703125\n",
      "epoch 36, val_loss 455.3108825683594\n",
      "epoch 37, train_loss 1277.7354736328125\n",
      "epoch 37, val_loss 455.310791015625\n",
      "epoch 38, train_loss 1277.7352294921875\n",
      "epoch 38, val_loss 455.3107604980469\n",
      "epoch 39, train_loss 1277.7352294921875\n",
      "epoch 39, val_loss 455.3105773925781\n",
      "epoch 40, train_loss 1277.735107421875\n",
      "epoch 40, val_loss 455.31048583984375\n",
      "epoch 41, train_loss 1277.73486328125\n",
      "epoch 41, val_loss 455.31036376953125\n",
      "epoch 42, train_loss 1277.73486328125\n",
      "epoch 42, val_loss 455.31036376953125\n",
      "epoch 43, train_loss 1277.734619140625\n",
      "epoch 43, val_loss 455.3101806640625\n",
      "epoch 44, train_loss 1277.7344970703125\n",
      "epoch 44, val_loss 455.3100891113281\n",
      "epoch 45, train_loss 1277.734375\n",
      "epoch 45, val_loss 455.30999755859375\n",
      "epoch 46, train_loss 1277.734130859375\n",
      "epoch 46, val_loss 455.3099060058594\n",
      "epoch 47, train_loss 1277.734130859375\n",
      "epoch 47, val_loss 455.3097839355469\n",
      "epoch 48, train_loss 1277.7340087890625\n",
      "epoch 48, val_loss 455.30975341796875\n",
      "epoch 49, train_loss 1277.73388671875\n",
      "epoch 49, val_loss 455.30963134765625\n",
      "epoch 50, train_loss 1277.7337646484375\n",
      "epoch 50, val_loss 455.30950927734375\n",
      "epoch 51, train_loss 1277.733642578125\n",
      "epoch 51, val_loss 455.30938720703125\n",
      "epoch 52, train_loss 1277.7335205078125\n",
      "epoch 52, val_loss 455.3092956542969\n",
      "epoch 53, train_loss 1277.7335205078125\n",
      "epoch 53, val_loss 455.3091125488281\n",
      "epoch 54, train_loss 1277.7330322265625\n",
      "epoch 54, val_loss 455.30908203125\n",
      "epoch 55, train_loss 1277.7330322265625\n",
      "epoch 55, val_loss 455.3089904785156\n",
      "epoch 56, train_loss 1277.73291015625\n",
      "epoch 56, val_loss 455.3088684082031\n",
      "epoch 57, train_loss 1277.7325439453125\n",
      "epoch 57, val_loss 455.30877685546875\n",
      "epoch 58, train_loss 1277.7327880859375\n",
      "epoch 58, val_loss 455.3086242675781\n",
      "epoch 59, train_loss 1277.732421875\n",
      "epoch 59, val_loss 455.3086242675781\n",
      "epoch 60, train_loss 1277.7322998046875\n",
      "epoch 60, val_loss 455.3084716796875\n",
      "epoch 61, train_loss 1277.7322998046875\n",
      "epoch 61, val_loss 455.30841064453125\n",
      "epoch 62, train_loss 1277.732177734375\n",
      "epoch 62, val_loss 455.30828857421875\n",
      "epoch 63, train_loss 1277.73193359375\n",
      "epoch 63, val_loss 455.3081970214844\n",
      "epoch 64, train_loss 1277.731689453125\n",
      "epoch 64, val_loss 455.3080749511719\n",
      "epoch 65, train_loss 1277.731689453125\n",
      "epoch 65, val_loss 455.3080139160156\n",
      "epoch 66, train_loss 1277.7314453125\n",
      "epoch 66, val_loss 455.30792236328125\n",
      "epoch 67, train_loss 1277.7314453125\n",
      "epoch 67, val_loss 455.3077392578125\n",
      "epoch 68, train_loss 1277.731201171875\n",
      "epoch 68, val_loss 455.3076171875\n",
      "epoch 69, train_loss 1277.73095703125\n",
      "epoch 69, val_loss 455.30755615234375\n",
      "epoch 70, train_loss 1277.73095703125\n",
      "epoch 70, val_loss 455.3074951171875\n",
      "epoch 71, train_loss 1277.730712890625\n",
      "epoch 71, val_loss 455.3073425292969\n",
      "epoch 72, train_loss 1277.730712890625\n",
      "epoch 72, val_loss 455.3072204589844\n",
      "epoch 73, train_loss 1277.7305908203125\n",
      "epoch 73, val_loss 455.30712890625\n",
      "epoch 74, train_loss 1277.73046875\n",
      "epoch 74, val_loss 455.3070983886719\n",
      "epoch 75, train_loss 1277.7303466796875\n",
      "epoch 75, val_loss 455.30694580078125\n",
      "epoch 76, train_loss 1277.7301025390625\n",
      "epoch 76, val_loss 455.3069152832031\n",
      "epoch 77, train_loss 1277.72998046875\n",
      "epoch 77, val_loss 455.3067626953125\n",
      "epoch 78, train_loss 1277.7298583984375\n",
      "epoch 78, val_loss 455.3066711425781\n",
      "epoch 79, train_loss 1277.7296142578125\n",
      "epoch 79, val_loss 455.3065490722656\n",
      "epoch 80, train_loss 1277.7296142578125\n",
      "epoch 80, val_loss 455.3064880371094\n",
      "epoch 81, train_loss 1277.7293701171875\n",
      "epoch 81, val_loss 455.3064270019531\n",
      "epoch 82, train_loss 1277.7293701171875\n",
      "epoch 82, val_loss 455.3062438964844\n",
      "epoch 83, train_loss 1277.7291259765625\n",
      "epoch 83, val_loss 455.30615234375\n",
      "epoch 84, train_loss 1277.7288818359375\n",
      "epoch 84, val_loss 455.30609130859375\n",
      "epoch 85, train_loss 1277.7286376953125\n",
      "epoch 85, val_loss 455.3059997558594\n",
      "epoch 86, train_loss 1277.7288818359375\n",
      "epoch 86, val_loss 455.3058776855469\n",
      "epoch 87, train_loss 1277.7286376953125\n",
      "epoch 87, val_loss 455.3057556152344\n",
      "epoch 88, train_loss 1277.728515625\n",
      "epoch 88, val_loss 455.3056640625\n",
      "epoch 89, train_loss 1277.7283935546875\n",
      "epoch 89, val_loss 455.3055114746094\n",
      "epoch 90, train_loss 1277.7281494140625\n",
      "epoch 90, val_loss 455.3054504394531\n",
      "epoch 91, train_loss 1277.72802734375\n",
      "epoch 91, val_loss 455.3053894042969\n",
      "epoch 92, train_loss 1277.7279052734375\n",
      "epoch 92, val_loss 455.3052062988281\n",
      "epoch 93, train_loss 1277.727783203125\n",
      "epoch 93, val_loss 455.3052062988281\n",
      "epoch 94, train_loss 1277.7275390625\n",
      "epoch 94, val_loss 455.30499267578125\n",
      "epoch 95, train_loss 1277.7274169921875\n",
      "epoch 95, val_loss 455.30499267578125\n",
      "epoch 96, train_loss 1277.727294921875\n",
      "epoch 96, val_loss 455.3048400878906\n",
      "epoch 97, train_loss 1277.7271728515625\n",
      "epoch 97, val_loss 455.30474853515625\n",
      "epoch 98, train_loss 1277.727294921875\n",
      "epoch 98, val_loss 455.3046569824219\n",
      "epoch 99, train_loss 1277.726806640625\n",
      "epoch 99, val_loss 455.3045959472656\n",
      "Parameter containing:\n",
      "tensor([5.0768e-13], requires_grad=True)\n",
      "iter 61, train_loss_regularization 0.8623447418212891\n",
      "iter 61, val_loss_regularization 0.8623447418212891\n",
      "epoch 0, train_loss 1277.726806640625\n",
      "epoch 0, val_loss 455.304443359375\n",
      "epoch 1, train_loss 1277.7266845703125\n",
      "epoch 1, val_loss 455.3043212890625\n",
      "epoch 2, train_loss 1277.7266845703125\n",
      "epoch 2, val_loss 455.30426025390625\n",
      "epoch 3, train_loss 1277.7264404296875\n",
      "epoch 3, val_loss 455.3041687011719\n",
      "epoch 4, train_loss 1277.726318359375\n",
      "epoch 4, val_loss 455.3040466308594\n",
      "epoch 5, train_loss 1277.7261962890625\n",
      "epoch 5, val_loss 455.3039245605469\n",
      "epoch 6, train_loss 1277.7259521484375\n",
      "epoch 6, val_loss 455.3038330078125\n",
      "epoch 7, train_loss 1277.725830078125\n",
      "epoch 7, val_loss 455.3037414550781\n",
      "epoch 8, train_loss 1277.7257080078125\n",
      "epoch 8, val_loss 455.3036804199219\n",
      "epoch 9, train_loss 1277.7254638671875\n",
      "epoch 9, val_loss 455.3035888671875\n",
      "epoch 10, train_loss 1277.7254638671875\n",
      "epoch 10, val_loss 455.3034973144531\n",
      "epoch 11, train_loss 1277.7254638671875\n",
      "epoch 11, val_loss 455.3033447265625\n",
      "epoch 12, train_loss 1277.7252197265625\n",
      "epoch 12, val_loss 455.30328369140625\n",
      "epoch 13, train_loss 1277.72509765625\n",
      "epoch 13, val_loss 455.30316162109375\n",
      "epoch 14, train_loss 1277.7249755859375\n",
      "epoch 14, val_loss 455.3030700683594\n",
      "epoch 15, train_loss 1277.724853515625\n",
      "epoch 15, val_loss 455.30291748046875\n",
      "epoch 16, train_loss 1277.724609375\n",
      "epoch 16, val_loss 455.3028259277344\n",
      "epoch 17, train_loss 1277.724609375\n",
      "epoch 17, val_loss 455.3027648925781\n",
      "epoch 18, train_loss 1277.724365234375\n",
      "epoch 18, val_loss 455.30267333984375\n",
      "epoch 19, train_loss 1277.72412109375\n",
      "epoch 19, val_loss 455.30255126953125\n",
      "epoch 20, train_loss 1277.72412109375\n",
      "epoch 20, val_loss 455.3024597167969\n",
      "epoch 21, train_loss 1277.723876953125\n",
      "epoch 21, val_loss 455.3023376464844\n",
      "epoch 22, train_loss 1277.7239990234375\n",
      "epoch 22, val_loss 455.3022155761719\n",
      "epoch 23, train_loss 1277.723876953125\n",
      "epoch 23, val_loss 455.3021545410156\n",
      "epoch 24, train_loss 1277.7236328125\n",
      "epoch 24, val_loss 455.3020935058594\n",
      "epoch 25, train_loss 1277.723388671875\n",
      "epoch 25, val_loss 455.3019714355469\n",
      "epoch 26, train_loss 1277.723388671875\n",
      "epoch 26, val_loss 455.3018798828125\n",
      "epoch 27, train_loss 1277.72314453125\n",
      "epoch 27, val_loss 455.3017578125\n",
      "epoch 28, train_loss 1277.7230224609375\n",
      "epoch 28, val_loss 455.3016662597656\n",
      "epoch 29, train_loss 1277.7230224609375\n",
      "epoch 29, val_loss 455.3015441894531\n",
      "epoch 30, train_loss 1277.7227783203125\n",
      "epoch 30, val_loss 455.30145263671875\n",
      "epoch 31, train_loss 1277.72265625\n",
      "epoch 31, val_loss 455.3013610839844\n",
      "epoch 32, train_loss 1277.722412109375\n",
      "epoch 32, val_loss 455.3013000488281\n",
      "epoch 33, train_loss 1277.7225341796875\n",
      "epoch 33, val_loss 455.3011169433594\n",
      "epoch 34, train_loss 1277.7222900390625\n",
      "epoch 34, val_loss 455.3009948730469\n",
      "epoch 35, train_loss 1277.7220458984375\n",
      "epoch 35, val_loss 455.30096435546875\n",
      "epoch 36, train_loss 1277.7220458984375\n",
      "epoch 36, val_loss 455.30078125\n",
      "epoch 37, train_loss 1277.7218017578125\n",
      "epoch 37, val_loss 455.30078125\n",
      "epoch 38, train_loss 1277.7218017578125\n",
      "epoch 38, val_loss 455.3006286621094\n",
      "epoch 39, train_loss 1277.7215576171875\n",
      "epoch 39, val_loss 455.300537109375\n",
      "epoch 40, train_loss 1277.7213134765625\n",
      "epoch 40, val_loss 455.3004455566406\n",
      "epoch 41, train_loss 1277.72119140625\n",
      "epoch 41, val_loss 455.3003234863281\n",
      "epoch 42, train_loss 1277.72119140625\n",
      "epoch 42, val_loss 455.30029296875\n",
      "epoch 43, train_loss 1277.7210693359375\n",
      "epoch 43, val_loss 455.3001708984375\n",
      "epoch 44, train_loss 1277.7208251953125\n",
      "epoch 44, val_loss 455.300048828125\n",
      "epoch 45, train_loss 1277.720703125\n",
      "epoch 45, val_loss 455.2999572753906\n",
      "epoch 46, train_loss 1277.720458984375\n",
      "epoch 46, val_loss 455.2998352050781\n",
      "epoch 47, train_loss 1277.720458984375\n",
      "epoch 47, val_loss 455.2998046875\n",
      "epoch 48, train_loss 1277.720458984375\n",
      "epoch 48, val_loss 455.2996520996094\n",
      "epoch 49, train_loss 1277.7203369140625\n",
      "epoch 49, val_loss 455.2995910644531\n",
      "epoch 50, train_loss 1277.719970703125\n",
      "epoch 50, val_loss 455.2994079589844\n",
      "epoch 51, train_loss 1277.7198486328125\n",
      "epoch 51, val_loss 455.29937744140625\n",
      "epoch 52, train_loss 1277.7198486328125\n",
      "epoch 52, val_loss 455.2993469238281\n",
      "epoch 53, train_loss 1277.719482421875\n",
      "epoch 53, val_loss 455.2991638183594\n",
      "epoch 54, train_loss 1277.7196044921875\n",
      "epoch 54, val_loss 455.299072265625\n",
      "epoch 55, train_loss 1277.7193603515625\n",
      "epoch 55, val_loss 455.2989196777344\n",
      "epoch 56, train_loss 1277.7191162109375\n",
      "epoch 56, val_loss 455.298828125\n",
      "epoch 57, train_loss 1277.718994140625\n",
      "epoch 57, val_loss 455.2987976074219\n",
      "epoch 58, train_loss 1277.718994140625\n",
      "epoch 58, val_loss 455.2986145019531\n",
      "epoch 59, train_loss 1277.71875\n",
      "epoch 59, val_loss 455.298583984375\n",
      "epoch 60, train_loss 1277.7186279296875\n",
      "epoch 60, val_loss 455.2984924316406\n",
      "epoch 61, train_loss 1277.7186279296875\n",
      "epoch 61, val_loss 455.29833984375\n",
      "epoch 62, train_loss 1277.718505859375\n",
      "epoch 62, val_loss 455.29827880859375\n",
      "epoch 63, train_loss 1277.71826171875\n",
      "epoch 63, val_loss 455.2981262207031\n",
      "epoch 64, train_loss 1277.7181396484375\n",
      "epoch 64, val_loss 455.2981262207031\n",
      "epoch 65, train_loss 1277.718017578125\n",
      "epoch 65, val_loss 455.2979431152344\n",
      "epoch 66, train_loss 1277.7177734375\n",
      "epoch 66, val_loss 455.2978820800781\n",
      "epoch 67, train_loss 1277.717529296875\n",
      "epoch 67, val_loss 455.2977600097656\n",
      "epoch 68, train_loss 1277.7176513671875\n",
      "epoch 68, val_loss 455.29766845703125\n",
      "epoch 69, train_loss 1277.717529296875\n",
      "epoch 69, val_loss 455.2975769042969\n",
      "epoch 70, train_loss 1277.7174072265625\n",
      "epoch 70, val_loss 455.2974853515625\n",
      "epoch 71, train_loss 1277.71728515625\n",
      "epoch 71, val_loss 455.29742431640625\n",
      "epoch 72, train_loss 1277.717041015625\n",
      "epoch 72, val_loss 455.2972412109375\n",
      "epoch 73, train_loss 1277.717041015625\n",
      "epoch 73, val_loss 455.29718017578125\n",
      "epoch 74, train_loss 1277.7169189453125\n",
      "epoch 74, val_loss 455.2970886230469\n",
      "epoch 75, train_loss 1277.716552734375\n",
      "epoch 75, val_loss 455.2969970703125\n",
      "epoch 76, train_loss 1277.716552734375\n",
      "epoch 76, val_loss 455.2969055175781\n",
      "epoch 77, train_loss 1277.7164306640625\n",
      "epoch 77, val_loss 455.2967529296875\n",
      "epoch 78, train_loss 1277.7161865234375\n",
      "epoch 78, val_loss 455.296630859375\n",
      "epoch 79, train_loss 1277.716064453125\n",
      "epoch 79, val_loss 455.29656982421875\n",
      "epoch 80, train_loss 1277.7159423828125\n",
      "epoch 80, val_loss 455.29644775390625\n",
      "epoch 81, train_loss 1277.7156982421875\n",
      "epoch 81, val_loss 455.2964172363281\n",
      "epoch 82, train_loss 1277.7156982421875\n",
      "epoch 82, val_loss 455.2962646484375\n",
      "epoch 83, train_loss 1277.7156982421875\n",
      "epoch 83, val_loss 455.2961120605469\n",
      "epoch 84, train_loss 1277.7156982421875\n",
      "epoch 84, val_loss 455.29608154296875\n",
      "epoch 85, train_loss 1277.71533203125\n",
      "epoch 85, val_loss 455.2959899902344\n",
      "epoch 86, train_loss 1277.7152099609375\n",
      "epoch 86, val_loss 455.2958679199219\n",
      "epoch 87, train_loss 1277.7152099609375\n",
      "epoch 87, val_loss 455.2958068847656\n",
      "epoch 88, train_loss 1277.7149658203125\n",
      "epoch 88, val_loss 455.29559326171875\n",
      "epoch 89, train_loss 1277.7147216796875\n",
      "epoch 89, val_loss 455.29559326171875\n",
      "epoch 90, train_loss 1277.7147216796875\n",
      "epoch 90, val_loss 455.29547119140625\n",
      "epoch 91, train_loss 1277.7144775390625\n",
      "epoch 91, val_loss 455.29541015625\n",
      "epoch 92, train_loss 1277.7144775390625\n",
      "epoch 92, val_loss 455.2952880859375\n",
      "epoch 93, train_loss 1277.7142333984375\n",
      "epoch 93, val_loss 455.295166015625\n",
      "epoch 94, train_loss 1277.714111328125\n",
      "epoch 94, val_loss 455.2950439453125\n",
      "epoch 95, train_loss 1277.714111328125\n",
      "epoch 95, val_loss 455.2950134277344\n",
      "epoch 96, train_loss 1277.7138671875\n",
      "epoch 96, val_loss 455.29486083984375\n",
      "epoch 97, train_loss 1277.713623046875\n",
      "epoch 97, val_loss 455.2947998046875\n",
      "epoch 98, train_loss 1277.713623046875\n",
      "epoch 98, val_loss 455.2947082519531\n",
      "epoch 99, train_loss 1277.71337890625\n",
      "epoch 99, val_loss 455.2945861816406\n",
      "Parameter containing:\n",
      "tensor([3.3604e-13], requires_grad=True)\n",
      "iter 62, train_loss_regularization 0.8585591316223145\n",
      "iter 62, val_loss_regularization 0.8585591316223145\n",
      "epoch 0, train_loss 1277.71337890625\n",
      "epoch 0, val_loss 455.29449462890625\n",
      "epoch 1, train_loss 1277.7132568359375\n",
      "epoch 1, val_loss 455.2943420410156\n",
      "epoch 2, train_loss 1277.713134765625\n",
      "epoch 2, val_loss 455.2942810058594\n",
      "epoch 3, train_loss 1277.712890625\n",
      "epoch 3, val_loss 455.2941589355469\n",
      "epoch 4, train_loss 1277.7127685546875\n",
      "epoch 4, val_loss 455.29412841796875\n",
      "epoch 5, train_loss 1277.7127685546875\n",
      "epoch 5, val_loss 455.29400634765625\n",
      "epoch 6, train_loss 1277.7127685546875\n",
      "epoch 6, val_loss 455.29388427734375\n",
      "epoch 7, train_loss 1277.7122802734375\n",
      "epoch 7, val_loss 455.29376220703125\n",
      "epoch 8, train_loss 1277.7122802734375\n",
      "epoch 8, val_loss 455.2936706542969\n",
      "epoch 9, train_loss 1277.7122802734375\n",
      "epoch 9, val_loss 455.2935791015625\n",
      "epoch 10, train_loss 1277.7119140625\n",
      "epoch 10, val_loss 455.2934875488281\n",
      "epoch 11, train_loss 1277.7119140625\n",
      "epoch 11, val_loss 455.2933654785156\n",
      "epoch 12, train_loss 1277.7117919921875\n",
      "epoch 12, val_loss 455.2933044433594\n",
      "epoch 13, train_loss 1277.7115478515625\n",
      "epoch 13, val_loss 455.293212890625\n",
      "epoch 14, train_loss 1277.71142578125\n",
      "epoch 14, val_loss 455.2931213378906\n",
      "epoch 15, train_loss 1277.7113037109375\n",
      "epoch 15, val_loss 455.2929992675781\n",
      "epoch 16, train_loss 1277.711181640625\n",
      "epoch 16, val_loss 455.29290771484375\n",
      "epoch 17, train_loss 1277.711181640625\n",
      "epoch 17, val_loss 455.29278564453125\n",
      "epoch 18, train_loss 1277.7110595703125\n",
      "epoch 18, val_loss 455.2926940917969\n",
      "epoch 19, train_loss 1277.7108154296875\n",
      "epoch 19, val_loss 455.2926330566406\n",
      "epoch 20, train_loss 1277.710693359375\n",
      "epoch 20, val_loss 455.29254150390625\n",
      "epoch 21, train_loss 1277.710693359375\n",
      "epoch 21, val_loss 455.29241943359375\n",
      "epoch 22, train_loss 1277.7103271484375\n",
      "epoch 22, val_loss 455.29229736328125\n",
      "epoch 23, train_loss 1277.710205078125\n",
      "epoch 23, val_loss 455.2922058105469\n",
      "epoch 24, train_loss 1277.710205078125\n",
      "epoch 24, val_loss 455.2921142578125\n",
      "epoch 25, train_loss 1277.7099609375\n",
      "epoch 25, val_loss 455.29205322265625\n",
      "epoch 26, train_loss 1277.7099609375\n",
      "epoch 26, val_loss 455.29193115234375\n",
      "epoch 27, train_loss 1277.709716796875\n",
      "epoch 27, val_loss 455.2918395996094\n",
      "epoch 28, train_loss 1277.70947265625\n",
      "epoch 28, val_loss 455.2917175292969\n",
      "epoch 29, train_loss 1277.70947265625\n",
      "epoch 29, val_loss 455.2916564941406\n",
      "epoch 30, train_loss 1277.70947265625\n",
      "epoch 30, val_loss 455.2914733886719\n",
      "epoch 31, train_loss 1277.7093505859375\n",
      "epoch 31, val_loss 455.2914123535156\n",
      "epoch 32, train_loss 1277.7091064453125\n",
      "epoch 32, val_loss 455.29132080078125\n",
      "epoch 33, train_loss 1277.708984375\n",
      "epoch 33, val_loss 455.29119873046875\n",
      "epoch 34, train_loss 1277.7088623046875\n",
      "epoch 34, val_loss 455.2911376953125\n",
      "epoch 35, train_loss 1277.7086181640625\n",
      "epoch 35, val_loss 455.2909851074219\n",
      "epoch 36, train_loss 1277.7086181640625\n",
      "epoch 36, val_loss 455.2909240722656\n",
      "epoch 37, train_loss 1277.70849609375\n",
      "epoch 37, val_loss 455.29083251953125\n",
      "epoch 38, train_loss 1277.7083740234375\n",
      "epoch 38, val_loss 455.2907409667969\n",
      "epoch 39, train_loss 1277.7081298828125\n",
      "epoch 39, val_loss 455.2906188964844\n",
      "epoch 40, train_loss 1277.7081298828125\n",
      "epoch 40, val_loss 455.29046630859375\n",
      "epoch 41, train_loss 1277.7076416015625\n",
      "epoch 41, val_loss 455.29046630859375\n",
      "epoch 42, train_loss 1277.7078857421875\n",
      "epoch 42, val_loss 455.29034423828125\n",
      "epoch 43, train_loss 1277.7076416015625\n",
      "epoch 43, val_loss 455.290283203125\n",
      "epoch 44, train_loss 1277.7073974609375\n",
      "epoch 44, val_loss 455.2901611328125\n",
      "epoch 45, train_loss 1277.7073974609375\n",
      "epoch 45, val_loss 455.2900390625\n",
      "epoch 46, train_loss 1277.7071533203125\n",
      "epoch 46, val_loss 455.2899169921875\n",
      "epoch 47, train_loss 1277.7071533203125\n",
      "epoch 47, val_loss 455.2898864746094\n",
      "epoch 48, train_loss 1277.706787109375\n",
      "epoch 48, val_loss 455.2897644042969\n",
      "epoch 49, train_loss 1277.706787109375\n",
      "epoch 49, val_loss 455.2896728515625\n",
      "epoch 50, train_loss 1277.706787109375\n",
      "epoch 50, val_loss 455.28948974609375\n",
      "epoch 51, train_loss 1277.7064208984375\n",
      "epoch 51, val_loss 455.2894287109375\n",
      "epoch 52, train_loss 1277.7064208984375\n",
      "epoch 52, val_loss 455.2893371582031\n",
      "epoch 53, train_loss 1277.7064208984375\n",
      "epoch 53, val_loss 455.28924560546875\n",
      "epoch 54, train_loss 1277.7061767578125\n",
      "epoch 54, val_loss 455.2891540527344\n",
      "epoch 55, train_loss 1277.7059326171875\n",
      "epoch 55, val_loss 455.2890319824219\n",
      "epoch 56, train_loss 1277.7060546875\n",
      "epoch 56, val_loss 455.2889709472656\n",
      "epoch 57, train_loss 1277.7056884765625\n",
      "epoch 57, val_loss 455.288818359375\n",
      "epoch 58, train_loss 1277.70556640625\n",
      "epoch 58, val_loss 455.2887878417969\n",
      "epoch 59, train_loss 1277.70556640625\n",
      "epoch 59, val_loss 455.28863525390625\n",
      "epoch 60, train_loss 1277.705322265625\n",
      "epoch 60, val_loss 455.28857421875\n",
      "epoch 61, train_loss 1277.705078125\n",
      "epoch 61, val_loss 455.28851318359375\n",
      "epoch 62, train_loss 1277.7049560546875\n",
      "epoch 62, val_loss 455.2883605957031\n",
      "epoch 63, train_loss 1277.7049560546875\n",
      "epoch 63, val_loss 455.2882385253906\n",
      "epoch 64, train_loss 1277.7047119140625\n",
      "epoch 64, val_loss 455.2881164550781\n",
      "epoch 65, train_loss 1277.70458984375\n",
      "epoch 65, val_loss 455.2880859375\n",
      "epoch 66, train_loss 1277.70458984375\n",
      "epoch 66, val_loss 455.2879943847656\n",
      "epoch 67, train_loss 1277.704345703125\n",
      "epoch 67, val_loss 455.2878723144531\n",
      "epoch 68, train_loss 1277.7042236328125\n",
      "epoch 68, val_loss 455.287841796875\n",
      "epoch 69, train_loss 1277.7042236328125\n",
      "epoch 69, val_loss 455.28765869140625\n",
      "epoch 70, train_loss 1277.7039794921875\n",
      "epoch 70, val_loss 455.28759765625\n",
      "epoch 71, train_loss 1277.703857421875\n",
      "epoch 71, val_loss 455.2874450683594\n",
      "epoch 72, train_loss 1277.7037353515625\n",
      "epoch 72, val_loss 455.28741455078125\n",
      "epoch 73, train_loss 1277.7037353515625\n",
      "epoch 73, val_loss 455.2872619628906\n",
      "epoch 74, train_loss 1277.7034912109375\n",
      "epoch 74, val_loss 455.28717041015625\n",
      "epoch 75, train_loss 1277.703369140625\n",
      "epoch 75, val_loss 455.2870788574219\n",
      "epoch 76, train_loss 1277.703125\n",
      "epoch 76, val_loss 455.2869873046875\n",
      "epoch 77, train_loss 1277.703125\n",
      "epoch 77, val_loss 455.286865234375\n",
      "epoch 78, train_loss 1277.703125\n",
      "epoch 78, val_loss 455.2868347167969\n",
      "epoch 79, train_loss 1277.702880859375\n",
      "epoch 79, val_loss 455.2866516113281\n",
      "epoch 80, train_loss 1277.70263671875\n",
      "epoch 80, val_loss 455.28662109375\n",
      "epoch 81, train_loss 1277.70263671875\n",
      "epoch 81, val_loss 455.2864990234375\n",
      "epoch 82, train_loss 1277.702392578125\n",
      "epoch 82, val_loss 455.2864685058594\n",
      "epoch 83, train_loss 1277.702392578125\n",
      "epoch 83, val_loss 455.2862854003906\n",
      "epoch 84, train_loss 1277.7020263671875\n",
      "epoch 84, val_loss 455.28619384765625\n",
      "epoch 85, train_loss 1277.7020263671875\n",
      "epoch 85, val_loss 455.28607177734375\n",
      "epoch 86, train_loss 1277.701904296875\n",
      "epoch 86, val_loss 455.2860107421875\n",
      "epoch 87, train_loss 1277.7017822265625\n",
      "epoch 87, val_loss 455.285888671875\n",
      "epoch 88, train_loss 1277.7017822265625\n",
      "epoch 88, val_loss 455.2857971191406\n",
      "epoch 89, train_loss 1277.7012939453125\n",
      "epoch 89, val_loss 455.28570556640625\n",
      "epoch 90, train_loss 1277.7012939453125\n",
      "epoch 90, val_loss 455.28558349609375\n",
      "epoch 91, train_loss 1277.7012939453125\n",
      "epoch 91, val_loss 455.2855529785156\n",
      "epoch 92, train_loss 1277.7012939453125\n",
      "epoch 92, val_loss 455.2854309082031\n",
      "epoch 93, train_loss 1277.7010498046875\n",
      "epoch 93, val_loss 455.28533935546875\n",
      "epoch 94, train_loss 1277.7008056640625\n",
      "epoch 94, val_loss 455.28521728515625\n",
      "epoch 95, train_loss 1277.70068359375\n",
      "epoch 95, val_loss 455.2851257324219\n",
      "epoch 96, train_loss 1277.70068359375\n",
      "epoch 96, val_loss 455.2850036621094\n",
      "epoch 97, train_loss 1277.700439453125\n",
      "epoch 97, val_loss 455.2848815917969\n",
      "epoch 98, train_loss 1277.7003173828125\n",
      "epoch 98, val_loss 455.2848205566406\n",
      "epoch 99, train_loss 1277.7001953125\n",
      "epoch 99, val_loss 455.2846984863281\n",
      "Parameter containing:\n",
      "tensor([2.2269e-13], requires_grad=True)\n",
      "iter 63, train_loss_regularization 0.8548386693000793\n",
      "iter 63, val_loss_regularization 0.8548386693000793\n",
      "epoch 0, train_loss 1277.699951171875\n",
      "epoch 0, val_loss 455.28466796875\n",
      "epoch 1, train_loss 1277.699951171875\n",
      "epoch 1, val_loss 455.2845764160156\n",
      "epoch 2, train_loss 1277.699951171875\n",
      "epoch 2, val_loss 455.28436279296875\n",
      "epoch 3, train_loss 1277.69970703125\n",
      "epoch 3, val_loss 455.28436279296875\n",
      "epoch 4, train_loss 1277.699462890625\n",
      "epoch 4, val_loss 455.28424072265625\n",
      "epoch 5, train_loss 1277.6993408203125\n",
      "epoch 5, val_loss 455.2842102050781\n",
      "epoch 6, train_loss 1277.69921875\n",
      "epoch 6, val_loss 455.2840881347656\n",
      "epoch 7, train_loss 1277.69921875\n",
      "epoch 7, val_loss 455.2839050292969\n",
      "epoch 8, train_loss 1277.698974609375\n",
      "epoch 8, val_loss 455.2838439941406\n",
      "epoch 9, train_loss 1277.698974609375\n",
      "epoch 9, val_loss 455.28375244140625\n",
      "epoch 10, train_loss 1277.69873046875\n",
      "epoch 10, val_loss 455.2836608886719\n",
      "epoch 11, train_loss 1277.698486328125\n",
      "epoch 11, val_loss 455.2835693359375\n",
      "epoch 12, train_loss 1277.698486328125\n",
      "epoch 12, val_loss 455.2834167480469\n",
      "epoch 13, train_loss 1277.6983642578125\n",
      "epoch 13, val_loss 455.2833251953125\n",
      "epoch 14, train_loss 1277.6983642578125\n",
      "epoch 14, val_loss 455.283203125\n",
      "epoch 15, train_loss 1277.697998046875\n",
      "epoch 15, val_loss 455.2831726074219\n",
      "epoch 16, train_loss 1277.697998046875\n",
      "epoch 16, val_loss 455.2830810546875\n",
      "epoch 17, train_loss 1277.6978759765625\n",
      "epoch 17, val_loss 455.282958984375\n",
      "epoch 18, train_loss 1277.6976318359375\n",
      "epoch 18, val_loss 455.2828369140625\n",
      "epoch 19, train_loss 1277.6976318359375\n",
      "epoch 19, val_loss 455.2827453613281\n",
      "epoch 20, train_loss 1277.697509765625\n",
      "epoch 20, val_loss 455.28271484375\n",
      "epoch 21, train_loss 1277.6973876953125\n",
      "epoch 21, val_loss 455.28253173828125\n",
      "epoch 22, train_loss 1277.6971435546875\n",
      "epoch 22, val_loss 455.2825012207031\n",
      "epoch 23, train_loss 1277.697021484375\n",
      "epoch 23, val_loss 455.2823791503906\n",
      "epoch 24, train_loss 1277.697021484375\n",
      "epoch 24, val_loss 455.2823181152344\n",
      "epoch 25, train_loss 1277.69677734375\n",
      "epoch 25, val_loss 455.28216552734375\n",
      "epoch 26, train_loss 1277.6966552734375\n",
      "epoch 26, val_loss 455.2820739746094\n",
      "epoch 27, train_loss 1277.696533203125\n",
      "epoch 27, val_loss 455.2819519042969\n",
      "epoch 28, train_loss 1277.696533203125\n",
      "epoch 28, val_loss 455.2818603515625\n",
      "epoch 29, train_loss 1277.6962890625\n",
      "epoch 29, val_loss 455.28179931640625\n",
      "epoch 30, train_loss 1277.6961669921875\n",
      "epoch 30, val_loss 455.28173828125\n",
      "epoch 31, train_loss 1277.696044921875\n",
      "epoch 31, val_loss 455.28155517578125\n",
      "epoch 32, train_loss 1277.6959228515625\n",
      "epoch 32, val_loss 455.281494140625\n",
      "epoch 33, train_loss 1277.69580078125\n",
      "epoch 33, val_loss 455.2813720703125\n",
      "epoch 34, train_loss 1277.6956787109375\n",
      "epoch 34, val_loss 455.2813720703125\n",
      "epoch 35, train_loss 1277.695556640625\n",
      "epoch 35, val_loss 455.28125\n",
      "epoch 36, train_loss 1277.6953125\n",
      "epoch 36, val_loss 455.2810974121094\n",
      "epoch 37, train_loss 1277.6951904296875\n",
      "epoch 37, val_loss 455.281005859375\n",
      "epoch 38, train_loss 1277.6951904296875\n",
      "epoch 38, val_loss 455.2809143066406\n",
      "epoch 39, train_loss 1277.695068359375\n",
      "epoch 39, val_loss 455.2807922363281\n",
      "epoch 40, train_loss 1277.69482421875\n",
      "epoch 40, val_loss 455.2807922363281\n",
      "epoch 41, train_loss 1277.6947021484375\n",
      "epoch 41, val_loss 455.2806396484375\n",
      "epoch 42, train_loss 1277.6944580078125\n",
      "epoch 42, val_loss 455.2805480957031\n",
      "epoch 43, train_loss 1277.6944580078125\n",
      "epoch 43, val_loss 455.2804260253906\n",
      "epoch 44, train_loss 1277.6943359375\n",
      "epoch 44, val_loss 455.2803649902344\n",
      "epoch 45, train_loss 1277.6942138671875\n",
      "epoch 45, val_loss 455.28021240234375\n",
      "epoch 46, train_loss 1277.694091796875\n",
      "epoch 46, val_loss 455.2801818847656\n",
      "epoch 47, train_loss 1277.6939697265625\n",
      "epoch 47, val_loss 455.28009033203125\n",
      "epoch 48, train_loss 1277.6937255859375\n",
      "epoch 48, val_loss 455.2799987792969\n",
      "epoch 49, train_loss 1277.693603515625\n",
      "epoch 49, val_loss 455.27984619140625\n",
      "epoch 50, train_loss 1277.6937255859375\n",
      "epoch 50, val_loss 455.2797546386719\n",
      "epoch 51, train_loss 1277.693359375\n",
      "epoch 51, val_loss 455.2796630859375\n",
      "epoch 52, train_loss 1277.693115234375\n",
      "epoch 52, val_loss 455.2795715332031\n",
      "epoch 53, train_loss 1277.693115234375\n",
      "epoch 53, val_loss 455.2795104980469\n",
      "epoch 54, train_loss 1277.6929931640625\n",
      "epoch 54, val_loss 455.2793884277344\n",
      "epoch 55, train_loss 1277.69287109375\n",
      "epoch 55, val_loss 455.27923583984375\n",
      "epoch 56, train_loss 1277.6927490234375\n",
      "epoch 56, val_loss 455.2791748046875\n",
      "epoch 57, train_loss 1277.692626953125\n",
      "epoch 57, val_loss 455.2790832519531\n",
      "epoch 58, train_loss 1277.6923828125\n",
      "epoch 58, val_loss 455.27899169921875\n",
      "epoch 59, train_loss 1277.6922607421875\n",
      "epoch 59, val_loss 455.2789306640625\n",
      "epoch 60, train_loss 1277.6923828125\n",
      "epoch 60, val_loss 455.27874755859375\n",
      "epoch 61, train_loss 1277.692138671875\n",
      "epoch 61, val_loss 455.2786560058594\n",
      "epoch 62, train_loss 1277.69189453125\n",
      "epoch 62, val_loss 455.2785949707031\n",
      "epoch 63, train_loss 1277.6920166015625\n",
      "epoch 63, val_loss 455.27850341796875\n",
      "epoch 64, train_loss 1277.6917724609375\n",
      "epoch 64, val_loss 455.2784118652344\n",
      "epoch 65, train_loss 1277.691650390625\n",
      "epoch 65, val_loss 455.27825927734375\n",
      "epoch 66, train_loss 1277.69140625\n",
      "epoch 66, val_loss 455.2781677246094\n",
      "epoch 67, train_loss 1277.6912841796875\n",
      "epoch 67, val_loss 455.27813720703125\n",
      "epoch 68, train_loss 1277.691162109375\n",
      "epoch 68, val_loss 455.27801513671875\n",
      "epoch 69, train_loss 1277.6910400390625\n",
      "epoch 69, val_loss 455.2779235839844\n",
      "epoch 70, train_loss 1277.6910400390625\n",
      "epoch 70, val_loss 455.27783203125\n",
      "epoch 71, train_loss 1277.6907958984375\n",
      "epoch 71, val_loss 455.2777404785156\n",
      "epoch 72, train_loss 1277.690673828125\n",
      "epoch 72, val_loss 455.2776184082031\n",
      "epoch 73, train_loss 1277.690673828125\n",
      "epoch 73, val_loss 455.2774963378906\n",
      "epoch 74, train_loss 1277.6904296875\n",
      "epoch 74, val_loss 455.27740478515625\n",
      "epoch 75, train_loss 1277.6903076171875\n",
      "epoch 75, val_loss 455.27734375\n",
      "epoch 76, train_loss 1277.6903076171875\n",
      "epoch 76, val_loss 455.27716064453125\n",
      "epoch 77, train_loss 1277.68994140625\n",
      "epoch 77, val_loss 455.2771301269531\n",
      "epoch 78, train_loss 1277.689697265625\n",
      "epoch 78, val_loss 455.2770080566406\n",
      "epoch 79, train_loss 1277.6898193359375\n",
      "epoch 79, val_loss 455.2770080566406\n",
      "epoch 80, train_loss 1277.689697265625\n",
      "epoch 80, val_loss 455.27679443359375\n",
      "epoch 81, train_loss 1277.689453125\n",
      "epoch 81, val_loss 455.2767639160156\n",
      "epoch 82, train_loss 1277.689208984375\n",
      "epoch 82, val_loss 455.27667236328125\n",
      "epoch 83, train_loss 1277.689208984375\n",
      "epoch 83, val_loss 455.27655029296875\n",
      "epoch 84, train_loss 1277.68896484375\n",
      "epoch 84, val_loss 455.2764587402344\n",
      "epoch 85, train_loss 1277.68896484375\n",
      "epoch 85, val_loss 455.2763671875\n",
      "epoch 86, train_loss 1277.68896484375\n",
      "epoch 86, val_loss 455.2762451171875\n",
      "epoch 87, train_loss 1277.688720703125\n",
      "epoch 87, val_loss 455.2761535644531\n",
      "epoch 88, train_loss 1277.6884765625\n",
      "epoch 88, val_loss 455.2760925292969\n",
      "epoch 89, train_loss 1277.6884765625\n",
      "epoch 89, val_loss 455.2759094238281\n",
      "epoch 90, train_loss 1277.688232421875\n",
      "epoch 90, val_loss 455.27587890625\n",
      "epoch 91, train_loss 1277.6881103515625\n",
      "epoch 91, val_loss 455.2757568359375\n",
      "epoch 92, train_loss 1277.6881103515625\n",
      "epoch 92, val_loss 455.27569580078125\n",
      "epoch 93, train_loss 1277.68798828125\n",
      "epoch 93, val_loss 455.275634765625\n",
      "epoch 94, train_loss 1277.6876220703125\n",
      "epoch 94, val_loss 455.2754211425781\n",
      "epoch 95, train_loss 1277.6876220703125\n",
      "epoch 95, val_loss 455.2754211425781\n",
      "epoch 96, train_loss 1277.6875\n",
      "epoch 96, val_loss 455.2752990722656\n",
      "epoch 97, train_loss 1277.6875\n",
      "epoch 97, val_loss 455.2752380371094\n",
      "epoch 98, train_loss 1277.6871337890625\n",
      "epoch 98, val_loss 455.2751159667969\n",
      "epoch 99, train_loss 1277.6871337890625\n",
      "epoch 99, val_loss 455.2749938964844\n",
      "Parameter containing:\n",
      "tensor([1.4775e-13], requires_grad=True)\n",
      "iter 64, train_loss_regularization 0.8511775135993958\n",
      "iter 64, val_loss_regularization 0.8511775135993958\n",
      "epoch 0, train_loss 1277.6868896484375\n",
      "epoch 0, val_loss 455.27490234375\n",
      "epoch 1, train_loss 1277.6868896484375\n",
      "epoch 1, val_loss 455.2747802734375\n",
      "epoch 2, train_loss 1277.6868896484375\n",
      "epoch 2, val_loss 455.27471923828125\n",
      "epoch 3, train_loss 1277.6865234375\n",
      "epoch 3, val_loss 455.2746276855469\n",
      "epoch 4, train_loss 1277.6864013671875\n",
      "epoch 4, val_loss 455.2745056152344\n",
      "epoch 5, train_loss 1277.6864013671875\n",
      "epoch 5, val_loss 455.2744140625\n",
      "epoch 6, train_loss 1277.6861572265625\n",
      "epoch 6, val_loss 455.2742919921875\n",
      "epoch 7, train_loss 1277.68603515625\n",
      "epoch 7, val_loss 455.2742004394531\n",
      "epoch 8, train_loss 1277.6859130859375\n",
      "epoch 8, val_loss 455.2741394042969\n",
      "epoch 9, train_loss 1277.6856689453125\n",
      "epoch 9, val_loss 455.27398681640625\n",
      "epoch 10, train_loss 1277.6856689453125\n",
      "epoch 10, val_loss 455.2739562988281\n",
      "epoch 11, train_loss 1277.685546875\n",
      "epoch 11, val_loss 455.2738342285156\n",
      "epoch 12, train_loss 1277.685546875\n",
      "epoch 12, val_loss 455.27374267578125\n",
      "epoch 13, train_loss 1277.685302734375\n",
      "epoch 13, val_loss 455.27362060546875\n",
      "epoch 14, train_loss 1277.685302734375\n",
      "epoch 14, val_loss 455.27349853515625\n",
      "epoch 15, train_loss 1277.68505859375\n",
      "epoch 15, val_loss 455.2734069824219\n",
      "epoch 16, train_loss 1277.6849365234375\n",
      "epoch 16, val_loss 455.27337646484375\n",
      "epoch 17, train_loss 1277.6846923828125\n",
      "epoch 17, val_loss 455.27325439453125\n",
      "epoch 18, train_loss 1277.6846923828125\n",
      "epoch 18, val_loss 455.2731628417969\n",
      "epoch 19, train_loss 1277.6844482421875\n",
      "epoch 19, val_loss 455.2730407714844\n",
      "epoch 20, train_loss 1277.684326171875\n",
      "epoch 20, val_loss 455.27294921875\n",
      "epoch 21, train_loss 1277.6842041015625\n",
      "epoch 21, val_loss 455.27288818359375\n",
      "epoch 22, train_loss 1277.6842041015625\n",
      "epoch 22, val_loss 455.2727355957031\n",
      "epoch 23, train_loss 1277.6839599609375\n",
      "epoch 23, val_loss 455.272705078125\n",
      "epoch 24, train_loss 1277.6839599609375\n",
      "epoch 24, val_loss 455.2725524902344\n",
      "epoch 25, train_loss 1277.6837158203125\n",
      "epoch 25, val_loss 455.2724609375\n",
      "epoch 26, train_loss 1277.68359375\n",
      "epoch 26, val_loss 455.2724304199219\n",
      "epoch 27, train_loss 1277.68359375\n",
      "epoch 27, val_loss 455.27227783203125\n",
      "epoch 28, train_loss 1277.6834716796875\n",
      "epoch 28, val_loss 455.272216796875\n",
      "epoch 29, train_loss 1277.6832275390625\n",
      "epoch 29, val_loss 455.27203369140625\n",
      "epoch 30, train_loss 1277.68310546875\n",
      "epoch 30, val_loss 455.2720031738281\n",
      "epoch 31, train_loss 1277.6829833984375\n",
      "epoch 31, val_loss 455.2718811035156\n",
      "epoch 32, train_loss 1277.682861328125\n",
      "epoch 32, val_loss 455.2718200683594\n",
      "epoch 33, train_loss 1277.682861328125\n",
      "epoch 33, val_loss 455.2716979980469\n",
      "epoch 34, train_loss 1277.682861328125\n",
      "epoch 34, val_loss 455.27166748046875\n",
      "epoch 35, train_loss 1277.6824951171875\n",
      "epoch 35, val_loss 455.2715148925781\n",
      "epoch 36, train_loss 1277.682373046875\n",
      "epoch 36, val_loss 455.2714538574219\n",
      "epoch 37, train_loss 1277.6822509765625\n",
      "epoch 37, val_loss 455.2713317871094\n",
      "epoch 38, train_loss 1277.68212890625\n",
      "epoch 38, val_loss 455.2712097167969\n",
      "epoch 39, train_loss 1277.681884765625\n",
      "epoch 39, val_loss 455.2711181640625\n",
      "epoch 40, train_loss 1277.681884765625\n",
      "epoch 40, val_loss 455.2710876464844\n",
      "epoch 41, train_loss 1277.6817626953125\n",
      "epoch 41, val_loss 455.2709655761719\n",
      "epoch 42, train_loss 1277.6815185546875\n",
      "epoch 42, val_loss 455.2708740234375\n",
      "epoch 43, train_loss 1277.681396484375\n",
      "epoch 43, val_loss 455.2707214355469\n",
      "epoch 44, train_loss 1277.6812744140625\n",
      "epoch 44, val_loss 455.2706604003906\n",
      "epoch 45, train_loss 1277.6812744140625\n",
      "epoch 45, val_loss 455.27056884765625\n",
      "epoch 46, train_loss 1277.6810302734375\n",
      "epoch 46, val_loss 455.2705078125\n",
      "epoch 47, train_loss 1277.680908203125\n",
      "epoch 47, val_loss 455.2703857421875\n",
      "epoch 48, train_loss 1277.6807861328125\n",
      "epoch 48, val_loss 455.270263671875\n",
      "epoch 49, train_loss 1277.6805419921875\n",
      "epoch 49, val_loss 455.2701721191406\n",
      "epoch 50, train_loss 1277.6806640625\n",
      "epoch 50, val_loss 455.27008056640625\n",
      "epoch 51, train_loss 1277.6805419921875\n",
      "epoch 51, val_loss 455.2699890136719\n",
      "epoch 52, train_loss 1277.6802978515625\n",
      "epoch 52, val_loss 455.2699279785156\n",
      "epoch 53, train_loss 1277.68017578125\n",
      "epoch 53, val_loss 455.2698059082031\n",
      "epoch 54, train_loss 1277.6800537109375\n",
      "epoch 54, val_loss 455.2696533203125\n",
      "epoch 55, train_loss 1277.6798095703125\n",
      "epoch 55, val_loss 455.2696228027344\n",
      "epoch 56, train_loss 1277.679931640625\n",
      "epoch 56, val_loss 455.26953125\n",
      "epoch 57, train_loss 1277.6796875\n",
      "epoch 57, val_loss 455.2694091796875\n",
      "epoch 58, train_loss 1277.6796875\n",
      "epoch 58, val_loss 455.269287109375\n",
      "epoch 59, train_loss 1277.679443359375\n",
      "epoch 59, val_loss 455.2691955566406\n",
      "epoch 60, train_loss 1277.6793212890625\n",
      "epoch 60, val_loss 455.2691345214844\n",
      "epoch 61, train_loss 1277.6790771484375\n",
      "epoch 61, val_loss 455.26904296875\n",
      "epoch 62, train_loss 1277.678955078125\n",
      "epoch 62, val_loss 455.2689208984375\n",
      "epoch 63, train_loss 1277.678955078125\n",
      "epoch 63, val_loss 455.2688293457031\n",
      "epoch 64, train_loss 1277.6787109375\n",
      "epoch 64, val_loss 455.2687072753906\n",
      "epoch 65, train_loss 1277.6785888671875\n",
      "epoch 65, val_loss 455.26861572265625\n",
      "epoch 66, train_loss 1277.678466796875\n",
      "epoch 66, val_loss 455.26849365234375\n",
      "epoch 67, train_loss 1277.678466796875\n",
      "epoch 67, val_loss 455.2684020996094\n",
      "epoch 68, train_loss 1277.67822265625\n",
      "epoch 68, val_loss 455.2683410644531\n",
      "epoch 69, train_loss 1277.6781005859375\n",
      "epoch 69, val_loss 455.26824951171875\n",
      "epoch 70, train_loss 1277.6781005859375\n",
      "epoch 70, val_loss 455.26812744140625\n",
      "epoch 71, train_loss 1277.6778564453125\n",
      "epoch 71, val_loss 455.2680969238281\n",
      "epoch 72, train_loss 1277.6776123046875\n",
      "epoch 72, val_loss 455.2679443359375\n",
      "epoch 73, train_loss 1277.6776123046875\n",
      "epoch 73, val_loss 455.267822265625\n",
      "epoch 74, train_loss 1277.677490234375\n",
      "epoch 74, val_loss 455.2677917480469\n",
      "epoch 75, train_loss 1277.6773681640625\n",
      "epoch 75, val_loss 455.2676696777344\n",
      "epoch 76, train_loss 1277.67724609375\n",
      "epoch 76, val_loss 455.267578125\n",
      "epoch 77, train_loss 1277.6771240234375\n",
      "epoch 77, val_loss 455.2674865722656\n",
      "epoch 78, train_loss 1277.677001953125\n",
      "epoch 78, val_loss 455.2673645019531\n",
      "epoch 79, train_loss 1277.6768798828125\n",
      "epoch 79, val_loss 455.267333984375\n",
      "epoch 80, train_loss 1277.6767578125\n",
      "epoch 80, val_loss 455.2672119140625\n",
      "epoch 81, train_loss 1277.6766357421875\n",
      "epoch 81, val_loss 455.26708984375\n",
      "epoch 82, train_loss 1277.6763916015625\n",
      "epoch 82, val_loss 455.2669677734375\n",
      "epoch 83, train_loss 1277.67626953125\n",
      "epoch 83, val_loss 455.2668762207031\n",
      "epoch 84, train_loss 1277.67626953125\n",
      "epoch 84, val_loss 455.26678466796875\n",
      "epoch 85, train_loss 1277.6761474609375\n",
      "epoch 85, val_loss 455.26678466796875\n",
      "epoch 86, train_loss 1277.67578125\n",
      "epoch 86, val_loss 455.2666320800781\n",
      "epoch 87, train_loss 1277.67578125\n",
      "epoch 87, val_loss 455.2665100097656\n",
      "epoch 88, train_loss 1277.6756591796875\n",
      "epoch 88, val_loss 455.26641845703125\n",
      "epoch 89, train_loss 1277.675537109375\n",
      "epoch 89, val_loss 455.2663269042969\n",
      "epoch 90, train_loss 1277.6754150390625\n",
      "epoch 90, val_loss 455.2662353515625\n",
      "epoch 91, train_loss 1277.67529296875\n",
      "epoch 91, val_loss 455.26617431640625\n",
      "epoch 92, train_loss 1277.67529296875\n",
      "epoch 92, val_loss 455.26605224609375\n",
      "epoch 93, train_loss 1277.675048828125\n",
      "epoch 93, val_loss 455.26593017578125\n",
      "epoch 94, train_loss 1277.675048828125\n",
      "epoch 94, val_loss 455.2658386230469\n",
      "epoch 95, train_loss 1277.6748046875\n",
      "epoch 95, val_loss 455.2657775878906\n",
      "epoch 96, train_loss 1277.6748046875\n",
      "epoch 96, val_loss 455.2656555175781\n",
      "epoch 97, train_loss 1277.6746826171875\n",
      "epoch 97, val_loss 455.2655944824219\n",
      "epoch 98, train_loss 1277.67431640625\n",
      "epoch 98, val_loss 455.2654724121094\n",
      "epoch 99, train_loss 1277.67431640625\n",
      "epoch 99, val_loss 455.2654113769531\n",
      "Parameter containing:\n",
      "tensor([9.8135e-14], requires_grad=True)\n",
      "iter 65, train_loss_regularization 0.8475761413574219\n",
      "iter 65, val_loss_regularization 0.8475761413574219\n",
      "epoch 0, train_loss 1277.6741943359375\n",
      "epoch 0, val_loss 455.2652587890625\n",
      "epoch 1, train_loss 1277.6739501953125\n",
      "epoch 1, val_loss 455.26519775390625\n",
      "epoch 2, train_loss 1277.6739501953125\n",
      "epoch 2, val_loss 455.2650451660156\n",
      "epoch 3, train_loss 1277.6737060546875\n",
      "epoch 3, val_loss 455.2650146484375\n",
      "epoch 4, train_loss 1277.6737060546875\n",
      "epoch 4, val_loss 455.2649230957031\n",
      "epoch 5, train_loss 1277.673583984375\n",
      "epoch 5, val_loss 455.26483154296875\n",
      "epoch 6, train_loss 1277.6734619140625\n",
      "epoch 6, val_loss 455.26470947265625\n",
      "epoch 7, train_loss 1277.6734619140625\n",
      "epoch 7, val_loss 455.2646179199219\n",
      "epoch 8, train_loss 1277.6732177734375\n",
      "epoch 8, val_loss 455.2644958496094\n",
      "epoch 9, train_loss 1277.6729736328125\n",
      "epoch 9, val_loss 455.26446533203125\n",
      "epoch 10, train_loss 1277.6729736328125\n",
      "epoch 10, val_loss 455.26434326171875\n",
      "epoch 11, train_loss 1277.6728515625\n",
      "epoch 11, val_loss 455.26422119140625\n",
      "epoch 12, train_loss 1277.672607421875\n",
      "epoch 12, val_loss 455.2640686035156\n",
      "epoch 13, train_loss 1277.67236328125\n",
      "epoch 13, val_loss 455.2640380859375\n",
      "epoch 14, train_loss 1277.67236328125\n",
      "epoch 14, val_loss 455.2639465332031\n",
      "epoch 15, train_loss 1277.67236328125\n",
      "epoch 15, val_loss 455.2638854980469\n",
      "epoch 16, train_loss 1277.672119140625\n",
      "epoch 16, val_loss 455.2637634277344\n",
      "epoch 17, train_loss 1277.672119140625\n",
      "epoch 17, val_loss 455.26361083984375\n",
      "epoch 18, train_loss 1277.6719970703125\n",
      "epoch 18, val_loss 455.2635498046875\n",
      "epoch 19, train_loss 1277.671875\n",
      "epoch 19, val_loss 455.2634582519531\n",
      "epoch 20, train_loss 1277.671630859375\n",
      "epoch 20, val_loss 455.263427734375\n",
      "epoch 21, train_loss 1277.671630859375\n",
      "epoch 21, val_loss 455.2633056640625\n",
      "epoch 22, train_loss 1277.67138671875\n",
      "epoch 22, val_loss 455.2631530761719\n",
      "epoch 23, train_loss 1277.6712646484375\n",
      "epoch 23, val_loss 455.2630920410156\n",
      "epoch 24, train_loss 1277.671142578125\n",
      "epoch 24, val_loss 455.2629699707031\n",
      "epoch 25, train_loss 1277.6710205078125\n",
      "epoch 25, val_loss 455.2629089355469\n",
      "epoch 26, train_loss 1277.6708984375\n",
      "epoch 26, val_loss 455.26275634765625\n",
      "epoch 27, train_loss 1277.6707763671875\n",
      "epoch 27, val_loss 455.2626953125\n",
      "epoch 28, train_loss 1277.6705322265625\n",
      "epoch 28, val_loss 455.2625732421875\n",
      "epoch 29, train_loss 1277.6705322265625\n",
      "epoch 29, val_loss 455.26251220703125\n",
      "epoch 30, train_loss 1277.67041015625\n",
      "epoch 30, val_loss 455.2624206542969\n",
      "epoch 31, train_loss 1277.6702880859375\n",
      "epoch 31, val_loss 455.2623291015625\n",
      "epoch 32, train_loss 1277.6702880859375\n",
      "epoch 32, val_loss 455.2621765136719\n",
      "epoch 33, train_loss 1277.6700439453125\n",
      "epoch 33, val_loss 455.2621765136719\n",
      "epoch 34, train_loss 1277.669921875\n",
      "epoch 34, val_loss 455.2620849609375\n",
      "epoch 35, train_loss 1277.669677734375\n",
      "epoch 35, val_loss 455.261962890625\n",
      "epoch 36, train_loss 1277.669677734375\n",
      "epoch 36, val_loss 455.2618408203125\n",
      "epoch 37, train_loss 1277.6695556640625\n",
      "epoch 37, val_loss 455.2617492675781\n",
      "epoch 38, train_loss 1277.6693115234375\n",
      "epoch 38, val_loss 455.26165771484375\n",
      "epoch 39, train_loss 1277.669189453125\n",
      "epoch 39, val_loss 455.2615966796875\n",
      "epoch 40, train_loss 1277.6690673828125\n",
      "epoch 40, val_loss 455.2615051269531\n",
      "epoch 41, train_loss 1277.6689453125\n",
      "epoch 41, val_loss 455.2613220214844\n",
      "epoch 42, train_loss 1277.6688232421875\n",
      "epoch 42, val_loss 455.2611999511719\n",
      "epoch 43, train_loss 1277.6688232421875\n",
      "epoch 43, val_loss 455.2611999511719\n",
      "epoch 44, train_loss 1277.668701171875\n",
      "epoch 44, val_loss 455.2610778808594\n",
      "epoch 45, train_loss 1277.66845703125\n",
      "epoch 45, val_loss 455.26104736328125\n",
      "epoch 46, train_loss 1277.6683349609375\n",
      "epoch 46, val_loss 455.2608337402344\n",
      "epoch 47, train_loss 1277.6683349609375\n",
      "epoch 47, val_loss 455.26080322265625\n",
      "epoch 48, train_loss 1277.6680908203125\n",
      "epoch 48, val_loss 455.2607116699219\n",
      "epoch 49, train_loss 1277.66796875\n",
      "epoch 49, val_loss 455.2606201171875\n",
      "epoch 50, train_loss 1277.6678466796875\n",
      "epoch 50, val_loss 455.2605285644531\n",
      "epoch 51, train_loss 1277.6676025390625\n",
      "epoch 51, val_loss 455.2604064941406\n",
      "epoch 52, train_loss 1277.6673583984375\n",
      "epoch 52, val_loss 455.2603454589844\n",
      "epoch 53, train_loss 1277.6676025390625\n",
      "epoch 53, val_loss 455.2602844238281\n",
      "epoch 54, train_loss 1277.6673583984375\n",
      "epoch 54, val_loss 455.2601623535156\n",
      "epoch 55, train_loss 1277.6671142578125\n",
      "epoch 55, val_loss 455.26007080078125\n",
      "epoch 56, train_loss 1277.6671142578125\n",
      "epoch 56, val_loss 455.2599182128906\n",
      "epoch 57, train_loss 1277.6669921875\n",
      "epoch 57, val_loss 455.2598876953125\n",
      "epoch 58, train_loss 1277.6668701171875\n",
      "epoch 58, val_loss 455.2597351074219\n",
      "epoch 59, train_loss 1277.6668701171875\n",
      "epoch 59, val_loss 455.2596740722656\n",
      "epoch 60, train_loss 1277.6666259765625\n",
      "epoch 60, val_loss 455.2595520019531\n",
      "epoch 61, train_loss 1277.6663818359375\n",
      "epoch 61, val_loss 455.25946044921875\n",
      "epoch 62, train_loss 1277.6661376953125\n",
      "epoch 62, val_loss 455.25933837890625\n",
      "epoch 63, train_loss 1277.6661376953125\n",
      "epoch 63, val_loss 455.25927734375\n",
      "epoch 64, train_loss 1277.6658935546875\n",
      "epoch 64, val_loss 455.2592468261719\n",
      "epoch 65, train_loss 1277.6658935546875\n",
      "epoch 65, val_loss 455.25909423828125\n",
      "epoch 66, train_loss 1277.6658935546875\n",
      "epoch 66, val_loss 455.25897216796875\n",
      "epoch 67, train_loss 1277.6656494140625\n",
      "epoch 67, val_loss 455.2588806152344\n",
      "epoch 68, train_loss 1277.66552734375\n",
      "epoch 68, val_loss 455.2588195800781\n",
      "epoch 69, train_loss 1277.6654052734375\n",
      "epoch 69, val_loss 455.2586975097656\n",
      "epoch 70, train_loss 1277.665283203125\n",
      "epoch 70, val_loss 455.2585754394531\n",
      "epoch 71, train_loss 1277.6651611328125\n",
      "epoch 71, val_loss 455.2584533691406\n",
      "epoch 72, train_loss 1277.6649169921875\n",
      "epoch 72, val_loss 455.2584533691406\n",
      "epoch 73, train_loss 1277.6650390625\n",
      "epoch 73, val_loss 455.25830078125\n",
      "epoch 74, train_loss 1277.664794921875\n",
      "epoch 74, val_loss 455.25823974609375\n",
      "epoch 75, train_loss 1277.66455078125\n",
      "epoch 75, val_loss 455.25811767578125\n",
      "epoch 76, train_loss 1277.66455078125\n",
      "epoch 76, val_loss 455.25799560546875\n",
      "epoch 77, train_loss 1277.664306640625\n",
      "epoch 77, val_loss 455.2579650878906\n",
      "epoch 78, train_loss 1277.6641845703125\n",
      "epoch 78, val_loss 455.2578430175781\n",
      "epoch 79, train_loss 1277.6641845703125\n",
      "epoch 79, val_loss 455.2577819824219\n",
      "epoch 80, train_loss 1277.6640625\n",
      "epoch 80, val_loss 455.2577209472656\n",
      "epoch 81, train_loss 1277.6639404296875\n",
      "epoch 81, val_loss 455.257568359375\n",
      "epoch 82, train_loss 1277.6636962890625\n",
      "epoch 82, val_loss 455.2574462890625\n",
      "epoch 83, train_loss 1277.6636962890625\n",
      "epoch 83, val_loss 455.25738525390625\n",
      "epoch 84, train_loss 1277.6634521484375\n",
      "epoch 84, val_loss 455.25732421875\n",
      "epoch 85, train_loss 1277.6634521484375\n",
      "epoch 85, val_loss 455.2572021484375\n",
      "epoch 86, train_loss 1277.6632080078125\n",
      "epoch 86, val_loss 455.2571105957031\n",
      "epoch 87, train_loss 1277.6629638671875\n",
      "epoch 87, val_loss 455.2569885253906\n",
      "epoch 88, train_loss 1277.6629638671875\n",
      "epoch 88, val_loss 455.2568664550781\n",
      "epoch 89, train_loss 1277.6629638671875\n",
      "epoch 89, val_loss 455.2568054199219\n",
      "epoch 90, train_loss 1277.662841796875\n",
      "epoch 90, val_loss 455.2567138671875\n",
      "epoch 91, train_loss 1277.66259765625\n",
      "epoch 91, val_loss 455.25665283203125\n",
      "epoch 92, train_loss 1277.66259765625\n",
      "epoch 92, val_loss 455.2565002441406\n",
      "epoch 93, train_loss 1277.6624755859375\n",
      "epoch 93, val_loss 455.25640869140625\n",
      "epoch 94, train_loss 1277.662353515625\n",
      "epoch 94, val_loss 455.25634765625\n",
      "epoch 95, train_loss 1277.662109375\n",
      "epoch 95, val_loss 455.2562561035156\n",
      "epoch 96, train_loss 1277.662109375\n",
      "epoch 96, val_loss 455.2561340332031\n",
      "epoch 97, train_loss 1277.661865234375\n",
      "epoch 97, val_loss 455.2560729980469\n",
      "epoch 98, train_loss 1277.6617431640625\n",
      "epoch 98, val_loss 455.2560119628906\n",
      "epoch 99, train_loss 1277.66162109375\n",
      "epoch 99, val_loss 455.25592041015625\n",
      "Parameter containing:\n",
      "tensor([6.5255e-14], requires_grad=True)\n",
      "iter 66, train_loss_regularization 0.8440365791320801\n",
      "iter 66, val_loss_regularization 0.8440365791320801\n",
      "epoch 0, train_loss 1277.66162109375\n",
      "epoch 0, val_loss 455.25579833984375\n",
      "epoch 1, train_loss 1277.661376953125\n",
      "epoch 1, val_loss 455.255615234375\n",
      "epoch 2, train_loss 1277.6612548828125\n",
      "epoch 2, val_loss 455.255615234375\n",
      "epoch 3, train_loss 1277.661376953125\n",
      "epoch 3, val_loss 455.2554931640625\n",
      "epoch 4, train_loss 1277.6611328125\n",
      "epoch 4, val_loss 455.2554016113281\n",
      "epoch 5, train_loss 1277.660888671875\n",
      "epoch 5, val_loss 455.2552795410156\n",
      "epoch 6, train_loss 1277.6607666015625\n",
      "epoch 6, val_loss 455.2552185058594\n",
      "epoch 7, train_loss 1277.6605224609375\n",
      "epoch 7, val_loss 455.2550964355469\n",
      "epoch 8, train_loss 1277.6605224609375\n",
      "epoch 8, val_loss 455.2550048828125\n",
      "epoch 9, train_loss 1277.6602783203125\n",
      "epoch 9, val_loss 455.2550048828125\n",
      "epoch 10, train_loss 1277.6602783203125\n",
      "epoch 10, val_loss 455.2548828125\n",
      "epoch 11, train_loss 1277.66015625\n",
      "epoch 11, val_loss 455.2547607421875\n",
      "epoch 12, train_loss 1277.659912109375\n",
      "epoch 12, val_loss 455.2546691894531\n",
      "epoch 13, train_loss 1277.659912109375\n",
      "epoch 13, val_loss 455.25457763671875\n",
      "epoch 14, train_loss 1277.65966796875\n",
      "epoch 14, val_loss 455.2544860839844\n",
      "epoch 15, train_loss 1277.65966796875\n",
      "epoch 15, val_loss 455.2543029785156\n",
      "epoch 16, train_loss 1277.6595458984375\n",
      "epoch 16, val_loss 455.2542419433594\n",
      "epoch 17, train_loss 1277.6593017578125\n",
      "epoch 17, val_loss 455.25408935546875\n",
      "epoch 18, train_loss 1277.6593017578125\n",
      "epoch 18, val_loss 455.25408935546875\n",
      "epoch 19, train_loss 1277.6591796875\n",
      "epoch 19, val_loss 455.25396728515625\n",
      "epoch 20, train_loss 1277.6588134765625\n",
      "epoch 20, val_loss 455.25390625\n",
      "epoch 21, train_loss 1277.6588134765625\n",
      "epoch 21, val_loss 455.2537841796875\n",
      "epoch 22, train_loss 1277.65869140625\n",
      "epoch 22, val_loss 455.2537536621094\n",
      "epoch 23, train_loss 1277.65869140625\n",
      "epoch 23, val_loss 455.2535705566406\n",
      "epoch 24, train_loss 1277.658447265625\n",
      "epoch 24, val_loss 455.2535095214844\n",
      "epoch 25, train_loss 1277.658447265625\n",
      "epoch 25, val_loss 455.2533874511719\n",
      "epoch 26, train_loss 1277.658447265625\n",
      "epoch 26, val_loss 455.2532958984375\n",
      "epoch 27, train_loss 1277.657958984375\n",
      "epoch 27, val_loss 455.2532043457031\n",
      "epoch 28, train_loss 1277.6580810546875\n",
      "epoch 28, val_loss 455.253173828125\n",
      "epoch 29, train_loss 1277.657958984375\n",
      "epoch 29, val_loss 455.25311279296875\n",
      "epoch 30, train_loss 1277.6578369140625\n",
      "epoch 30, val_loss 455.2529296875\n",
      "epoch 31, train_loss 1277.6575927734375\n",
      "epoch 31, val_loss 455.2527770996094\n",
      "epoch 32, train_loss 1277.657470703125\n",
      "epoch 32, val_loss 455.25274658203125\n",
      "epoch 33, train_loss 1277.6573486328125\n",
      "epoch 33, val_loss 455.25262451171875\n",
      "epoch 34, train_loss 1277.6572265625\n",
      "epoch 34, val_loss 455.2525939941406\n",
      "epoch 35, train_loss 1277.6572265625\n",
      "epoch 35, val_loss 455.25250244140625\n",
      "epoch 36, train_loss 1277.6571044921875\n",
      "epoch 36, val_loss 455.25238037109375\n",
      "epoch 37, train_loss 1277.656982421875\n",
      "epoch 37, val_loss 455.2522888183594\n",
      "epoch 38, train_loss 1277.65673828125\n",
      "epoch 38, val_loss 455.252197265625\n",
      "epoch 39, train_loss 1277.65673828125\n",
      "epoch 39, val_loss 455.25213623046875\n",
      "epoch 40, train_loss 1277.6566162109375\n",
      "epoch 40, val_loss 455.2520446777344\n",
      "epoch 41, train_loss 1277.6563720703125\n",
      "epoch 41, val_loss 455.2519226074219\n",
      "epoch 42, train_loss 1277.65625\n",
      "epoch 42, val_loss 455.2518005371094\n",
      "epoch 43, train_loss 1277.6561279296875\n",
      "epoch 43, val_loss 455.251708984375\n",
      "epoch 44, train_loss 1277.6561279296875\n",
      "epoch 44, val_loss 455.2516784667969\n",
      "epoch 45, train_loss 1277.6558837890625\n",
      "epoch 45, val_loss 455.2514953613281\n",
      "epoch 46, train_loss 1277.6558837890625\n",
      "epoch 46, val_loss 455.25146484375\n",
      "epoch 47, train_loss 1277.6556396484375\n",
      "epoch 47, val_loss 455.25140380859375\n",
      "epoch 48, train_loss 1277.6556396484375\n",
      "epoch 48, val_loss 455.2512512207031\n",
      "epoch 49, train_loss 1277.6552734375\n",
      "epoch 49, val_loss 455.251220703125\n",
      "epoch 50, train_loss 1277.6552734375\n",
      "epoch 50, val_loss 455.25103759765625\n",
      "epoch 51, train_loss 1277.6552734375\n",
      "epoch 51, val_loss 455.2510070800781\n",
      "epoch 52, train_loss 1277.655029296875\n",
      "epoch 52, val_loss 455.25091552734375\n",
      "epoch 53, train_loss 1277.6549072265625\n",
      "epoch 53, val_loss 455.2508239746094\n",
      "epoch 54, train_loss 1277.6546630859375\n",
      "epoch 54, val_loss 455.25067138671875\n",
      "epoch 55, train_loss 1277.65478515625\n",
      "epoch 55, val_loss 455.2505798339844\n",
      "epoch 56, train_loss 1277.654541015625\n",
      "epoch 56, val_loss 455.25048828125\n",
      "epoch 57, train_loss 1277.654296875\n",
      "epoch 57, val_loss 455.25042724609375\n",
      "epoch 58, train_loss 1277.654296875\n",
      "epoch 58, val_loss 455.2503356933594\n",
      "epoch 59, train_loss 1277.654296875\n",
      "epoch 59, val_loss 455.250244140625\n",
      "epoch 60, train_loss 1277.654052734375\n",
      "epoch 60, val_loss 455.2500915527344\n",
      "epoch 61, train_loss 1277.654052734375\n",
      "epoch 61, val_loss 455.25\n",
      "epoch 62, train_loss 1277.65380859375\n",
      "epoch 62, val_loss 455.2499084472656\n",
      "epoch 63, train_loss 1277.6536865234375\n",
      "epoch 63, val_loss 455.2498474121094\n",
      "epoch 64, train_loss 1277.6536865234375\n",
      "epoch 64, val_loss 455.249755859375\n",
      "epoch 65, train_loss 1277.6534423828125\n",
      "epoch 65, val_loss 455.24969482421875\n",
      "epoch 66, train_loss 1277.6533203125\n",
      "epoch 66, val_loss 455.24951171875\n",
      "epoch 67, train_loss 1277.6531982421875\n",
      "epoch 67, val_loss 455.24951171875\n",
      "epoch 68, train_loss 1277.6529541015625\n",
      "epoch 68, val_loss 455.2493896484375\n",
      "epoch 69, train_loss 1277.65283203125\n",
      "epoch 69, val_loss 455.2492370605469\n",
      "epoch 70, train_loss 1277.65283203125\n",
      "epoch 70, val_loss 455.24920654296875\n",
      "epoch 71, train_loss 1277.652587890625\n",
      "epoch 71, val_loss 455.2491149902344\n",
      "epoch 72, train_loss 1277.652587890625\n",
      "epoch 72, val_loss 455.2489929199219\n",
      "epoch 73, train_loss 1277.6524658203125\n",
      "epoch 73, val_loss 455.2489318847656\n",
      "epoch 74, train_loss 1277.65234375\n",
      "epoch 74, val_loss 455.24884033203125\n",
      "epoch 75, train_loss 1277.6522216796875\n",
      "epoch 75, val_loss 455.24871826171875\n",
      "epoch 76, train_loss 1277.652099609375\n",
      "epoch 76, val_loss 455.2486267089844\n",
      "epoch 77, train_loss 1277.6519775390625\n",
      "epoch 77, val_loss 455.24859619140625\n",
      "epoch 78, train_loss 1277.6519775390625\n",
      "epoch 78, val_loss 455.2484130859375\n",
      "epoch 79, train_loss 1277.651611328125\n",
      "epoch 79, val_loss 455.2483825683594\n",
      "epoch 80, train_loss 1277.6514892578125\n",
      "epoch 80, val_loss 455.2481994628906\n",
      "epoch 81, train_loss 1277.6514892578125\n",
      "epoch 81, val_loss 455.2481689453125\n",
      "epoch 82, train_loss 1277.6512451171875\n",
      "epoch 82, val_loss 455.2480773925781\n",
      "epoch 83, train_loss 1277.651123046875\n",
      "epoch 83, val_loss 455.248046875\n",
      "epoch 84, train_loss 1277.651123046875\n",
      "epoch 84, val_loss 455.2479248046875\n",
      "epoch 85, train_loss 1277.6510009765625\n",
      "epoch 85, val_loss 455.24774169921875\n",
      "epoch 86, train_loss 1277.65087890625\n",
      "epoch 86, val_loss 455.2477111816406\n",
      "epoch 87, train_loss 1277.650634765625\n",
      "epoch 87, val_loss 455.2475891113281\n",
      "epoch 88, train_loss 1277.650634765625\n",
      "epoch 88, val_loss 455.24749755859375\n",
      "epoch 89, train_loss 1277.6505126953125\n",
      "epoch 89, val_loss 455.2474670410156\n",
      "epoch 90, train_loss 1277.6502685546875\n",
      "epoch 90, val_loss 455.24725341796875\n",
      "epoch 91, train_loss 1277.650146484375\n",
      "epoch 91, val_loss 455.2472229003906\n",
      "epoch 92, train_loss 1277.6500244140625\n",
      "epoch 92, val_loss 455.24713134765625\n",
      "epoch 93, train_loss 1277.6500244140625\n",
      "epoch 93, val_loss 455.2470397949219\n",
      "epoch 94, train_loss 1277.64990234375\n",
      "epoch 94, val_loss 455.2469177246094\n",
      "epoch 95, train_loss 1277.649658203125\n",
      "epoch 95, val_loss 455.246826171875\n",
      "epoch 96, train_loss 1277.6495361328125\n",
      "epoch 96, val_loss 455.24676513671875\n",
      "epoch 97, train_loss 1277.6495361328125\n",
      "epoch 97, val_loss 455.2466735839844\n",
      "epoch 98, train_loss 1277.6494140625\n",
      "epoch 98, val_loss 455.24658203125\n",
      "epoch 99, train_loss 1277.6492919921875\n",
      "epoch 99, val_loss 455.2464904785156\n",
      "Parameter containing:\n",
      "tensor([4.3439e-14], requires_grad=True)\n",
      "iter 67, train_loss_regularization 0.8405587673187256\n",
      "iter 67, val_loss_regularization 0.8405587673187256\n",
      "epoch 0, train_loss 1277.649169921875\n",
      "epoch 0, val_loss 455.2463684082031\n",
      "epoch 1, train_loss 1277.6490478515625\n",
      "epoch 1, val_loss 455.246337890625\n",
      "epoch 2, train_loss 1277.6488037109375\n",
      "epoch 2, val_loss 455.2462158203125\n",
      "epoch 3, train_loss 1277.6488037109375\n",
      "epoch 3, val_loss 455.24609375\n",
      "epoch 4, train_loss 1277.6488037109375\n",
      "epoch 4, val_loss 455.2460021972656\n",
      "epoch 5, train_loss 1277.6484375\n",
      "epoch 5, val_loss 455.24591064453125\n",
      "epoch 6, train_loss 1277.6483154296875\n",
      "epoch 6, val_loss 455.24578857421875\n",
      "epoch 7, train_loss 1277.648193359375\n",
      "epoch 7, val_loss 455.2457580566406\n",
      "epoch 8, train_loss 1277.648193359375\n",
      "epoch 8, val_loss 455.24566650390625\n",
      "epoch 9, train_loss 1277.64794921875\n",
      "epoch 9, val_loss 455.24554443359375\n",
      "epoch 10, train_loss 1277.6478271484375\n",
      "epoch 10, val_loss 455.245361328125\n",
      "epoch 11, train_loss 1277.6478271484375\n",
      "epoch 11, val_loss 455.2453308105469\n",
      "epoch 12, train_loss 1277.6475830078125\n",
      "epoch 12, val_loss 455.24530029296875\n",
      "epoch 13, train_loss 1277.6474609375\n",
      "epoch 13, val_loss 455.2452087402344\n",
      "epoch 14, train_loss 1277.6474609375\n",
      "epoch 14, val_loss 455.2451171875\n",
      "epoch 15, train_loss 1277.647216796875\n",
      "epoch 15, val_loss 455.2449951171875\n",
      "epoch 16, train_loss 1277.647216796875\n",
      "epoch 16, val_loss 455.2449035644531\n",
      "epoch 17, train_loss 1277.64697265625\n",
      "epoch 17, val_loss 455.2447814941406\n",
      "epoch 18, train_loss 1277.6468505859375\n",
      "epoch 18, val_loss 455.2447204589844\n",
      "epoch 19, train_loss 1277.6468505859375\n",
      "epoch 19, val_loss 455.24462890625\n",
      "epoch 20, train_loss 1277.646728515625\n",
      "epoch 20, val_loss 455.2445068359375\n",
      "epoch 21, train_loss 1277.6466064453125\n",
      "epoch 21, val_loss 455.244384765625\n",
      "epoch 22, train_loss 1277.6463623046875\n",
      "epoch 22, val_loss 455.2442932128906\n",
      "epoch 23, train_loss 1277.6463623046875\n",
      "epoch 23, val_loss 455.2442626953125\n",
      "epoch 24, train_loss 1277.646240234375\n",
      "epoch 24, val_loss 455.2441101074219\n",
      "epoch 25, train_loss 1277.64599609375\n",
      "epoch 25, val_loss 455.2440490722656\n",
      "epoch 26, train_loss 1277.6458740234375\n",
      "epoch 26, val_loss 455.24395751953125\n",
      "epoch 27, train_loss 1277.6458740234375\n",
      "epoch 27, val_loss 455.2439270019531\n",
      "epoch 28, train_loss 1277.6458740234375\n",
      "epoch 28, val_loss 455.2437438964844\n",
      "epoch 29, train_loss 1277.6456298828125\n",
      "epoch 29, val_loss 455.24365234375\n",
      "epoch 30, train_loss 1277.6455078125\n",
      "epoch 30, val_loss 455.2435302734375\n",
      "epoch 31, train_loss 1277.6453857421875\n",
      "epoch 31, val_loss 455.2434997558594\n",
      "epoch 32, train_loss 1277.6451416015625\n",
      "epoch 32, val_loss 455.243408203125\n",
      "epoch 33, train_loss 1277.64501953125\n",
      "epoch 33, val_loss 455.2432861328125\n",
      "epoch 34, train_loss 1277.6451416015625\n",
      "epoch 34, val_loss 455.2431945800781\n",
      "epoch 35, train_loss 1277.644775390625\n",
      "epoch 35, val_loss 455.2430725097656\n",
      "epoch 36, train_loss 1277.6446533203125\n",
      "epoch 36, val_loss 455.2430114746094\n",
      "epoch 37, train_loss 1277.6446533203125\n",
      "epoch 37, val_loss 455.2429504394531\n",
      "epoch 38, train_loss 1277.6444091796875\n",
      "epoch 38, val_loss 455.2428894042969\n",
      "epoch 39, train_loss 1277.644287109375\n",
      "epoch 39, val_loss 455.24273681640625\n",
      "epoch 40, train_loss 1277.64404296875\n",
      "epoch 40, val_loss 455.2425842285156\n",
      "epoch 41, train_loss 1277.644287109375\n",
      "epoch 41, val_loss 455.2425842285156\n",
      "epoch 42, train_loss 1277.64404296875\n",
      "epoch 42, val_loss 455.242431640625\n",
      "epoch 43, train_loss 1277.643798828125\n",
      "epoch 43, val_loss 455.242431640625\n",
      "epoch 44, train_loss 1277.643798828125\n",
      "epoch 44, val_loss 455.24224853515625\n",
      "epoch 45, train_loss 1277.6435546875\n",
      "epoch 45, val_loss 455.2422180175781\n",
      "epoch 46, train_loss 1277.6434326171875\n",
      "epoch 46, val_loss 455.2420349121094\n",
      "epoch 47, train_loss 1277.6435546875\n",
      "epoch 47, val_loss 455.24200439453125\n",
      "epoch 48, train_loss 1277.643310546875\n",
      "epoch 48, val_loss 455.24188232421875\n",
      "epoch 49, train_loss 1277.64306640625\n",
      "epoch 49, val_loss 455.2418212890625\n",
      "epoch 50, train_loss 1277.6429443359375\n",
      "epoch 50, val_loss 455.24176025390625\n",
      "epoch 51, train_loss 1277.6429443359375\n",
      "epoch 51, val_loss 455.2416687011719\n",
      "epoch 52, train_loss 1277.6427001953125\n",
      "epoch 52, val_loss 455.2414855957031\n",
      "epoch 53, train_loss 1277.642578125\n",
      "epoch 53, val_loss 455.2414245605469\n",
      "epoch 54, train_loss 1277.642578125\n",
      "epoch 54, val_loss 455.2413330078125\n",
      "epoch 55, train_loss 1277.6424560546875\n",
      "epoch 55, val_loss 455.2412414550781\n",
      "epoch 56, train_loss 1277.642333984375\n",
      "epoch 56, val_loss 455.2411804199219\n",
      "epoch 57, train_loss 1277.6422119140625\n",
      "epoch 57, val_loss 455.2410888671875\n",
      "epoch 58, train_loss 1277.6419677734375\n",
      "epoch 58, val_loss 455.24102783203125\n",
      "epoch 59, train_loss 1277.641845703125\n",
      "epoch 59, val_loss 455.2408752441406\n",
      "epoch 60, train_loss 1277.6417236328125\n",
      "epoch 60, val_loss 455.24078369140625\n",
      "epoch 61, train_loss 1277.6417236328125\n",
      "epoch 61, val_loss 455.24072265625\n",
      "epoch 62, train_loss 1277.6416015625\n",
      "epoch 62, val_loss 455.24066162109375\n",
      "epoch 63, train_loss 1277.641357421875\n",
      "epoch 63, val_loss 455.24053955078125\n",
      "epoch 64, train_loss 1277.641357421875\n",
      "epoch 64, val_loss 455.24041748046875\n",
      "epoch 65, train_loss 1277.64111328125\n",
      "epoch 65, val_loss 455.2403259277344\n",
      "epoch 66, train_loss 1277.640869140625\n",
      "epoch 66, val_loss 455.2402648925781\n",
      "epoch 67, train_loss 1277.640869140625\n",
      "epoch 67, val_loss 455.2401123046875\n",
      "epoch 68, train_loss 1277.640869140625\n",
      "epoch 68, val_loss 455.2400817871094\n",
      "epoch 69, train_loss 1277.640625\n",
      "epoch 69, val_loss 455.24005126953125\n",
      "epoch 70, train_loss 1277.640625\n",
      "epoch 70, val_loss 455.2398376464844\n",
      "epoch 71, train_loss 1277.640380859375\n",
      "epoch 71, val_loss 455.23974609375\n",
      "epoch 72, train_loss 1277.640380859375\n",
      "epoch 72, val_loss 455.2397155761719\n",
      "epoch 73, train_loss 1277.64013671875\n",
      "epoch 73, val_loss 455.2395935058594\n",
      "epoch 74, train_loss 1277.64013671875\n",
      "epoch 74, val_loss 455.2395935058594\n",
      "epoch 75, train_loss 1277.639892578125\n",
      "epoch 75, val_loss 455.2393798828125\n",
      "epoch 76, train_loss 1277.6396484375\n",
      "epoch 76, val_loss 455.23931884765625\n",
      "epoch 77, train_loss 1277.6396484375\n",
      "epoch 77, val_loss 455.23919677734375\n",
      "epoch 78, train_loss 1277.6395263671875\n",
      "epoch 78, val_loss 455.2391357421875\n",
      "epoch 79, train_loss 1277.6395263671875\n",
      "epoch 79, val_loss 455.239013671875\n",
      "epoch 80, train_loss 1277.6392822265625\n",
      "epoch 80, val_loss 455.23895263671875\n",
      "epoch 81, train_loss 1277.6390380859375\n",
      "epoch 81, val_loss 455.23883056640625\n",
      "epoch 82, train_loss 1277.6392822265625\n",
      "epoch 82, val_loss 455.2387390136719\n",
      "epoch 83, train_loss 1277.6390380859375\n",
      "epoch 83, val_loss 455.2386169433594\n",
      "epoch 84, train_loss 1277.638916015625\n",
      "epoch 84, val_loss 455.2385559082031\n",
      "epoch 85, train_loss 1277.6387939453125\n",
      "epoch 85, val_loss 455.23846435546875\n",
      "epoch 86, train_loss 1277.6385498046875\n",
      "epoch 86, val_loss 455.23846435546875\n",
      "epoch 87, train_loss 1277.6385498046875\n",
      "epoch 87, val_loss 455.23834228515625\n",
      "epoch 88, train_loss 1277.6383056640625\n",
      "epoch 88, val_loss 455.2382507324219\n",
      "epoch 89, train_loss 1277.6383056640625\n",
      "epoch 89, val_loss 455.23809814453125\n",
      "epoch 90, train_loss 1277.63818359375\n",
      "epoch 90, val_loss 455.2380065917969\n",
      "epoch 91, train_loss 1277.637939453125\n",
      "epoch 91, val_loss 455.2379150390625\n",
      "epoch 92, train_loss 1277.637939453125\n",
      "epoch 92, val_loss 455.2378845214844\n",
      "epoch 93, train_loss 1277.6378173828125\n",
      "epoch 93, val_loss 455.2377624511719\n",
      "epoch 94, train_loss 1277.6375732421875\n",
      "epoch 94, val_loss 455.2377014160156\n",
      "epoch 95, train_loss 1277.6375732421875\n",
      "epoch 95, val_loss 455.23748779296875\n",
      "epoch 96, train_loss 1277.637451171875\n",
      "epoch 96, val_loss 455.2374572753906\n",
      "epoch 97, train_loss 1277.63720703125\n",
      "epoch 97, val_loss 455.23736572265625\n",
      "epoch 98, train_loss 1277.63720703125\n",
      "epoch 98, val_loss 455.2373046875\n",
      "epoch 99, train_loss 1277.636962890625\n",
      "epoch 99, val_loss 455.23712158203125\n",
      "Parameter containing:\n",
      "tensor([2.8947e-14], requires_grad=True)\n",
      "iter 68, train_loss_regularization 0.837141215801239\n",
      "iter 68, val_loss_regularization 0.837141215801239\n",
      "epoch 0, train_loss 1277.636962890625\n",
      "epoch 0, val_loss 455.23712158203125\n",
      "epoch 1, train_loss 1277.63671875\n",
      "epoch 1, val_loss 455.2369689941406\n",
      "epoch 2, train_loss 1277.63671875\n",
      "epoch 2, val_loss 455.2369079589844\n",
      "epoch 3, train_loss 1277.636474609375\n",
      "epoch 3, val_loss 455.2368469238281\n",
      "epoch 4, train_loss 1277.636474609375\n",
      "epoch 4, val_loss 455.23675537109375\n",
      "epoch 5, train_loss 1277.6363525390625\n",
      "epoch 5, val_loss 455.23663330078125\n",
      "epoch 6, train_loss 1277.63623046875\n",
      "epoch 6, val_loss 455.2365417480469\n",
      "epoch 7, train_loss 1277.6361083984375\n",
      "epoch 7, val_loss 455.2364196777344\n",
      "epoch 8, train_loss 1277.6358642578125\n",
      "epoch 8, val_loss 455.23638916015625\n",
      "epoch 9, train_loss 1277.6358642578125\n",
      "epoch 9, val_loss 455.2362060546875\n",
      "epoch 10, train_loss 1277.6356201171875\n",
      "epoch 10, val_loss 455.2361755371094\n",
      "epoch 11, train_loss 1277.635498046875\n",
      "epoch 11, val_loss 455.236083984375\n",
      "epoch 12, train_loss 1277.635498046875\n",
      "epoch 12, val_loss 455.2359924316406\n",
      "epoch 13, train_loss 1277.6353759765625\n",
      "epoch 13, val_loss 455.2359313964844\n",
      "epoch 14, train_loss 1277.6351318359375\n",
      "epoch 14, val_loss 455.2357482910156\n",
      "epoch 15, train_loss 1277.6351318359375\n",
      "epoch 15, val_loss 455.23565673828125\n",
      "epoch 16, train_loss 1277.635009765625\n",
      "epoch 16, val_loss 455.23565673828125\n",
      "epoch 17, train_loss 1277.635009765625\n",
      "epoch 17, val_loss 455.2355041503906\n",
      "epoch 18, train_loss 1277.6346435546875\n",
      "epoch 18, val_loss 455.2354431152344\n",
      "epoch 19, train_loss 1277.634521484375\n",
      "epoch 19, val_loss 455.23541259765625\n",
      "epoch 20, train_loss 1277.634521484375\n",
      "epoch 20, val_loss 455.2352600097656\n",
      "epoch 21, train_loss 1277.63427734375\n",
      "epoch 21, val_loss 455.2351379394531\n",
      "epoch 22, train_loss 1277.6341552734375\n",
      "epoch 22, val_loss 455.2350769042969\n",
      "epoch 23, train_loss 1277.63427734375\n",
      "epoch 23, val_loss 455.2349853515625\n",
      "epoch 24, train_loss 1277.634033203125\n",
      "epoch 24, val_loss 455.23492431640625\n",
      "epoch 25, train_loss 1277.6337890625\n",
      "epoch 25, val_loss 455.23480224609375\n",
      "epoch 26, train_loss 1277.6337890625\n",
      "epoch 26, val_loss 455.23468017578125\n",
      "epoch 27, train_loss 1277.6336669921875\n",
      "epoch 27, val_loss 455.234619140625\n",
      "epoch 28, train_loss 1277.633544921875\n",
      "epoch 28, val_loss 455.2344970703125\n",
      "epoch 29, train_loss 1277.6334228515625\n",
      "epoch 29, val_loss 455.2344665527344\n",
      "epoch 30, train_loss 1277.63330078125\n",
      "epoch 30, val_loss 455.2343444824219\n",
      "epoch 31, train_loss 1277.6331787109375\n",
      "epoch 31, val_loss 455.2342834472656\n",
      "epoch 32, train_loss 1277.633056640625\n",
      "epoch 32, val_loss 455.234130859375\n",
      "epoch 33, train_loss 1277.6329345703125\n",
      "epoch 33, val_loss 455.23406982421875\n",
      "epoch 34, train_loss 1277.6328125\n",
      "epoch 34, val_loss 455.2340087890625\n",
      "epoch 35, train_loss 1277.6326904296875\n",
      "epoch 35, val_loss 455.23388671875\n",
      "epoch 36, train_loss 1277.6326904296875\n",
      "epoch 36, val_loss 455.2337951660156\n",
      "epoch 37, train_loss 1277.6324462890625\n",
      "epoch 37, val_loss 455.23370361328125\n",
      "epoch 38, train_loss 1277.6322021484375\n",
      "epoch 38, val_loss 455.2336120605469\n",
      "epoch 39, train_loss 1277.6322021484375\n",
      "epoch 39, val_loss 455.2335510253906\n",
      "epoch 40, train_loss 1277.6322021484375\n",
      "epoch 40, val_loss 455.2333679199219\n",
      "epoch 41, train_loss 1277.6319580078125\n",
      "epoch 41, val_loss 455.23333740234375\n",
      "epoch 42, train_loss 1277.6317138671875\n",
      "epoch 42, val_loss 455.23321533203125\n",
      "epoch 43, train_loss 1277.6318359375\n",
      "epoch 43, val_loss 455.233154296875\n",
      "epoch 44, train_loss 1277.6317138671875\n",
      "epoch 44, val_loss 455.23309326171875\n",
      "epoch 45, train_loss 1277.63134765625\n",
      "epoch 45, val_loss 455.23291015625\n",
      "epoch 46, train_loss 1277.63134765625\n",
      "epoch 46, val_loss 455.2328186035156\n",
      "epoch 47, train_loss 1277.6312255859375\n",
      "epoch 47, val_loss 455.2327880859375\n",
      "epoch 48, train_loss 1277.6309814453125\n",
      "epoch 48, val_loss 455.232666015625\n",
      "epoch 49, train_loss 1277.6309814453125\n",
      "epoch 49, val_loss 455.2326354980469\n",
      "epoch 50, train_loss 1277.6309814453125\n",
      "epoch 50, val_loss 455.2324523925781\n",
      "epoch 51, train_loss 1277.630859375\n",
      "epoch 51, val_loss 455.232421875\n",
      "epoch 52, train_loss 1277.630615234375\n",
      "epoch 52, val_loss 455.2322998046875\n",
      "epoch 53, train_loss 1277.6304931640625\n",
      "epoch 53, val_loss 455.2322082519531\n",
      "epoch 54, train_loss 1277.63037109375\n",
      "epoch 54, val_loss 455.23211669921875\n",
      "epoch 55, train_loss 1277.63037109375\n",
      "epoch 55, val_loss 455.2320556640625\n",
      "epoch 56, train_loss 1277.630126953125\n",
      "epoch 56, val_loss 455.2319030761719\n",
      "epoch 57, train_loss 1277.6300048828125\n",
      "epoch 57, val_loss 455.23187255859375\n",
      "epoch 58, train_loss 1277.6298828125\n",
      "epoch 58, val_loss 455.2317199707031\n",
      "epoch 59, train_loss 1277.6298828125\n",
      "epoch 59, val_loss 455.2317199707031\n",
      "epoch 60, train_loss 1277.629638671875\n",
      "epoch 60, val_loss 455.2315368652344\n",
      "epoch 61, train_loss 1277.629638671875\n",
      "epoch 61, val_loss 455.23150634765625\n",
      "epoch 62, train_loss 1277.6295166015625\n",
      "epoch 62, val_loss 455.2314147949219\n",
      "epoch 63, train_loss 1277.6292724609375\n",
      "epoch 63, val_loss 455.2313232421875\n",
      "epoch 64, train_loss 1277.6292724609375\n",
      "epoch 64, val_loss 455.231201171875\n",
      "epoch 65, train_loss 1277.6290283203125\n",
      "epoch 65, val_loss 455.2310791015625\n",
      "epoch 66, train_loss 1277.6290283203125\n",
      "epoch 66, val_loss 455.2310485839844\n",
      "epoch 67, train_loss 1277.6290283203125\n",
      "epoch 67, val_loss 455.23095703125\n",
      "epoch 68, train_loss 1277.628662109375\n",
      "epoch 68, val_loss 455.2309265136719\n",
      "epoch 69, train_loss 1277.6285400390625\n",
      "epoch 69, val_loss 455.2307434082031\n",
      "epoch 70, train_loss 1277.6285400390625\n",
      "epoch 70, val_loss 455.230712890625\n",
      "epoch 71, train_loss 1277.6282958984375\n",
      "epoch 71, val_loss 455.23052978515625\n",
      "epoch 72, train_loss 1277.628173828125\n",
      "epoch 72, val_loss 455.2304992675781\n",
      "epoch 73, train_loss 1277.628173828125\n",
      "epoch 73, val_loss 455.2303771972656\n",
      "epoch 74, train_loss 1277.628173828125\n",
      "epoch 74, val_loss 455.2303466796875\n",
      "epoch 75, train_loss 1277.6279296875\n",
      "epoch 75, val_loss 455.23016357421875\n",
      "epoch 76, train_loss 1277.6278076171875\n",
      "epoch 76, val_loss 455.2300720214844\n",
      "epoch 77, train_loss 1277.6278076171875\n",
      "epoch 77, val_loss 455.23004150390625\n",
      "epoch 78, train_loss 1277.6275634765625\n",
      "epoch 78, val_loss 455.2299499511719\n",
      "epoch 79, train_loss 1277.6273193359375\n",
      "epoch 79, val_loss 455.2298889160156\n",
      "epoch 80, train_loss 1277.627197265625\n",
      "epoch 80, val_loss 455.229736328125\n",
      "epoch 81, train_loss 1277.627197265625\n",
      "epoch 81, val_loss 455.22967529296875\n",
      "epoch 82, train_loss 1277.626953125\n",
      "epoch 82, val_loss 455.2295837402344\n",
      "epoch 83, train_loss 1277.626953125\n",
      "epoch 83, val_loss 455.22943115234375\n",
      "epoch 84, train_loss 1277.6268310546875\n",
      "epoch 84, val_loss 455.22943115234375\n",
      "epoch 85, train_loss 1277.6265869140625\n",
      "epoch 85, val_loss 455.2292785644531\n",
      "epoch 86, train_loss 1277.626708984375\n",
      "epoch 86, val_loss 455.2292175292969\n",
      "epoch 87, train_loss 1277.62646484375\n",
      "epoch 87, val_loss 455.2290954589844\n",
      "epoch 88, train_loss 1277.6263427734375\n",
      "epoch 88, val_loss 455.2290344238281\n",
      "epoch 89, train_loss 1277.626220703125\n",
      "epoch 89, val_loss 455.2289123535156\n",
      "epoch 90, train_loss 1277.6260986328125\n",
      "epoch 90, val_loss 455.2288818359375\n",
      "epoch 91, train_loss 1277.6259765625\n",
      "epoch 91, val_loss 455.22869873046875\n",
      "epoch 92, train_loss 1277.6259765625\n",
      "epoch 92, val_loss 455.2286682128906\n",
      "epoch 93, train_loss 1277.6256103515625\n",
      "epoch 93, val_loss 455.22857666015625\n",
      "epoch 94, train_loss 1277.6256103515625\n",
      "epoch 94, val_loss 455.2284851074219\n",
      "epoch 95, train_loss 1277.6253662109375\n",
      "epoch 95, val_loss 455.2283630371094\n",
      "epoch 96, train_loss 1277.6253662109375\n",
      "epoch 96, val_loss 455.2282409667969\n",
      "epoch 97, train_loss 1277.625244140625\n",
      "epoch 97, val_loss 455.2281799316406\n",
      "epoch 98, train_loss 1277.625\n",
      "epoch 98, val_loss 455.22808837890625\n",
      "epoch 99, train_loss 1277.625\n",
      "epoch 99, val_loss 455.22802734375\n",
      "Parameter containing:\n",
      "tensor([1.9311e-14], requires_grad=True)\n",
      "iter 69, train_loss_regularization 0.8337826728820801\n",
      "iter 69, val_loss_regularization 0.8337826728820801\n",
      "epoch 0, train_loss 1277.6248779296875\n",
      "epoch 0, val_loss 455.2279052734375\n",
      "epoch 1, train_loss 1277.6246337890625\n",
      "epoch 1, val_loss 455.227783203125\n",
      "epoch 2, train_loss 1277.6246337890625\n",
      "epoch 2, val_loss 455.2277526855469\n",
      "epoch 3, train_loss 1277.62451171875\n",
      "epoch 3, val_loss 455.2276611328125\n",
      "epoch 4, train_loss 1277.62451171875\n",
      "epoch 4, val_loss 455.2275695800781\n",
      "epoch 5, train_loss 1277.6243896484375\n",
      "epoch 5, val_loss 455.2275085449219\n",
      "epoch 6, train_loss 1277.6240234375\n",
      "epoch 6, val_loss 455.2273864746094\n",
      "epoch 7, train_loss 1277.6240234375\n",
      "epoch 7, val_loss 455.2272644042969\n",
      "epoch 8, train_loss 1277.6239013671875\n",
      "epoch 8, val_loss 455.2272033691406\n",
      "epoch 9, train_loss 1277.623779296875\n",
      "epoch 9, val_loss 455.22711181640625\n",
      "epoch 10, train_loss 1277.623779296875\n",
      "epoch 10, val_loss 455.22705078125\n",
      "epoch 11, train_loss 1277.62353515625\n",
      "epoch 11, val_loss 455.2269287109375\n",
      "epoch 12, train_loss 1277.62353515625\n",
      "epoch 12, val_loss 455.226806640625\n",
      "epoch 13, train_loss 1277.623291015625\n",
      "epoch 13, val_loss 455.22674560546875\n",
      "epoch 14, train_loss 1277.623291015625\n",
      "epoch 14, val_loss 455.22662353515625\n",
      "epoch 15, train_loss 1277.6231689453125\n",
      "epoch 15, val_loss 455.2265930175781\n",
      "epoch 16, train_loss 1277.623046875\n",
      "epoch 16, val_loss 455.2264709472656\n",
      "epoch 17, train_loss 1277.622802734375\n",
      "epoch 17, val_loss 455.2264099121094\n",
      "epoch 18, train_loss 1277.6226806640625\n",
      "epoch 18, val_loss 455.226318359375\n",
      "epoch 19, train_loss 1277.62255859375\n",
      "epoch 19, val_loss 455.2261657714844\n",
      "epoch 20, train_loss 1277.62255859375\n",
      "epoch 20, val_loss 455.22607421875\n",
      "epoch 21, train_loss 1277.6221923828125\n",
      "epoch 21, val_loss 455.22601318359375\n",
      "epoch 22, train_loss 1277.6221923828125\n",
      "epoch 22, val_loss 455.2259216308594\n",
      "epoch 23, train_loss 1277.6221923828125\n",
      "epoch 23, val_loss 455.225830078125\n",
      "epoch 24, train_loss 1277.6219482421875\n",
      "epoch 24, val_loss 455.2257385253906\n",
      "epoch 25, train_loss 1277.621826171875\n",
      "epoch 25, val_loss 455.2256774902344\n",
      "epoch 26, train_loss 1277.6217041015625\n",
      "epoch 26, val_loss 455.2254638671875\n",
      "epoch 27, train_loss 1277.6217041015625\n",
      "epoch 27, val_loss 455.2254638671875\n",
      "epoch 28, train_loss 1277.6214599609375\n",
      "epoch 28, val_loss 455.2253723144531\n",
      "epoch 29, train_loss 1277.621337890625\n",
      "epoch 29, val_loss 455.22528076171875\n",
      "epoch 30, train_loss 1277.6214599609375\n",
      "epoch 30, val_loss 455.2252197265625\n",
      "epoch 31, train_loss 1277.6212158203125\n",
      "epoch 31, val_loss 455.22515869140625\n",
      "epoch 32, train_loss 1277.6209716796875\n",
      "epoch 32, val_loss 455.2250061035156\n",
      "epoch 33, train_loss 1277.6209716796875\n",
      "epoch 33, val_loss 455.22491455078125\n",
      "epoch 34, train_loss 1277.620849609375\n",
      "epoch 34, val_loss 455.2248229980469\n",
      "epoch 35, train_loss 1277.6207275390625\n",
      "epoch 35, val_loss 455.2247009277344\n",
      "epoch 36, train_loss 1277.62060546875\n",
      "epoch 36, val_loss 455.2246398925781\n",
      "epoch 37, train_loss 1277.6204833984375\n",
      "epoch 37, val_loss 455.22454833984375\n",
      "epoch 38, train_loss 1277.620361328125\n",
      "epoch 38, val_loss 455.22442626953125\n",
      "epoch 39, train_loss 1277.6202392578125\n",
      "epoch 39, val_loss 455.224365234375\n",
      "epoch 40, train_loss 1277.6201171875\n",
      "epoch 40, val_loss 455.22430419921875\n",
      "epoch 41, train_loss 1277.6201171875\n",
      "epoch 41, val_loss 455.2241516113281\n",
      "epoch 42, train_loss 1277.619873046875\n",
      "epoch 42, val_loss 455.22412109375\n",
      "epoch 43, train_loss 1277.619873046875\n",
      "epoch 43, val_loss 455.2239990234375\n",
      "epoch 44, train_loss 1277.61962890625\n",
      "epoch 44, val_loss 455.2239074707031\n",
      "epoch 45, train_loss 1277.619384765625\n",
      "epoch 45, val_loss 455.2238464355469\n",
      "epoch 46, train_loss 1277.619384765625\n",
      "epoch 46, val_loss 455.22369384765625\n",
      "epoch 47, train_loss 1277.619384765625\n",
      "epoch 47, val_loss 455.2236328125\n",
      "epoch 48, train_loss 1277.619384765625\n",
      "epoch 48, val_loss 455.22357177734375\n",
      "epoch 49, train_loss 1277.619140625\n",
      "epoch 49, val_loss 455.2235107421875\n",
      "epoch 50, train_loss 1277.618896484375\n",
      "epoch 50, val_loss 455.2234191894531\n",
      "epoch 51, train_loss 1277.6187744140625\n",
      "epoch 51, val_loss 455.2232360839844\n",
      "epoch 52, train_loss 1277.6185302734375\n",
      "epoch 52, val_loss 455.22320556640625\n",
      "epoch 53, train_loss 1277.6185302734375\n",
      "epoch 53, val_loss 455.22308349609375\n",
      "epoch 54, train_loss 1277.6185302734375\n",
      "epoch 54, val_loss 455.2230529785156\n",
      "epoch 55, train_loss 1277.6182861328125\n",
      "epoch 55, val_loss 455.2229309082031\n",
      "epoch 56, train_loss 1277.6181640625\n",
      "epoch 56, val_loss 455.22283935546875\n",
      "epoch 57, train_loss 1277.6180419921875\n",
      "epoch 57, val_loss 455.22271728515625\n",
      "epoch 58, train_loss 1277.6181640625\n",
      "epoch 58, val_loss 455.2226257324219\n",
      "epoch 59, train_loss 1277.617919921875\n",
      "epoch 59, val_loss 455.2225341796875\n",
      "epoch 60, train_loss 1277.6177978515625\n",
      "epoch 60, val_loss 455.2225036621094\n",
      "epoch 61, train_loss 1277.6177978515625\n",
      "epoch 61, val_loss 455.2223815917969\n",
      "epoch 62, train_loss 1277.6175537109375\n",
      "epoch 62, val_loss 455.2222900390625\n",
      "epoch 63, train_loss 1277.6173095703125\n",
      "epoch 63, val_loss 455.22216796875\n",
      "epoch 64, train_loss 1277.6173095703125\n",
      "epoch 64, val_loss 455.22216796875\n",
      "epoch 65, train_loss 1277.6173095703125\n",
      "epoch 65, val_loss 455.2220458984375\n",
      "epoch 66, train_loss 1277.616943359375\n",
      "epoch 66, val_loss 455.2219543457031\n",
      "epoch 67, train_loss 1277.616943359375\n",
      "epoch 67, val_loss 455.2218322753906\n",
      "epoch 68, train_loss 1277.61669921875\n",
      "epoch 68, val_loss 455.22174072265625\n",
      "epoch 69, train_loss 1277.61669921875\n",
      "epoch 69, val_loss 455.2216796875\n",
      "epoch 70, train_loss 1277.6165771484375\n",
      "epoch 70, val_loss 455.2215881347656\n",
      "epoch 71, train_loss 1277.616455078125\n",
      "epoch 71, val_loss 455.22149658203125\n",
      "epoch 72, train_loss 1277.616455078125\n",
      "epoch 72, val_loss 455.2213439941406\n",
      "epoch 73, train_loss 1277.6162109375\n",
      "epoch 73, val_loss 455.2213439941406\n",
      "epoch 74, train_loss 1277.6162109375\n",
      "epoch 74, val_loss 455.2212219238281\n",
      "epoch 75, train_loss 1277.6160888671875\n",
      "epoch 75, val_loss 455.2212219238281\n",
      "epoch 76, train_loss 1277.6158447265625\n",
      "epoch 76, val_loss 455.22100830078125\n",
      "epoch 77, train_loss 1277.61572265625\n",
      "epoch 77, val_loss 455.2209167480469\n",
      "epoch 78, train_loss 1277.6156005859375\n",
      "epoch 78, val_loss 455.2208251953125\n",
      "epoch 79, train_loss 1277.6153564453125\n",
      "epoch 79, val_loss 455.22076416015625\n",
      "epoch 80, train_loss 1277.6153564453125\n",
      "epoch 80, val_loss 455.220703125\n",
      "epoch 81, train_loss 1277.6153564453125\n",
      "epoch 81, val_loss 455.2206115722656\n",
      "epoch 82, train_loss 1277.615234375\n",
      "epoch 82, val_loss 455.2204284667969\n",
      "epoch 83, train_loss 1277.6148681640625\n",
      "epoch 83, val_loss 455.2203674316406\n",
      "epoch 84, train_loss 1277.614990234375\n",
      "epoch 84, val_loss 455.2203063964844\n",
      "epoch 85, train_loss 1277.6148681640625\n",
      "epoch 85, val_loss 455.22021484375\n",
      "epoch 86, train_loss 1277.6146240234375\n",
      "epoch 86, val_loss 455.2200927734375\n",
      "epoch 87, train_loss 1277.614501953125\n",
      "epoch 87, val_loss 455.22003173828125\n",
      "epoch 88, train_loss 1277.6143798828125\n",
      "epoch 88, val_loss 455.21990966796875\n",
      "epoch 89, train_loss 1277.6143798828125\n",
      "epoch 89, val_loss 455.2198181152344\n",
      "epoch 90, train_loss 1277.6141357421875\n",
      "epoch 90, val_loss 455.2196960449219\n",
      "epoch 91, train_loss 1277.6141357421875\n",
      "epoch 91, val_loss 455.21966552734375\n",
      "epoch 92, train_loss 1277.614013671875\n",
      "epoch 92, val_loss 455.2195739746094\n",
      "epoch 93, train_loss 1277.6138916015625\n",
      "epoch 93, val_loss 455.2195129394531\n",
      "epoch 94, train_loss 1277.61376953125\n",
      "epoch 94, val_loss 455.21942138671875\n",
      "epoch 95, train_loss 1277.6136474609375\n",
      "epoch 95, val_loss 455.21929931640625\n",
      "epoch 96, train_loss 1277.613525390625\n",
      "epoch 96, val_loss 455.21923828125\n",
      "epoch 97, train_loss 1277.61328125\n",
      "epoch 97, val_loss 455.2191162109375\n",
      "epoch 98, train_loss 1277.61328125\n",
      "epoch 98, val_loss 455.21905517578125\n",
      "epoch 99, train_loss 1277.6131591796875\n",
      "epoch 99, val_loss 455.2189025878906\n",
      "Parameter containing:\n",
      "tensor([1.2896e-14], requires_grad=True)\n",
      "iter 70, train_loss_regularization 0.8304823040962219\n",
      "iter 70, val_loss_regularization 0.8304823040962219\n",
      "epoch 0, train_loss 1277.613037109375\n",
      "epoch 0, val_loss 455.2188720703125\n",
      "epoch 1, train_loss 1277.6129150390625\n",
      "epoch 1, val_loss 455.21875\n",
      "epoch 2, train_loss 1277.61279296875\n",
      "epoch 2, val_loss 455.2187194824219\n",
      "epoch 3, train_loss 1277.61279296875\n",
      "epoch 3, val_loss 455.2185974121094\n",
      "epoch 4, train_loss 1277.612548828125\n",
      "epoch 4, val_loss 455.2185363769531\n",
      "epoch 5, train_loss 1277.6124267578125\n",
      "epoch 5, val_loss 455.2183837890625\n",
      "epoch 6, train_loss 1277.6123046875\n",
      "epoch 6, val_loss 455.2183837890625\n",
      "epoch 7, train_loss 1277.6121826171875\n",
      "epoch 7, val_loss 455.21826171875\n",
      "epoch 8, train_loss 1277.6123046875\n",
      "epoch 8, val_loss 455.2181396484375\n",
      "epoch 9, train_loss 1277.612060546875\n",
      "epoch 9, val_loss 455.2179870605469\n",
      "epoch 10, train_loss 1277.6119384765625\n",
      "epoch 10, val_loss 455.21795654296875\n",
      "epoch 11, train_loss 1277.6116943359375\n",
      "epoch 11, val_loss 455.2179260253906\n",
      "epoch 12, train_loss 1277.6116943359375\n",
      "epoch 12, val_loss 455.21783447265625\n",
      "epoch 13, train_loss 1277.6114501953125\n",
      "epoch 13, val_loss 455.21771240234375\n",
      "epoch 14, train_loss 1277.611328125\n",
      "epoch 14, val_loss 455.21759033203125\n",
      "epoch 15, train_loss 1277.6112060546875\n",
      "epoch 15, val_loss 455.21746826171875\n",
      "epoch 16, train_loss 1277.6112060546875\n",
      "epoch 16, val_loss 455.21746826171875\n",
      "epoch 17, train_loss 1277.611083984375\n",
      "epoch 17, val_loss 455.21734619140625\n",
      "epoch 18, train_loss 1277.6109619140625\n",
      "epoch 18, val_loss 455.2172546386719\n",
      "epoch 19, train_loss 1277.61083984375\n",
      "epoch 19, val_loss 455.2171630859375\n",
      "epoch 20, train_loss 1277.6107177734375\n",
      "epoch 20, val_loss 455.2170715332031\n",
      "epoch 21, train_loss 1277.6107177734375\n",
      "epoch 21, val_loss 455.2169189453125\n",
      "epoch 22, train_loss 1277.6104736328125\n",
      "epoch 22, val_loss 455.2169189453125\n",
      "epoch 23, train_loss 1277.6103515625\n",
      "epoch 23, val_loss 455.216796875\n",
      "epoch 24, train_loss 1277.6103515625\n",
      "epoch 24, val_loss 455.21673583984375\n",
      "epoch 25, train_loss 1277.610107421875\n",
      "epoch 25, val_loss 455.21661376953125\n",
      "epoch 26, train_loss 1277.6099853515625\n",
      "epoch 26, val_loss 455.21649169921875\n",
      "epoch 27, train_loss 1277.60986328125\n",
      "epoch 27, val_loss 455.2164306640625\n",
      "epoch 28, train_loss 1277.60986328125\n",
      "epoch 28, val_loss 455.2163391113281\n",
      "epoch 29, train_loss 1277.609619140625\n",
      "epoch 29, val_loss 455.21624755859375\n",
      "epoch 30, train_loss 1277.609619140625\n",
      "epoch 30, val_loss 455.2161560058594\n",
      "epoch 31, train_loss 1277.609619140625\n",
      "epoch 31, val_loss 455.21612548828125\n",
      "epoch 32, train_loss 1277.609375\n",
      "epoch 32, val_loss 455.21600341796875\n",
      "epoch 33, train_loss 1277.609130859375\n",
      "epoch 33, val_loss 455.2159118652344\n",
      "epoch 34, train_loss 1277.6090087890625\n",
      "epoch 34, val_loss 455.21575927734375\n",
      "epoch 35, train_loss 1277.609130859375\n",
      "epoch 35, val_loss 455.2156982421875\n",
      "epoch 36, train_loss 1277.60888671875\n",
      "epoch 36, val_loss 455.21563720703125\n",
      "epoch 37, train_loss 1277.608642578125\n",
      "epoch 37, val_loss 455.215576171875\n",
      "epoch 38, train_loss 1277.608642578125\n",
      "epoch 38, val_loss 455.21551513671875\n",
      "epoch 39, train_loss 1277.6083984375\n",
      "epoch 39, val_loss 455.21533203125\n",
      "epoch 40, train_loss 1277.6082763671875\n",
      "epoch 40, val_loss 455.2152404785156\n",
      "epoch 41, train_loss 1277.6082763671875\n",
      "epoch 41, val_loss 455.2151794433594\n",
      "epoch 42, train_loss 1277.6082763671875\n",
      "epoch 42, val_loss 455.21502685546875\n",
      "epoch 43, train_loss 1277.608154296875\n",
      "epoch 43, val_loss 455.2149963378906\n",
      "epoch 44, train_loss 1277.6077880859375\n",
      "epoch 44, val_loss 455.2148742675781\n",
      "epoch 45, train_loss 1277.6077880859375\n",
      "epoch 45, val_loss 455.21484375\n",
      "epoch 46, train_loss 1277.607666015625\n",
      "epoch 46, val_loss 455.2147216796875\n",
      "epoch 47, train_loss 1277.607666015625\n",
      "epoch 47, val_loss 455.2146301269531\n",
      "epoch 48, train_loss 1277.6075439453125\n",
      "epoch 48, val_loss 455.2145690917969\n",
      "epoch 49, train_loss 1277.6072998046875\n",
      "epoch 49, val_loss 455.2144470214844\n",
      "epoch 50, train_loss 1277.6072998046875\n",
      "epoch 50, val_loss 455.2143859863281\n",
      "epoch 51, train_loss 1277.6070556640625\n",
      "epoch 51, val_loss 455.21429443359375\n",
      "epoch 52, train_loss 1277.60693359375\n",
      "epoch 52, val_loss 455.2142028808594\n",
      "epoch 53, train_loss 1277.60693359375\n",
      "epoch 53, val_loss 455.2140808105469\n",
      "epoch 54, train_loss 1277.606689453125\n",
      "epoch 54, val_loss 455.2139892578125\n",
      "epoch 55, train_loss 1277.6065673828125\n",
      "epoch 55, val_loss 455.21392822265625\n",
      "epoch 56, train_loss 1277.606689453125\n",
      "epoch 56, val_loss 455.2138671875\n",
      "epoch 57, train_loss 1277.6063232421875\n",
      "epoch 57, val_loss 455.2137145996094\n",
      "epoch 58, train_loss 1277.6063232421875\n",
      "epoch 58, val_loss 455.2136535644531\n",
      "epoch 59, train_loss 1277.606201171875\n",
      "epoch 59, val_loss 455.2135314941406\n",
      "epoch 60, train_loss 1277.6060791015625\n",
      "epoch 60, val_loss 455.2135009765625\n",
      "epoch 61, train_loss 1277.6058349609375\n",
      "epoch 61, val_loss 455.21337890625\n",
      "epoch 62, train_loss 1277.605712890625\n",
      "epoch 62, val_loss 455.2133483886719\n",
      "epoch 63, train_loss 1277.605712890625\n",
      "epoch 63, val_loss 455.2132568359375\n",
      "epoch 64, train_loss 1277.6055908203125\n",
      "epoch 64, val_loss 455.2131652832031\n",
      "epoch 65, train_loss 1277.60546875\n",
      "epoch 65, val_loss 455.2130126953125\n",
      "epoch 66, train_loss 1277.60546875\n",
      "epoch 66, val_loss 455.2129211425781\n",
      "epoch 67, train_loss 1277.605224609375\n",
      "epoch 67, val_loss 455.21282958984375\n",
      "epoch 68, train_loss 1277.6051025390625\n",
      "epoch 68, val_loss 455.2127380371094\n",
      "epoch 69, train_loss 1277.6051025390625\n",
      "epoch 69, val_loss 455.21270751953125\n",
      "epoch 70, train_loss 1277.60498046875\n",
      "epoch 70, val_loss 455.21258544921875\n",
      "epoch 71, train_loss 1277.604736328125\n",
      "epoch 71, val_loss 455.2124938964844\n",
      "epoch 72, train_loss 1277.6046142578125\n",
      "epoch 72, val_loss 455.2123718261719\n",
      "epoch 73, train_loss 1277.6044921875\n",
      "epoch 73, val_loss 455.2122802734375\n",
      "epoch 74, train_loss 1277.6043701171875\n",
      "epoch 74, val_loss 455.2122497558594\n",
      "epoch 75, train_loss 1277.604248046875\n",
      "epoch 75, val_loss 455.212158203125\n",
      "epoch 76, train_loss 1277.604248046875\n",
      "epoch 76, val_loss 455.21209716796875\n",
      "epoch 77, train_loss 1277.6041259765625\n",
      "epoch 77, val_loss 455.2120056152344\n",
      "epoch 78, train_loss 1277.6038818359375\n",
      "epoch 78, val_loss 455.2118225097656\n",
      "epoch 79, train_loss 1277.6038818359375\n",
      "epoch 79, val_loss 455.2118225097656\n",
      "epoch 80, train_loss 1277.6038818359375\n",
      "epoch 80, val_loss 455.211669921875\n",
      "epoch 81, train_loss 1277.6036376953125\n",
      "epoch 81, val_loss 455.2115783691406\n",
      "epoch 82, train_loss 1277.603515625\n",
      "epoch 82, val_loss 455.21148681640625\n",
      "epoch 83, train_loss 1277.603271484375\n",
      "epoch 83, val_loss 455.21142578125\n",
      "epoch 84, train_loss 1277.603271484375\n",
      "epoch 84, val_loss 455.2113342285156\n",
      "epoch 85, train_loss 1277.60302734375\n",
      "epoch 85, val_loss 455.2112121582031\n",
      "epoch 86, train_loss 1277.60302734375\n",
      "epoch 86, val_loss 455.21112060546875\n",
      "epoch 87, train_loss 1277.6029052734375\n",
      "epoch 87, val_loss 455.2110900878906\n",
      "epoch 88, train_loss 1277.602783203125\n",
      "epoch 88, val_loss 455.2109680175781\n",
      "epoch 89, train_loss 1277.6026611328125\n",
      "epoch 89, val_loss 455.2109069824219\n",
      "epoch 90, train_loss 1277.6025390625\n",
      "epoch 90, val_loss 455.21075439453125\n",
      "epoch 91, train_loss 1277.6025390625\n",
      "epoch 91, val_loss 455.210693359375\n",
      "epoch 92, train_loss 1277.602294921875\n",
      "epoch 92, val_loss 455.2105712890625\n",
      "epoch 93, train_loss 1277.602294921875\n",
      "epoch 93, val_loss 455.2105407714844\n",
      "epoch 94, train_loss 1277.60205078125\n",
      "epoch 94, val_loss 455.21044921875\n",
      "epoch 95, train_loss 1277.60205078125\n",
      "epoch 95, val_loss 455.2104187011719\n",
      "epoch 96, train_loss 1277.601806640625\n",
      "epoch 96, val_loss 455.2102966308594\n",
      "epoch 97, train_loss 1277.601806640625\n",
      "epoch 97, val_loss 455.2101135253906\n",
      "epoch 98, train_loss 1277.6016845703125\n",
      "epoch 98, val_loss 455.2100830078125\n",
      "epoch 99, train_loss 1277.6014404296875\n",
      "epoch 99, val_loss 455.2099914550781\n",
      "Parameter containing:\n",
      "tensor([8.6205e-15], requires_grad=True)\n",
      "iter 71, train_loss_regularization 0.8272392153739929\n",
      "iter 71, val_loss_regularization 0.8272392153739929\n",
      "epoch 0, train_loss 1277.6014404296875\n",
      "epoch 0, val_loss 455.2099609375\n",
      "epoch 1, train_loss 1277.6011962890625\n",
      "epoch 1, val_loss 455.20977783203125\n",
      "epoch 2, train_loss 1277.6011962890625\n",
      "epoch 2, val_loss 455.2097473144531\n",
      "epoch 3, train_loss 1277.6009521484375\n",
      "epoch 3, val_loss 455.2096252441406\n",
      "epoch 4, train_loss 1277.6009521484375\n",
      "epoch 4, val_loss 455.2095947265625\n",
      "epoch 5, train_loss 1277.600830078125\n",
      "epoch 5, val_loss 455.2095031738281\n",
      "epoch 6, train_loss 1277.6005859375\n",
      "epoch 6, val_loss 455.20941162109375\n",
      "epoch 7, train_loss 1277.6005859375\n",
      "epoch 7, val_loss 455.20928955078125\n",
      "epoch 8, train_loss 1277.6005859375\n",
      "epoch 8, val_loss 455.2092590332031\n",
      "epoch 9, train_loss 1277.6004638671875\n",
      "epoch 9, val_loss 455.2090759277344\n",
      "epoch 10, train_loss 1277.6002197265625\n",
      "epoch 10, val_loss 455.20904541015625\n",
      "epoch 11, train_loss 1277.6002197265625\n",
      "epoch 11, val_loss 455.2088623046875\n",
      "epoch 12, train_loss 1277.60009765625\n",
      "epoch 12, val_loss 455.2088317871094\n",
      "epoch 13, train_loss 1277.5999755859375\n",
      "epoch 13, val_loss 455.2087097167969\n",
      "epoch 14, train_loss 1277.5997314453125\n",
      "epoch 14, val_loss 455.20867919921875\n",
      "epoch 15, train_loss 1277.5997314453125\n",
      "epoch 15, val_loss 455.2085876464844\n",
      "epoch 16, train_loss 1277.599609375\n",
      "epoch 16, val_loss 455.2084655761719\n",
      "epoch 17, train_loss 1277.599365234375\n",
      "epoch 17, val_loss 455.2083740234375\n",
      "epoch 18, train_loss 1277.599365234375\n",
      "epoch 18, val_loss 455.2083435058594\n",
      "epoch 19, train_loss 1277.59912109375\n",
      "epoch 19, val_loss 455.2082214355469\n",
      "epoch 20, train_loss 1277.5989990234375\n",
      "epoch 20, val_loss 455.2082214355469\n",
      "epoch 21, train_loss 1277.59912109375\n",
      "epoch 21, val_loss 455.20806884765625\n",
      "epoch 22, train_loss 1277.598876953125\n",
      "epoch 22, val_loss 455.2079162597656\n",
      "epoch 23, train_loss 1277.5986328125\n",
      "epoch 23, val_loss 455.2078857421875\n",
      "epoch 24, train_loss 1277.5986328125\n",
      "epoch 24, val_loss 455.2077941894531\n",
      "epoch 25, train_loss 1277.5986328125\n",
      "epoch 25, val_loss 455.2076721191406\n",
      "epoch 26, train_loss 1277.598388671875\n",
      "epoch 26, val_loss 455.2076110839844\n",
      "epoch 27, train_loss 1277.598388671875\n",
      "epoch 27, val_loss 455.2075500488281\n",
      "epoch 28, train_loss 1277.5982666015625\n",
      "epoch 28, val_loss 455.2074279785156\n",
      "epoch 29, train_loss 1277.5980224609375\n",
      "epoch 29, val_loss 455.2073059082031\n",
      "epoch 30, train_loss 1277.597900390625\n",
      "epoch 30, val_loss 455.2072448730469\n",
      "epoch 31, train_loss 1277.5977783203125\n",
      "epoch 31, val_loss 455.20721435546875\n",
      "epoch 32, train_loss 1277.5977783203125\n",
      "epoch 32, val_loss 455.20703125\n",
      "epoch 33, train_loss 1277.59765625\n",
      "epoch 33, val_loss 455.2070007324219\n",
      "epoch 34, train_loss 1277.5975341796875\n",
      "epoch 34, val_loss 455.2068786621094\n",
      "epoch 35, train_loss 1277.5972900390625\n",
      "epoch 35, val_loss 455.20684814453125\n",
      "epoch 36, train_loss 1277.5972900390625\n",
      "epoch 36, val_loss 455.2066650390625\n",
      "epoch 37, train_loss 1277.59716796875\n",
      "epoch 37, val_loss 455.2066345214844\n",
      "epoch 38, train_loss 1277.5970458984375\n",
      "epoch 38, val_loss 455.2064514160156\n",
      "epoch 39, train_loss 1277.5970458984375\n",
      "epoch 39, val_loss 455.2065124511719\n",
      "epoch 40, train_loss 1277.5968017578125\n",
      "epoch 40, val_loss 455.20635986328125\n",
      "epoch 41, train_loss 1277.5966796875\n",
      "epoch 41, val_loss 455.20623779296875\n",
      "epoch 42, train_loss 1277.5965576171875\n",
      "epoch 42, val_loss 455.2061767578125\n",
      "epoch 43, train_loss 1277.596435546875\n",
      "epoch 43, val_loss 455.2060546875\n",
      "epoch 44, train_loss 1277.596435546875\n",
      "epoch 44, val_loss 455.2060546875\n",
      "epoch 45, train_loss 1277.5963134765625\n",
      "epoch 45, val_loss 455.2059020996094\n",
      "epoch 46, train_loss 1277.5960693359375\n",
      "epoch 46, val_loss 455.20587158203125\n",
      "epoch 47, train_loss 1277.595947265625\n",
      "epoch 47, val_loss 455.20574951171875\n",
      "epoch 48, train_loss 1277.595947265625\n",
      "epoch 48, val_loss 455.2056579589844\n",
      "epoch 49, train_loss 1277.5958251953125\n",
      "epoch 49, val_loss 455.20550537109375\n",
      "epoch 50, train_loss 1277.595703125\n",
      "epoch 50, val_loss 455.20550537109375\n",
      "epoch 51, train_loss 1277.5955810546875\n",
      "epoch 51, val_loss 455.20538330078125\n",
      "epoch 52, train_loss 1277.595458984375\n",
      "epoch 52, val_loss 455.20538330078125\n",
      "epoch 53, train_loss 1277.59521484375\n",
      "epoch 53, val_loss 455.2051696777344\n",
      "epoch 54, train_loss 1277.59521484375\n",
      "epoch 54, val_loss 455.20513916015625\n",
      "epoch 55, train_loss 1277.59521484375\n",
      "epoch 55, val_loss 455.2049865722656\n",
      "epoch 56, train_loss 1277.594970703125\n",
      "epoch 56, val_loss 455.2049255371094\n",
      "epoch 57, train_loss 1277.5947265625\n",
      "epoch 57, val_loss 455.2048645019531\n",
      "epoch 58, train_loss 1277.5947265625\n",
      "epoch 58, val_loss 455.2047424316406\n",
      "epoch 59, train_loss 1277.5947265625\n",
      "epoch 59, val_loss 455.2046203613281\n",
      "epoch 60, train_loss 1277.5947265625\n",
      "epoch 60, val_loss 455.2046203613281\n",
      "epoch 61, train_loss 1277.594482421875\n",
      "epoch 61, val_loss 455.20452880859375\n",
      "epoch 62, train_loss 1277.59423828125\n",
      "epoch 62, val_loss 455.20440673828125\n",
      "epoch 63, train_loss 1277.5941162109375\n",
      "epoch 63, val_loss 455.20428466796875\n",
      "epoch 64, train_loss 1277.593994140625\n",
      "epoch 64, val_loss 455.2042541503906\n",
      "epoch 65, train_loss 1277.5938720703125\n",
      "epoch 65, val_loss 455.20416259765625\n",
      "epoch 66, train_loss 1277.5938720703125\n",
      "epoch 66, val_loss 455.2040710449219\n",
      "epoch 67, train_loss 1277.5936279296875\n",
      "epoch 67, val_loss 455.2039489746094\n",
      "epoch 68, train_loss 1277.5936279296875\n",
      "epoch 68, val_loss 455.2038269042969\n",
      "epoch 69, train_loss 1277.593505859375\n",
      "epoch 69, val_loss 455.2037353515625\n",
      "epoch 70, train_loss 1277.59326171875\n",
      "epoch 70, val_loss 455.2037048339844\n",
      "epoch 71, train_loss 1277.59326171875\n",
      "epoch 71, val_loss 455.20367431640625\n",
      "epoch 72, train_loss 1277.5931396484375\n",
      "epoch 72, val_loss 455.2034912109375\n",
      "epoch 73, train_loss 1277.5931396484375\n",
      "epoch 73, val_loss 455.2034606933594\n",
      "epoch 74, train_loss 1277.593017578125\n",
      "epoch 74, val_loss 455.2032775878906\n",
      "epoch 75, train_loss 1277.5927734375\n",
      "epoch 75, val_loss 455.2032470703125\n",
      "epoch 76, train_loss 1277.5927734375\n",
      "epoch 76, val_loss 455.2031555175781\n",
      "epoch 77, train_loss 1277.5926513671875\n",
      "epoch 77, val_loss 455.2030944824219\n",
      "epoch 78, train_loss 1277.5924072265625\n",
      "epoch 78, val_loss 455.2030029296875\n",
      "epoch 79, train_loss 1277.5924072265625\n",
      "epoch 79, val_loss 455.2029113769531\n",
      "epoch 80, train_loss 1277.5921630859375\n",
      "epoch 80, val_loss 455.20281982421875\n",
      "epoch 81, train_loss 1277.592041015625\n",
      "epoch 81, val_loss 455.2027587890625\n",
      "epoch 82, train_loss 1277.592041015625\n",
      "epoch 82, val_loss 455.20263671875\n",
      "epoch 83, train_loss 1277.5919189453125\n",
      "epoch 83, val_loss 455.20257568359375\n",
      "epoch 84, train_loss 1277.591796875\n",
      "epoch 84, val_loss 455.2025146484375\n",
      "epoch 85, train_loss 1277.5916748046875\n",
      "epoch 85, val_loss 455.20233154296875\n",
      "epoch 86, train_loss 1277.591552734375\n",
      "epoch 86, val_loss 455.20233154296875\n",
      "epoch 87, train_loss 1277.591552734375\n",
      "epoch 87, val_loss 455.20220947265625\n",
      "epoch 88, train_loss 1277.5911865234375\n",
      "epoch 88, val_loss 455.2021179199219\n",
      "epoch 89, train_loss 1277.59130859375\n",
      "epoch 89, val_loss 455.2019958496094\n",
      "epoch 90, train_loss 1277.5911865234375\n",
      "epoch 90, val_loss 455.201904296875\n",
      "epoch 91, train_loss 1277.5909423828125\n",
      "epoch 91, val_loss 455.20184326171875\n",
      "epoch 92, train_loss 1277.5908203125\n",
      "epoch 92, val_loss 455.2017822265625\n",
      "epoch 93, train_loss 1277.5906982421875\n",
      "epoch 93, val_loss 455.20166015625\n",
      "epoch 94, train_loss 1277.5906982421875\n",
      "epoch 94, val_loss 455.2015686035156\n",
      "epoch 95, train_loss 1277.5904541015625\n",
      "epoch 95, val_loss 455.2014465332031\n",
      "epoch 96, train_loss 1277.59033203125\n",
      "epoch 96, val_loss 455.201416015625\n",
      "epoch 97, train_loss 1277.59033203125\n",
      "epoch 97, val_loss 455.2012939453125\n",
      "epoch 98, train_loss 1277.5902099609375\n",
      "epoch 98, val_loss 455.2012634277344\n",
      "epoch 99, train_loss 1277.590087890625\n",
      "epoch 99, val_loss 455.2012023925781\n",
      "Parameter containing:\n",
      "tensor([5.7684e-15], requires_grad=True)\n",
      "iter 72, train_loss_regularization 0.8240529298782349\n",
      "iter 72, val_loss_regularization 0.8240529298782349\n",
      "epoch 0, train_loss 1277.5899658203125\n",
      "epoch 0, val_loss 455.20098876953125\n",
      "epoch 1, train_loss 1277.5899658203125\n",
      "epoch 1, val_loss 455.2009582519531\n",
      "epoch 2, train_loss 1277.5897216796875\n",
      "epoch 2, val_loss 455.20086669921875\n",
      "epoch 3, train_loss 1277.5897216796875\n",
      "epoch 3, val_loss 455.2008361816406\n",
      "epoch 4, train_loss 1277.5894775390625\n",
      "epoch 4, val_loss 455.20074462890625\n",
      "epoch 5, train_loss 1277.5894775390625\n",
      "epoch 5, val_loss 455.20062255859375\n",
      "epoch 6, train_loss 1277.5892333984375\n",
      "epoch 6, val_loss 455.20050048828125\n",
      "epoch 7, train_loss 1277.589111328125\n",
      "epoch 7, val_loss 455.2004699707031\n",
      "epoch 8, train_loss 1277.589111328125\n",
      "epoch 8, val_loss 455.20037841796875\n",
      "epoch 9, train_loss 1277.5888671875\n",
      "epoch 9, val_loss 455.20025634765625\n",
      "epoch 10, train_loss 1277.5888671875\n",
      "epoch 10, val_loss 455.2001647949219\n",
      "epoch 11, train_loss 1277.5888671875\n",
      "epoch 11, val_loss 455.2000732421875\n",
      "epoch 12, train_loss 1277.588623046875\n",
      "epoch 12, val_loss 455.20001220703125\n",
      "epoch 13, train_loss 1277.58837890625\n",
      "epoch 13, val_loss 455.1999206542969\n",
      "epoch 14, train_loss 1277.58837890625\n",
      "epoch 14, val_loss 455.1998291015625\n",
      "epoch 15, train_loss 1277.5882568359375\n",
      "epoch 15, val_loss 455.19970703125\n",
      "epoch 16, train_loss 1277.588134765625\n",
      "epoch 16, val_loss 455.1996154785156\n",
      "epoch 17, train_loss 1277.5880126953125\n",
      "epoch 17, val_loss 455.1995849609375\n",
      "epoch 18, train_loss 1277.587890625\n",
      "epoch 18, val_loss 455.1994934082031\n",
      "epoch 19, train_loss 1277.587890625\n",
      "epoch 19, val_loss 455.1993713378906\n",
      "epoch 20, train_loss 1277.5875244140625\n",
      "epoch 20, val_loss 455.1993408203125\n",
      "epoch 21, train_loss 1277.5875244140625\n",
      "epoch 21, val_loss 455.19921875\n",
      "epoch 22, train_loss 1277.5875244140625\n",
      "epoch 22, val_loss 455.1991271972656\n",
      "epoch 23, train_loss 1277.58740234375\n",
      "epoch 23, val_loss 455.19903564453125\n",
      "epoch 24, train_loss 1277.5872802734375\n",
      "epoch 24, val_loss 455.1989440917969\n",
      "epoch 25, train_loss 1277.587158203125\n",
      "epoch 25, val_loss 455.19891357421875\n",
      "epoch 26, train_loss 1277.5870361328125\n",
      "epoch 26, val_loss 455.19879150390625\n",
      "epoch 27, train_loss 1277.5869140625\n",
      "epoch 27, val_loss 455.1986389160156\n",
      "epoch 28, train_loss 1277.5867919921875\n",
      "epoch 28, val_loss 455.1985778808594\n",
      "epoch 29, train_loss 1277.586669921875\n",
      "epoch 29, val_loss 455.1984558105469\n",
      "epoch 30, train_loss 1277.5865478515625\n",
      "epoch 30, val_loss 455.1984558105469\n",
      "epoch 31, train_loss 1277.5863037109375\n",
      "epoch 31, val_loss 455.1983642578125\n",
      "epoch 32, train_loss 1277.5863037109375\n",
      "epoch 32, val_loss 455.1982421875\n",
      "epoch 33, train_loss 1277.5863037109375\n",
      "epoch 33, val_loss 455.1981201171875\n",
      "epoch 34, train_loss 1277.5860595703125\n",
      "epoch 34, val_loss 455.1980895996094\n",
      "epoch 35, train_loss 1277.5860595703125\n",
      "epoch 35, val_loss 455.1979675292969\n",
      "epoch 36, train_loss 1277.585693359375\n",
      "epoch 36, val_loss 455.1979064941406\n",
      "epoch 37, train_loss 1277.5858154296875\n",
      "epoch 37, val_loss 455.1977844238281\n",
      "epoch 38, train_loss 1277.5855712890625\n",
      "epoch 38, val_loss 455.19775390625\n",
      "epoch 39, train_loss 1277.5855712890625\n",
      "epoch 39, val_loss 455.1976318359375\n",
      "epoch 40, train_loss 1277.58544921875\n",
      "epoch 40, val_loss 455.1975402832031\n",
      "epoch 41, train_loss 1277.5853271484375\n",
      "epoch 41, val_loss 455.197509765625\n",
      "epoch 42, train_loss 1277.585205078125\n",
      "epoch 42, val_loss 455.19732666015625\n",
      "epoch 43, train_loss 1277.5850830078125\n",
      "epoch 43, val_loss 455.1972961425781\n",
      "epoch 44, train_loss 1277.5849609375\n",
      "epoch 44, val_loss 455.19720458984375\n",
      "epoch 45, train_loss 1277.5849609375\n",
      "epoch 45, val_loss 455.19708251953125\n",
      "epoch 46, train_loss 1277.5848388671875\n",
      "epoch 46, val_loss 455.1969909667969\n",
      "epoch 47, train_loss 1277.584716796875\n",
      "epoch 47, val_loss 455.1969299316406\n",
      "epoch 48, train_loss 1277.58447265625\n",
      "epoch 48, val_loss 455.19683837890625\n",
      "epoch 49, train_loss 1277.5843505859375\n",
      "epoch 49, val_loss 455.1967468261719\n",
      "epoch 50, train_loss 1277.5843505859375\n",
      "epoch 50, val_loss 455.1966552734375\n",
      "epoch 51, train_loss 1277.584228515625\n",
      "epoch 51, val_loss 455.19659423828125\n",
      "epoch 52, train_loss 1277.583984375\n",
      "epoch 52, val_loss 455.1965026855469\n",
      "epoch 53, train_loss 1277.583984375\n",
      "epoch 53, val_loss 455.1963806152344\n",
      "epoch 54, train_loss 1277.5838623046875\n",
      "epoch 54, val_loss 455.1963195800781\n",
      "epoch 55, train_loss 1277.5838623046875\n",
      "epoch 55, val_loss 455.1961975097656\n",
      "epoch 56, train_loss 1277.5836181640625\n",
      "epoch 56, val_loss 455.1961669921875\n",
      "epoch 57, train_loss 1277.5836181640625\n",
      "epoch 57, val_loss 455.1961364746094\n",
      "epoch 58, train_loss 1277.5833740234375\n",
      "epoch 58, val_loss 455.1959228515625\n",
      "epoch 59, train_loss 1277.5833740234375\n",
      "epoch 59, val_loss 455.19586181640625\n",
      "epoch 60, train_loss 1277.5831298828125\n",
      "epoch 60, val_loss 455.1958312988281\n",
      "epoch 61, train_loss 1277.5828857421875\n",
      "epoch 61, val_loss 455.1957092285156\n",
      "epoch 62, train_loss 1277.5831298828125\n",
      "epoch 62, val_loss 455.1956787109375\n",
      "epoch 63, train_loss 1277.582763671875\n",
      "epoch 63, val_loss 455.195556640625\n",
      "epoch 64, train_loss 1277.5826416015625\n",
      "epoch 64, val_loss 455.1954650878906\n",
      "epoch 65, train_loss 1277.58251953125\n",
      "epoch 65, val_loss 455.1953430175781\n",
      "epoch 66, train_loss 1277.58251953125\n",
      "epoch 66, val_loss 455.19525146484375\n",
      "epoch 67, train_loss 1277.582275390625\n",
      "epoch 67, val_loss 455.1951599121094\n",
      "epoch 68, train_loss 1277.582275390625\n",
      "epoch 68, val_loss 455.1950378417969\n",
      "epoch 69, train_loss 1277.582275390625\n",
      "epoch 69, val_loss 455.1950378417969\n",
      "epoch 70, train_loss 1277.5821533203125\n",
      "epoch 70, val_loss 455.1949462890625\n",
      "epoch 71, train_loss 1277.58203125\n",
      "epoch 71, val_loss 455.1949157714844\n",
      "epoch 72, train_loss 1277.581787109375\n",
      "epoch 72, val_loss 455.19476318359375\n",
      "epoch 73, train_loss 1277.581787109375\n",
      "epoch 73, val_loss 455.1946716308594\n",
      "epoch 74, train_loss 1277.58154296875\n",
      "epoch 74, val_loss 455.1945495605469\n",
      "epoch 75, train_loss 1277.5814208984375\n",
      "epoch 75, val_loss 455.1944885253906\n",
      "epoch 76, train_loss 1277.581298828125\n",
      "epoch 76, val_loss 455.1944274902344\n",
      "epoch 77, train_loss 1277.5811767578125\n",
      "epoch 77, val_loss 455.1943664550781\n",
      "epoch 78, train_loss 1277.5811767578125\n",
      "epoch 78, val_loss 455.1942138671875\n",
      "epoch 79, train_loss 1277.5810546875\n",
      "epoch 79, val_loss 455.194091796875\n",
      "epoch 80, train_loss 1277.5810546875\n",
      "epoch 80, val_loss 455.1940002441406\n",
      "epoch 81, train_loss 1277.5809326171875\n",
      "epoch 81, val_loss 455.1939697265625\n",
      "epoch 82, train_loss 1277.580810546875\n",
      "epoch 82, val_loss 455.1938781738281\n",
      "epoch 83, train_loss 1277.58056640625\n",
      "epoch 83, val_loss 455.19378662109375\n",
      "epoch 84, train_loss 1277.5804443359375\n",
      "epoch 84, val_loss 455.19366455078125\n",
      "epoch 85, train_loss 1277.5804443359375\n",
      "epoch 85, val_loss 455.1935729980469\n",
      "epoch 86, train_loss 1277.5802001953125\n",
      "epoch 86, val_loss 455.19354248046875\n",
      "epoch 87, train_loss 1277.5802001953125\n",
      "epoch 87, val_loss 455.19342041015625\n",
      "epoch 88, train_loss 1277.5799560546875\n",
      "epoch 88, val_loss 455.19342041015625\n",
      "epoch 89, train_loss 1277.5799560546875\n",
      "epoch 89, val_loss 455.1932373046875\n",
      "epoch 90, train_loss 1277.579833984375\n",
      "epoch 90, val_loss 455.1932067871094\n",
      "epoch 91, train_loss 1277.579833984375\n",
      "epoch 91, val_loss 455.19305419921875\n",
      "epoch 92, train_loss 1277.57958984375\n",
      "epoch 92, val_loss 455.1929931640625\n",
      "epoch 93, train_loss 1277.5794677734375\n",
      "epoch 93, val_loss 455.1929626464844\n",
      "epoch 94, train_loss 1277.5794677734375\n",
      "epoch 94, val_loss 455.1928405761719\n",
      "epoch 95, train_loss 1277.5792236328125\n",
      "epoch 95, val_loss 455.1927185058594\n",
      "epoch 96, train_loss 1277.5792236328125\n",
      "epoch 96, val_loss 455.1926574707031\n",
      "epoch 97, train_loss 1277.5791015625\n",
      "epoch 97, val_loss 455.1925964355469\n",
      "epoch 98, train_loss 1277.5789794921875\n",
      "epoch 98, val_loss 455.19244384765625\n",
      "epoch 99, train_loss 1277.578857421875\n",
      "epoch 99, val_loss 455.1923828125\n",
      "Parameter containing:\n",
      "tensor([3.8637e-15], requires_grad=True)\n",
      "iter 73, train_loss_regularization 0.8209221363067627\n",
      "iter 73, val_loss_regularization 0.8209221363067627\n",
      "epoch 0, train_loss 1277.57861328125\n",
      "epoch 0, val_loss 455.1922912597656\n",
      "epoch 1, train_loss 1277.5784912109375\n",
      "epoch 1, val_loss 455.1922607421875\n",
      "epoch 2, train_loss 1277.5784912109375\n",
      "epoch 2, val_loss 455.1921691894531\n",
      "epoch 3, train_loss 1277.578369140625\n",
      "epoch 3, val_loss 455.19207763671875\n",
      "epoch 4, train_loss 1277.578369140625\n",
      "epoch 4, val_loss 455.1919860839844\n",
      "epoch 5, train_loss 1277.578125\n",
      "epoch 5, val_loss 455.1918640136719\n",
      "epoch 6, train_loss 1277.5780029296875\n",
      "epoch 6, val_loss 455.1918029785156\n",
      "epoch 7, train_loss 1277.577880859375\n",
      "epoch 7, val_loss 455.19171142578125\n",
      "epoch 8, train_loss 1277.5777587890625\n",
      "epoch 8, val_loss 455.19158935546875\n",
      "epoch 9, train_loss 1277.5777587890625\n",
      "epoch 9, val_loss 455.1915283203125\n",
      "epoch 10, train_loss 1277.57763671875\n",
      "epoch 10, val_loss 455.19146728515625\n",
      "epoch 11, train_loss 1277.577392578125\n",
      "epoch 11, val_loss 455.19134521484375\n",
      "epoch 12, train_loss 1277.577392578125\n",
      "epoch 12, val_loss 455.1912536621094\n",
      "epoch 13, train_loss 1277.577392578125\n",
      "epoch 13, val_loss 455.191162109375\n",
      "epoch 14, train_loss 1277.5770263671875\n",
      "epoch 14, val_loss 455.1910705566406\n",
      "epoch 15, train_loss 1277.5770263671875\n",
      "epoch 15, val_loss 455.1910705566406\n",
      "epoch 16, train_loss 1277.576904296875\n",
      "epoch 16, val_loss 455.1908874511719\n",
      "epoch 17, train_loss 1277.5767822265625\n",
      "epoch 17, val_loss 455.1908264160156\n",
      "epoch 18, train_loss 1277.5767822265625\n",
      "epoch 18, val_loss 455.19073486328125\n",
      "epoch 19, train_loss 1277.57666015625\n",
      "epoch 19, val_loss 455.1907043457031\n",
      "epoch 20, train_loss 1277.5765380859375\n",
      "epoch 20, val_loss 455.1905822753906\n",
      "epoch 21, train_loss 1277.5762939453125\n",
      "epoch 21, val_loss 455.1904602050781\n",
      "epoch 22, train_loss 1277.5762939453125\n",
      "epoch 22, val_loss 455.19036865234375\n",
      "epoch 23, train_loss 1277.5762939453125\n",
      "epoch 23, val_loss 455.1903381347656\n",
      "epoch 24, train_loss 1277.5758056640625\n",
      "epoch 24, val_loss 455.19024658203125\n",
      "epoch 25, train_loss 1277.5758056640625\n",
      "epoch 25, val_loss 455.19012451171875\n",
      "epoch 26, train_loss 1277.5758056640625\n",
      "epoch 26, val_loss 455.1900329589844\n",
      "epoch 27, train_loss 1277.5758056640625\n",
      "epoch 27, val_loss 455.1899108886719\n",
      "epoch 28, train_loss 1277.575439453125\n",
      "epoch 28, val_loss 455.18988037109375\n",
      "epoch 29, train_loss 1277.575439453125\n",
      "epoch 29, val_loss 455.1897888183594\n",
      "epoch 30, train_loss 1277.5753173828125\n",
      "epoch 30, val_loss 455.189697265625\n",
      "epoch 31, train_loss 1277.5753173828125\n",
      "epoch 31, val_loss 455.1896667480469\n",
      "epoch 32, train_loss 1277.5751953125\n",
      "epoch 32, val_loss 455.18951416015625\n",
      "epoch 33, train_loss 1277.5750732421875\n",
      "epoch 33, val_loss 455.189453125\n",
      "epoch 34, train_loss 1277.574951171875\n",
      "epoch 34, val_loss 455.1893615722656\n",
      "epoch 35, train_loss 1277.5748291015625\n",
      "epoch 35, val_loss 455.1893310546875\n",
      "epoch 36, train_loss 1277.5745849609375\n",
      "epoch 36, val_loss 455.189208984375\n",
      "epoch 37, train_loss 1277.5745849609375\n",
      "epoch 37, val_loss 455.1890869140625\n",
      "epoch 38, train_loss 1277.5745849609375\n",
      "epoch 38, val_loss 455.1889953613281\n",
      "epoch 39, train_loss 1277.5743408203125\n",
      "epoch 39, val_loss 455.18890380859375\n",
      "epoch 40, train_loss 1277.57421875\n",
      "epoch 40, val_loss 455.18878173828125\n",
      "epoch 41, train_loss 1277.57421875\n",
      "epoch 41, val_loss 455.18878173828125\n",
      "epoch 42, train_loss 1277.573974609375\n",
      "epoch 42, val_loss 455.1887512207031\n",
      "epoch 43, train_loss 1277.5738525390625\n",
      "epoch 43, val_loss 455.1885681152344\n",
      "epoch 44, train_loss 1277.5738525390625\n",
      "epoch 44, val_loss 455.1885070800781\n",
      "epoch 45, train_loss 1277.57373046875\n",
      "epoch 45, val_loss 455.18841552734375\n",
      "epoch 46, train_loss 1277.573486328125\n",
      "epoch 46, val_loss 455.1883239746094\n",
      "epoch 47, train_loss 1277.5733642578125\n",
      "epoch 47, val_loss 455.1882629394531\n",
      "epoch 48, train_loss 1277.5732421875\n",
      "epoch 48, val_loss 455.18817138671875\n",
      "epoch 49, train_loss 1277.5731201171875\n",
      "epoch 49, val_loss 455.1880798339844\n",
      "epoch 50, train_loss 1277.5731201171875\n",
      "epoch 50, val_loss 455.18798828125\n",
      "epoch 51, train_loss 1277.5732421875\n",
      "epoch 51, val_loss 455.1878662109375\n",
      "epoch 52, train_loss 1277.5728759765625\n",
      "epoch 52, val_loss 455.1878356933594\n",
      "epoch 53, train_loss 1277.57275390625\n",
      "epoch 53, val_loss 455.1876525878906\n",
      "epoch 54, train_loss 1277.5726318359375\n",
      "epoch 54, val_loss 455.1876220703125\n",
      "epoch 55, train_loss 1277.5726318359375\n",
      "epoch 55, val_loss 455.1875305175781\n",
      "epoch 56, train_loss 1277.5723876953125\n",
      "epoch 56, val_loss 455.1875\n",
      "epoch 57, train_loss 1277.572265625\n",
      "epoch 57, val_loss 455.1873779296875\n",
      "epoch 58, train_loss 1277.572265625\n",
      "epoch 58, val_loss 455.1872863769531\n",
      "epoch 59, train_loss 1277.5721435546875\n",
      "epoch 59, val_loss 455.1871643066406\n",
      "epoch 60, train_loss 1277.57177734375\n",
      "epoch 60, val_loss 455.1871337890625\n",
      "epoch 61, train_loss 1277.5718994140625\n",
      "epoch 61, val_loss 455.18701171875\n",
      "epoch 62, train_loss 1277.57177734375\n",
      "epoch 62, val_loss 455.18701171875\n",
      "epoch 63, train_loss 1277.5716552734375\n",
      "epoch 63, val_loss 455.18682861328125\n",
      "epoch 64, train_loss 1277.571533203125\n",
      "epoch 64, val_loss 455.1867980957031\n",
      "epoch 65, train_loss 1277.571533203125\n",
      "epoch 65, val_loss 455.18670654296875\n",
      "epoch 66, train_loss 1277.571533203125\n",
      "epoch 66, val_loss 455.18658447265625\n",
      "epoch 67, train_loss 1277.5714111328125\n",
      "epoch 67, val_loss 455.1865539550781\n",
      "epoch 68, train_loss 1277.571044921875\n",
      "epoch 68, val_loss 455.18646240234375\n",
      "epoch 69, train_loss 1277.571044921875\n",
      "epoch 69, val_loss 455.18634033203125\n",
      "epoch 70, train_loss 1277.5709228515625\n",
      "epoch 70, val_loss 455.1862487792969\n",
      "epoch 71, train_loss 1277.57080078125\n",
      "epoch 71, val_loss 455.1861572265625\n",
      "epoch 72, train_loss 1277.5706787109375\n",
      "epoch 72, val_loss 455.18609619140625\n",
      "epoch 73, train_loss 1277.570556640625\n",
      "epoch 73, val_loss 455.18603515625\n",
      "epoch 74, train_loss 1277.570556640625\n",
      "epoch 74, val_loss 455.1858825683594\n",
      "epoch 75, train_loss 1277.5701904296875\n",
      "epoch 75, val_loss 455.1858215332031\n",
      "epoch 76, train_loss 1277.5701904296875\n",
      "epoch 76, val_loss 455.1857604980469\n",
      "epoch 77, train_loss 1277.5701904296875\n",
      "epoch 77, val_loss 455.1856384277344\n",
      "epoch 78, train_loss 1277.5699462890625\n",
      "epoch 78, val_loss 455.1855773925781\n",
      "epoch 79, train_loss 1277.5699462890625\n",
      "epoch 79, val_loss 455.18548583984375\n",
      "epoch 80, train_loss 1277.5697021484375\n",
      "epoch 80, val_loss 455.18536376953125\n",
      "epoch 81, train_loss 1277.569580078125\n",
      "epoch 81, val_loss 455.1853332519531\n",
      "epoch 82, train_loss 1277.5694580078125\n",
      "epoch 82, val_loss 455.18524169921875\n",
      "epoch 83, train_loss 1277.569580078125\n",
      "epoch 83, val_loss 455.18511962890625\n",
      "epoch 84, train_loss 1277.5693359375\n",
      "epoch 84, val_loss 455.1850280761719\n",
      "epoch 85, train_loss 1277.5692138671875\n",
      "epoch 85, val_loss 455.1849670410156\n",
      "epoch 86, train_loss 1277.569091796875\n",
      "epoch 86, val_loss 455.1848449707031\n",
      "epoch 87, train_loss 1277.569091796875\n",
      "epoch 87, val_loss 455.1847839355469\n",
      "epoch 88, train_loss 1277.5689697265625\n",
      "epoch 88, val_loss 455.1847229003906\n",
      "epoch 89, train_loss 1277.56884765625\n",
      "epoch 89, val_loss 455.1846618652344\n",
      "epoch 90, train_loss 1277.5687255859375\n",
      "epoch 90, val_loss 455.18450927734375\n",
      "epoch 91, train_loss 1277.5687255859375\n",
      "epoch 91, val_loss 455.1844482421875\n",
      "epoch 92, train_loss 1277.568359375\n",
      "epoch 92, val_loss 455.18438720703125\n",
      "epoch 93, train_loss 1277.568359375\n",
      "epoch 93, val_loss 455.1842956542969\n",
      "epoch 94, train_loss 1277.568115234375\n",
      "epoch 94, val_loss 455.1842041015625\n",
      "epoch 95, train_loss 1277.568115234375\n",
      "epoch 95, val_loss 455.18408203125\n",
      "epoch 96, train_loss 1277.568115234375\n",
      "epoch 96, val_loss 455.1839904785156\n",
      "epoch 97, train_loss 1277.56787109375\n",
      "epoch 97, val_loss 455.1839294433594\n",
      "epoch 98, train_loss 1277.5679931640625\n",
      "epoch 98, val_loss 455.183837890625\n",
      "epoch 99, train_loss 1277.5677490234375\n",
      "epoch 99, val_loss 455.1837463378906\n",
      "Parameter containing:\n",
      "tensor([2.5905e-15], requires_grad=True)\n",
      "iter 74, train_loss_regularization 0.8178461194038391\n",
      "iter 74, val_loss_regularization 0.8178461194038391\n",
      "epoch 0, train_loss 1277.567626953125\n",
      "epoch 0, val_loss 455.1837158203125\n",
      "epoch 1, train_loss 1277.5673828125\n",
      "epoch 1, val_loss 455.18359375\n",
      "epoch 2, train_loss 1277.5675048828125\n",
      "epoch 2, val_loss 455.1835021972656\n",
      "epoch 3, train_loss 1277.5673828125\n",
      "epoch 3, val_loss 455.18341064453125\n",
      "epoch 4, train_loss 1277.5670166015625\n",
      "epoch 4, val_loss 455.18328857421875\n",
      "epoch 5, train_loss 1277.567138671875\n",
      "epoch 5, val_loss 455.1832580566406\n",
      "epoch 6, train_loss 1277.5670166015625\n",
      "epoch 6, val_loss 455.18316650390625\n",
      "epoch 7, train_loss 1277.5667724609375\n",
      "epoch 7, val_loss 455.18304443359375\n",
      "epoch 8, train_loss 1277.5665283203125\n",
      "epoch 8, val_loss 455.1830139160156\n",
      "epoch 9, train_loss 1277.5665283203125\n",
      "epoch 9, val_loss 455.18292236328125\n",
      "epoch 10, train_loss 1277.5665283203125\n",
      "epoch 10, val_loss 455.18280029296875\n",
      "epoch 11, train_loss 1277.5665283203125\n",
      "epoch 11, val_loss 455.1826171875\n",
      "epoch 12, train_loss 1277.5662841796875\n",
      "epoch 12, val_loss 455.1826171875\n",
      "epoch 13, train_loss 1277.5662841796875\n",
      "epoch 13, val_loss 455.18255615234375\n",
      "epoch 14, train_loss 1277.5660400390625\n",
      "epoch 14, val_loss 455.1824951171875\n",
      "epoch 15, train_loss 1277.5660400390625\n",
      "epoch 15, val_loss 455.182373046875\n",
      "epoch 16, train_loss 1277.56591796875\n",
      "epoch 16, val_loss 455.182373046875\n",
      "epoch 17, train_loss 1277.5657958984375\n",
      "epoch 17, val_loss 455.1821594238281\n",
      "epoch 18, train_loss 1277.5657958984375\n",
      "epoch 18, val_loss 455.18212890625\n",
      "epoch 19, train_loss 1277.5654296875\n",
      "epoch 19, val_loss 455.1820068359375\n",
      "epoch 20, train_loss 1277.5654296875\n",
      "epoch 20, val_loss 455.18194580078125\n",
      "epoch 21, train_loss 1277.5653076171875\n",
      "epoch 21, val_loss 455.1819152832031\n",
      "epoch 22, train_loss 1277.5653076171875\n",
      "epoch 22, val_loss 455.1817932128906\n",
      "epoch 23, train_loss 1277.5650634765625\n",
      "epoch 23, val_loss 455.18170166015625\n",
      "epoch 24, train_loss 1277.56494140625\n",
      "epoch 24, val_loss 455.1816101074219\n",
      "epoch 25, train_loss 1277.56494140625\n",
      "epoch 25, val_loss 455.1814880371094\n",
      "epoch 26, train_loss 1277.56494140625\n",
      "epoch 26, val_loss 455.18145751953125\n",
      "epoch 27, train_loss 1277.564697265625\n",
      "epoch 27, val_loss 455.18133544921875\n",
      "epoch 28, train_loss 1277.564453125\n",
      "epoch 28, val_loss 455.1812438964844\n",
      "epoch 29, train_loss 1277.564453125\n",
      "epoch 29, val_loss 455.18121337890625\n",
      "epoch 30, train_loss 1277.5643310546875\n",
      "epoch 30, val_loss 455.18109130859375\n",
      "epoch 31, train_loss 1277.564208984375\n",
      "epoch 31, val_loss 455.1810302734375\n",
      "epoch 32, train_loss 1277.5640869140625\n",
      "epoch 32, val_loss 455.1809997558594\n",
      "epoch 33, train_loss 1277.56396484375\n",
      "epoch 33, val_loss 455.18084716796875\n",
      "epoch 34, train_loss 1277.56396484375\n",
      "epoch 34, val_loss 455.1807556152344\n",
      "epoch 35, train_loss 1277.56396484375\n",
      "epoch 35, val_loss 455.1806640625\n",
      "epoch 36, train_loss 1277.563720703125\n",
      "epoch 36, val_loss 455.1806335449219\n",
      "epoch 37, train_loss 1277.563720703125\n",
      "epoch 37, val_loss 455.1805114746094\n",
      "epoch 38, train_loss 1277.5634765625\n",
      "epoch 38, val_loss 455.180419921875\n",
      "epoch 39, train_loss 1277.5634765625\n",
      "epoch 39, val_loss 455.1802978515625\n",
      "epoch 40, train_loss 1277.5633544921875\n",
      "epoch 40, val_loss 455.18023681640625\n",
      "epoch 41, train_loss 1277.5631103515625\n",
      "epoch 41, val_loss 455.18017578125\n",
      "epoch 42, train_loss 1277.5631103515625\n",
      "epoch 42, val_loss 455.1800842285156\n",
      "epoch 43, train_loss 1277.56298828125\n",
      "epoch 43, val_loss 455.17999267578125\n",
      "epoch 44, train_loss 1277.5628662109375\n",
      "epoch 44, val_loss 455.17987060546875\n",
      "epoch 45, train_loss 1277.5626220703125\n",
      "epoch 45, val_loss 455.1797790527344\n",
      "epoch 46, train_loss 1277.5626220703125\n",
      "epoch 46, val_loss 455.1797180175781\n",
      "epoch 47, train_loss 1277.5626220703125\n",
      "epoch 47, val_loss 455.17962646484375\n",
      "epoch 48, train_loss 1277.5625\n",
      "epoch 48, val_loss 455.1795959472656\n",
      "epoch 49, train_loss 1277.562255859375\n",
      "epoch 49, val_loss 455.17950439453125\n",
      "epoch 50, train_loss 1277.5621337890625\n",
      "epoch 50, val_loss 455.17938232421875\n",
      "epoch 51, train_loss 1277.5621337890625\n",
      "epoch 51, val_loss 455.1793212890625\n",
      "epoch 52, train_loss 1277.56201171875\n",
      "epoch 52, val_loss 455.17919921875\n",
      "epoch 53, train_loss 1277.561767578125\n",
      "epoch 53, val_loss 455.17913818359375\n",
      "epoch 54, train_loss 1277.5618896484375\n",
      "epoch 54, val_loss 455.1789855957031\n",
      "epoch 55, train_loss 1277.5616455078125\n",
      "epoch 55, val_loss 455.1789855957031\n",
      "epoch 56, train_loss 1277.5614013671875\n",
      "epoch 56, val_loss 455.1788330078125\n",
      "epoch 57, train_loss 1277.5614013671875\n",
      "epoch 57, val_loss 455.1788024902344\n",
      "epoch 58, train_loss 1277.5614013671875\n",
      "epoch 58, val_loss 455.1786804199219\n",
      "epoch 59, train_loss 1277.561279296875\n",
      "epoch 59, val_loss 455.1786193847656\n",
      "epoch 60, train_loss 1277.56103515625\n",
      "epoch 60, val_loss 455.1784973144531\n",
      "epoch 61, train_loss 1277.56103515625\n",
      "epoch 61, val_loss 455.178466796875\n",
      "epoch 62, train_loss 1277.56103515625\n",
      "epoch 62, val_loss 455.1783752441406\n",
      "epoch 63, train_loss 1277.560791015625\n",
      "epoch 63, val_loss 455.1783447265625\n",
      "epoch 64, train_loss 1277.560546875\n",
      "epoch 64, val_loss 455.1781311035156\n",
      "epoch 65, train_loss 1277.560546875\n",
      "epoch 65, val_loss 455.17803955078125\n",
      "epoch 66, train_loss 1277.5604248046875\n",
      "epoch 66, val_loss 455.1780090332031\n",
      "epoch 67, train_loss 1277.560302734375\n",
      "epoch 67, val_loss 455.1779479980469\n",
      "epoch 68, train_loss 1277.5601806640625\n",
      "epoch 68, val_loss 455.1778869628906\n",
      "epoch 69, train_loss 1277.5601806640625\n",
      "epoch 69, val_loss 455.17779541015625\n",
      "epoch 70, train_loss 1277.56005859375\n",
      "epoch 70, val_loss 455.17767333984375\n",
      "epoch 71, train_loss 1277.5599365234375\n",
      "epoch 71, val_loss 455.1775817871094\n",
      "epoch 72, train_loss 1277.559814453125\n",
      "epoch 72, val_loss 455.177490234375\n",
      "epoch 73, train_loss 1277.5596923828125\n",
      "epoch 73, val_loss 455.17742919921875\n",
      "epoch 74, train_loss 1277.5594482421875\n",
      "epoch 74, val_loss 455.1773376464844\n",
      "epoch 75, train_loss 1277.5594482421875\n",
      "epoch 75, val_loss 455.1772766113281\n",
      "epoch 76, train_loss 1277.5594482421875\n",
      "epoch 76, val_loss 455.1771240234375\n",
      "epoch 77, train_loss 1277.5592041015625\n",
      "epoch 77, val_loss 455.1770935058594\n",
      "epoch 78, train_loss 1277.5589599609375\n",
      "epoch 78, val_loss 455.1769104003906\n",
      "epoch 79, train_loss 1277.5589599609375\n",
      "epoch 79, val_loss 455.1769104003906\n",
      "epoch 80, train_loss 1277.558837890625\n",
      "epoch 80, val_loss 455.17681884765625\n",
      "epoch 81, train_loss 1277.558837890625\n",
      "epoch 81, val_loss 455.1767578125\n",
      "epoch 82, train_loss 1277.5587158203125\n",
      "epoch 82, val_loss 455.1766662597656\n",
      "epoch 83, train_loss 1277.5587158203125\n",
      "epoch 83, val_loss 455.1766357421875\n",
      "epoch 84, train_loss 1277.5584716796875\n",
      "epoch 84, val_loss 455.176513671875\n",
      "epoch 85, train_loss 1277.558349609375\n",
      "epoch 85, val_loss 455.1764221191406\n",
      "epoch 86, train_loss 1277.5582275390625\n",
      "epoch 86, val_loss 455.1763000488281\n",
      "epoch 87, train_loss 1277.5582275390625\n",
      "epoch 87, val_loss 455.1762390136719\n",
      "epoch 88, train_loss 1277.557861328125\n",
      "epoch 88, val_loss 455.1761779785156\n",
      "epoch 89, train_loss 1277.557861328125\n",
      "epoch 89, val_loss 455.17608642578125\n",
      "epoch 90, train_loss 1277.557861328125\n",
      "epoch 90, val_loss 455.1759948730469\n",
      "epoch 91, train_loss 1277.5576171875\n",
      "epoch 91, val_loss 455.17584228515625\n",
      "epoch 92, train_loss 1277.5574951171875\n",
      "epoch 92, val_loss 455.17578125\n",
      "epoch 93, train_loss 1277.557373046875\n",
      "epoch 93, val_loss 455.17572021484375\n",
      "epoch 94, train_loss 1277.557373046875\n",
      "epoch 94, val_loss 455.1756591796875\n",
      "epoch 95, train_loss 1277.557373046875\n",
      "epoch 95, val_loss 455.17559814453125\n",
      "epoch 96, train_loss 1277.5572509765625\n",
      "epoch 96, val_loss 455.1755065917969\n",
      "epoch 97, train_loss 1277.55712890625\n",
      "epoch 97, val_loss 455.1753845214844\n",
      "epoch 98, train_loss 1277.556884765625\n",
      "epoch 98, val_loss 455.1752624511719\n",
      "epoch 99, train_loss 1277.556884765625\n",
      "epoch 99, val_loss 455.1752624511719\n",
      "Parameter containing:\n",
      "tensor([1.7385e-15], requires_grad=True)\n",
      "iter 75, train_loss_regularization 0.8148252367973328\n",
      "iter 75, val_loss_regularization 0.8148252367973328\n",
      "epoch 0, train_loss 1277.556640625\n",
      "epoch 0, val_loss 455.17510986328125\n",
      "epoch 1, train_loss 1277.556640625\n",
      "epoch 1, val_loss 455.1750793457031\n",
      "epoch 2, train_loss 1277.556396484375\n",
      "epoch 2, val_loss 455.1749267578125\n",
      "epoch 3, train_loss 1277.5562744140625\n",
      "epoch 3, val_loss 455.17486572265625\n",
      "epoch 4, train_loss 1277.5562744140625\n",
      "epoch 4, val_loss 455.17474365234375\n",
      "epoch 5, train_loss 1277.55615234375\n",
      "epoch 5, val_loss 455.1747131347656\n",
      "epoch 6, train_loss 1277.55615234375\n",
      "epoch 6, val_loss 455.1745910644531\n",
      "epoch 7, train_loss 1277.5560302734375\n",
      "epoch 7, val_loss 455.17449951171875\n",
      "epoch 8, train_loss 1277.555908203125\n",
      "epoch 8, val_loss 455.1744689941406\n",
      "epoch 9, train_loss 1277.5556640625\n",
      "epoch 9, val_loss 455.17437744140625\n",
      "epoch 10, train_loss 1277.5556640625\n",
      "epoch 10, val_loss 455.1743469238281\n",
      "epoch 11, train_loss 1277.5556640625\n",
      "epoch 11, val_loss 455.1741943359375\n",
      "epoch 12, train_loss 1277.5552978515625\n",
      "epoch 12, val_loss 455.174072265625\n",
      "epoch 13, train_loss 1277.5552978515625\n",
      "epoch 13, val_loss 455.17401123046875\n",
      "epoch 14, train_loss 1277.5552978515625\n",
      "epoch 14, val_loss 455.1739501953125\n",
      "epoch 15, train_loss 1277.5550537109375\n",
      "epoch 15, val_loss 455.173828125\n",
      "epoch 16, train_loss 1277.5548095703125\n",
      "epoch 16, val_loss 455.1737365722656\n",
      "epoch 17, train_loss 1277.554931640625\n",
      "epoch 17, val_loss 455.1737060546875\n",
      "epoch 18, train_loss 1277.5548095703125\n",
      "epoch 18, val_loss 455.173583984375\n",
      "epoch 19, train_loss 1277.5548095703125\n",
      "epoch 19, val_loss 455.1734924316406\n",
      "epoch 20, train_loss 1277.554443359375\n",
      "epoch 20, val_loss 455.1734313964844\n",
      "epoch 21, train_loss 1277.554443359375\n",
      "epoch 21, val_loss 455.17333984375\n",
      "epoch 22, train_loss 1277.554443359375\n",
      "epoch 22, val_loss 455.17327880859375\n",
      "epoch 23, train_loss 1277.55419921875\n",
      "epoch 23, val_loss 455.1732177734375\n",
      "epoch 24, train_loss 1277.5540771484375\n",
      "epoch 24, val_loss 455.173095703125\n",
      "epoch 25, train_loss 1277.553955078125\n",
      "epoch 25, val_loss 455.1730041503906\n",
      "epoch 26, train_loss 1277.5538330078125\n",
      "epoch 26, val_loss 455.17291259765625\n",
      "epoch 27, train_loss 1277.5537109375\n",
      "epoch 27, val_loss 455.1728210449219\n",
      "epoch 28, train_loss 1277.5537109375\n",
      "epoch 28, val_loss 455.1727600097656\n",
      "epoch 29, train_loss 1277.553466796875\n",
      "epoch 29, val_loss 455.1726379394531\n",
      "epoch 30, train_loss 1277.553466796875\n",
      "epoch 30, val_loss 455.1725769042969\n",
      "epoch 31, train_loss 1277.553466796875\n",
      "epoch 31, val_loss 455.1724853515625\n",
      "epoch 32, train_loss 1277.55322265625\n",
      "epoch 32, val_loss 455.1723327636719\n",
      "epoch 33, train_loss 1277.55322265625\n",
      "epoch 33, val_loss 455.1723327636719\n",
      "epoch 34, train_loss 1277.552978515625\n",
      "epoch 34, val_loss 455.1722106933594\n",
      "epoch 35, train_loss 1277.552978515625\n",
      "epoch 35, val_loss 455.172119140625\n",
      "epoch 36, train_loss 1277.552978515625\n",
      "epoch 36, val_loss 455.1720886230469\n",
      "epoch 37, train_loss 1277.552734375\n",
      "epoch 37, val_loss 455.1719970703125\n",
      "epoch 38, train_loss 1277.552490234375\n",
      "epoch 38, val_loss 455.1719665527344\n",
      "epoch 39, train_loss 1277.552490234375\n",
      "epoch 39, val_loss 455.1717834472656\n",
      "epoch 40, train_loss 1277.5523681640625\n",
      "epoch 40, val_loss 455.1717224121094\n",
      "epoch 41, train_loss 1277.55224609375\n",
      "epoch 41, val_loss 455.171630859375\n",
      "epoch 42, train_loss 1277.55224609375\n",
      "epoch 42, val_loss 455.17156982421875\n",
      "epoch 43, train_loss 1277.5521240234375\n",
      "epoch 43, val_loss 455.17144775390625\n",
      "epoch 44, train_loss 1277.552001953125\n",
      "epoch 44, val_loss 455.1714172363281\n",
      "epoch 45, train_loss 1277.5517578125\n",
      "epoch 45, val_loss 455.1712951660156\n",
      "epoch 46, train_loss 1277.5517578125\n",
      "epoch 46, val_loss 455.17120361328125\n",
      "epoch 47, train_loss 1277.5516357421875\n",
      "epoch 47, val_loss 455.1711120605469\n",
      "epoch 48, train_loss 1277.5516357421875\n",
      "epoch 48, val_loss 455.1710510253906\n",
      "epoch 49, train_loss 1277.5513916015625\n",
      "epoch 49, val_loss 455.1710510253906\n",
      "epoch 50, train_loss 1277.5513916015625\n",
      "epoch 50, val_loss 455.1708679199219\n",
      "epoch 51, train_loss 1277.5513916015625\n",
      "epoch 51, val_loss 455.17083740234375\n",
      "epoch 52, train_loss 1277.551025390625\n",
      "epoch 52, val_loss 455.17071533203125\n",
      "epoch 53, train_loss 1277.551025390625\n",
      "epoch 53, val_loss 455.1706237792969\n",
      "epoch 54, train_loss 1277.551025390625\n",
      "epoch 54, val_loss 455.1705017089844\n",
      "epoch 55, train_loss 1277.55078125\n",
      "epoch 55, val_loss 455.1705017089844\n",
      "epoch 56, train_loss 1277.550537109375\n",
      "epoch 56, val_loss 455.1703796386719\n",
      "epoch 57, train_loss 1277.550537109375\n",
      "epoch 57, val_loss 455.1702880859375\n",
      "epoch 58, train_loss 1277.550537109375\n",
      "epoch 58, val_loss 455.1702575683594\n",
      "epoch 59, train_loss 1277.5504150390625\n",
      "epoch 59, val_loss 455.1701354980469\n",
      "epoch 60, train_loss 1277.5501708984375\n",
      "epoch 60, val_loss 455.1700744628906\n",
      "epoch 61, train_loss 1277.5501708984375\n",
      "epoch 61, val_loss 455.169921875\n",
      "epoch 62, train_loss 1277.550048828125\n",
      "epoch 62, val_loss 455.16986083984375\n",
      "epoch 63, train_loss 1277.5498046875\n",
      "epoch 63, val_loss 455.1697998046875\n",
      "epoch 64, train_loss 1277.5498046875\n",
      "epoch 64, val_loss 455.16973876953125\n",
      "epoch 65, train_loss 1277.5496826171875\n",
      "epoch 65, val_loss 455.169677734375\n",
      "epoch 66, train_loss 1277.5496826171875\n",
      "epoch 66, val_loss 455.16949462890625\n",
      "epoch 67, train_loss 1277.549560546875\n",
      "epoch 67, val_loss 455.16937255859375\n",
      "epoch 68, train_loss 1277.549560546875\n",
      "epoch 68, val_loss 455.16937255859375\n",
      "epoch 69, train_loss 1277.54931640625\n",
      "epoch 69, val_loss 455.16925048828125\n",
      "epoch 70, train_loss 1277.5491943359375\n",
      "epoch 70, val_loss 455.1692199707031\n",
      "epoch 71, train_loss 1277.549072265625\n",
      "epoch 71, val_loss 455.1691589355469\n",
      "epoch 72, train_loss 1277.549072265625\n",
      "epoch 72, val_loss 455.1690368652344\n",
      "epoch 73, train_loss 1277.5487060546875\n",
      "epoch 73, val_loss 455.1689147949219\n",
      "epoch 74, train_loss 1277.548828125\n",
      "epoch 74, val_loss 455.1688232421875\n",
      "epoch 75, train_loss 1277.548583984375\n",
      "epoch 75, val_loss 455.1687927246094\n",
      "epoch 76, train_loss 1277.5484619140625\n",
      "epoch 76, val_loss 455.168701171875\n",
      "epoch 77, train_loss 1277.54833984375\n",
      "epoch 77, val_loss 455.16864013671875\n",
      "epoch 78, train_loss 1277.548583984375\n",
      "epoch 78, val_loss 455.1684875488281\n",
      "epoch 79, train_loss 1277.5482177734375\n",
      "epoch 79, val_loss 455.16845703125\n",
      "epoch 80, train_loss 1277.548095703125\n",
      "epoch 80, val_loss 455.1683349609375\n",
      "epoch 81, train_loss 1277.5479736328125\n",
      "epoch 81, val_loss 455.1683044433594\n",
      "epoch 82, train_loss 1277.5478515625\n",
      "epoch 82, val_loss 455.1681213378906\n",
      "epoch 83, train_loss 1277.5478515625\n",
      "epoch 83, val_loss 455.1680908203125\n",
      "epoch 84, train_loss 1277.547607421875\n",
      "epoch 84, val_loss 455.16802978515625\n",
      "epoch 85, train_loss 1277.5474853515625\n",
      "epoch 85, val_loss 455.16790771484375\n",
      "epoch 86, train_loss 1277.5474853515625\n",
      "epoch 86, val_loss 455.1678771972656\n",
      "epoch 87, train_loss 1277.54736328125\n",
      "epoch 87, val_loss 455.1677551269531\n",
      "epoch 88, train_loss 1277.54736328125\n",
      "epoch 88, val_loss 455.16766357421875\n",
      "epoch 89, train_loss 1277.54736328125\n",
      "epoch 89, val_loss 455.16754150390625\n",
      "epoch 90, train_loss 1277.547119140625\n",
      "epoch 90, val_loss 455.1675109863281\n",
      "epoch 91, train_loss 1277.5469970703125\n",
      "epoch 91, val_loss 455.1674499511719\n",
      "epoch 92, train_loss 1277.5469970703125\n",
      "epoch 92, val_loss 455.16741943359375\n",
      "epoch 93, train_loss 1277.5467529296875\n",
      "epoch 93, val_loss 455.167236328125\n",
      "epoch 94, train_loss 1277.546630859375\n",
      "epoch 94, val_loss 455.1671142578125\n",
      "epoch 95, train_loss 1277.546630859375\n",
      "epoch 95, val_loss 455.1671142578125\n",
      "epoch 96, train_loss 1277.5465087890625\n",
      "epoch 96, val_loss 455.1669921875\n",
      "epoch 97, train_loss 1277.54638671875\n",
      "epoch 97, val_loss 455.1669616699219\n",
      "epoch 98, train_loss 1277.546142578125\n",
      "epoch 98, val_loss 455.1668395996094\n",
      "epoch 99, train_loss 1277.546142578125\n",
      "epoch 99, val_loss 455.166748046875\n",
      "Parameter containing:\n",
      "tensor([1.1678e-15], requires_grad=True)\n",
      "iter 76, train_loss_regularization 0.8118574023246765\n",
      "iter 76, val_loss_regularization 0.8118574023246765\n",
      "epoch 0, train_loss 1277.546142578125\n",
      "epoch 0, val_loss 455.1666564941406\n",
      "epoch 1, train_loss 1277.5460205078125\n",
      "epoch 1, val_loss 455.1666259765625\n",
      "epoch 2, train_loss 1277.5458984375\n",
      "epoch 2, val_loss 455.16650390625\n",
      "epoch 3, train_loss 1277.545654296875\n",
      "epoch 3, val_loss 455.1664733886719\n",
      "epoch 4, train_loss 1277.5455322265625\n",
      "epoch 4, val_loss 455.16632080078125\n",
      "epoch 5, train_loss 1277.5455322265625\n",
      "epoch 5, val_loss 455.166259765625\n",
      "epoch 6, train_loss 1277.5452880859375\n",
      "epoch 6, val_loss 455.16619873046875\n",
      "epoch 7, train_loss 1277.5452880859375\n",
      "epoch 7, val_loss 455.16607666015625\n",
      "epoch 8, train_loss 1277.545166015625\n",
      "epoch 8, val_loss 455.1660461425781\n",
      "epoch 9, train_loss 1277.5450439453125\n",
      "epoch 9, val_loss 455.1659240722656\n",
      "epoch 10, train_loss 1277.544921875\n",
      "epoch 10, val_loss 455.16583251953125\n",
      "epoch 11, train_loss 1277.544921875\n",
      "epoch 11, val_loss 455.16571044921875\n",
      "epoch 12, train_loss 1277.5447998046875\n",
      "epoch 12, val_loss 455.1656799316406\n",
      "epoch 13, train_loss 1277.544677734375\n",
      "epoch 13, val_loss 455.16558837890625\n",
      "epoch 14, train_loss 1277.5445556640625\n",
      "epoch 14, val_loss 455.16558837890625\n",
      "epoch 15, train_loss 1277.5445556640625\n",
      "epoch 15, val_loss 455.16534423828125\n",
      "epoch 16, train_loss 1277.54443359375\n",
      "epoch 16, val_loss 455.16534423828125\n",
      "epoch 17, train_loss 1277.544189453125\n",
      "epoch 17, val_loss 455.1652526855469\n",
      "epoch 18, train_loss 1277.544189453125\n",
      "epoch 18, val_loss 455.16522216796875\n",
      "epoch 19, train_loss 1277.5440673828125\n",
      "epoch 19, val_loss 455.1651306152344\n",
      "epoch 20, train_loss 1277.5438232421875\n",
      "epoch 20, val_loss 455.1650085449219\n",
      "epoch 21, train_loss 1277.5438232421875\n",
      "epoch 21, val_loss 455.1649169921875\n",
      "epoch 22, train_loss 1277.543701171875\n",
      "epoch 22, val_loss 455.1648254394531\n",
      "epoch 23, train_loss 1277.54345703125\n",
      "epoch 23, val_loss 455.1647644042969\n",
      "epoch 24, train_loss 1277.54345703125\n",
      "epoch 24, val_loss 455.1646728515625\n",
      "epoch 25, train_loss 1277.5435791015625\n",
      "epoch 25, val_loss 455.16461181640625\n",
      "epoch 26, train_loss 1277.543212890625\n",
      "epoch 26, val_loss 455.1644592285156\n",
      "epoch 27, train_loss 1277.5430908203125\n",
      "epoch 27, val_loss 455.1644287109375\n",
      "epoch 28, train_loss 1277.5430908203125\n",
      "epoch 28, val_loss 455.16436767578125\n",
      "epoch 29, train_loss 1277.54296875\n",
      "epoch 29, val_loss 455.16424560546875\n",
      "epoch 30, train_loss 1277.54296875\n",
      "epoch 30, val_loss 455.1642150878906\n",
      "epoch 31, train_loss 1277.5426025390625\n",
      "epoch 31, val_loss 455.1640319824219\n",
      "epoch 32, train_loss 1277.5426025390625\n",
      "epoch 32, val_loss 455.16400146484375\n",
      "epoch 33, train_loss 1277.5426025390625\n",
      "epoch 33, val_loss 455.1639099121094\n",
      "epoch 34, train_loss 1277.5423583984375\n",
      "epoch 34, val_loss 455.16387939453125\n",
      "epoch 35, train_loss 1277.542236328125\n",
      "epoch 35, val_loss 455.16375732421875\n",
      "epoch 36, train_loss 1277.5423583984375\n",
      "epoch 36, val_loss 455.1636657714844\n",
      "epoch 37, train_loss 1277.5419921875\n",
      "epoch 37, val_loss 455.16351318359375\n",
      "epoch 38, train_loss 1277.5419921875\n",
      "epoch 38, val_loss 455.16351318359375\n",
      "epoch 39, train_loss 1277.5418701171875\n",
      "epoch 39, val_loss 455.1634216308594\n",
      "epoch 40, train_loss 1277.5418701171875\n",
      "epoch 40, val_loss 455.163330078125\n",
      "epoch 41, train_loss 1277.541748046875\n",
      "epoch 41, val_loss 455.1632995605469\n",
      "epoch 42, train_loss 1277.5416259765625\n",
      "epoch 42, val_loss 455.1631774902344\n",
      "epoch 43, train_loss 1277.5413818359375\n",
      "epoch 43, val_loss 455.1630859375\n",
      "epoch 44, train_loss 1277.5413818359375\n",
      "epoch 44, val_loss 455.1629943847656\n",
      "epoch 45, train_loss 1277.5413818359375\n",
      "epoch 45, val_loss 455.16290283203125\n",
      "epoch 46, train_loss 1277.5411376953125\n",
      "epoch 46, val_loss 455.162841796875\n",
      "epoch 47, train_loss 1277.541015625\n",
      "epoch 47, val_loss 455.16278076171875\n",
      "epoch 48, train_loss 1277.5411376953125\n",
      "epoch 48, val_loss 455.16265869140625\n",
      "epoch 49, train_loss 1277.5408935546875\n",
      "epoch 49, val_loss 455.16253662109375\n",
      "epoch 50, train_loss 1277.5406494140625\n",
      "epoch 50, val_loss 455.1624450683594\n",
      "epoch 51, train_loss 1277.54052734375\n",
      "epoch 51, val_loss 455.1623840332031\n",
      "epoch 52, train_loss 1277.54052734375\n",
      "epoch 52, val_loss 455.1623229980469\n",
      "epoch 53, train_loss 1277.54052734375\n",
      "epoch 53, val_loss 455.16217041015625\n",
      "epoch 54, train_loss 1277.540283203125\n",
      "epoch 54, val_loss 455.16217041015625\n",
      "epoch 55, train_loss 1277.540283203125\n",
      "epoch 55, val_loss 455.16204833984375\n",
      "epoch 56, train_loss 1277.5400390625\n",
      "epoch 56, val_loss 455.1619567871094\n",
      "epoch 57, train_loss 1277.539794921875\n",
      "epoch 57, val_loss 455.16192626953125\n",
      "epoch 58, train_loss 1277.539794921875\n",
      "epoch 58, val_loss 455.16180419921875\n",
      "epoch 59, train_loss 1277.5396728515625\n",
      "epoch 59, val_loss 455.1617431640625\n",
      "epoch 60, train_loss 1277.5396728515625\n",
      "epoch 60, val_loss 455.1616516113281\n",
      "epoch 61, train_loss 1277.53955078125\n",
      "epoch 61, val_loss 455.16162109375\n",
      "epoch 62, train_loss 1277.53955078125\n",
      "epoch 62, val_loss 455.1614990234375\n",
      "epoch 63, train_loss 1277.539306640625\n",
      "epoch 63, val_loss 455.1614074707031\n",
      "epoch 64, train_loss 1277.539306640625\n",
      "epoch 64, val_loss 455.1612854003906\n",
      "epoch 65, train_loss 1277.5391845703125\n",
      "epoch 65, val_loss 455.1612548828125\n",
      "epoch 66, train_loss 1277.5390625\n",
      "epoch 66, val_loss 455.1611328125\n",
      "epoch 67, train_loss 1277.5389404296875\n",
      "epoch 67, val_loss 455.1610412597656\n",
      "epoch 68, train_loss 1277.5386962890625\n",
      "epoch 68, val_loss 455.1610107421875\n",
      "epoch 69, train_loss 1277.5386962890625\n",
      "epoch 69, val_loss 455.1609191894531\n",
      "epoch 70, train_loss 1277.5386962890625\n",
      "epoch 70, val_loss 455.16082763671875\n",
      "epoch 71, train_loss 1277.5384521484375\n",
      "epoch 71, val_loss 455.1607360839844\n",
      "epoch 72, train_loss 1277.538330078125\n",
      "epoch 72, val_loss 455.16070556640625\n",
      "epoch 73, train_loss 1277.538330078125\n",
      "epoch 73, val_loss 455.16058349609375\n",
      "epoch 74, train_loss 1277.5382080078125\n",
      "epoch 74, val_loss 455.1605529785156\n",
      "epoch 75, train_loss 1277.5379638671875\n",
      "epoch 75, val_loss 455.1603698730469\n",
      "epoch 76, train_loss 1277.5379638671875\n",
      "epoch 76, val_loss 455.16033935546875\n",
      "epoch 77, train_loss 1277.5379638671875\n",
      "epoch 77, val_loss 455.16021728515625\n",
      "epoch 78, train_loss 1277.537841796875\n",
      "epoch 78, val_loss 455.16021728515625\n",
      "epoch 79, train_loss 1277.5377197265625\n",
      "epoch 79, val_loss 455.16009521484375\n",
      "epoch 80, train_loss 1277.5374755859375\n",
      "epoch 80, val_loss 455.15997314453125\n",
      "epoch 81, train_loss 1277.5374755859375\n",
      "epoch 81, val_loss 455.159912109375\n",
      "epoch 82, train_loss 1277.537353515625\n",
      "epoch 82, val_loss 455.1598205566406\n",
      "epoch 83, train_loss 1277.5372314453125\n",
      "epoch 83, val_loss 455.1597595214844\n",
      "epoch 84, train_loss 1277.537109375\n",
      "epoch 84, val_loss 455.15966796875\n",
      "epoch 85, train_loss 1277.537109375\n",
      "epoch 85, val_loss 455.1596374511719\n",
      "epoch 86, train_loss 1277.536865234375\n",
      "epoch 86, val_loss 455.15948486328125\n",
      "epoch 87, train_loss 1277.5369873046875\n",
      "epoch 87, val_loss 455.159423828125\n",
      "epoch 88, train_loss 1277.53662109375\n",
      "epoch 88, val_loss 455.1593017578125\n",
      "epoch 89, train_loss 1277.53662109375\n",
      "epoch 89, val_loss 455.1593017578125\n",
      "epoch 90, train_loss 1277.5364990234375\n",
      "epoch 90, val_loss 455.1592102050781\n",
      "epoch 91, train_loss 1277.536376953125\n",
      "epoch 91, val_loss 455.1590270996094\n",
      "epoch 92, train_loss 1277.5362548828125\n",
      "epoch 92, val_loss 455.15899658203125\n",
      "epoch 93, train_loss 1277.5361328125\n",
      "epoch 93, val_loss 455.1589050292969\n",
      "epoch 94, train_loss 1277.5361328125\n",
      "epoch 94, val_loss 455.1588439941406\n",
      "epoch 95, train_loss 1277.5360107421875\n",
      "epoch 95, val_loss 455.15875244140625\n",
      "epoch 96, train_loss 1277.535888671875\n",
      "epoch 96, val_loss 455.1586608886719\n",
      "epoch 97, train_loss 1277.5357666015625\n",
      "epoch 97, val_loss 455.1585388183594\n",
      "epoch 98, train_loss 1277.5357666015625\n",
      "epoch 98, val_loss 455.158447265625\n",
      "epoch 99, train_loss 1277.5355224609375\n",
      "epoch 99, val_loss 455.1584167480469\n",
      "Parameter containing:\n",
      "tensor([7.8514e-16], requires_grad=True)\n",
      "iter 77, train_loss_regularization 0.8089421987533569\n",
      "iter 77, val_loss_regularization 0.8089421987533569\n",
      "epoch 0, train_loss 1277.5352783203125\n",
      "epoch 0, val_loss 455.15838623046875\n",
      "epoch 1, train_loss 1277.5352783203125\n",
      "epoch 1, val_loss 455.1582946777344\n",
      "epoch 2, train_loss 1277.5352783203125\n",
      "epoch 2, val_loss 455.1581726074219\n",
      "epoch 3, train_loss 1277.53515625\n",
      "epoch 3, val_loss 455.1581115722656\n",
      "epoch 4, train_loss 1277.534912109375\n",
      "epoch 4, val_loss 455.1579895019531\n",
      "epoch 5, train_loss 1277.5347900390625\n",
      "epoch 5, val_loss 455.1579284667969\n",
      "epoch 6, train_loss 1277.5347900390625\n",
      "epoch 6, val_loss 455.1578369140625\n",
      "epoch 7, train_loss 1277.53466796875\n",
      "epoch 7, val_loss 455.1577453613281\n",
      "epoch 8, train_loss 1277.53466796875\n",
      "epoch 8, val_loss 455.1576232910156\n",
      "epoch 9, train_loss 1277.5345458984375\n",
      "epoch 9, val_loss 455.1576232910156\n",
      "epoch 10, train_loss 1277.5343017578125\n",
      "epoch 10, val_loss 455.1575012207031\n",
      "epoch 11, train_loss 1277.5343017578125\n",
      "epoch 11, val_loss 455.15740966796875\n",
      "epoch 12, train_loss 1277.5343017578125\n",
      "epoch 12, val_loss 455.1573791503906\n",
      "epoch 13, train_loss 1277.5340576171875\n",
      "epoch 13, val_loss 455.15728759765625\n",
      "epoch 14, train_loss 1277.533935546875\n",
      "epoch 14, val_loss 455.15716552734375\n",
      "epoch 15, train_loss 1277.533935546875\n",
      "epoch 15, val_loss 455.1571350097656\n",
      "epoch 16, train_loss 1277.53369140625\n",
      "epoch 16, val_loss 455.15704345703125\n",
      "epoch 17, train_loss 1277.53369140625\n",
      "epoch 17, val_loss 455.1569519042969\n",
      "epoch 18, train_loss 1277.533447265625\n",
      "epoch 18, val_loss 455.1568603515625\n",
      "epoch 19, train_loss 1277.533447265625\n",
      "epoch 19, val_loss 455.15673828125\n",
      "epoch 20, train_loss 1277.533447265625\n",
      "epoch 20, val_loss 455.15667724609375\n",
      "epoch 21, train_loss 1277.533203125\n",
      "epoch 21, val_loss 455.1565856933594\n",
      "epoch 22, train_loss 1277.5330810546875\n",
      "epoch 22, val_loss 455.15655517578125\n",
      "epoch 23, train_loss 1277.5330810546875\n",
      "epoch 23, val_loss 455.1564636230469\n",
      "epoch 24, train_loss 1277.532958984375\n",
      "epoch 24, val_loss 455.1563720703125\n",
      "epoch 25, train_loss 1277.5328369140625\n",
      "epoch 25, val_loss 455.1563415527344\n",
      "epoch 26, train_loss 1277.53271484375\n",
      "epoch 26, val_loss 455.1561584472656\n",
      "epoch 27, train_loss 1277.5325927734375\n",
      "epoch 27, val_loss 455.1561279296875\n",
      "epoch 28, train_loss 1277.5325927734375\n",
      "epoch 28, val_loss 455.156005859375\n",
      "epoch 29, train_loss 1277.532470703125\n",
      "epoch 29, val_loss 455.1559143066406\n",
      "epoch 30, train_loss 1277.5322265625\n",
      "epoch 30, val_loss 455.1558837890625\n",
      "epoch 31, train_loss 1277.5322265625\n",
      "epoch 31, val_loss 455.1557922363281\n",
      "epoch 32, train_loss 1277.5323486328125\n",
      "epoch 32, val_loss 455.15570068359375\n",
      "epoch 33, train_loss 1277.5318603515625\n",
      "epoch 33, val_loss 455.15557861328125\n",
      "epoch 34, train_loss 1277.5318603515625\n",
      "epoch 34, val_loss 455.1554870605469\n",
      "epoch 35, train_loss 1277.53173828125\n",
      "epoch 35, val_loss 455.1554260253906\n",
      "epoch 36, train_loss 1277.5318603515625\n",
      "epoch 36, val_loss 455.15533447265625\n",
      "epoch 37, train_loss 1277.5316162109375\n",
      "epoch 37, val_loss 455.1553039550781\n",
      "epoch 38, train_loss 1277.5313720703125\n",
      "epoch 38, val_loss 455.1552429199219\n",
      "epoch 39, train_loss 1277.5313720703125\n",
      "epoch 39, val_loss 455.15509033203125\n",
      "epoch 40, train_loss 1277.5311279296875\n",
      "epoch 40, val_loss 455.15496826171875\n",
      "epoch 41, train_loss 1277.5311279296875\n",
      "epoch 41, val_loss 455.15496826171875\n",
      "epoch 42, train_loss 1277.5311279296875\n",
      "epoch 42, val_loss 455.15484619140625\n",
      "epoch 43, train_loss 1277.5311279296875\n",
      "epoch 43, val_loss 455.15478515625\n",
      "epoch 44, train_loss 1277.531005859375\n",
      "epoch 44, val_loss 455.1546936035156\n",
      "epoch 45, train_loss 1277.5308837890625\n",
      "epoch 45, val_loss 455.1546325683594\n",
      "epoch 46, train_loss 1277.5306396484375\n",
      "epoch 46, val_loss 455.154541015625\n",
      "epoch 47, train_loss 1277.5306396484375\n",
      "epoch 47, val_loss 455.1544189453125\n",
      "epoch 48, train_loss 1277.5303955078125\n",
      "epoch 48, val_loss 455.1543273925781\n",
      "epoch 49, train_loss 1277.5302734375\n",
      "epoch 49, val_loss 455.154296875\n",
      "epoch 50, train_loss 1277.5302734375\n",
      "epoch 50, val_loss 455.1541748046875\n",
      "epoch 51, train_loss 1277.5302734375\n",
      "epoch 51, val_loss 455.15411376953125\n",
      "epoch 52, train_loss 1277.530029296875\n",
      "epoch 52, val_loss 455.15399169921875\n",
      "epoch 53, train_loss 1277.5299072265625\n",
      "epoch 53, val_loss 455.1539306640625\n",
      "epoch 54, train_loss 1277.5299072265625\n",
      "epoch 54, val_loss 455.15386962890625\n",
      "epoch 55, train_loss 1277.52978515625\n",
      "epoch 55, val_loss 455.1537780761719\n",
      "epoch 56, train_loss 1277.5296630859375\n",
      "epoch 56, val_loss 455.1537170410156\n",
      "epoch 57, train_loss 1277.5296630859375\n",
      "epoch 57, val_loss 455.1535949707031\n",
      "epoch 58, train_loss 1277.529541015625\n",
      "epoch 58, val_loss 455.1535339355469\n",
      "epoch 59, train_loss 1277.529296875\n",
      "epoch 59, val_loss 455.1534729003906\n",
      "epoch 60, train_loss 1277.529296875\n",
      "epoch 60, val_loss 455.1534118652344\n",
      "epoch 61, train_loss 1277.529296875\n",
      "epoch 61, val_loss 455.15325927734375\n",
      "epoch 62, train_loss 1277.529052734375\n",
      "epoch 62, val_loss 455.15325927734375\n",
      "epoch 63, train_loss 1277.5289306640625\n",
      "epoch 63, val_loss 455.15313720703125\n",
      "epoch 64, train_loss 1277.52880859375\n",
      "epoch 64, val_loss 455.1530456542969\n",
      "epoch 65, train_loss 1277.52880859375\n",
      "epoch 65, val_loss 455.15301513671875\n",
      "epoch 66, train_loss 1277.5284423828125\n",
      "epoch 66, val_loss 455.1529235839844\n",
      "epoch 67, train_loss 1277.5284423828125\n",
      "epoch 67, val_loss 455.15283203125\n",
      "epoch 68, train_loss 1277.5284423828125\n",
      "epoch 68, val_loss 455.1526794433594\n",
      "epoch 69, train_loss 1277.5284423828125\n",
      "epoch 69, val_loss 455.1526794433594\n",
      "epoch 70, train_loss 1277.5281982421875\n",
      "epoch 70, val_loss 455.15252685546875\n",
      "epoch 71, train_loss 1277.528076171875\n",
      "epoch 71, val_loss 455.1524963378906\n",
      "epoch 72, train_loss 1277.5279541015625\n",
      "epoch 72, val_loss 455.15240478515625\n",
      "epoch 73, train_loss 1277.5279541015625\n",
      "epoch 73, val_loss 455.15234375\n",
      "epoch 74, train_loss 1277.52783203125\n",
      "epoch 74, val_loss 455.1522216796875\n",
      "epoch 75, train_loss 1277.527587890625\n",
      "epoch 75, val_loss 455.1520690917969\n",
      "epoch 76, train_loss 1277.5274658203125\n",
      "epoch 76, val_loss 455.1520690917969\n",
      "epoch 77, train_loss 1277.5274658203125\n",
      "epoch 77, val_loss 455.1519470214844\n",
      "epoch 78, train_loss 1277.5274658203125\n",
      "epoch 78, val_loss 455.1518859863281\n",
      "epoch 79, train_loss 1277.5272216796875\n",
      "epoch 79, val_loss 455.1517639160156\n",
      "epoch 80, train_loss 1277.5272216796875\n",
      "epoch 80, val_loss 455.1517639160156\n",
      "epoch 81, train_loss 1277.5269775390625\n",
      "epoch 81, val_loss 455.1517028808594\n",
      "epoch 82, train_loss 1277.5269775390625\n",
      "epoch 82, val_loss 455.1515808105469\n",
      "epoch 83, train_loss 1277.52685546875\n",
      "epoch 83, val_loss 455.1514892578125\n",
      "epoch 84, train_loss 1277.5267333984375\n",
      "epoch 84, val_loss 455.1513671875\n",
      "epoch 85, train_loss 1277.5267333984375\n",
      "epoch 85, val_loss 455.1513366699219\n",
      "epoch 86, train_loss 1277.526611328125\n",
      "epoch 86, val_loss 455.1512451171875\n",
      "epoch 87, train_loss 1277.5263671875\n",
      "epoch 87, val_loss 455.1511535644531\n",
      "epoch 88, train_loss 1277.5263671875\n",
      "epoch 88, val_loss 455.1510925292969\n",
      "epoch 89, train_loss 1277.526123046875\n",
      "epoch 89, val_loss 455.1510009765625\n",
      "epoch 90, train_loss 1277.526123046875\n",
      "epoch 90, val_loss 455.15087890625\n",
      "epoch 91, train_loss 1277.526123046875\n",
      "epoch 91, val_loss 455.1507873535156\n",
      "epoch 92, train_loss 1277.52587890625\n",
      "epoch 92, val_loss 455.1507568359375\n",
      "epoch 93, train_loss 1277.52587890625\n",
      "epoch 93, val_loss 455.1506652832031\n",
      "epoch 94, train_loss 1277.5257568359375\n",
      "epoch 94, val_loss 455.1506652832031\n",
      "epoch 95, train_loss 1277.525634765625\n",
      "epoch 95, val_loss 455.1505432128906\n",
      "epoch 96, train_loss 1277.525390625\n",
      "epoch 96, val_loss 455.1504211425781\n",
      "epoch 97, train_loss 1277.5255126953125\n",
      "epoch 97, val_loss 455.1502990722656\n",
      "epoch 98, train_loss 1277.525390625\n",
      "epoch 98, val_loss 455.1502380371094\n",
      "epoch 99, train_loss 1277.5252685546875\n",
      "epoch 99, val_loss 455.1501770019531\n",
      "Parameter containing:\n",
      "tensor([5.2836e-16], requires_grad=True)\n",
      "iter 78, train_loss_regularization 0.8060782551765442\n",
      "iter 78, val_loss_regularization 0.8060782551765442\n",
      "epoch 0, train_loss 1277.5250244140625\n",
      "epoch 0, val_loss 455.1501159667969\n",
      "epoch 1, train_loss 1277.5250244140625\n",
      "epoch 1, val_loss 455.1499938964844\n",
      "epoch 2, train_loss 1277.52490234375\n",
      "epoch 2, val_loss 455.14990234375\n",
      "epoch 3, train_loss 1277.524658203125\n",
      "epoch 3, val_loss 455.1498718261719\n",
      "epoch 4, train_loss 1277.5247802734375\n",
      "epoch 4, val_loss 455.1497497558594\n",
      "epoch 5, train_loss 1277.524658203125\n",
      "epoch 5, val_loss 455.14971923828125\n",
      "epoch 6, train_loss 1277.5242919921875\n",
      "epoch 6, val_loss 455.1495361328125\n",
      "epoch 7, train_loss 1277.5242919921875\n",
      "epoch 7, val_loss 455.1495361328125\n",
      "epoch 8, train_loss 1277.5242919921875\n",
      "epoch 8, val_loss 455.1494445800781\n",
      "epoch 9, train_loss 1277.524169921875\n",
      "epoch 9, val_loss 455.1493835449219\n",
      "epoch 10, train_loss 1277.5240478515625\n",
      "epoch 10, val_loss 455.1492919921875\n",
      "epoch 11, train_loss 1277.5240478515625\n",
      "epoch 11, val_loss 455.1492004394531\n",
      "epoch 12, train_loss 1277.5238037109375\n",
      "epoch 12, val_loss 455.1490783691406\n",
      "epoch 13, train_loss 1277.523681640625\n",
      "epoch 13, val_loss 455.14898681640625\n",
      "epoch 14, train_loss 1277.5235595703125\n",
      "epoch 14, val_loss 455.1489562988281\n",
      "epoch 15, train_loss 1277.5235595703125\n",
      "epoch 15, val_loss 455.1488342285156\n",
      "epoch 16, train_loss 1277.5235595703125\n",
      "epoch 16, val_loss 455.1488342285156\n",
      "epoch 17, train_loss 1277.5233154296875\n",
      "epoch 17, val_loss 455.148681640625\n",
      "epoch 18, train_loss 1277.523193359375\n",
      "epoch 18, val_loss 455.14862060546875\n",
      "epoch 19, train_loss 1277.523193359375\n",
      "epoch 19, val_loss 455.14849853515625\n",
      "epoch 20, train_loss 1277.5230712890625\n",
      "epoch 20, val_loss 455.1484680175781\n",
      "epoch 21, train_loss 1277.5228271484375\n",
      "epoch 21, val_loss 455.14837646484375\n",
      "epoch 22, train_loss 1277.5228271484375\n",
      "epoch 22, val_loss 455.14825439453125\n",
      "epoch 23, train_loss 1277.522705078125\n",
      "epoch 23, val_loss 455.148193359375\n",
      "epoch 24, train_loss 1277.522705078125\n",
      "epoch 24, val_loss 455.1480712890625\n",
      "epoch 25, train_loss 1277.5224609375\n",
      "epoch 25, val_loss 455.1480407714844\n",
      "epoch 26, train_loss 1277.5223388671875\n",
      "epoch 26, val_loss 455.1479187011719\n",
      "epoch 27, train_loss 1277.5224609375\n",
      "epoch 27, val_loss 455.1479187011719\n",
      "epoch 28, train_loss 1277.5224609375\n",
      "epoch 28, val_loss 455.1477966308594\n",
      "epoch 29, train_loss 1277.5220947265625\n",
      "epoch 29, val_loss 455.1476745605469\n",
      "epoch 30, train_loss 1277.52197265625\n",
      "epoch 30, val_loss 455.1476745605469\n",
      "epoch 31, train_loss 1277.52197265625\n",
      "epoch 31, val_loss 455.1475830078125\n",
      "epoch 32, train_loss 1277.5218505859375\n",
      "epoch 32, val_loss 455.1474914550781\n",
      "epoch 33, train_loss 1277.5216064453125\n",
      "epoch 33, val_loss 455.1474304199219\n",
      "epoch 34, train_loss 1277.5216064453125\n",
      "epoch 34, val_loss 455.14727783203125\n",
      "epoch 35, train_loss 1277.5216064453125\n",
      "epoch 35, val_loss 455.147216796875\n",
      "epoch 36, train_loss 1277.5213623046875\n",
      "epoch 36, val_loss 455.14715576171875\n",
      "epoch 37, train_loss 1277.5211181640625\n",
      "epoch 37, val_loss 455.1470947265625\n",
      "epoch 38, train_loss 1277.5211181640625\n",
      "epoch 38, val_loss 455.1470031738281\n",
      "epoch 39, train_loss 1277.5211181640625\n",
      "epoch 39, val_loss 455.1468811035156\n",
      "epoch 40, train_loss 1277.5208740234375\n",
      "epoch 40, val_loss 455.1467590332031\n",
      "epoch 41, train_loss 1277.52099609375\n",
      "epoch 41, val_loss 455.1466979980469\n",
      "epoch 42, train_loss 1277.5208740234375\n",
      "epoch 42, val_loss 455.14666748046875\n",
      "epoch 43, train_loss 1277.5206298828125\n",
      "epoch 43, val_loss 455.1465759277344\n",
      "epoch 44, train_loss 1277.5205078125\n",
      "epoch 44, val_loss 455.1464538574219\n",
      "epoch 45, train_loss 1277.5205078125\n",
      "epoch 45, val_loss 455.14642333984375\n",
      "epoch 46, train_loss 1277.5203857421875\n",
      "epoch 46, val_loss 455.1463317871094\n",
      "epoch 47, train_loss 1277.5203857421875\n",
      "epoch 47, val_loss 455.146240234375\n",
      "epoch 48, train_loss 1277.52001953125\n",
      "epoch 48, val_loss 455.1462097167969\n",
      "epoch 49, train_loss 1277.52001953125\n",
      "epoch 49, val_loss 455.1461181640625\n",
      "epoch 50, train_loss 1277.519775390625\n",
      "epoch 50, val_loss 455.14599609375\n",
      "epoch 51, train_loss 1277.519775390625\n",
      "epoch 51, val_loss 455.1459045410156\n",
      "epoch 52, train_loss 1277.519775390625\n",
      "epoch 52, val_loss 455.1458435058594\n",
      "epoch 53, train_loss 1277.519775390625\n",
      "epoch 53, val_loss 455.1457214355469\n",
      "epoch 54, train_loss 1277.51953125\n",
      "epoch 54, val_loss 455.1456604003906\n",
      "epoch 55, train_loss 1277.5194091796875\n",
      "epoch 55, val_loss 455.14556884765625\n",
      "epoch 56, train_loss 1277.519287109375\n",
      "epoch 56, val_loss 455.14544677734375\n",
      "epoch 57, train_loss 1277.519287109375\n",
      "epoch 57, val_loss 455.1454162597656\n",
      "epoch 58, train_loss 1277.519287109375\n",
      "epoch 58, val_loss 455.14532470703125\n",
      "epoch 59, train_loss 1277.51904296875\n",
      "epoch 59, val_loss 455.14532470703125\n",
      "epoch 60, train_loss 1277.5189208984375\n",
      "epoch 60, val_loss 455.14520263671875\n",
      "epoch 61, train_loss 1277.518798828125\n",
      "epoch 61, val_loss 455.14508056640625\n",
      "epoch 62, train_loss 1277.518798828125\n",
      "epoch 62, val_loss 455.1449890136719\n",
      "epoch 63, train_loss 1277.5185546875\n",
      "epoch 63, val_loss 455.14495849609375\n",
      "epoch 64, train_loss 1277.5184326171875\n",
      "epoch 64, val_loss 455.14483642578125\n",
      "epoch 65, train_loss 1277.5185546875\n",
      "epoch 65, val_loss 455.1448059082031\n",
      "epoch 66, train_loss 1277.518310546875\n",
      "epoch 66, val_loss 455.1447448730469\n",
      "epoch 67, train_loss 1277.51806640625\n",
      "epoch 67, val_loss 455.14459228515625\n",
      "epoch 68, train_loss 1277.5179443359375\n",
      "epoch 68, val_loss 455.14453125\n",
      "epoch 69, train_loss 1277.51806640625\n",
      "epoch 69, val_loss 455.14447021484375\n",
      "epoch 70, train_loss 1277.5179443359375\n",
      "epoch 70, val_loss 455.1443786621094\n",
      "epoch 71, train_loss 1277.5177001953125\n",
      "epoch 71, val_loss 455.14434814453125\n",
      "epoch 72, train_loss 1277.517822265625\n",
      "epoch 72, val_loss 455.1442565917969\n",
      "epoch 73, train_loss 1277.5174560546875\n",
      "epoch 73, val_loss 455.1441650390625\n",
      "epoch 74, train_loss 1277.517578125\n",
      "epoch 74, val_loss 455.14404296875\n",
      "epoch 75, train_loss 1277.5174560546875\n",
      "epoch 75, val_loss 455.1439514160156\n",
      "epoch 76, train_loss 1277.517333984375\n",
      "epoch 76, val_loss 455.14385986328125\n",
      "epoch 77, train_loss 1277.5172119140625\n",
      "epoch 77, val_loss 455.14385986328125\n",
      "epoch 78, train_loss 1277.51708984375\n",
      "epoch 78, val_loss 455.1436767578125\n",
      "epoch 79, train_loss 1277.5169677734375\n",
      "epoch 79, val_loss 455.14361572265625\n",
      "epoch 80, train_loss 1277.5169677734375\n",
      "epoch 80, val_loss 455.1435852050781\n",
      "epoch 81, train_loss 1277.516845703125\n",
      "epoch 81, val_loss 455.14349365234375\n",
      "epoch 82, train_loss 1277.516845703125\n",
      "epoch 82, val_loss 455.14337158203125\n",
      "epoch 83, train_loss 1277.5167236328125\n",
      "epoch 83, val_loss 455.1432800292969\n",
      "epoch 84, train_loss 1277.5166015625\n",
      "epoch 84, val_loss 455.1432189941406\n",
      "epoch 85, train_loss 1277.516357421875\n",
      "epoch 85, val_loss 455.1431579589844\n",
      "epoch 86, train_loss 1277.516357421875\n",
      "epoch 86, val_loss 455.14312744140625\n",
      "epoch 87, train_loss 1277.5162353515625\n",
      "epoch 87, val_loss 455.1430358886719\n",
      "epoch 88, train_loss 1277.51611328125\n",
      "epoch 88, val_loss 455.1429138183594\n",
      "epoch 89, train_loss 1277.5159912109375\n",
      "epoch 89, val_loss 455.1427917480469\n",
      "epoch 90, train_loss 1277.5159912109375\n",
      "epoch 90, val_loss 455.14276123046875\n",
      "epoch 91, train_loss 1277.5157470703125\n",
      "epoch 91, val_loss 455.1427001953125\n",
      "epoch 92, train_loss 1277.515625\n",
      "epoch 92, val_loss 455.142578125\n",
      "epoch 93, train_loss 1277.515625\n",
      "epoch 93, val_loss 455.1424560546875\n",
      "epoch 94, train_loss 1277.5155029296875\n",
      "epoch 94, val_loss 455.1424865722656\n",
      "epoch 95, train_loss 1277.515380859375\n",
      "epoch 95, val_loss 455.142333984375\n",
      "epoch 96, train_loss 1277.515380859375\n",
      "epoch 96, val_loss 455.1422424316406\n",
      "epoch 97, train_loss 1277.51513671875\n",
      "epoch 97, val_loss 455.1421203613281\n",
      "epoch 98, train_loss 1277.51513671875\n",
      "epoch 98, val_loss 455.1421203613281\n",
      "epoch 99, train_loss 1277.51513671875\n",
      "epoch 99, val_loss 455.14202880859375\n",
      "Parameter containing:\n",
      "tensor([3.5587e-16], requires_grad=True)\n",
      "iter 79, train_loss_regularization 0.8032654523849487\n",
      "iter 79, val_loss_regularization 0.8032654523849487\n",
      "epoch 0, train_loss 1277.5150146484375\n",
      "epoch 0, val_loss 455.14190673828125\n",
      "epoch 1, train_loss 1277.514892578125\n",
      "epoch 1, val_loss 455.1418762207031\n",
      "epoch 2, train_loss 1277.514892578125\n",
      "epoch 2, val_loss 455.1417541503906\n",
      "epoch 3, train_loss 1277.5145263671875\n",
      "epoch 3, val_loss 455.1416320800781\n",
      "epoch 4, train_loss 1277.5145263671875\n",
      "epoch 4, val_loss 455.14166259765625\n",
      "epoch 5, train_loss 1277.5142822265625\n",
      "epoch 5, val_loss 455.14154052734375\n",
      "epoch 6, train_loss 1277.5142822265625\n",
      "epoch 6, val_loss 455.1414489746094\n",
      "epoch 7, train_loss 1277.51416015625\n",
      "epoch 7, val_loss 455.1413879394531\n",
      "epoch 8, train_loss 1277.5140380859375\n",
      "epoch 8, val_loss 455.1413269042969\n",
      "epoch 9, train_loss 1277.513916015625\n",
      "epoch 9, val_loss 455.1412353515625\n",
      "epoch 10, train_loss 1277.5137939453125\n",
      "epoch 10, val_loss 455.14111328125\n",
      "epoch 11, train_loss 1277.5137939453125\n",
      "epoch 11, val_loss 455.14105224609375\n",
      "epoch 12, train_loss 1277.5137939453125\n",
      "epoch 12, val_loss 455.1409606933594\n",
      "epoch 13, train_loss 1277.513671875\n",
      "epoch 13, val_loss 455.14093017578125\n",
      "epoch 14, train_loss 1277.5135498046875\n",
      "epoch 14, val_loss 455.1408386230469\n",
      "epoch 15, train_loss 1277.513427734375\n",
      "epoch 15, val_loss 455.1407775878906\n",
      "epoch 16, train_loss 1277.5133056640625\n",
      "epoch 16, val_loss 455.1406555175781\n",
      "epoch 17, train_loss 1277.51318359375\n",
      "epoch 17, val_loss 455.1405029296875\n",
      "epoch 18, train_loss 1277.512939453125\n",
      "epoch 18, val_loss 455.1404113769531\n",
      "epoch 19, train_loss 1277.512939453125\n",
      "epoch 19, val_loss 455.1404113769531\n",
      "epoch 20, train_loss 1277.512939453125\n",
      "epoch 20, val_loss 455.14031982421875\n",
      "epoch 21, train_loss 1277.5128173828125\n",
      "epoch 21, val_loss 455.1402587890625\n",
      "epoch 22, train_loss 1277.5126953125\n",
      "epoch 22, val_loss 455.14019775390625\n",
      "epoch 23, train_loss 1277.512451171875\n",
      "epoch 23, val_loss 455.14007568359375\n",
      "epoch 24, train_loss 1277.5125732421875\n",
      "epoch 24, val_loss 455.13995361328125\n",
      "epoch 25, train_loss 1277.512451171875\n",
      "epoch 25, val_loss 455.13983154296875\n",
      "epoch 26, train_loss 1277.51220703125\n",
      "epoch 26, val_loss 455.1398620605469\n",
      "epoch 27, train_loss 1277.51220703125\n",
      "epoch 27, val_loss 455.1397399902344\n",
      "epoch 28, train_loss 1277.5120849609375\n",
      "epoch 28, val_loss 455.1396789550781\n",
      "epoch 29, train_loss 1277.511962890625\n",
      "epoch 29, val_loss 455.1396179199219\n",
      "epoch 30, train_loss 1277.5118408203125\n",
      "epoch 30, val_loss 455.1395568847656\n",
      "epoch 31, train_loss 1277.5118408203125\n",
      "epoch 31, val_loss 455.139404296875\n",
      "epoch 32, train_loss 1277.51171875\n",
      "epoch 32, val_loss 455.13934326171875\n",
      "epoch 33, train_loss 1277.5115966796875\n",
      "epoch 33, val_loss 455.1392822265625\n",
      "epoch 34, train_loss 1277.511474609375\n",
      "epoch 34, val_loss 455.13916015625\n",
      "epoch 35, train_loss 1277.51123046875\n",
      "epoch 35, val_loss 455.1391296386719\n",
      "epoch 36, train_loss 1277.511474609375\n",
      "epoch 36, val_loss 455.1390380859375\n",
      "epoch 37, train_loss 1277.51123046875\n",
      "epoch 37, val_loss 455.1389465332031\n",
      "epoch 38, train_loss 1277.5111083984375\n",
      "epoch 38, val_loss 455.1388244628906\n",
      "epoch 39, train_loss 1277.5111083984375\n",
      "epoch 39, val_loss 455.1387939453125\n",
      "epoch 40, train_loss 1277.5108642578125\n",
      "epoch 40, val_loss 455.1387023925781\n",
      "epoch 41, train_loss 1277.5106201171875\n",
      "epoch 41, val_loss 455.13861083984375\n",
      "epoch 42, train_loss 1277.5106201171875\n",
      "epoch 42, val_loss 455.1385498046875\n",
      "epoch 43, train_loss 1277.510498046875\n",
      "epoch 43, val_loss 455.13848876953125\n",
      "epoch 44, train_loss 1277.510498046875\n",
      "epoch 44, val_loss 455.13836669921875\n",
      "epoch 45, train_loss 1277.5103759765625\n",
      "epoch 45, val_loss 455.1383056640625\n",
      "epoch 46, train_loss 1277.51025390625\n",
      "epoch 46, val_loss 455.1381530761719\n",
      "epoch 47, train_loss 1277.5101318359375\n",
      "epoch 47, val_loss 455.13812255859375\n",
      "epoch 48, train_loss 1277.510009765625\n",
      "epoch 48, val_loss 455.1380310058594\n",
      "epoch 49, train_loss 1277.5098876953125\n",
      "epoch 49, val_loss 455.1379699707031\n",
      "epoch 50, train_loss 1277.5098876953125\n",
      "epoch 50, val_loss 455.1379089355469\n",
      "epoch 51, train_loss 1277.5098876953125\n",
      "epoch 51, val_loss 455.1378479003906\n",
      "epoch 52, train_loss 1277.5096435546875\n",
      "epoch 52, val_loss 455.1376953125\n",
      "epoch 53, train_loss 1277.509521484375\n",
      "epoch 53, val_loss 455.13763427734375\n",
      "epoch 54, train_loss 1277.509521484375\n",
      "epoch 54, val_loss 455.1375732421875\n",
      "epoch 55, train_loss 1277.50927734375\n",
      "epoch 55, val_loss 455.13751220703125\n",
      "epoch 56, train_loss 1277.5091552734375\n",
      "epoch 56, val_loss 455.1374206542969\n",
      "epoch 57, train_loss 1277.509033203125\n",
      "epoch 57, val_loss 455.1373291015625\n",
      "epoch 58, train_loss 1277.509033203125\n",
      "epoch 58, val_loss 455.1372985839844\n",
      "epoch 59, train_loss 1277.5089111328125\n",
      "epoch 59, val_loss 455.1371765136719\n",
      "epoch 60, train_loss 1277.5086669921875\n",
      "epoch 60, val_loss 455.1370849609375\n",
      "epoch 61, train_loss 1277.5087890625\n",
      "epoch 61, val_loss 455.1369934082031\n",
      "epoch 62, train_loss 1277.5087890625\n",
      "epoch 62, val_loss 455.13690185546875\n",
      "epoch 63, train_loss 1277.508544921875\n",
      "epoch 63, val_loss 455.1368408203125\n",
      "epoch 64, train_loss 1277.5084228515625\n",
      "epoch 64, val_loss 455.13677978515625\n",
      "epoch 65, train_loss 1277.50830078125\n",
      "epoch 65, val_loss 455.1367492675781\n",
      "epoch 66, train_loss 1277.5081787109375\n",
      "epoch 66, val_loss 455.1366271972656\n",
      "epoch 67, train_loss 1277.508056640625\n",
      "epoch 67, val_loss 455.1365966796875\n",
      "epoch 68, train_loss 1277.508056640625\n",
      "epoch 68, val_loss 455.1364440917969\n",
      "epoch 69, train_loss 1277.5078125\n",
      "epoch 69, val_loss 455.1363220214844\n",
      "epoch 70, train_loss 1277.5076904296875\n",
      "epoch 70, val_loss 455.1363220214844\n",
      "epoch 71, train_loss 1277.5076904296875\n",
      "epoch 71, val_loss 455.1361999511719\n",
      "epoch 72, train_loss 1277.507568359375\n",
      "epoch 72, val_loss 455.13616943359375\n",
      "epoch 73, train_loss 1277.5074462890625\n",
      "epoch 73, val_loss 455.13604736328125\n",
      "epoch 74, train_loss 1277.5074462890625\n",
      "epoch 74, val_loss 455.1359558105469\n",
      "epoch 75, train_loss 1277.50732421875\n",
      "epoch 75, val_loss 455.1358642578125\n",
      "epoch 76, train_loss 1277.50732421875\n",
      "epoch 76, val_loss 455.13580322265625\n",
      "epoch 77, train_loss 1277.5072021484375\n",
      "epoch 77, val_loss 455.1357116699219\n",
      "epoch 78, train_loss 1277.5069580078125\n",
      "epoch 78, val_loss 455.13568115234375\n",
      "epoch 79, train_loss 1277.5069580078125\n",
      "epoch 79, val_loss 455.1355895996094\n",
      "epoch 80, train_loss 1277.5069580078125\n",
      "epoch 80, val_loss 455.1354675292969\n",
      "epoch 81, train_loss 1277.5067138671875\n",
      "epoch 81, val_loss 455.1354064941406\n",
      "epoch 82, train_loss 1277.5064697265625\n",
      "epoch 82, val_loss 455.1353454589844\n",
      "epoch 83, train_loss 1277.506591796875\n",
      "epoch 83, val_loss 455.13525390625\n",
      "epoch 84, train_loss 1277.5064697265625\n",
      "epoch 84, val_loss 455.1351318359375\n",
      "epoch 85, train_loss 1277.50634765625\n",
      "epoch 85, val_loss 455.13507080078125\n",
      "epoch 86, train_loss 1277.5062255859375\n",
      "epoch 86, val_loss 455.1350402832031\n",
      "epoch 87, train_loss 1277.5062255859375\n",
      "epoch 87, val_loss 455.13494873046875\n",
      "epoch 88, train_loss 1277.506103515625\n",
      "epoch 88, val_loss 455.1348876953125\n",
      "epoch 89, train_loss 1277.505859375\n",
      "epoch 89, val_loss 455.1347351074219\n",
      "epoch 90, train_loss 1277.505859375\n",
      "epoch 90, val_loss 455.13470458984375\n",
      "epoch 91, train_loss 1277.505615234375\n",
      "epoch 91, val_loss 455.13458251953125\n",
      "epoch 92, train_loss 1277.505615234375\n",
      "epoch 92, val_loss 455.1344909667969\n",
      "epoch 93, train_loss 1277.505615234375\n",
      "epoch 93, val_loss 455.1344299316406\n",
      "epoch 94, train_loss 1277.50537109375\n",
      "epoch 94, val_loss 455.13433837890625\n",
      "epoch 95, train_loss 1277.505126953125\n",
      "epoch 95, val_loss 455.13433837890625\n",
      "epoch 96, train_loss 1277.505126953125\n",
      "epoch 96, val_loss 455.1341552734375\n",
      "epoch 97, train_loss 1277.505126953125\n",
      "epoch 97, val_loss 455.1341247558594\n",
      "epoch 98, train_loss 1277.505126953125\n",
      "epoch 98, val_loss 455.134033203125\n",
      "epoch 99, train_loss 1277.5050048828125\n",
      "epoch 99, val_loss 455.13397216796875\n",
      "Parameter containing:\n",
      "tensor([2.3990e-16], requires_grad=True)\n",
      "iter 80, train_loss_regularization 0.8005024194717407\n",
      "iter 80, val_loss_regularization 0.8005024194717407\n",
      "epoch 0, train_loss 1277.5048828125\n",
      "epoch 0, val_loss 455.1338806152344\n",
      "epoch 1, train_loss 1277.5048828125\n",
      "epoch 1, val_loss 455.1337890625\n",
      "epoch 2, train_loss 1277.504638671875\n",
      "epoch 2, val_loss 455.1336669921875\n",
      "epoch 3, train_loss 1277.5045166015625\n",
      "epoch 3, val_loss 455.1335754394531\n",
      "epoch 4, train_loss 1277.5042724609375\n",
      "epoch 4, val_loss 455.1335754394531\n",
      "epoch 5, train_loss 1277.5042724609375\n",
      "epoch 5, val_loss 455.1335144042969\n",
      "epoch 6, train_loss 1277.50439453125\n",
      "epoch 6, val_loss 455.1334228515625\n",
      "epoch 7, train_loss 1277.5042724609375\n",
      "epoch 7, val_loss 455.13330078125\n",
      "epoch 8, train_loss 1277.5040283203125\n",
      "epoch 8, val_loss 455.13330078125\n",
      "epoch 9, train_loss 1277.5040283203125\n",
      "epoch 9, val_loss 455.1331787109375\n",
      "epoch 10, train_loss 1277.50390625\n",
      "epoch 10, val_loss 455.1330871582031\n",
      "epoch 11, train_loss 1277.5037841796875\n",
      "epoch 11, val_loss 455.1329650878906\n",
      "epoch 12, train_loss 1277.5037841796875\n",
      "epoch 12, val_loss 455.1329650878906\n",
      "epoch 13, train_loss 1277.503662109375\n",
      "epoch 13, val_loss 455.1328430175781\n",
      "epoch 14, train_loss 1277.5035400390625\n",
      "epoch 14, val_loss 455.1327819824219\n",
      "epoch 15, train_loss 1277.5032958984375\n",
      "epoch 15, val_loss 455.1326599121094\n",
      "epoch 16, train_loss 1277.503173828125\n",
      "epoch 16, val_loss 455.13262939453125\n",
      "epoch 17, train_loss 1277.503173828125\n",
      "epoch 17, val_loss 455.132568359375\n",
      "epoch 18, train_loss 1277.5030517578125\n",
      "epoch 18, val_loss 455.13238525390625\n",
      "epoch 19, train_loss 1277.5029296875\n",
      "epoch 19, val_loss 455.13238525390625\n",
      "epoch 20, train_loss 1277.5028076171875\n",
      "epoch 20, val_loss 455.13226318359375\n",
      "epoch 21, train_loss 1277.5028076171875\n",
      "epoch 21, val_loss 455.1322021484375\n",
      "epoch 22, train_loss 1277.5028076171875\n",
      "epoch 22, val_loss 455.1321716308594\n",
      "epoch 23, train_loss 1277.5025634765625\n",
      "epoch 23, val_loss 455.1320495605469\n",
      "epoch 24, train_loss 1277.50244140625\n",
      "epoch 24, val_loss 455.1319274902344\n",
      "epoch 25, train_loss 1277.50244140625\n",
      "epoch 25, val_loss 455.1318359375\n",
      "epoch 26, train_loss 1277.50244140625\n",
      "epoch 26, val_loss 455.1318359375\n",
      "epoch 27, train_loss 1277.5020751953125\n",
      "epoch 27, val_loss 455.1317443847656\n",
      "epoch 28, train_loss 1277.501953125\n",
      "epoch 28, val_loss 455.13165283203125\n",
      "epoch 29, train_loss 1277.501953125\n",
      "epoch 29, val_loss 455.13153076171875\n",
      "epoch 30, train_loss 1277.5018310546875\n",
      "epoch 30, val_loss 455.1314697265625\n",
      "epoch 31, train_loss 1277.501708984375\n",
      "epoch 31, val_loss 455.1314697265625\n",
      "epoch 32, train_loss 1277.501708984375\n",
      "epoch 32, val_loss 455.13128662109375\n",
      "epoch 33, train_loss 1277.5015869140625\n",
      "epoch 33, val_loss 455.13128662109375\n",
      "epoch 34, train_loss 1277.5015869140625\n",
      "epoch 34, val_loss 455.1311340332031\n",
      "epoch 35, train_loss 1277.50146484375\n",
      "epoch 35, val_loss 455.1310729980469\n",
      "epoch 36, train_loss 1277.5013427734375\n",
      "epoch 36, val_loss 455.13104248046875\n",
      "epoch 37, train_loss 1277.501220703125\n",
      "epoch 37, val_loss 455.1309509277344\n",
      "epoch 38, train_loss 1277.501220703125\n",
      "epoch 38, val_loss 455.1308288574219\n",
      "epoch 39, train_loss 1277.5009765625\n",
      "epoch 39, val_loss 455.1307373046875\n",
      "epoch 40, train_loss 1277.5009765625\n",
      "epoch 40, val_loss 455.1307067871094\n",
      "epoch 41, train_loss 1277.500732421875\n",
      "epoch 41, val_loss 455.130615234375\n",
      "epoch 42, train_loss 1277.5006103515625\n",
      "epoch 42, val_loss 455.1304931640625\n",
      "epoch 43, train_loss 1277.5006103515625\n",
      "epoch 43, val_loss 455.1304626464844\n",
      "epoch 44, train_loss 1277.5006103515625\n",
      "epoch 44, val_loss 455.13037109375\n",
      "epoch 45, train_loss 1277.50048828125\n",
      "epoch 45, val_loss 455.13037109375\n",
      "epoch 46, train_loss 1277.5003662109375\n",
      "epoch 46, val_loss 455.1301574707031\n",
      "epoch 47, train_loss 1277.500244140625\n",
      "epoch 47, val_loss 455.130126953125\n",
      "epoch 48, train_loss 1277.5001220703125\n",
      "epoch 48, val_loss 455.1300354003906\n",
      "epoch 49, train_loss 1277.4998779296875\n",
      "epoch 49, val_loss 455.12994384765625\n",
      "epoch 50, train_loss 1277.4998779296875\n",
      "epoch 50, val_loss 455.1299133300781\n",
      "epoch 51, train_loss 1277.4996337890625\n",
      "epoch 51, val_loss 455.12982177734375\n",
      "epoch 52, train_loss 1277.4996337890625\n",
      "epoch 52, val_loss 455.1297607421875\n",
      "epoch 53, train_loss 1277.4996337890625\n",
      "epoch 53, val_loss 455.1296691894531\n",
      "epoch 54, train_loss 1277.4993896484375\n",
      "epoch 54, val_loss 455.12957763671875\n",
      "epoch 55, train_loss 1277.4993896484375\n",
      "epoch 55, val_loss 455.1294860839844\n",
      "epoch 56, train_loss 1277.4993896484375\n",
      "epoch 56, val_loss 455.1294250488281\n",
      "epoch 57, train_loss 1277.4993896484375\n",
      "epoch 57, val_loss 455.1293029785156\n",
      "epoch 58, train_loss 1277.4990234375\n",
      "epoch 58, val_loss 455.1292419433594\n",
      "epoch 59, train_loss 1277.4989013671875\n",
      "epoch 59, val_loss 455.1291809082031\n",
      "epoch 60, train_loss 1277.4989013671875\n",
      "epoch 60, val_loss 455.12908935546875\n",
      "epoch 61, train_loss 1277.4989013671875\n",
      "epoch 61, val_loss 455.1290283203125\n",
      "epoch 62, train_loss 1277.49853515625\n",
      "epoch 62, val_loss 455.12890625\n",
      "epoch 63, train_loss 1277.49853515625\n",
      "epoch 63, val_loss 455.12890625\n",
      "epoch 64, train_loss 1277.4984130859375\n",
      "epoch 64, val_loss 455.1287536621094\n",
      "epoch 65, train_loss 1277.498291015625\n",
      "epoch 65, val_loss 455.12872314453125\n",
      "epoch 66, train_loss 1277.4981689453125\n",
      "epoch 66, val_loss 455.1286315917969\n",
      "epoch 67, train_loss 1277.4981689453125\n",
      "epoch 67, val_loss 455.1285705566406\n",
      "epoch 68, train_loss 1277.498046875\n",
      "epoch 68, val_loss 455.1285095214844\n",
      "epoch 69, train_loss 1277.4981689453125\n",
      "epoch 69, val_loss 455.1283264160156\n",
      "epoch 70, train_loss 1277.498046875\n",
      "epoch 70, val_loss 455.1282958984375\n",
      "epoch 71, train_loss 1277.4976806640625\n",
      "epoch 71, val_loss 455.128173828125\n",
      "epoch 72, train_loss 1277.4976806640625\n",
      "epoch 72, val_loss 455.128173828125\n",
      "epoch 73, train_loss 1277.4976806640625\n",
      "epoch 73, val_loss 455.1280822753906\n",
      "epoch 74, train_loss 1277.49755859375\n",
      "epoch 74, val_loss 455.12799072265625\n",
      "epoch 75, train_loss 1277.49755859375\n",
      "epoch 75, val_loss 455.1279296875\n",
      "epoch 76, train_loss 1277.4971923828125\n",
      "epoch 76, val_loss 455.1277770996094\n",
      "epoch 77, train_loss 1277.4971923828125\n",
      "epoch 77, val_loss 455.1277770996094\n",
      "epoch 78, train_loss 1277.4970703125\n",
      "epoch 78, val_loss 455.1276550292969\n",
      "epoch 79, train_loss 1277.4969482421875\n",
      "epoch 79, val_loss 455.1275939941406\n",
      "epoch 80, train_loss 1277.4967041015625\n",
      "epoch 80, val_loss 455.12750244140625\n",
      "epoch 81, train_loss 1277.496826171875\n",
      "epoch 81, val_loss 455.1274719238281\n",
      "epoch 82, train_loss 1277.4967041015625\n",
      "epoch 82, val_loss 455.1273193359375\n",
      "epoch 83, train_loss 1277.4967041015625\n",
      "epoch 83, val_loss 455.12725830078125\n",
      "epoch 84, train_loss 1277.4964599609375\n",
      "epoch 84, val_loss 455.127197265625\n",
      "epoch 85, train_loss 1277.4964599609375\n",
      "epoch 85, val_loss 455.1271667480469\n",
      "epoch 86, train_loss 1277.496337890625\n",
      "epoch 86, val_loss 455.1270446777344\n",
      "epoch 87, train_loss 1277.4962158203125\n",
      "epoch 87, val_loss 455.126953125\n",
      "epoch 88, train_loss 1277.49609375\n",
      "epoch 88, val_loss 455.1269226074219\n",
      "epoch 89, train_loss 1277.49609375\n",
      "epoch 89, val_loss 455.1267395019531\n",
      "epoch 90, train_loss 1277.4959716796875\n",
      "epoch 90, val_loss 455.1267395019531\n",
      "epoch 91, train_loss 1277.495849609375\n",
      "epoch 91, val_loss 455.1266784667969\n",
      "epoch 92, train_loss 1277.4957275390625\n",
      "epoch 92, val_loss 455.1264953613281\n",
      "epoch 93, train_loss 1277.4957275390625\n",
      "epoch 93, val_loss 455.12646484375\n",
      "epoch 94, train_loss 1277.4954833984375\n",
      "epoch 94, val_loss 455.1263732910156\n",
      "epoch 95, train_loss 1277.495361328125\n",
      "epoch 95, val_loss 455.1263427734375\n",
      "epoch 96, train_loss 1277.4952392578125\n",
      "epoch 96, val_loss 455.1262512207031\n",
      "epoch 97, train_loss 1277.4952392578125\n",
      "epoch 97, val_loss 455.1261291503906\n",
      "epoch 98, train_loss 1277.4951171875\n",
      "epoch 98, val_loss 455.12603759765625\n",
      "epoch 99, train_loss 1277.4949951171875\n",
      "epoch 99, val_loss 455.1259460449219\n",
      "Parameter containing:\n",
      "tensor([1.6187e-16], requires_grad=True)\n",
      "iter 81, train_loss_regularization 0.7977890968322754\n",
      "iter 81, val_loss_regularization 0.7977890968322754\n",
      "epoch 0, train_loss 1277.4951171875\n",
      "epoch 0, val_loss 455.1259460449219\n",
      "epoch 1, train_loss 1277.494873046875\n",
      "epoch 1, val_loss 455.1258239746094\n",
      "epoch 2, train_loss 1277.49462890625\n",
      "epoch 2, val_loss 455.1258239746094\n",
      "epoch 3, train_loss 1277.49462890625\n",
      "epoch 3, val_loss 455.1257019042969\n",
      "epoch 4, train_loss 1277.4947509765625\n",
      "epoch 4, val_loss 455.1255798339844\n",
      "epoch 5, train_loss 1277.49462890625\n",
      "epoch 5, val_loss 455.12548828125\n",
      "epoch 6, train_loss 1277.494384765625\n",
      "epoch 6, val_loss 455.12542724609375\n",
      "epoch 7, train_loss 1277.4942626953125\n",
      "epoch 7, val_loss 455.1253662109375\n",
      "epoch 8, train_loss 1277.494140625\n",
      "epoch 8, val_loss 455.1253356933594\n",
      "epoch 9, train_loss 1277.494140625\n",
      "epoch 9, val_loss 455.1252136230469\n",
      "epoch 10, train_loss 1277.493896484375\n",
      "epoch 10, val_loss 455.1251220703125\n",
      "epoch 11, train_loss 1277.493896484375\n",
      "epoch 11, val_loss 455.1250915527344\n",
      "epoch 12, train_loss 1277.4937744140625\n",
      "epoch 12, val_loss 455.1249694824219\n",
      "epoch 13, train_loss 1277.49365234375\n",
      "epoch 13, val_loss 455.1249084472656\n",
      "epoch 14, train_loss 1277.4935302734375\n",
      "epoch 14, val_loss 455.1247863769531\n",
      "epoch 15, train_loss 1277.4932861328125\n",
      "epoch 15, val_loss 455.12469482421875\n",
      "epoch 16, train_loss 1277.4935302734375\n",
      "epoch 16, val_loss 455.12469482421875\n",
      "epoch 17, train_loss 1277.4935302734375\n",
      "epoch 17, val_loss 455.12457275390625\n",
      "epoch 18, train_loss 1277.4931640625\n",
      "epoch 18, val_loss 455.12451171875\n",
      "epoch 19, train_loss 1277.4930419921875\n",
      "epoch 19, val_loss 455.1244201660156\n",
      "epoch 20, train_loss 1277.4932861328125\n",
      "epoch 20, val_loss 455.1242980957031\n",
      "epoch 21, train_loss 1277.4930419921875\n",
      "epoch 21, val_loss 455.1242370605469\n",
      "epoch 22, train_loss 1277.4927978515625\n",
      "epoch 22, val_loss 455.1241760253906\n",
      "epoch 23, train_loss 1277.4927978515625\n",
      "epoch 23, val_loss 455.1241149902344\n",
      "epoch 24, train_loss 1277.49267578125\n",
      "epoch 24, val_loss 455.1240539550781\n",
      "epoch 25, train_loss 1277.49267578125\n",
      "epoch 25, val_loss 455.1239929199219\n",
      "epoch 26, train_loss 1277.492431640625\n",
      "epoch 26, val_loss 455.12384033203125\n",
      "epoch 27, train_loss 1277.4923095703125\n",
      "epoch 27, val_loss 455.12384033203125\n",
      "epoch 28, train_loss 1277.4923095703125\n",
      "epoch 28, val_loss 455.1236572265625\n",
      "epoch 29, train_loss 1277.4921875\n",
      "epoch 29, val_loss 455.1236267089844\n",
      "epoch 30, train_loss 1277.4920654296875\n",
      "epoch 30, val_loss 455.12359619140625\n",
      "epoch 31, train_loss 1277.4921875\n",
      "epoch 31, val_loss 455.1234436035156\n",
      "epoch 32, train_loss 1277.491943359375\n",
      "epoch 32, val_loss 455.1233825683594\n",
      "epoch 33, train_loss 1277.4918212890625\n",
      "epoch 33, val_loss 455.1233215332031\n",
      "epoch 34, train_loss 1277.49169921875\n",
      "epoch 34, val_loss 455.1231994628906\n",
      "epoch 35, train_loss 1277.491455078125\n",
      "epoch 35, val_loss 455.1231689453125\n",
      "epoch 36, train_loss 1277.491455078125\n",
      "epoch 36, val_loss 455.1230773925781\n",
      "epoch 37, train_loss 1277.4913330078125\n",
      "epoch 37, val_loss 455.12298583984375\n",
      "epoch 38, train_loss 1277.4912109375\n",
      "epoch 38, val_loss 455.1229553222656\n",
      "epoch 39, train_loss 1277.4912109375\n",
      "epoch 39, val_loss 455.12286376953125\n",
      "epoch 40, train_loss 1277.4912109375\n",
      "epoch 40, val_loss 455.122802734375\n",
      "epoch 41, train_loss 1277.490966796875\n",
      "epoch 41, val_loss 455.1226806640625\n",
      "epoch 42, train_loss 1277.4908447265625\n",
      "epoch 42, val_loss 455.1225891113281\n",
      "epoch 43, train_loss 1277.490966796875\n",
      "epoch 43, val_loss 455.12249755859375\n",
      "epoch 44, train_loss 1277.49072265625\n",
      "epoch 44, val_loss 455.1224670410156\n",
      "epoch 45, train_loss 1277.4906005859375\n",
      "epoch 45, val_loss 455.12237548828125\n",
      "epoch 46, train_loss 1277.490478515625\n",
      "epoch 46, val_loss 455.1222839355469\n",
      "epoch 47, train_loss 1277.4903564453125\n",
      "epoch 47, val_loss 455.12225341796875\n",
      "epoch 48, train_loss 1277.4901123046875\n",
      "epoch 48, val_loss 455.1221618652344\n",
      "epoch 49, train_loss 1277.4901123046875\n",
      "epoch 49, val_loss 455.12200927734375\n",
      "epoch 50, train_loss 1277.489990234375\n",
      "epoch 50, val_loss 455.1219482421875\n",
      "epoch 51, train_loss 1277.4901123046875\n",
      "epoch 51, val_loss 455.12188720703125\n",
      "epoch 52, train_loss 1277.4898681640625\n",
      "epoch 52, val_loss 455.121826171875\n",
      "epoch 53, train_loss 1277.4896240234375\n",
      "epoch 53, val_loss 455.1217956542969\n",
      "epoch 54, train_loss 1277.48974609375\n",
      "epoch 54, val_loss 455.1216735839844\n",
      "epoch 55, train_loss 1277.4896240234375\n",
      "epoch 55, val_loss 455.12158203125\n",
      "epoch 56, train_loss 1277.489501953125\n",
      "epoch 56, val_loss 455.1214904785156\n",
      "epoch 57, train_loss 1277.4893798828125\n",
      "epoch 57, val_loss 455.1214294433594\n",
      "epoch 58, train_loss 1277.4892578125\n",
      "epoch 58, val_loss 455.121337890625\n",
      "epoch 59, train_loss 1277.4891357421875\n",
      "epoch 59, val_loss 455.12127685546875\n",
      "epoch 60, train_loss 1277.4891357421875\n",
      "epoch 60, val_loss 455.12115478515625\n",
      "epoch 61, train_loss 1277.4891357421875\n",
      "epoch 61, val_loss 455.12109375\n",
      "epoch 62, train_loss 1277.4888916015625\n",
      "epoch 62, val_loss 455.12103271484375\n",
      "epoch 63, train_loss 1277.4888916015625\n",
      "epoch 63, val_loss 455.1209716796875\n",
      "epoch 64, train_loss 1277.48876953125\n",
      "epoch 64, val_loss 455.1208801269531\n",
      "epoch 65, train_loss 1277.488525390625\n",
      "epoch 65, val_loss 455.12078857421875\n",
      "epoch 66, train_loss 1277.488525390625\n",
      "epoch 66, val_loss 455.1206970214844\n",
      "epoch 67, train_loss 1277.488525390625\n",
      "epoch 67, val_loss 455.12066650390625\n",
      "epoch 68, train_loss 1277.48828125\n",
      "epoch 68, val_loss 455.12054443359375\n",
      "epoch 69, train_loss 1277.48828125\n",
      "epoch 69, val_loss 455.12054443359375\n",
      "epoch 70, train_loss 1277.4881591796875\n",
      "epoch 70, val_loss 455.12042236328125\n",
      "epoch 71, train_loss 1277.488037109375\n",
      "epoch 71, val_loss 455.1203308105469\n",
      "epoch 72, train_loss 1277.488037109375\n",
      "epoch 72, val_loss 455.1202087402344\n",
      "epoch 73, train_loss 1277.48779296875\n",
      "epoch 73, val_loss 455.12017822265625\n",
      "epoch 74, train_loss 1277.48779296875\n",
      "epoch 74, val_loss 455.1200866699219\n",
      "epoch 75, train_loss 1277.48779296875\n",
      "epoch 75, val_loss 455.12005615234375\n",
      "epoch 76, train_loss 1277.487548828125\n",
      "epoch 76, val_loss 455.1199645996094\n",
      "epoch 77, train_loss 1277.487548828125\n",
      "epoch 77, val_loss 455.119873046875\n",
      "epoch 78, train_loss 1277.4874267578125\n",
      "epoch 78, val_loss 455.1197509765625\n",
      "epoch 79, train_loss 1277.4871826171875\n",
      "epoch 79, val_loss 455.1196594238281\n",
      "epoch 80, train_loss 1277.4871826171875\n",
      "epoch 80, val_loss 455.1196594238281\n",
      "epoch 81, train_loss 1277.487060546875\n",
      "epoch 81, val_loss 455.1195373535156\n",
      "epoch 82, train_loss 1277.487060546875\n",
      "epoch 82, val_loss 455.1194152832031\n",
      "epoch 83, train_loss 1277.48681640625\n",
      "epoch 83, val_loss 455.1194152832031\n",
      "epoch 84, train_loss 1277.4866943359375\n",
      "epoch 84, val_loss 455.11932373046875\n",
      "epoch 85, train_loss 1277.4866943359375\n",
      "epoch 85, val_loss 455.11920166015625\n",
      "epoch 86, train_loss 1277.4866943359375\n",
      "epoch 86, val_loss 455.1191101074219\n",
      "epoch 87, train_loss 1277.4864501953125\n",
      "epoch 87, val_loss 455.1190490722656\n",
      "epoch 88, train_loss 1277.486328125\n",
      "epoch 88, val_loss 455.11895751953125\n",
      "epoch 89, train_loss 1277.486328125\n",
      "epoch 89, val_loss 455.1188659667969\n",
      "epoch 90, train_loss 1277.4862060546875\n",
      "epoch 90, val_loss 455.1188659667969\n",
      "epoch 91, train_loss 1277.486083984375\n",
      "epoch 91, val_loss 455.1187438964844\n",
      "epoch 92, train_loss 1277.4859619140625\n",
      "epoch 92, val_loss 455.11865234375\n",
      "epoch 93, train_loss 1277.48583984375\n",
      "epoch 93, val_loss 455.11859130859375\n",
      "epoch 94, train_loss 1277.4857177734375\n",
      "epoch 94, val_loss 455.1184997558594\n",
      "epoch 95, train_loss 1277.4857177734375\n",
      "epoch 95, val_loss 455.118408203125\n",
      "epoch 96, train_loss 1277.4857177734375\n",
      "epoch 96, val_loss 455.11834716796875\n",
      "epoch 97, train_loss 1277.4854736328125\n",
      "epoch 97, val_loss 455.11834716796875\n",
      "epoch 98, train_loss 1277.4852294921875\n",
      "epoch 98, val_loss 455.1182556152344\n",
      "epoch 99, train_loss 1277.4852294921875\n",
      "epoch 99, val_loss 455.1181640625\n",
      "Parameter containing:\n",
      "tensor([1.0930e-16], requires_grad=True)\n",
      "iter 82, train_loss_regularization 0.7951241731643677\n",
      "iter 82, val_loss_regularization 0.7951241731643677\n",
      "epoch 0, train_loss 1277.4853515625\n",
      "epoch 0, val_loss 455.1180114746094\n",
      "epoch 1, train_loss 1277.485107421875\n",
      "epoch 1, val_loss 455.1179504394531\n",
      "epoch 2, train_loss 1277.485107421875\n",
      "epoch 2, val_loss 455.1178894042969\n",
      "epoch 3, train_loss 1277.48486328125\n",
      "epoch 3, val_loss 455.1178283691406\n",
      "epoch 4, train_loss 1277.48486328125\n",
      "epoch 4, val_loss 455.1177978515625\n",
      "epoch 5, train_loss 1277.484619140625\n",
      "epoch 5, val_loss 455.1177062988281\n",
      "epoch 6, train_loss 1277.484619140625\n",
      "epoch 6, val_loss 455.11761474609375\n",
      "epoch 7, train_loss 1277.484619140625\n",
      "epoch 7, val_loss 455.11749267578125\n",
      "epoch 8, train_loss 1277.484619140625\n",
      "epoch 8, val_loss 455.117431640625\n",
      "epoch 9, train_loss 1277.484375\n",
      "epoch 9, val_loss 455.1173400878906\n",
      "epoch 10, train_loss 1277.484130859375\n",
      "epoch 10, val_loss 455.11724853515625\n",
      "epoch 11, train_loss 1277.484130859375\n",
      "epoch 11, val_loss 455.1171569824219\n",
      "epoch 12, train_loss 1277.4840087890625\n",
      "epoch 12, val_loss 455.11712646484375\n",
      "epoch 13, train_loss 1277.4840087890625\n",
      "epoch 13, val_loss 455.1170959472656\n",
      "epoch 14, train_loss 1277.48388671875\n",
      "epoch 14, val_loss 455.11700439453125\n",
      "epoch 15, train_loss 1277.48388671875\n",
      "epoch 15, val_loss 455.1168212890625\n",
      "epoch 16, train_loss 1277.4837646484375\n",
      "epoch 16, val_loss 455.1168212890625\n",
      "epoch 17, train_loss 1277.483642578125\n",
      "epoch 17, val_loss 455.11669921875\n",
      "epoch 18, train_loss 1277.4835205078125\n",
      "epoch 18, val_loss 455.1165771484375\n",
      "epoch 19, train_loss 1277.4835205078125\n",
      "epoch 19, val_loss 455.1165771484375\n",
      "epoch 20, train_loss 1277.4833984375\n",
      "epoch 20, val_loss 455.1165466308594\n",
      "epoch 21, train_loss 1277.4832763671875\n",
      "epoch 21, val_loss 455.1164245605469\n",
      "epoch 22, train_loss 1277.4830322265625\n",
      "epoch 22, val_loss 455.1163330078125\n",
      "epoch 23, train_loss 1277.4830322265625\n",
      "epoch 23, val_loss 455.1162414550781\n",
      "epoch 24, train_loss 1277.4827880859375\n",
      "epoch 24, val_loss 455.1161804199219\n",
      "epoch 25, train_loss 1277.4827880859375\n",
      "epoch 25, val_loss 455.1160888671875\n",
      "epoch 26, train_loss 1277.4827880859375\n",
      "epoch 26, val_loss 455.11602783203125\n",
      "epoch 27, train_loss 1277.4825439453125\n",
      "epoch 27, val_loss 455.1159973144531\n",
      "epoch 28, train_loss 1277.4827880859375\n",
      "epoch 28, val_loss 455.1158752441406\n",
      "epoch 29, train_loss 1277.4825439453125\n",
      "epoch 29, val_loss 455.1157531738281\n",
      "epoch 30, train_loss 1277.482421875\n",
      "epoch 30, val_loss 455.11572265625\n",
      "epoch 31, train_loss 1277.4822998046875\n",
      "epoch 31, val_loss 455.11566162109375\n",
      "epoch 32, train_loss 1277.4822998046875\n",
      "epoch 32, val_loss 455.11553955078125\n",
      "epoch 33, train_loss 1277.4820556640625\n",
      "epoch 33, val_loss 455.1155090332031\n",
      "epoch 34, train_loss 1277.4820556640625\n",
      "epoch 34, val_loss 455.1153869628906\n",
      "epoch 35, train_loss 1277.4820556640625\n",
      "epoch 35, val_loss 455.1153259277344\n",
      "epoch 36, train_loss 1277.4818115234375\n",
      "epoch 36, val_loss 455.1152648925781\n",
      "epoch 37, train_loss 1277.481689453125\n",
      "epoch 37, val_loss 455.11517333984375\n",
      "epoch 38, train_loss 1277.4815673828125\n",
      "epoch 38, val_loss 455.1150817871094\n",
      "epoch 39, train_loss 1277.4815673828125\n",
      "epoch 39, val_loss 455.11505126953125\n",
      "epoch 40, train_loss 1277.4814453125\n",
      "epoch 40, val_loss 455.11492919921875\n",
      "epoch 41, train_loss 1277.4813232421875\n",
      "epoch 41, val_loss 455.1148681640625\n",
      "epoch 42, train_loss 1277.481201171875\n",
      "epoch 42, val_loss 455.1147766113281\n",
      "epoch 43, train_loss 1277.481201171875\n",
      "epoch 43, val_loss 455.1147155761719\n",
      "epoch 44, train_loss 1277.4810791015625\n",
      "epoch 44, val_loss 455.1146545410156\n",
      "epoch 45, train_loss 1277.4808349609375\n",
      "epoch 45, val_loss 455.1145935058594\n",
      "epoch 46, train_loss 1277.4808349609375\n",
      "epoch 46, val_loss 455.1144104003906\n",
      "epoch 47, train_loss 1277.480712890625\n",
      "epoch 47, val_loss 455.1143798828125\n",
      "epoch 48, train_loss 1277.480712890625\n",
      "epoch 48, val_loss 455.1142883300781\n",
      "epoch 49, train_loss 1277.48046875\n",
      "epoch 49, val_loss 455.1142578125\n",
      "epoch 50, train_loss 1277.48046875\n",
      "epoch 50, val_loss 455.1141662597656\n",
      "epoch 51, train_loss 1277.48046875\n",
      "epoch 51, val_loss 455.11407470703125\n",
      "epoch 52, train_loss 1277.4803466796875\n",
      "epoch 52, val_loss 455.114013671875\n",
      "epoch 53, train_loss 1277.480224609375\n",
      "epoch 53, val_loss 455.1138610839844\n",
      "epoch 54, train_loss 1277.4801025390625\n",
      "epoch 54, val_loss 455.1138000488281\n",
      "epoch 55, train_loss 1277.47998046875\n",
      "epoch 55, val_loss 455.1138000488281\n",
      "epoch 56, train_loss 1277.4798583984375\n",
      "epoch 56, val_loss 455.11370849609375\n",
      "epoch 57, train_loss 1277.4798583984375\n",
      "epoch 57, val_loss 455.11358642578125\n",
      "epoch 58, train_loss 1277.4796142578125\n",
      "epoch 58, val_loss 455.1135559082031\n",
      "epoch 59, train_loss 1277.4796142578125\n",
      "epoch 59, val_loss 455.1134033203125\n",
      "epoch 60, train_loss 1277.4796142578125\n",
      "epoch 60, val_loss 455.11334228515625\n",
      "epoch 61, train_loss 1277.4794921875\n",
      "epoch 61, val_loss 455.11328125\n",
      "epoch 62, train_loss 1277.479248046875\n",
      "epoch 62, val_loss 455.1132507324219\n",
      "epoch 63, train_loss 1277.479248046875\n",
      "epoch 63, val_loss 455.1131286621094\n",
      "epoch 64, train_loss 1277.4791259765625\n",
      "epoch 64, val_loss 455.11309814453125\n",
      "epoch 65, train_loss 1277.47900390625\n",
      "epoch 65, val_loss 455.1130065917969\n",
      "epoch 66, train_loss 1277.4788818359375\n",
      "epoch 66, val_loss 455.1129150390625\n",
      "epoch 67, train_loss 1277.4788818359375\n",
      "epoch 67, val_loss 455.1128234863281\n",
      "epoch 68, train_loss 1277.4786376953125\n",
      "epoch 68, val_loss 455.1127624511719\n",
      "epoch 69, train_loss 1277.4786376953125\n",
      "epoch 69, val_loss 455.1126708984375\n",
      "epoch 70, train_loss 1277.4786376953125\n",
      "epoch 70, val_loss 455.1125793457031\n",
      "epoch 71, train_loss 1277.478515625\n",
      "epoch 71, val_loss 455.112548828125\n",
      "epoch 72, train_loss 1277.4783935546875\n",
      "epoch 72, val_loss 455.1124572753906\n",
      "epoch 73, train_loss 1277.4783935546875\n",
      "epoch 73, val_loss 455.1124267578125\n",
      "epoch 74, train_loss 1277.478271484375\n",
      "epoch 74, val_loss 455.11224365234375\n",
      "epoch 75, train_loss 1277.47802734375\n",
      "epoch 75, val_loss 455.1121520996094\n",
      "epoch 76, train_loss 1277.47802734375\n",
      "epoch 76, val_loss 455.11212158203125\n",
      "epoch 77, train_loss 1277.477783203125\n",
      "epoch 77, val_loss 455.1120300292969\n",
      "epoch 78, train_loss 1277.477783203125\n",
      "epoch 78, val_loss 455.11199951171875\n",
      "epoch 79, train_loss 1277.4776611328125\n",
      "epoch 79, val_loss 455.11187744140625\n",
      "epoch 80, train_loss 1277.4775390625\n",
      "epoch 80, val_loss 455.1118469238281\n",
      "epoch 81, train_loss 1277.477294921875\n",
      "epoch 81, val_loss 455.11175537109375\n",
      "epoch 82, train_loss 1277.477294921875\n",
      "epoch 82, val_loss 455.1116943359375\n",
      "epoch 83, train_loss 1277.477294921875\n",
      "epoch 83, val_loss 455.11163330078125\n",
      "epoch 84, train_loss 1277.4771728515625\n",
      "epoch 84, val_loss 455.11151123046875\n",
      "epoch 85, train_loss 1277.4771728515625\n",
      "epoch 85, val_loss 455.1114501953125\n",
      "epoch 86, train_loss 1277.47705078125\n",
      "epoch 86, val_loss 455.1114196777344\n",
      "epoch 87, train_loss 1277.476806640625\n",
      "epoch 87, val_loss 455.1112976074219\n",
      "epoch 88, train_loss 1277.476806640625\n",
      "epoch 88, val_loss 455.1112060546875\n",
      "epoch 89, train_loss 1277.4766845703125\n",
      "epoch 89, val_loss 455.111083984375\n",
      "epoch 90, train_loss 1277.4766845703125\n",
      "epoch 90, val_loss 455.111083984375\n",
      "epoch 91, train_loss 1277.4765625\n",
      "epoch 91, val_loss 455.1109619140625\n",
      "epoch 92, train_loss 1277.476318359375\n",
      "epoch 92, val_loss 455.1108703613281\n",
      "epoch 93, train_loss 1277.476318359375\n",
      "epoch 93, val_loss 455.11077880859375\n",
      "epoch 94, train_loss 1277.4761962890625\n",
      "epoch 94, val_loss 455.1107177734375\n",
      "epoch 95, train_loss 1277.47607421875\n",
      "epoch 95, val_loss 455.11065673828125\n",
      "epoch 96, train_loss 1277.4759521484375\n",
      "epoch 96, val_loss 455.110595703125\n",
      "epoch 97, train_loss 1277.4759521484375\n",
      "epoch 97, val_loss 455.11053466796875\n",
      "epoch 98, train_loss 1277.4759521484375\n",
      "epoch 98, val_loss 455.11041259765625\n",
      "epoch 99, train_loss 1277.475830078125\n",
      "epoch 99, val_loss 455.1103210449219\n",
      "Parameter containing:\n",
      "tensor([7.3871e-17], requires_grad=True)\n",
      "iter 83, train_loss_regularization 0.7925070524215698\n",
      "iter 83, val_loss_regularization 0.7925070524215698\n",
      "epoch 0, train_loss 1277.4757080078125\n",
      "epoch 0, val_loss 455.11029052734375\n",
      "epoch 1, train_loss 1277.4754638671875\n",
      "epoch 1, val_loss 455.1101989746094\n",
      "epoch 2, train_loss 1277.4754638671875\n",
      "epoch 2, val_loss 455.1101379394531\n",
      "epoch 3, train_loss 1277.4754638671875\n",
      "epoch 3, val_loss 455.1100769042969\n",
      "epoch 4, train_loss 1277.475341796875\n",
      "epoch 4, val_loss 455.1099548339844\n",
      "epoch 5, train_loss 1277.475341796875\n",
      "epoch 5, val_loss 455.1098327636719\n",
      "epoch 6, train_loss 1277.4749755859375\n",
      "epoch 6, val_loss 455.1098327636719\n",
      "epoch 7, train_loss 1277.4749755859375\n",
      "epoch 7, val_loss 455.1097106933594\n",
      "epoch 8, train_loss 1277.4749755859375\n",
      "epoch 8, val_loss 455.109619140625\n",
      "epoch 9, train_loss 1277.474853515625\n",
      "epoch 9, val_loss 455.1095275878906\n",
      "epoch 10, train_loss 1277.474609375\n",
      "epoch 10, val_loss 455.1094970703125\n",
      "epoch 11, train_loss 1277.474609375\n",
      "epoch 11, val_loss 455.1094055175781\n",
      "epoch 12, train_loss 1277.474609375\n",
      "epoch 12, val_loss 455.1093444824219\n",
      "epoch 13, train_loss 1277.474365234375\n",
      "epoch 13, val_loss 455.1092834472656\n",
      "epoch 14, train_loss 1277.474365234375\n",
      "epoch 14, val_loss 455.1091613769531\n",
      "epoch 15, train_loss 1277.47412109375\n",
      "epoch 15, val_loss 455.10906982421875\n",
      "epoch 16, train_loss 1277.47412109375\n",
      "epoch 16, val_loss 455.1090087890625\n",
      "epoch 17, train_loss 1277.47412109375\n",
      "epoch 17, val_loss 455.10894775390625\n",
      "epoch 18, train_loss 1277.4739990234375\n",
      "epoch 18, val_loss 455.1089172363281\n",
      "epoch 19, train_loss 1277.4737548828125\n",
      "epoch 19, val_loss 455.1087951660156\n",
      "epoch 20, train_loss 1277.4736328125\n",
      "epoch 20, val_loss 455.10870361328125\n",
      "epoch 21, train_loss 1277.4737548828125\n",
      "epoch 21, val_loss 455.1086120605469\n",
      "epoch 22, train_loss 1277.4735107421875\n",
      "epoch 22, val_loss 455.10858154296875\n",
      "epoch 23, train_loss 1277.4735107421875\n",
      "epoch 23, val_loss 455.1084899902344\n",
      "epoch 24, train_loss 1277.4736328125\n",
      "epoch 24, val_loss 455.10845947265625\n",
      "epoch 25, train_loss 1277.4735107421875\n",
      "epoch 25, val_loss 455.1083679199219\n",
      "epoch 26, train_loss 1277.47314453125\n",
      "epoch 26, val_loss 455.10833740234375\n",
      "epoch 27, train_loss 1277.47314453125\n",
      "epoch 27, val_loss 455.108154296875\n",
      "epoch 28, train_loss 1277.4730224609375\n",
      "epoch 28, val_loss 455.10809326171875\n",
      "epoch 29, train_loss 1277.4730224609375\n",
      "epoch 29, val_loss 455.1080322265625\n",
      "epoch 30, train_loss 1277.4727783203125\n",
      "epoch 30, val_loss 455.10791015625\n",
      "epoch 31, train_loss 1277.4727783203125\n",
      "epoch 31, val_loss 455.10791015625\n",
      "epoch 32, train_loss 1277.47265625\n",
      "epoch 32, val_loss 455.1078186035156\n",
      "epoch 33, train_loss 1277.4725341796875\n",
      "epoch 33, val_loss 455.1076965332031\n",
      "epoch 34, train_loss 1277.472412109375\n",
      "epoch 34, val_loss 455.107666015625\n",
      "epoch 35, train_loss 1277.472412109375\n",
      "epoch 35, val_loss 455.1075439453125\n",
      "epoch 36, train_loss 1277.4725341796875\n",
      "epoch 36, val_loss 455.1075439453125\n",
      "epoch 37, train_loss 1277.4722900390625\n",
      "epoch 37, val_loss 455.107421875\n",
      "epoch 38, train_loss 1277.4720458984375\n",
      "epoch 38, val_loss 455.10736083984375\n",
      "epoch 39, train_loss 1277.4720458984375\n",
      "epoch 39, val_loss 455.1072998046875\n",
      "epoch 40, train_loss 1277.4720458984375\n",
      "epoch 40, val_loss 455.1072082519531\n",
      "epoch 41, train_loss 1277.4718017578125\n",
      "epoch 41, val_loss 455.10711669921875\n",
      "epoch 42, train_loss 1277.4718017578125\n",
      "epoch 42, val_loss 455.1070556640625\n",
      "epoch 43, train_loss 1277.4715576171875\n",
      "epoch 43, val_loss 455.1069641113281\n",
      "epoch 44, train_loss 1277.4715576171875\n",
      "epoch 44, val_loss 455.1069030761719\n",
      "epoch 45, train_loss 1277.4715576171875\n",
      "epoch 45, val_loss 455.1067810058594\n",
      "epoch 46, train_loss 1277.4713134765625\n",
      "epoch 46, val_loss 455.10675048828125\n",
      "epoch 47, train_loss 1277.4713134765625\n",
      "epoch 47, val_loss 455.1066589355469\n",
      "epoch 48, train_loss 1277.47119140625\n",
      "epoch 48, val_loss 455.1065979003906\n",
      "epoch 49, train_loss 1277.47119140625\n",
      "epoch 49, val_loss 455.1064453125\n",
      "epoch 50, train_loss 1277.4708251953125\n",
      "epoch 50, val_loss 455.10638427734375\n",
      "epoch 51, train_loss 1277.470947265625\n",
      "epoch 51, val_loss 455.10638427734375\n",
      "epoch 52, train_loss 1277.470703125\n",
      "epoch 52, val_loss 455.10626220703125\n",
      "epoch 53, train_loss 1277.4708251953125\n",
      "epoch 53, val_loss 455.106201171875\n",
      "epoch 54, train_loss 1277.470703125\n",
      "epoch 54, val_loss 455.10614013671875\n",
      "epoch 55, train_loss 1277.470458984375\n",
      "epoch 55, val_loss 455.1060485839844\n",
      "epoch 56, train_loss 1277.470458984375\n",
      "epoch 56, val_loss 455.1059265136719\n",
      "epoch 57, train_loss 1277.470458984375\n",
      "epoch 57, val_loss 455.1058349609375\n",
      "epoch 58, train_loss 1277.4703369140625\n",
      "epoch 58, val_loss 455.1058044433594\n",
      "epoch 59, train_loss 1277.469970703125\n",
      "epoch 59, val_loss 455.1057434082031\n",
      "epoch 60, train_loss 1277.4700927734375\n",
      "epoch 60, val_loss 455.105712890625\n",
      "epoch 61, train_loss 1277.4698486328125\n",
      "epoch 61, val_loss 455.1055908203125\n",
      "epoch 62, train_loss 1277.469970703125\n",
      "epoch 62, val_loss 455.1054992675781\n",
      "epoch 63, train_loss 1277.4697265625\n",
      "epoch 63, val_loss 455.10540771484375\n",
      "epoch 64, train_loss 1277.4697265625\n",
      "epoch 64, val_loss 455.1053466796875\n",
      "epoch 65, train_loss 1277.469482421875\n",
      "epoch 65, val_loss 455.10528564453125\n",
      "epoch 66, train_loss 1277.4696044921875\n",
      "epoch 66, val_loss 455.10516357421875\n",
      "epoch 67, train_loss 1277.4693603515625\n",
      "epoch 67, val_loss 455.1051330566406\n",
      "epoch 68, train_loss 1277.46923828125\n",
      "epoch 68, val_loss 455.10504150390625\n",
      "epoch 69, train_loss 1277.46923828125\n",
      "epoch 69, val_loss 455.1049499511719\n",
      "epoch 70, train_loss 1277.4691162109375\n",
      "epoch 70, val_loss 455.10491943359375\n",
      "epoch 71, train_loss 1277.468994140625\n",
      "epoch 71, val_loss 455.104736328125\n",
      "epoch 72, train_loss 1277.4688720703125\n",
      "epoch 72, val_loss 455.104736328125\n",
      "epoch 73, train_loss 1277.4686279296875\n",
      "epoch 73, val_loss 455.10467529296875\n",
      "epoch 74, train_loss 1277.4686279296875\n",
      "epoch 74, val_loss 455.1045837402344\n",
      "epoch 75, train_loss 1277.4686279296875\n",
      "epoch 75, val_loss 455.1044616699219\n",
      "epoch 76, train_loss 1277.4686279296875\n",
      "epoch 76, val_loss 455.1044616699219\n",
      "epoch 77, train_loss 1277.4683837890625\n",
      "epoch 77, val_loss 455.1043701171875\n",
      "epoch 78, train_loss 1277.46826171875\n",
      "epoch 78, val_loss 455.104248046875\n",
      "epoch 79, train_loss 1277.4681396484375\n",
      "epoch 79, val_loss 455.1042175292969\n",
      "epoch 80, train_loss 1277.4681396484375\n",
      "epoch 80, val_loss 455.1040344238281\n",
      "epoch 81, train_loss 1277.468017578125\n",
      "epoch 81, val_loss 455.1040344238281\n",
      "epoch 82, train_loss 1277.4681396484375\n",
      "epoch 82, val_loss 455.1039733886719\n",
      "epoch 83, train_loss 1277.4678955078125\n",
      "epoch 83, val_loss 455.1039123535156\n",
      "epoch 84, train_loss 1277.4678955078125\n",
      "epoch 84, val_loss 455.10382080078125\n",
      "epoch 85, train_loss 1277.467529296875\n",
      "epoch 85, val_loss 455.103759765625\n",
      "epoch 86, train_loss 1277.467529296875\n",
      "epoch 86, val_loss 455.10357666015625\n",
      "epoch 87, train_loss 1277.4674072265625\n",
      "epoch 87, val_loss 455.1035461425781\n",
      "epoch 88, train_loss 1277.4674072265625\n",
      "epoch 88, val_loss 455.1034851074219\n",
      "epoch 89, train_loss 1277.4674072265625\n",
      "epoch 89, val_loss 455.1034240722656\n",
      "epoch 90, train_loss 1277.4671630859375\n",
      "epoch 90, val_loss 455.1033630371094\n",
      "epoch 91, train_loss 1277.4669189453125\n",
      "epoch 91, val_loss 455.10333251953125\n",
      "epoch 92, train_loss 1277.467041015625\n",
      "epoch 92, val_loss 455.10321044921875\n",
      "epoch 93, train_loss 1277.466796875\n",
      "epoch 93, val_loss 455.1031799316406\n",
      "epoch 94, train_loss 1277.466796875\n",
      "epoch 94, val_loss 455.1029968261719\n",
      "epoch 95, train_loss 1277.466796875\n",
      "epoch 95, val_loss 455.10296630859375\n",
      "epoch 96, train_loss 1277.466796875\n",
      "epoch 96, val_loss 455.1029052734375\n",
      "epoch 97, train_loss 1277.466552734375\n",
      "epoch 97, val_loss 455.102783203125\n",
      "epoch 98, train_loss 1277.466552734375\n",
      "epoch 98, val_loss 455.1027526855469\n",
      "epoch 99, train_loss 1277.46630859375\n",
      "epoch 99, val_loss 455.1026611328125\n",
      "Parameter containing:\n",
      "tensor([4.9965e-17], requires_grad=True)\n",
      "iter 84, train_loss_regularization 0.7899374961853027\n",
      "iter 84, val_loss_regularization 0.7899374961853027\n",
      "epoch 0, train_loss 1277.46630859375\n",
      "epoch 0, val_loss 455.1026306152344\n",
      "epoch 1, train_loss 1277.46630859375\n",
      "epoch 1, val_loss 455.1025085449219\n",
      "epoch 2, train_loss 1277.46630859375\n",
      "epoch 2, val_loss 455.1024475097656\n",
      "epoch 3, train_loss 1277.4659423828125\n",
      "epoch 3, val_loss 455.102294921875\n",
      "epoch 4, train_loss 1277.4658203125\n",
      "epoch 4, val_loss 455.102294921875\n",
      "epoch 5, train_loss 1277.4656982421875\n",
      "epoch 5, val_loss 455.1022033691406\n",
      "epoch 6, train_loss 1277.4656982421875\n",
      "epoch 6, val_loss 455.10211181640625\n",
      "epoch 7, train_loss 1277.4656982421875\n",
      "epoch 7, val_loss 455.1020812988281\n",
      "epoch 8, train_loss 1277.4654541015625\n",
      "epoch 8, val_loss 455.10198974609375\n",
      "epoch 9, train_loss 1277.4654541015625\n",
      "epoch 9, val_loss 455.1018371582031\n",
      "epoch 10, train_loss 1277.4654541015625\n",
      "epoch 10, val_loss 455.101806640625\n",
      "epoch 11, train_loss 1277.46533203125\n",
      "epoch 11, val_loss 455.10174560546875\n",
      "epoch 12, train_loss 1277.465087890625\n",
      "epoch 12, val_loss 455.1017150878906\n",
      "epoch 13, train_loss 1277.4649658203125\n",
      "epoch 13, val_loss 455.10162353515625\n",
      "epoch 14, train_loss 1277.4649658203125\n",
      "epoch 14, val_loss 455.1015319824219\n",
      "epoch 15, train_loss 1277.4649658203125\n",
      "epoch 15, val_loss 455.1014099121094\n",
      "epoch 16, train_loss 1277.4647216796875\n",
      "epoch 16, val_loss 455.101318359375\n",
      "epoch 17, train_loss 1277.464599609375\n",
      "epoch 17, val_loss 455.1012878417969\n",
      "epoch 18, train_loss 1277.4647216796875\n",
      "epoch 18, val_loss 455.1011962890625\n",
      "epoch 19, train_loss 1277.4647216796875\n",
      "epoch 19, val_loss 455.1011657714844\n",
      "epoch 20, train_loss 1277.4644775390625\n",
      "epoch 20, val_loss 455.10101318359375\n",
      "epoch 21, train_loss 1277.4642333984375\n",
      "epoch 21, val_loss 455.10101318359375\n",
      "epoch 22, train_loss 1277.4642333984375\n",
      "epoch 22, val_loss 455.1008605957031\n",
      "epoch 23, train_loss 1277.46435546875\n",
      "epoch 23, val_loss 455.100830078125\n",
      "epoch 24, train_loss 1277.4638671875\n",
      "epoch 24, val_loss 455.1007995605469\n",
      "epoch 25, train_loss 1277.4639892578125\n",
      "epoch 25, val_loss 455.1006774902344\n",
      "epoch 26, train_loss 1277.4638671875\n",
      "epoch 26, val_loss 455.1006164550781\n",
      "epoch 27, train_loss 1277.4638671875\n",
      "epoch 27, val_loss 455.1004943847656\n",
      "epoch 28, train_loss 1277.463623046875\n",
      "epoch 28, val_loss 455.1004638671875\n",
      "epoch 29, train_loss 1277.463623046875\n",
      "epoch 29, val_loss 455.10040283203125\n",
      "epoch 30, train_loss 1277.4635009765625\n",
      "epoch 30, val_loss 455.10028076171875\n",
      "epoch 31, train_loss 1277.4635009765625\n",
      "epoch 31, val_loss 455.1002502441406\n",
      "epoch 32, train_loss 1277.46337890625\n",
      "epoch 32, val_loss 455.10015869140625\n",
      "epoch 33, train_loss 1277.46337890625\n",
      "epoch 33, val_loss 455.10003662109375\n",
      "epoch 34, train_loss 1277.4632568359375\n",
      "epoch 34, val_loss 455.09991455078125\n",
      "epoch 35, train_loss 1277.463134765625\n",
      "epoch 35, val_loss 455.09991455078125\n",
      "epoch 36, train_loss 1277.4630126953125\n",
      "epoch 36, val_loss 455.0998229980469\n",
      "epoch 37, train_loss 1277.462890625\n",
      "epoch 37, val_loss 455.09979248046875\n",
      "epoch 38, train_loss 1277.462890625\n",
      "epoch 38, val_loss 455.0997009277344\n",
      "epoch 39, train_loss 1277.4627685546875\n",
      "epoch 39, val_loss 455.0995788574219\n",
      "epoch 40, train_loss 1277.4625244140625\n",
      "epoch 40, val_loss 455.0994873046875\n",
      "epoch 41, train_loss 1277.4625244140625\n",
      "epoch 41, val_loss 455.0994567871094\n",
      "epoch 42, train_loss 1277.4625244140625\n",
      "epoch 42, val_loss 455.099365234375\n",
      "epoch 43, train_loss 1277.46240234375\n",
      "epoch 43, val_loss 455.0993347167969\n",
      "epoch 44, train_loss 1277.462158203125\n",
      "epoch 44, val_loss 455.0992126464844\n",
      "epoch 45, train_loss 1277.4622802734375\n",
      "epoch 45, val_loss 455.0991516113281\n",
      "epoch 46, train_loss 1277.4620361328125\n",
      "epoch 46, val_loss 455.0990295410156\n",
      "epoch 47, train_loss 1277.4620361328125\n",
      "epoch 47, val_loss 455.0989990234375\n",
      "epoch 48, train_loss 1277.4617919921875\n",
      "epoch 48, val_loss 455.0989074707031\n",
      "epoch 49, train_loss 1277.461669921875\n",
      "epoch 49, val_loss 455.098876953125\n",
      "epoch 50, train_loss 1277.4615478515625\n",
      "epoch 50, val_loss 455.0987548828125\n",
      "epoch 51, train_loss 1277.461669921875\n",
      "epoch 51, val_loss 455.09869384765625\n",
      "epoch 52, train_loss 1277.4613037109375\n",
      "epoch 52, val_loss 455.0986633300781\n",
      "epoch 53, train_loss 1277.4613037109375\n",
      "epoch 53, val_loss 455.0985412597656\n",
      "epoch 54, train_loss 1277.4613037109375\n",
      "epoch 54, val_loss 455.0985412597656\n",
      "epoch 55, train_loss 1277.4613037109375\n",
      "epoch 55, val_loss 455.0984191894531\n",
      "epoch 56, train_loss 1277.461181640625\n",
      "epoch 56, val_loss 455.0982971191406\n",
      "epoch 57, train_loss 1277.4609375\n",
      "epoch 57, val_loss 455.09820556640625\n",
      "epoch 58, train_loss 1277.4609375\n",
      "epoch 58, val_loss 455.0981750488281\n",
      "epoch 59, train_loss 1277.4608154296875\n",
      "epoch 59, val_loss 455.09808349609375\n",
      "epoch 60, train_loss 1277.4608154296875\n",
      "epoch 60, val_loss 455.0979919433594\n",
      "epoch 61, train_loss 1277.4605712890625\n",
      "epoch 61, val_loss 455.09796142578125\n",
      "epoch 62, train_loss 1277.4605712890625\n",
      "epoch 62, val_loss 455.09783935546875\n",
      "epoch 63, train_loss 1277.46044921875\n",
      "epoch 63, val_loss 455.0977478027344\n",
      "epoch 64, train_loss 1277.460205078125\n",
      "epoch 64, val_loss 455.09771728515625\n",
      "epoch 65, train_loss 1277.460205078125\n",
      "epoch 65, val_loss 455.0976257324219\n",
      "epoch 66, train_loss 1277.4600830078125\n",
      "epoch 66, val_loss 455.09759521484375\n",
      "epoch 67, train_loss 1277.4599609375\n",
      "epoch 67, val_loss 455.0975341796875\n",
      "epoch 68, train_loss 1277.460205078125\n",
      "epoch 68, val_loss 455.097412109375\n",
      "epoch 69, train_loss 1277.4599609375\n",
      "epoch 69, val_loss 455.0973815917969\n",
      "epoch 70, train_loss 1277.459716796875\n",
      "epoch 70, val_loss 455.0971984863281\n",
      "epoch 71, train_loss 1277.459716796875\n",
      "epoch 71, val_loss 455.0971984863281\n",
      "epoch 72, train_loss 1277.4595947265625\n",
      "epoch 72, val_loss 455.0971374511719\n",
      "epoch 73, train_loss 1277.45947265625\n",
      "epoch 73, val_loss 455.0970458984375\n",
      "epoch 74, train_loss 1277.45947265625\n",
      "epoch 74, val_loss 455.096923828125\n",
      "epoch 75, train_loss 1277.45947265625\n",
      "epoch 75, val_loss 455.09686279296875\n",
      "epoch 76, train_loss 1277.4591064453125\n",
      "epoch 76, val_loss 455.0968322753906\n",
      "epoch 77, train_loss 1277.4591064453125\n",
      "epoch 77, val_loss 455.09674072265625\n",
      "epoch 78, train_loss 1277.4591064453125\n",
      "epoch 78, val_loss 455.0966796875\n",
      "epoch 79, train_loss 1277.4588623046875\n",
      "epoch 79, val_loss 455.0965881347656\n",
      "epoch 80, train_loss 1277.4588623046875\n",
      "epoch 80, val_loss 455.0964660644531\n",
      "epoch 81, train_loss 1277.4588623046875\n",
      "epoch 81, val_loss 455.0964050292969\n",
      "epoch 82, train_loss 1277.458740234375\n",
      "epoch 82, val_loss 455.09637451171875\n",
      "epoch 83, train_loss 1277.4586181640625\n",
      "epoch 83, val_loss 455.0962829589844\n",
      "epoch 84, train_loss 1277.45849609375\n",
      "epoch 84, val_loss 455.0962219238281\n",
      "epoch 85, train_loss 1277.4583740234375\n",
      "epoch 85, val_loss 455.0960693359375\n",
      "epoch 86, train_loss 1277.4583740234375\n",
      "epoch 86, val_loss 455.09600830078125\n",
      "epoch 87, train_loss 1277.458251953125\n",
      "epoch 87, val_loss 455.095947265625\n",
      "epoch 88, train_loss 1277.4581298828125\n",
      "epoch 88, val_loss 455.09588623046875\n",
      "epoch 89, train_loss 1277.4580078125\n",
      "epoch 89, val_loss 455.0958251953125\n",
      "epoch 90, train_loss 1277.4580078125\n",
      "epoch 90, val_loss 455.0957946777344\n",
      "epoch 91, train_loss 1277.4578857421875\n",
      "epoch 91, val_loss 455.0956115722656\n",
      "epoch 92, train_loss 1277.4576416015625\n",
      "epoch 92, val_loss 455.0955810546875\n",
      "epoch 93, train_loss 1277.457763671875\n",
      "epoch 93, val_loss 455.0954895019531\n",
      "epoch 94, train_loss 1277.4576416015625\n",
      "epoch 94, val_loss 455.0954284667969\n",
      "epoch 95, train_loss 1277.45751953125\n",
      "epoch 95, val_loss 455.0953674316406\n",
      "epoch 96, train_loss 1277.4573974609375\n",
      "epoch 96, val_loss 455.0953063964844\n",
      "epoch 97, train_loss 1277.457275390625\n",
      "epoch 97, val_loss 455.0952453613281\n",
      "epoch 98, train_loss 1277.4571533203125\n",
      "epoch 98, val_loss 455.0951232910156\n",
      "epoch 99, train_loss 1277.45703125\n",
      "epoch 99, val_loss 455.09503173828125\n",
      "Parameter containing:\n",
      "tensor([3.3822e-17], requires_grad=True)\n",
      "iter 85, train_loss_regularization 0.7874122858047485\n",
      "iter 85, val_loss_regularization 0.7874122858047485\n",
      "epoch 0, train_loss 1277.45703125\n",
      "epoch 0, val_loss 455.0950012207031\n",
      "epoch 1, train_loss 1277.4569091796875\n",
      "epoch 1, val_loss 455.09490966796875\n",
      "epoch 2, train_loss 1277.456787109375\n",
      "epoch 2, val_loss 455.0948181152344\n",
      "epoch 3, train_loss 1277.4566650390625\n",
      "epoch 3, val_loss 455.09478759765625\n",
      "epoch 4, train_loss 1277.45654296875\n",
      "epoch 4, val_loss 455.0946960449219\n",
      "epoch 5, train_loss 1277.45654296875\n",
      "epoch 5, val_loss 455.0946350097656\n",
      "epoch 6, train_loss 1277.45654296875\n",
      "epoch 6, val_loss 455.09454345703125\n",
      "epoch 7, train_loss 1277.456298828125\n",
      "epoch 7, val_loss 455.0945129394531\n",
      "epoch 8, train_loss 1277.456298828125\n",
      "epoch 8, val_loss 455.0943603515625\n",
      "epoch 9, train_loss 1277.456298828125\n",
      "epoch 9, val_loss 455.0943298339844\n",
      "epoch 10, train_loss 1277.4560546875\n",
      "epoch 10, val_loss 455.0942077636719\n",
      "epoch 11, train_loss 1277.4559326171875\n",
      "epoch 11, val_loss 455.09417724609375\n",
      "epoch 12, train_loss 1277.4559326171875\n",
      "epoch 12, val_loss 455.0940856933594\n",
      "epoch 13, train_loss 1277.4559326171875\n",
      "epoch 13, val_loss 455.093994140625\n",
      "epoch 14, train_loss 1277.455810546875\n",
      "epoch 14, val_loss 455.0939636230469\n",
      "epoch 15, train_loss 1277.455810546875\n",
      "epoch 15, val_loss 455.0938415527344\n",
      "epoch 16, train_loss 1277.4554443359375\n",
      "epoch 16, val_loss 455.09375\n",
      "epoch 17, train_loss 1277.45556640625\n",
      "epoch 17, val_loss 455.0936584472656\n",
      "epoch 18, train_loss 1277.4554443359375\n",
      "epoch 18, val_loss 455.0936279296875\n",
      "epoch 19, train_loss 1277.455322265625\n",
      "epoch 19, val_loss 455.0935974121094\n",
      "epoch 20, train_loss 1277.4552001953125\n",
      "epoch 20, val_loss 455.09344482421875\n",
      "epoch 21, train_loss 1277.4552001953125\n",
      "epoch 21, val_loss 455.09344482421875\n",
      "epoch 22, train_loss 1277.4552001953125\n",
      "epoch 22, val_loss 455.09332275390625\n",
      "epoch 23, train_loss 1277.4549560546875\n",
      "epoch 23, val_loss 455.09326171875\n",
      "epoch 24, train_loss 1277.454833984375\n",
      "epoch 24, val_loss 455.09320068359375\n",
      "epoch 25, train_loss 1277.4547119140625\n",
      "epoch 25, val_loss 455.0931396484375\n",
      "epoch 26, train_loss 1277.4547119140625\n",
      "epoch 26, val_loss 455.0930480957031\n",
      "epoch 27, train_loss 1277.45458984375\n",
      "epoch 27, val_loss 455.0929260253906\n",
      "epoch 28, train_loss 1277.45458984375\n",
      "epoch 28, val_loss 455.0928649902344\n",
      "epoch 29, train_loss 1277.4544677734375\n",
      "epoch 29, val_loss 455.0928039550781\n",
      "epoch 30, train_loss 1277.4544677734375\n",
      "epoch 30, val_loss 455.0927429199219\n",
      "epoch 31, train_loss 1277.4542236328125\n",
      "epoch 31, val_loss 455.0926818847656\n",
      "epoch 32, train_loss 1277.4542236328125\n",
      "epoch 32, val_loss 455.09259033203125\n",
      "epoch 33, train_loss 1277.4539794921875\n",
      "epoch 33, val_loss 455.0924987792969\n",
      "epoch 34, train_loss 1277.4541015625\n",
      "epoch 34, val_loss 455.0924072265625\n",
      "epoch 35, train_loss 1277.453857421875\n",
      "epoch 35, val_loss 455.0923767089844\n",
      "epoch 36, train_loss 1277.4537353515625\n",
      "epoch 36, val_loss 455.0922546386719\n",
      "epoch 37, train_loss 1277.45361328125\n",
      "epoch 37, val_loss 455.0922546386719\n",
      "epoch 38, train_loss 1277.45361328125\n",
      "epoch 38, val_loss 455.0921630859375\n",
      "epoch 39, train_loss 1277.45361328125\n",
      "epoch 39, val_loss 455.0920104980469\n",
      "epoch 40, train_loss 1277.4532470703125\n",
      "epoch 40, val_loss 455.0919189453125\n",
      "epoch 41, train_loss 1277.453369140625\n",
      "epoch 41, val_loss 455.0918884277344\n",
      "epoch 42, train_loss 1277.4532470703125\n",
      "epoch 42, val_loss 455.091796875\n",
      "epoch 43, train_loss 1277.4530029296875\n",
      "epoch 43, val_loss 455.091796875\n",
      "epoch 44, train_loss 1277.452880859375\n",
      "epoch 44, val_loss 455.0916748046875\n",
      "epoch 45, train_loss 1277.4530029296875\n",
      "epoch 45, val_loss 455.09161376953125\n",
      "epoch 46, train_loss 1277.452880859375\n",
      "epoch 46, val_loss 455.09149169921875\n",
      "epoch 47, train_loss 1277.45263671875\n",
      "epoch 47, val_loss 455.0914611816406\n",
      "epoch 48, train_loss 1277.45263671875\n",
      "epoch 48, val_loss 455.0913391113281\n",
      "epoch 49, train_loss 1277.45263671875\n",
      "epoch 49, val_loss 455.0913391113281\n",
      "epoch 50, train_loss 1277.4525146484375\n",
      "epoch 50, val_loss 455.0912170410156\n",
      "epoch 51, train_loss 1277.4522705078125\n",
      "epoch 51, val_loss 455.09112548828125\n",
      "epoch 52, train_loss 1277.4522705078125\n",
      "epoch 52, val_loss 455.0910949707031\n",
      "epoch 53, train_loss 1277.4522705078125\n",
      "epoch 53, val_loss 455.09100341796875\n",
      "epoch 54, train_loss 1277.4522705078125\n",
      "epoch 54, val_loss 455.0909118652344\n",
      "epoch 55, train_loss 1277.4521484375\n",
      "epoch 55, val_loss 455.09088134765625\n",
      "epoch 56, train_loss 1277.4517822265625\n",
      "epoch 56, val_loss 455.0907897949219\n",
      "epoch 57, train_loss 1277.4517822265625\n",
      "epoch 57, val_loss 455.0906982421875\n",
      "epoch 58, train_loss 1277.4517822265625\n",
      "epoch 58, val_loss 455.09063720703125\n",
      "epoch 59, train_loss 1277.4515380859375\n",
      "epoch 59, val_loss 455.090576171875\n",
      "epoch 60, train_loss 1277.4515380859375\n",
      "epoch 60, val_loss 455.0904541015625\n",
      "epoch 61, train_loss 1277.4515380859375\n",
      "epoch 61, val_loss 455.0904235839844\n",
      "epoch 62, train_loss 1277.4515380859375\n",
      "epoch 62, val_loss 455.0903015136719\n",
      "epoch 63, train_loss 1277.4512939453125\n",
      "epoch 63, val_loss 455.0902099609375\n",
      "epoch 64, train_loss 1277.4512939453125\n",
      "epoch 64, val_loss 455.0901794433594\n",
      "epoch 65, train_loss 1277.4510498046875\n",
      "epoch 65, val_loss 455.090087890625\n",
      "epoch 66, train_loss 1277.4510498046875\n",
      "epoch 66, val_loss 455.09002685546875\n",
      "epoch 67, train_loss 1277.4510498046875\n",
      "epoch 67, val_loss 455.0899963378906\n",
      "epoch 68, train_loss 1277.4508056640625\n",
      "epoch 68, val_loss 455.08984375\n",
      "epoch 69, train_loss 1277.45068359375\n",
      "epoch 69, val_loss 455.0897521972656\n",
      "epoch 70, train_loss 1277.45068359375\n",
      "epoch 70, val_loss 455.0897216796875\n",
      "epoch 71, train_loss 1277.450439453125\n",
      "epoch 71, val_loss 455.08966064453125\n",
      "epoch 72, train_loss 1277.450439453125\n",
      "epoch 72, val_loss 455.0896301269531\n",
      "epoch 73, train_loss 1277.4501953125\n",
      "epoch 73, val_loss 455.0895080566406\n",
      "epoch 74, train_loss 1277.4501953125\n",
      "epoch 74, val_loss 455.08941650390625\n",
      "epoch 75, train_loss 1277.4501953125\n",
      "epoch 75, val_loss 455.08929443359375\n",
      "epoch 76, train_loss 1277.4500732421875\n",
      "epoch 76, val_loss 455.08929443359375\n",
      "epoch 77, train_loss 1277.4500732421875\n",
      "epoch 77, val_loss 455.08917236328125\n",
      "epoch 78, train_loss 1277.449951171875\n",
      "epoch 78, val_loss 455.0890808105469\n",
      "epoch 79, train_loss 1277.4498291015625\n",
      "epoch 79, val_loss 455.08905029296875\n",
      "epoch 80, train_loss 1277.44970703125\n",
      "epoch 80, val_loss 455.0889587402344\n",
      "epoch 81, train_loss 1277.44970703125\n",
      "epoch 81, val_loss 455.08892822265625\n",
      "epoch 82, train_loss 1277.44970703125\n",
      "epoch 82, val_loss 455.0888366699219\n",
      "epoch 83, train_loss 1277.449462890625\n",
      "epoch 83, val_loss 455.08880615234375\n",
      "epoch 84, train_loss 1277.4493408203125\n",
      "epoch 84, val_loss 455.0886535644531\n",
      "epoch 85, train_loss 1277.44921875\n",
      "epoch 85, val_loss 455.088623046875\n",
      "epoch 86, train_loss 1277.44921875\n",
      "epoch 86, val_loss 455.0885925292969\n",
      "epoch 87, train_loss 1277.448974609375\n",
      "epoch 87, val_loss 455.0884704589844\n",
      "epoch 88, train_loss 1277.448974609375\n",
      "epoch 88, val_loss 455.08837890625\n",
      "epoch 89, train_loss 1277.44921875\n",
      "epoch 89, val_loss 455.0882873535156\n",
      "epoch 90, train_loss 1277.4488525390625\n",
      "epoch 90, val_loss 455.0882568359375\n",
      "epoch 91, train_loss 1277.44873046875\n",
      "epoch 91, val_loss 455.08819580078125\n",
      "epoch 92, train_loss 1277.4486083984375\n",
      "epoch 92, val_loss 455.08807373046875\n",
      "epoch 93, train_loss 1277.448486328125\n",
      "epoch 93, val_loss 455.08795166015625\n",
      "epoch 94, train_loss 1277.448486328125\n",
      "epoch 94, val_loss 455.0878601074219\n",
      "epoch 95, train_loss 1277.4483642578125\n",
      "epoch 95, val_loss 455.08782958984375\n",
      "epoch 96, train_loss 1277.4481201171875\n",
      "epoch 96, val_loss 455.08782958984375\n",
      "epoch 97, train_loss 1277.4481201171875\n",
      "epoch 97, val_loss 455.08770751953125\n",
      "epoch 98, train_loss 1277.4481201171875\n",
      "epoch 98, val_loss 455.0876159667969\n",
      "epoch 99, train_loss 1277.4478759765625\n",
      "epoch 99, val_loss 455.0875549316406\n",
      "Parameter containing:\n",
      "tensor([2.2912e-17], requires_grad=True)\n",
      "iter 86, train_loss_regularization 0.7849264144897461\n",
      "iter 86, val_loss_regularization 0.7849264144897461\n",
      "epoch 0, train_loss 1277.4478759765625\n",
      "epoch 0, val_loss 455.0874938964844\n",
      "epoch 1, train_loss 1277.4478759765625\n",
      "epoch 1, val_loss 455.0873718261719\n",
      "epoch 2, train_loss 1277.4478759765625\n",
      "epoch 2, val_loss 455.0872802734375\n",
      "epoch 3, train_loss 1277.4476318359375\n",
      "epoch 3, val_loss 455.0872802734375\n",
      "epoch 4, train_loss 1277.4476318359375\n",
      "epoch 4, val_loss 455.087158203125\n",
      "epoch 5, train_loss 1277.4473876953125\n",
      "epoch 5, val_loss 455.08709716796875\n",
      "epoch 6, train_loss 1277.4473876953125\n",
      "epoch 6, val_loss 455.0870361328125\n",
      "epoch 7, train_loss 1277.4473876953125\n",
      "epoch 7, val_loss 455.0870056152344\n",
      "epoch 8, train_loss 1277.4471435546875\n",
      "epoch 8, val_loss 455.0868225097656\n",
      "epoch 9, train_loss 1277.447265625\n",
      "epoch 9, val_loss 455.0867919921875\n",
      "epoch 10, train_loss 1277.447021484375\n",
      "epoch 10, val_loss 455.0867614746094\n",
      "epoch 11, train_loss 1277.44677734375\n",
      "epoch 11, val_loss 455.0866394042969\n",
      "epoch 12, train_loss 1277.4468994140625\n",
      "epoch 12, val_loss 455.0865783691406\n",
      "epoch 13, train_loss 1277.4468994140625\n",
      "epoch 13, val_loss 455.08648681640625\n",
      "epoch 14, train_loss 1277.44677734375\n",
      "epoch 14, val_loss 455.0864562988281\n",
      "epoch 15, train_loss 1277.4466552734375\n",
      "epoch 15, val_loss 455.08642578125\n",
      "epoch 16, train_loss 1277.446533203125\n",
      "epoch 16, val_loss 455.08624267578125\n",
      "epoch 17, train_loss 1277.446533203125\n",
      "epoch 17, val_loss 455.0862121582031\n",
      "epoch 18, train_loss 1277.4462890625\n",
      "epoch 18, val_loss 455.08612060546875\n",
      "epoch 19, train_loss 1277.4462890625\n",
      "epoch 19, val_loss 455.0860290527344\n",
      "epoch 20, train_loss 1277.446044921875\n",
      "epoch 20, val_loss 455.0860290527344\n",
      "epoch 21, train_loss 1277.446044921875\n",
      "epoch 21, val_loss 455.0859680175781\n",
      "epoch 22, train_loss 1277.4461669921875\n",
      "epoch 22, val_loss 455.08587646484375\n",
      "epoch 23, train_loss 1277.4456787109375\n",
      "epoch 23, val_loss 455.08575439453125\n",
      "epoch 24, train_loss 1277.4456787109375\n",
      "epoch 24, val_loss 455.0856628417969\n",
      "epoch 25, train_loss 1277.4456787109375\n",
      "epoch 25, val_loss 455.08563232421875\n",
      "epoch 26, train_loss 1277.44580078125\n",
      "epoch 26, val_loss 455.0855407714844\n",
      "epoch 27, train_loss 1277.445556640625\n",
      "epoch 27, val_loss 455.0855407714844\n",
      "epoch 28, train_loss 1277.445556640625\n",
      "epoch 28, val_loss 455.0853271484375\n",
      "epoch 29, train_loss 1277.4454345703125\n",
      "epoch 29, val_loss 455.0853271484375\n",
      "epoch 30, train_loss 1277.4451904296875\n",
      "epoch 30, val_loss 455.0852355957031\n",
      "epoch 31, train_loss 1277.4451904296875\n",
      "epoch 31, val_loss 455.0851745605469\n",
      "epoch 32, train_loss 1277.4451904296875\n",
      "epoch 32, val_loss 455.0851135253906\n",
      "epoch 33, train_loss 1277.4449462890625\n",
      "epoch 33, val_loss 455.0849914550781\n",
      "epoch 34, train_loss 1277.4449462890625\n",
      "epoch 34, val_loss 455.0849609375\n",
      "epoch 35, train_loss 1277.4447021484375\n",
      "epoch 35, val_loss 455.0848693847656\n",
      "epoch 36, train_loss 1277.4447021484375\n",
      "epoch 36, val_loss 455.0848388671875\n",
      "epoch 37, train_loss 1277.444580078125\n",
      "epoch 37, val_loss 455.0847473144531\n",
      "epoch 38, train_loss 1277.4444580078125\n",
      "epoch 38, val_loss 455.08465576171875\n",
      "epoch 39, train_loss 1277.4444580078125\n",
      "epoch 39, val_loss 455.0845947265625\n",
      "epoch 40, train_loss 1277.4444580078125\n",
      "epoch 40, val_loss 455.08453369140625\n",
      "epoch 41, train_loss 1277.4444580078125\n",
      "epoch 41, val_loss 455.0843811035156\n",
      "epoch 42, train_loss 1277.4442138671875\n",
      "epoch 42, val_loss 455.0843200683594\n",
      "epoch 43, train_loss 1277.4439697265625\n",
      "epoch 43, val_loss 455.08428955078125\n",
      "epoch 44, train_loss 1277.4439697265625\n",
      "epoch 44, val_loss 455.0842590332031\n",
      "epoch 45, train_loss 1277.44384765625\n",
      "epoch 45, val_loss 455.08416748046875\n",
      "epoch 46, train_loss 1277.44384765625\n",
      "epoch 46, val_loss 455.0841369628906\n",
      "epoch 47, train_loss 1277.4437255859375\n",
      "epoch 47, val_loss 455.0839538574219\n",
      "epoch 48, train_loss 1277.4434814453125\n",
      "epoch 48, val_loss 455.0838623046875\n",
      "epoch 49, train_loss 1277.4434814453125\n",
      "epoch 49, val_loss 455.0838317871094\n",
      "epoch 50, train_loss 1277.443603515625\n",
      "epoch 50, val_loss 455.083740234375\n",
      "epoch 51, train_loss 1277.443359375\n",
      "epoch 51, val_loss 455.0837097167969\n",
      "epoch 52, train_loss 1277.4432373046875\n",
      "epoch 52, val_loss 455.08367919921875\n",
      "epoch 53, train_loss 1277.4432373046875\n",
      "epoch 53, val_loss 455.0835266113281\n",
      "epoch 54, train_loss 1277.443115234375\n",
      "epoch 54, val_loss 455.0834655761719\n",
      "epoch 55, train_loss 1277.443115234375\n",
      "epoch 55, val_loss 455.0833740234375\n",
      "epoch 56, train_loss 1277.4427490234375\n",
      "epoch 56, val_loss 455.0833435058594\n",
      "epoch 57, train_loss 1277.4427490234375\n",
      "epoch 57, val_loss 455.0832214355469\n",
      "epoch 58, train_loss 1277.4427490234375\n",
      "epoch 58, val_loss 455.0832214355469\n",
      "epoch 59, train_loss 1277.442626953125\n",
      "epoch 59, val_loss 455.0830383300781\n",
      "epoch 60, train_loss 1277.4425048828125\n",
      "epoch 60, val_loss 455.0830383300781\n",
      "epoch 61, train_loss 1277.4423828125\n",
      "epoch 61, val_loss 455.08294677734375\n",
      "epoch 62, train_loss 1277.4423828125\n",
      "epoch 62, val_loss 455.0829162597656\n",
      "epoch 63, train_loss 1277.4425048828125\n",
      "epoch 63, val_loss 455.0827941894531\n",
      "epoch 64, train_loss 1277.442138671875\n",
      "epoch 64, val_loss 455.0826721191406\n",
      "epoch 65, train_loss 1277.442138671875\n",
      "epoch 65, val_loss 455.08258056640625\n",
      "epoch 66, train_loss 1277.4420166015625\n",
      "epoch 66, val_loss 455.0826110839844\n",
      "epoch 67, train_loss 1277.44189453125\n",
      "epoch 67, val_loss 455.0824890136719\n",
      "epoch 68, train_loss 1277.4417724609375\n",
      "epoch 68, val_loss 455.08245849609375\n",
      "epoch 69, train_loss 1277.4417724609375\n",
      "epoch 69, val_loss 455.08233642578125\n",
      "epoch 70, train_loss 1277.4415283203125\n",
      "epoch 70, val_loss 455.08233642578125\n",
      "epoch 71, train_loss 1277.4415283203125\n",
      "epoch 71, val_loss 455.0821228027344\n",
      "epoch 72, train_loss 1277.44140625\n",
      "epoch 72, val_loss 455.0821533203125\n",
      "epoch 73, train_loss 1277.4412841796875\n",
      "epoch 73, val_loss 455.08209228515625\n",
      "epoch 74, train_loss 1277.4412841796875\n",
      "epoch 74, val_loss 455.08197021484375\n",
      "epoch 75, train_loss 1277.4412841796875\n",
      "epoch 75, val_loss 455.08197021484375\n",
      "epoch 76, train_loss 1277.4410400390625\n",
      "epoch 76, val_loss 455.0818786621094\n",
      "epoch 77, train_loss 1277.4410400390625\n",
      "epoch 77, val_loss 455.0817565917969\n",
      "epoch 78, train_loss 1277.4410400390625\n",
      "epoch 78, val_loss 455.0816955566406\n",
      "epoch 79, train_loss 1277.4407958984375\n",
      "epoch 79, val_loss 455.0815734863281\n",
      "epoch 80, train_loss 1277.440673828125\n",
      "epoch 80, val_loss 455.08154296875\n",
      "epoch 81, train_loss 1277.440673828125\n",
      "epoch 81, val_loss 455.0815124511719\n",
      "epoch 82, train_loss 1277.4405517578125\n",
      "epoch 82, val_loss 455.0814208984375\n",
      "epoch 83, train_loss 1277.4405517578125\n",
      "epoch 83, val_loss 455.081298828125\n",
      "epoch 84, train_loss 1277.4403076171875\n",
      "epoch 84, val_loss 455.08123779296875\n",
      "epoch 85, train_loss 1277.4403076171875\n",
      "epoch 85, val_loss 455.0811767578125\n",
      "epoch 86, train_loss 1277.4403076171875\n",
      "epoch 86, val_loss 455.0810852050781\n",
      "epoch 87, train_loss 1277.4403076171875\n",
      "epoch 87, val_loss 455.0810546875\n",
      "epoch 88, train_loss 1277.4400634765625\n",
      "epoch 88, val_loss 455.0809631347656\n",
      "epoch 89, train_loss 1277.4400634765625\n",
      "epoch 89, val_loss 455.08087158203125\n",
      "epoch 90, train_loss 1277.43994140625\n",
      "epoch 90, val_loss 455.0808410644531\n",
      "epoch 91, train_loss 1277.439697265625\n",
      "epoch 91, val_loss 455.0807189941406\n",
      "epoch 92, train_loss 1277.439697265625\n",
      "epoch 92, val_loss 455.0807189941406\n",
      "epoch 93, train_loss 1277.4395751953125\n",
      "epoch 93, val_loss 455.0805969238281\n",
      "epoch 94, train_loss 1277.439453125\n",
      "epoch 94, val_loss 455.08050537109375\n",
      "epoch 95, train_loss 1277.439453125\n",
      "epoch 95, val_loss 455.0804138183594\n",
      "epoch 96, train_loss 1277.4393310546875\n",
      "epoch 96, val_loss 455.08038330078125\n",
      "epoch 97, train_loss 1277.439208984375\n",
      "epoch 97, val_loss 455.0802917480469\n",
      "epoch 98, train_loss 1277.43896484375\n",
      "epoch 98, val_loss 455.0802001953125\n",
      "epoch 99, train_loss 1277.4390869140625\n",
      "epoch 99, val_loss 455.0801696777344\n",
      "Parameter containing:\n",
      "tensor([1.5534e-17], requires_grad=True)\n",
      "iter 87, train_loss_regularization 0.7824856042861938\n",
      "iter 87, val_loss_regularization 0.7824856042861938\n",
      "epoch 0, train_loss 1277.439208984375\n",
      "epoch 0, val_loss 455.080078125\n",
      "epoch 1, train_loss 1277.43896484375\n",
      "epoch 1, val_loss 455.0799255371094\n",
      "epoch 2, train_loss 1277.4388427734375\n",
      "epoch 2, val_loss 455.0799255371094\n",
      "epoch 3, train_loss 1277.4388427734375\n",
      "epoch 3, val_loss 455.079833984375\n",
      "epoch 4, train_loss 1277.4384765625\n",
      "epoch 4, val_loss 455.0798034667969\n",
      "epoch 5, train_loss 1277.4385986328125\n",
      "epoch 5, val_loss 455.0796813964844\n",
      "epoch 6, train_loss 1277.4384765625\n",
      "epoch 6, val_loss 455.0796203613281\n",
      "epoch 7, train_loss 1277.4383544921875\n",
      "epoch 7, val_loss 455.07952880859375\n",
      "epoch 8, train_loss 1277.438232421875\n",
      "epoch 8, val_loss 455.0794677734375\n",
      "epoch 9, train_loss 1277.438232421875\n",
      "epoch 9, val_loss 455.0793762207031\n",
      "epoch 10, train_loss 1277.4383544921875\n",
      "epoch 10, val_loss 455.07928466796875\n",
      "epoch 11, train_loss 1277.43798828125\n",
      "epoch 11, val_loss 455.0792541503906\n",
      "epoch 12, train_loss 1277.43798828125\n",
      "epoch 12, val_loss 455.0792541503906\n",
      "epoch 13, train_loss 1277.4378662109375\n",
      "epoch 13, val_loss 455.07904052734375\n",
      "epoch 14, train_loss 1277.4378662109375\n",
      "epoch 14, val_loss 455.07904052734375\n",
      "epoch 15, train_loss 1277.4376220703125\n",
      "epoch 15, val_loss 455.0789489746094\n",
      "epoch 16, train_loss 1277.4375\n",
      "epoch 16, val_loss 455.07891845703125\n",
      "epoch 17, train_loss 1277.4376220703125\n",
      "epoch 17, val_loss 455.0787353515625\n",
      "epoch 18, train_loss 1277.4375\n",
      "epoch 18, val_loss 455.0787353515625\n",
      "epoch 19, train_loss 1277.4373779296875\n",
      "epoch 19, val_loss 455.07861328125\n",
      "epoch 20, train_loss 1277.4371337890625\n",
      "epoch 20, val_loss 455.0785827636719\n",
      "epoch 21, train_loss 1277.4371337890625\n",
      "epoch 21, val_loss 455.07855224609375\n",
      "epoch 22, train_loss 1277.4371337890625\n",
      "epoch 22, val_loss 455.07843017578125\n",
      "epoch 23, train_loss 1277.4371337890625\n",
      "epoch 23, val_loss 455.078369140625\n",
      "epoch 24, train_loss 1277.4368896484375\n",
      "epoch 24, val_loss 455.0783386230469\n",
      "epoch 25, train_loss 1277.436767578125\n",
      "epoch 25, val_loss 455.0782165527344\n",
      "epoch 26, train_loss 1277.4366455078125\n",
      "epoch 26, val_loss 455.078125\n",
      "epoch 27, train_loss 1277.4366455078125\n",
      "epoch 27, val_loss 455.0780334472656\n",
      "epoch 28, train_loss 1277.4365234375\n",
      "epoch 28, val_loss 455.0779724121094\n",
      "epoch 29, train_loss 1277.4364013671875\n",
      "epoch 29, val_loss 455.0779724121094\n",
      "epoch 30, train_loss 1277.4364013671875\n",
      "epoch 30, val_loss 455.077880859375\n",
      "epoch 31, train_loss 1277.436279296875\n",
      "epoch 31, val_loss 455.0777893066406\n",
      "epoch 32, train_loss 1277.43603515625\n",
      "epoch 32, val_loss 455.0776672363281\n",
      "epoch 33, train_loss 1277.43603515625\n",
      "epoch 33, val_loss 455.07757568359375\n",
      "epoch 34, train_loss 1277.4359130859375\n",
      "epoch 34, val_loss 455.0775451660156\n",
      "epoch 35, train_loss 1277.43603515625\n",
      "epoch 35, val_loss 455.0775146484375\n",
      "epoch 36, train_loss 1277.435791015625\n",
      "epoch 36, val_loss 455.07745361328125\n",
      "epoch 37, train_loss 1277.435791015625\n",
      "epoch 37, val_loss 455.0773010253906\n",
      "epoch 38, train_loss 1277.435546875\n",
      "epoch 38, val_loss 455.0773010253906\n",
      "epoch 39, train_loss 1277.435546875\n",
      "epoch 39, val_loss 455.07720947265625\n",
      "epoch 40, train_loss 1277.435546875\n",
      "epoch 40, val_loss 455.0771789550781\n",
      "epoch 41, train_loss 1277.435302734375\n",
      "epoch 41, val_loss 455.07708740234375\n",
      "epoch 42, train_loss 1277.435302734375\n",
      "epoch 42, val_loss 455.07696533203125\n",
      "epoch 43, train_loss 1277.4351806640625\n",
      "epoch 43, val_loss 455.076904296875\n",
      "epoch 44, train_loss 1277.435302734375\n",
      "epoch 44, val_loss 455.0767822265625\n",
      "epoch 45, train_loss 1277.43505859375\n",
      "epoch 45, val_loss 455.0767517089844\n",
      "epoch 46, train_loss 1277.4349365234375\n",
      "epoch 46, val_loss 455.07666015625\n",
      "epoch 47, train_loss 1277.434814453125\n",
      "epoch 47, val_loss 455.0766296386719\n",
      "epoch 48, train_loss 1277.4346923828125\n",
      "epoch 48, val_loss 455.0765075683594\n",
      "epoch 49, train_loss 1277.4345703125\n",
      "epoch 49, val_loss 455.076416015625\n",
      "epoch 50, train_loss 1277.4345703125\n",
      "epoch 50, val_loss 455.0763854980469\n",
      "epoch 51, train_loss 1277.4344482421875\n",
      "epoch 51, val_loss 455.0763244628906\n",
      "epoch 52, train_loss 1277.434326171875\n",
      "epoch 52, val_loss 455.0762634277344\n",
      "epoch 53, train_loss 1277.4342041015625\n",
      "epoch 53, val_loss 455.076171875\n",
      "epoch 54, train_loss 1277.4342041015625\n",
      "epoch 54, val_loss 455.07611083984375\n",
      "epoch 55, train_loss 1277.4342041015625\n",
      "epoch 55, val_loss 455.0760498046875\n",
      "epoch 56, train_loss 1277.43408203125\n",
      "epoch 56, val_loss 455.075927734375\n",
      "epoch 57, train_loss 1277.433837890625\n",
      "epoch 57, val_loss 455.0758361816406\n",
      "epoch 58, train_loss 1277.4337158203125\n",
      "epoch 58, val_loss 455.0758361816406\n",
      "epoch 59, train_loss 1277.433837890625\n",
      "epoch 59, val_loss 455.0757141113281\n",
      "epoch 60, train_loss 1277.4337158203125\n",
      "epoch 60, val_loss 455.0756530761719\n",
      "epoch 61, train_loss 1277.4337158203125\n",
      "epoch 61, val_loss 455.0755920410156\n",
      "epoch 62, train_loss 1277.4334716796875\n",
      "epoch 62, val_loss 455.07550048828125\n",
      "epoch 63, train_loss 1277.4334716796875\n",
      "epoch 63, val_loss 455.0754699707031\n",
      "epoch 64, train_loss 1277.4334716796875\n",
      "epoch 64, val_loss 455.0754089355469\n",
      "epoch 65, train_loss 1277.433349609375\n",
      "epoch 65, val_loss 455.0752868652344\n",
      "epoch 66, train_loss 1277.43310546875\n",
      "epoch 66, val_loss 455.0751953125\n",
      "epoch 67, train_loss 1277.432861328125\n",
      "epoch 67, val_loss 455.07513427734375\n",
      "epoch 68, train_loss 1277.4329833984375\n",
      "epoch 68, val_loss 455.0750427246094\n",
      "epoch 69, train_loss 1277.4329833984375\n",
      "epoch 69, val_loss 455.07501220703125\n",
      "epoch 70, train_loss 1277.4327392578125\n",
      "epoch 70, val_loss 455.07501220703125\n",
      "epoch 71, train_loss 1277.4326171875\n",
      "epoch 71, val_loss 455.0748291015625\n",
      "epoch 72, train_loss 1277.4326171875\n",
      "epoch 72, val_loss 455.0747375488281\n",
      "epoch 73, train_loss 1277.4324951171875\n",
      "epoch 73, val_loss 455.0746765136719\n",
      "epoch 74, train_loss 1277.4326171875\n",
      "epoch 74, val_loss 455.0746154785156\n",
      "epoch 75, train_loss 1277.4324951171875\n",
      "epoch 75, val_loss 455.0745544433594\n",
      "epoch 76, train_loss 1277.4322509765625\n",
      "epoch 76, val_loss 455.074462890625\n",
      "epoch 77, train_loss 1277.4322509765625\n",
      "epoch 77, val_loss 455.07440185546875\n",
      "epoch 78, train_loss 1277.4322509765625\n",
      "epoch 78, val_loss 455.0743713378906\n",
      "epoch 79, train_loss 1277.4320068359375\n",
      "epoch 79, val_loss 455.0742492675781\n",
      "epoch 80, train_loss 1277.431884765625\n",
      "epoch 80, val_loss 455.07415771484375\n",
      "epoch 81, train_loss 1277.431884765625\n",
      "epoch 81, val_loss 455.0741271972656\n",
      "epoch 82, train_loss 1277.431640625\n",
      "epoch 82, val_loss 455.0740051269531\n",
      "epoch 83, train_loss 1277.4317626953125\n",
      "epoch 83, val_loss 455.07391357421875\n",
      "epoch 84, train_loss 1277.4317626953125\n",
      "epoch 84, val_loss 455.07391357421875\n",
      "epoch 85, train_loss 1277.4315185546875\n",
      "epoch 85, val_loss 455.0738220214844\n",
      "epoch 86, train_loss 1277.431396484375\n",
      "epoch 86, val_loss 455.07366943359375\n",
      "epoch 87, train_loss 1277.4312744140625\n",
      "epoch 87, val_loss 455.07366943359375\n",
      "epoch 88, train_loss 1277.4312744140625\n",
      "epoch 88, val_loss 455.0735778808594\n",
      "epoch 89, train_loss 1277.4310302734375\n",
      "epoch 89, val_loss 455.073486328125\n",
      "epoch 90, train_loss 1277.4310302734375\n",
      "epoch 90, val_loss 455.07354736328125\n",
      "epoch 91, train_loss 1277.4310302734375\n",
      "epoch 91, val_loss 455.0733337402344\n",
      "epoch 92, train_loss 1277.430908203125\n",
      "epoch 92, val_loss 455.0733337402344\n",
      "epoch 93, train_loss 1277.4307861328125\n",
      "epoch 93, val_loss 455.0732116699219\n",
      "epoch 94, train_loss 1277.4307861328125\n",
      "epoch 94, val_loss 455.0732116699219\n",
      "epoch 95, train_loss 1277.4305419921875\n",
      "epoch 95, val_loss 455.0730895996094\n",
      "epoch 96, train_loss 1277.4305419921875\n",
      "epoch 96, val_loss 455.072998046875\n",
      "epoch 97, train_loss 1277.4305419921875\n",
      "epoch 97, val_loss 455.0729675292969\n",
      "epoch 98, train_loss 1277.430419921875\n",
      "epoch 98, val_loss 455.0728454589844\n",
      "epoch 99, train_loss 1277.4302978515625\n",
      "epoch 99, val_loss 455.0727844238281\n",
      "Parameter containing:\n",
      "tensor([1.0539e-17], requires_grad=True)\n",
      "iter 88, train_loss_regularization 0.7800893187522888\n",
      "iter 88, val_loss_regularization 0.7800893187522888\n",
      "epoch 0, train_loss 1277.43017578125\n",
      "epoch 0, val_loss 455.0726623535156\n",
      "epoch 1, train_loss 1277.4300537109375\n",
      "epoch 1, val_loss 455.0726623535156\n",
      "epoch 2, train_loss 1277.4300537109375\n",
      "epoch 2, val_loss 455.0726318359375\n",
      "epoch 3, train_loss 1277.4300537109375\n",
      "epoch 3, val_loss 455.072509765625\n",
      "epoch 4, train_loss 1277.4298095703125\n",
      "epoch 4, val_loss 455.0724182128906\n",
      "epoch 5, train_loss 1277.4298095703125\n",
      "epoch 5, val_loss 455.07232666015625\n",
      "epoch 6, train_loss 1277.4298095703125\n",
      "epoch 6, val_loss 455.0722351074219\n",
      "epoch 7, train_loss 1277.4295654296875\n",
      "epoch 7, val_loss 455.0722351074219\n",
      "epoch 8, train_loss 1277.429443359375\n",
      "epoch 8, val_loss 455.0721130371094\n",
      "epoch 9, train_loss 1277.429443359375\n",
      "epoch 9, val_loss 455.07208251953125\n",
      "epoch 10, train_loss 1277.429443359375\n",
      "epoch 10, val_loss 455.0719909667969\n",
      "epoch 11, train_loss 1277.42919921875\n",
      "epoch 11, val_loss 455.0719299316406\n",
      "epoch 12, train_loss 1277.42919921875\n",
      "epoch 12, val_loss 455.0717468261719\n",
      "epoch 13, train_loss 1277.428955078125\n",
      "epoch 13, val_loss 455.0717468261719\n",
      "epoch 14, train_loss 1277.4288330078125\n",
      "epoch 14, val_loss 455.07171630859375\n",
      "epoch 15, train_loss 1277.428955078125\n",
      "epoch 15, val_loss 455.07159423828125\n",
      "epoch 16, train_loss 1277.428955078125\n",
      "epoch 16, val_loss 455.07159423828125\n",
      "epoch 17, train_loss 1277.4285888671875\n",
      "epoch 17, val_loss 455.071533203125\n",
      "epoch 18, train_loss 1277.428466796875\n",
      "epoch 18, val_loss 455.0714111328125\n",
      "epoch 19, train_loss 1277.4285888671875\n",
      "epoch 19, val_loss 455.0714111328125\n",
      "epoch 20, train_loss 1277.428466796875\n",
      "epoch 20, val_loss 455.0712585449219\n",
      "epoch 21, train_loss 1277.42822265625\n",
      "epoch 21, val_loss 455.0711669921875\n",
      "epoch 22, train_loss 1277.42822265625\n",
      "epoch 22, val_loss 455.0710754394531\n",
      "epoch 23, train_loss 1277.42822265625\n",
      "epoch 23, val_loss 455.071044921875\n",
      "epoch 24, train_loss 1277.4281005859375\n",
      "epoch 24, val_loss 455.0710144042969\n",
      "epoch 25, train_loss 1277.4281005859375\n",
      "epoch 25, val_loss 455.0709228515625\n",
      "epoch 26, train_loss 1277.4278564453125\n",
      "epoch 26, val_loss 455.0708312988281\n",
      "epoch 27, train_loss 1277.427734375\n",
      "epoch 27, val_loss 455.07073974609375\n",
      "epoch 28, train_loss 1277.427734375\n",
      "epoch 28, val_loss 455.0707092285156\n",
      "epoch 29, train_loss 1277.427734375\n",
      "epoch 29, val_loss 455.0705871582031\n",
      "epoch 30, train_loss 1277.427490234375\n",
      "epoch 30, val_loss 455.070556640625\n",
      "epoch 31, train_loss 1277.4273681640625\n",
      "epoch 31, val_loss 455.07049560546875\n",
      "epoch 32, train_loss 1277.4273681640625\n",
      "epoch 32, val_loss 455.07037353515625\n",
      "epoch 33, train_loss 1277.4273681640625\n",
      "epoch 33, val_loss 455.07037353515625\n",
      "epoch 34, train_loss 1277.4273681640625\n",
      "epoch 34, val_loss 455.0702819824219\n",
      "epoch 35, train_loss 1277.4271240234375\n",
      "epoch 35, val_loss 455.0701599121094\n",
      "epoch 36, train_loss 1277.427001953125\n",
      "epoch 36, val_loss 455.070068359375\n",
      "epoch 37, train_loss 1277.427001953125\n",
      "epoch 37, val_loss 455.07000732421875\n",
      "epoch 38, train_loss 1277.427001953125\n",
      "epoch 38, val_loss 455.0699462890625\n",
      "epoch 39, train_loss 1277.4266357421875\n",
      "epoch 39, val_loss 455.0699462890625\n",
      "epoch 40, train_loss 1277.4267578125\n",
      "epoch 40, val_loss 455.06982421875\n",
      "epoch 41, train_loss 1277.426513671875\n",
      "epoch 41, val_loss 455.06976318359375\n",
      "epoch 42, train_loss 1277.426513671875\n",
      "epoch 42, val_loss 455.0697021484375\n",
      "epoch 43, train_loss 1277.4263916015625\n",
      "epoch 43, val_loss 455.069580078125\n",
      "epoch 44, train_loss 1277.4263916015625\n",
      "epoch 44, val_loss 455.0695495605469\n",
      "epoch 45, train_loss 1277.42626953125\n",
      "epoch 45, val_loss 455.0694580078125\n",
      "epoch 46, train_loss 1277.42626953125\n",
      "epoch 46, val_loss 455.0694274902344\n",
      "epoch 47, train_loss 1277.426025390625\n",
      "epoch 47, val_loss 455.0692443847656\n",
      "epoch 48, train_loss 1277.4261474609375\n",
      "epoch 48, val_loss 455.0692443847656\n",
      "epoch 49, train_loss 1277.4259033203125\n",
      "epoch 49, val_loss 455.06915283203125\n",
      "epoch 50, train_loss 1277.42578125\n",
      "epoch 50, val_loss 455.069091796875\n",
      "epoch 51, train_loss 1277.4256591796875\n",
      "epoch 51, val_loss 455.06903076171875\n",
      "epoch 52, train_loss 1277.4256591796875\n",
      "epoch 52, val_loss 455.06890869140625\n",
      "epoch 53, train_loss 1277.4256591796875\n",
      "epoch 53, val_loss 455.0688781738281\n",
      "epoch 54, train_loss 1277.4256591796875\n",
      "epoch 54, val_loss 455.0687561035156\n",
      "epoch 55, train_loss 1277.42529296875\n",
      "epoch 55, val_loss 455.0686950683594\n",
      "epoch 56, train_loss 1277.4251708984375\n",
      "epoch 56, val_loss 455.06866455078125\n",
      "epoch 57, train_loss 1277.42529296875\n",
      "epoch 57, val_loss 455.0685729980469\n",
      "epoch 58, train_loss 1277.42529296875\n",
      "epoch 58, val_loss 455.06854248046875\n",
      "epoch 59, train_loss 1277.425048828125\n",
      "epoch 59, val_loss 455.06842041015625\n",
      "epoch 60, train_loss 1277.425048828125\n",
      "epoch 60, val_loss 455.0683288574219\n",
      "epoch 61, train_loss 1277.4249267578125\n",
      "epoch 61, val_loss 455.06829833984375\n",
      "epoch 62, train_loss 1277.4248046875\n",
      "epoch 62, val_loss 455.0682067871094\n",
      "epoch 63, train_loss 1277.4248046875\n",
      "epoch 63, val_loss 455.06817626953125\n",
      "epoch 64, train_loss 1277.4246826171875\n",
      "epoch 64, val_loss 455.068115234375\n",
      "epoch 65, train_loss 1277.424560546875\n",
      "epoch 65, val_loss 455.0679931640625\n",
      "epoch 66, train_loss 1277.424560546875\n",
      "epoch 66, val_loss 455.0679016113281\n",
      "epoch 67, train_loss 1277.424560546875\n",
      "epoch 67, val_loss 455.0678405761719\n",
      "epoch 68, train_loss 1277.42431640625\n",
      "epoch 68, val_loss 455.0677795410156\n",
      "epoch 69, train_loss 1277.4241943359375\n",
      "epoch 69, val_loss 455.0677185058594\n",
      "epoch 70, train_loss 1277.4241943359375\n",
      "epoch 70, val_loss 455.0676574707031\n",
      "epoch 71, train_loss 1277.4241943359375\n",
      "epoch 71, val_loss 455.0675354003906\n",
      "epoch 72, train_loss 1277.4239501953125\n",
      "epoch 72, val_loss 455.06744384765625\n",
      "epoch 73, train_loss 1277.423828125\n",
      "epoch 73, val_loss 455.06744384765625\n",
      "epoch 74, train_loss 1277.423828125\n",
      "epoch 74, val_loss 455.0673828125\n",
      "epoch 75, train_loss 1277.4237060546875\n",
      "epoch 75, val_loss 455.0672607421875\n",
      "epoch 76, train_loss 1277.423583984375\n",
      "epoch 76, val_loss 455.0672607421875\n",
      "epoch 77, train_loss 1277.4234619140625\n",
      "epoch 77, val_loss 455.067138671875\n",
      "epoch 78, train_loss 1277.4234619140625\n",
      "epoch 78, val_loss 455.06707763671875\n",
      "epoch 79, train_loss 1277.4234619140625\n",
      "epoch 79, val_loss 455.06695556640625\n",
      "epoch 80, train_loss 1277.4232177734375\n",
      "epoch 80, val_loss 455.0669250488281\n",
      "epoch 81, train_loss 1277.4232177734375\n",
      "epoch 81, val_loss 455.0668029785156\n",
      "epoch 82, train_loss 1277.423095703125\n",
      "epoch 82, val_loss 455.0668029785156\n",
      "epoch 83, train_loss 1277.4229736328125\n",
      "epoch 83, val_loss 455.0666809082031\n",
      "epoch 84, train_loss 1277.4228515625\n",
      "epoch 84, val_loss 455.0666198730469\n",
      "epoch 85, train_loss 1277.4229736328125\n",
      "epoch 85, val_loss 455.0665283203125\n",
      "epoch 86, train_loss 1277.4228515625\n",
      "epoch 86, val_loss 455.06646728515625\n",
      "epoch 87, train_loss 1277.422607421875\n",
      "epoch 87, val_loss 455.06640625\n",
      "epoch 88, train_loss 1277.4224853515625\n",
      "epoch 88, val_loss 455.0663757324219\n",
      "epoch 89, train_loss 1277.422607421875\n",
      "epoch 89, val_loss 455.0662536621094\n",
      "epoch 90, train_loss 1277.4222412109375\n",
      "epoch 90, val_loss 455.066162109375\n",
      "epoch 91, train_loss 1277.4222412109375\n",
      "epoch 91, val_loss 455.0660705566406\n",
      "epoch 92, train_loss 1277.422119140625\n",
      "epoch 92, val_loss 455.0660400390625\n",
      "epoch 93, train_loss 1277.422119140625\n",
      "epoch 93, val_loss 455.0660095214844\n",
      "epoch 94, train_loss 1277.421875\n",
      "epoch 94, val_loss 455.0658874511719\n",
      "epoch 95, train_loss 1277.4219970703125\n",
      "epoch 95, val_loss 455.0658264160156\n",
      "epoch 96, train_loss 1277.421875\n",
      "epoch 96, val_loss 455.06573486328125\n",
      "epoch 97, train_loss 1277.421630859375\n",
      "epoch 97, val_loss 455.065673828125\n",
      "epoch 98, train_loss 1277.421630859375\n",
      "epoch 98, val_loss 455.065673828125\n",
      "epoch 99, train_loss 1277.4217529296875\n",
      "epoch 99, val_loss 455.06549072265625\n",
      "Parameter containing:\n",
      "tensor([7.1559e-18], requires_grad=True)\n",
      "iter 89, train_loss_regularization 0.7777369618415833\n",
      "iter 89, val_loss_regularization 0.7777369618415833\n",
      "epoch 0, train_loss 1277.421630859375\n",
      "epoch 0, val_loss 455.0654602050781\n",
      "epoch 1, train_loss 1277.4215087890625\n",
      "epoch 1, val_loss 455.0654296875\n",
      "epoch 2, train_loss 1277.42138671875\n",
      "epoch 2, val_loss 455.0653381347656\n",
      "epoch 3, train_loss 1277.4212646484375\n",
      "epoch 3, val_loss 455.06524658203125\n",
      "epoch 4, train_loss 1277.421142578125\n",
      "epoch 4, val_loss 455.0651550292969\n",
      "epoch 5, train_loss 1277.421142578125\n",
      "epoch 5, val_loss 455.0651550292969\n",
      "epoch 6, train_loss 1277.4210205078125\n",
      "epoch 6, val_loss 455.0650329589844\n",
      "epoch 7, train_loss 1277.4210205078125\n",
      "epoch 7, val_loss 455.0649719238281\n",
      "epoch 8, train_loss 1277.4208984375\n",
      "epoch 8, val_loss 455.06488037109375\n",
      "epoch 9, train_loss 1277.4207763671875\n",
      "epoch 9, val_loss 455.0647888183594\n",
      "epoch 10, train_loss 1277.4205322265625\n",
      "epoch 10, val_loss 455.06475830078125\n",
      "epoch 11, train_loss 1277.4205322265625\n",
      "epoch 11, val_loss 455.06463623046875\n",
      "epoch 12, train_loss 1277.42041015625\n",
      "epoch 12, val_loss 455.06463623046875\n",
      "epoch 13, train_loss 1277.42041015625\n",
      "epoch 13, val_loss 455.0645751953125\n",
      "epoch 14, train_loss 1277.4202880859375\n",
      "epoch 14, val_loss 455.064453125\n",
      "epoch 15, train_loss 1277.4202880859375\n",
      "epoch 15, val_loss 455.0643615722656\n",
      "epoch 16, train_loss 1277.4200439453125\n",
      "epoch 16, val_loss 455.0643005371094\n",
      "epoch 17, train_loss 1277.4200439453125\n",
      "epoch 17, val_loss 455.0642395019531\n",
      "epoch 18, train_loss 1277.419921875\n",
      "epoch 18, val_loss 455.0641784667969\n",
      "epoch 19, train_loss 1277.4197998046875\n",
      "epoch 19, val_loss 455.0640869140625\n",
      "epoch 20, train_loss 1277.4197998046875\n",
      "epoch 20, val_loss 455.0639953613281\n",
      "epoch 21, train_loss 1277.4197998046875\n",
      "epoch 21, val_loss 455.06390380859375\n",
      "epoch 22, train_loss 1277.419677734375\n",
      "epoch 22, val_loss 455.06390380859375\n",
      "epoch 23, train_loss 1277.4195556640625\n",
      "epoch 23, val_loss 455.0637512207031\n",
      "epoch 24, train_loss 1277.4195556640625\n",
      "epoch 24, val_loss 455.0637512207031\n",
      "epoch 25, train_loss 1277.419189453125\n",
      "epoch 25, val_loss 455.06365966796875\n",
      "epoch 26, train_loss 1277.4193115234375\n",
      "epoch 26, val_loss 455.0636291503906\n",
      "epoch 27, train_loss 1277.419189453125\n",
      "epoch 27, val_loss 455.06353759765625\n",
      "epoch 28, train_loss 1277.419189453125\n",
      "epoch 28, val_loss 455.0634460449219\n",
      "epoch 29, train_loss 1277.4189453125\n",
      "epoch 29, val_loss 455.06341552734375\n",
      "epoch 30, train_loss 1277.4189453125\n",
      "epoch 30, val_loss 455.0633850097656\n",
      "epoch 31, train_loss 1277.4189453125\n",
      "epoch 31, val_loss 455.0632019042969\n",
      "epoch 32, train_loss 1277.418701171875\n",
      "epoch 32, val_loss 455.0632019042969\n",
      "epoch 33, train_loss 1277.418701171875\n",
      "epoch 33, val_loss 455.0631103515625\n",
      "epoch 34, train_loss 1277.4185791015625\n",
      "epoch 34, val_loss 455.06298828125\n",
      "epoch 35, train_loss 1277.4185791015625\n",
      "epoch 35, val_loss 455.0629577636719\n",
      "epoch 36, train_loss 1277.41845703125\n",
      "epoch 36, val_loss 455.0628662109375\n",
      "epoch 37, train_loss 1277.41845703125\n",
      "epoch 37, val_loss 455.0628356933594\n",
      "epoch 38, train_loss 1277.418212890625\n",
      "epoch 38, val_loss 455.062744140625\n",
      "epoch 39, train_loss 1277.4180908203125\n",
      "epoch 39, val_loss 455.0626220703125\n",
      "epoch 40, train_loss 1277.418212890625\n",
      "epoch 40, val_loss 455.0625305175781\n",
      "epoch 41, train_loss 1277.4180908203125\n",
      "epoch 41, val_loss 455.0625\n",
      "epoch 42, train_loss 1277.41796875\n",
      "epoch 42, val_loss 455.0625\n",
      "epoch 43, train_loss 1277.41796875\n",
      "epoch 43, val_loss 455.0623779296875\n",
      "epoch 44, train_loss 1277.417724609375\n",
      "epoch 44, val_loss 455.0623474121094\n",
      "epoch 45, train_loss 1277.4178466796875\n",
      "epoch 45, val_loss 455.062255859375\n",
      "epoch 46, train_loss 1277.417724609375\n",
      "epoch 46, val_loss 455.0621643066406\n",
      "epoch 47, train_loss 1277.41748046875\n",
      "epoch 47, val_loss 455.06207275390625\n",
      "epoch 48, train_loss 1277.4173583984375\n",
      "epoch 48, val_loss 455.06201171875\n",
      "epoch 49, train_loss 1277.4173583984375\n",
      "epoch 49, val_loss 455.0619201660156\n",
      "epoch 50, train_loss 1277.4173583984375\n",
      "epoch 50, val_loss 455.0619201660156\n",
      "epoch 51, train_loss 1277.4171142578125\n",
      "epoch 51, val_loss 455.0617370605469\n",
      "epoch 52, train_loss 1277.4171142578125\n",
      "epoch 52, val_loss 455.06170654296875\n",
      "epoch 53, train_loss 1277.4168701171875\n",
      "epoch 53, val_loss 455.0616760253906\n",
      "epoch 54, train_loss 1277.4168701171875\n",
      "epoch 54, val_loss 455.06158447265625\n",
      "epoch 55, train_loss 1277.416748046875\n",
      "epoch 55, val_loss 455.06158447265625\n",
      "epoch 56, train_loss 1277.4168701171875\n",
      "epoch 56, val_loss 455.06146240234375\n",
      "epoch 57, train_loss 1277.4166259765625\n",
      "epoch 57, val_loss 455.0613708496094\n",
      "epoch 58, train_loss 1277.416748046875\n",
      "epoch 58, val_loss 455.061279296875\n",
      "epoch 59, train_loss 1277.41650390625\n",
      "epoch 59, val_loss 455.06121826171875\n",
      "epoch 60, train_loss 1277.4163818359375\n",
      "epoch 60, val_loss 455.0611572265625\n",
      "epoch 61, train_loss 1277.4163818359375\n",
      "epoch 61, val_loss 455.06109619140625\n",
      "epoch 62, train_loss 1277.4163818359375\n",
      "epoch 62, val_loss 455.06109619140625\n",
      "epoch 63, train_loss 1277.4161376953125\n",
      "epoch 63, val_loss 455.0608215332031\n",
      "epoch 64, train_loss 1277.4161376953125\n",
      "epoch 64, val_loss 455.0608215332031\n",
      "epoch 65, train_loss 1277.416015625\n",
      "epoch 65, val_loss 455.060791015625\n",
      "epoch 66, train_loss 1277.4158935546875\n",
      "epoch 66, val_loss 455.0606994628906\n",
      "epoch 67, train_loss 1277.4158935546875\n",
      "epoch 67, val_loss 455.0606689453125\n",
      "epoch 68, train_loss 1277.4156494140625\n",
      "epoch 68, val_loss 455.0606384277344\n",
      "epoch 69, train_loss 1277.4156494140625\n",
      "epoch 69, val_loss 455.060546875\n",
      "epoch 70, train_loss 1277.4156494140625\n",
      "epoch 70, val_loss 455.0604553222656\n",
      "epoch 71, train_loss 1277.4156494140625\n",
      "epoch 71, val_loss 455.0603332519531\n",
      "epoch 72, train_loss 1277.415283203125\n",
      "epoch 72, val_loss 455.060302734375\n",
      "epoch 73, train_loss 1277.415283203125\n",
      "epoch 73, val_loss 455.0602111816406\n",
      "epoch 74, train_loss 1277.4151611328125\n",
      "epoch 74, val_loss 455.06011962890625\n",
      "epoch 75, train_loss 1277.4151611328125\n",
      "epoch 75, val_loss 455.06011962890625\n",
      "epoch 76, train_loss 1277.4150390625\n",
      "epoch 76, val_loss 455.0600280761719\n",
      "epoch 77, train_loss 1277.414794921875\n",
      "epoch 77, val_loss 455.0599670410156\n",
      "epoch 78, train_loss 1277.414794921875\n",
      "epoch 78, val_loss 455.05987548828125\n",
      "epoch 79, train_loss 1277.414794921875\n",
      "epoch 79, val_loss 455.0597839355469\n",
      "epoch 80, train_loss 1277.4146728515625\n",
      "epoch 80, val_loss 455.05975341796875\n",
      "epoch 81, train_loss 1277.41455078125\n",
      "epoch 81, val_loss 455.0596618652344\n",
      "epoch 82, train_loss 1277.4144287109375\n",
      "epoch 82, val_loss 455.0595703125\n",
      "epoch 83, train_loss 1277.4144287109375\n",
      "epoch 83, val_loss 455.05950927734375\n",
      "epoch 84, train_loss 1277.414306640625\n",
      "epoch 84, val_loss 455.0594482421875\n",
      "epoch 85, train_loss 1277.414306640625\n",
      "epoch 85, val_loss 455.05938720703125\n",
      "epoch 86, train_loss 1277.4140625\n",
      "epoch 86, val_loss 455.0592956542969\n",
      "epoch 87, train_loss 1277.4139404296875\n",
      "epoch 87, val_loss 455.05926513671875\n",
      "epoch 88, train_loss 1277.4140625\n",
      "epoch 88, val_loss 455.0591125488281\n",
      "epoch 89, train_loss 1277.413818359375\n",
      "epoch 89, val_loss 455.05908203125\n",
      "epoch 90, train_loss 1277.413818359375\n",
      "epoch 90, val_loss 455.0589599609375\n",
      "epoch 91, train_loss 1277.4136962890625\n",
      "epoch 91, val_loss 455.0589294433594\n",
      "epoch 92, train_loss 1277.4134521484375\n",
      "epoch 92, val_loss 455.0589294433594\n",
      "epoch 93, train_loss 1277.41357421875\n",
      "epoch 93, val_loss 455.058837890625\n",
      "epoch 94, train_loss 1277.4134521484375\n",
      "epoch 94, val_loss 455.05877685546875\n",
      "epoch 95, train_loss 1277.4134521484375\n",
      "epoch 95, val_loss 455.05865478515625\n",
      "epoch 96, train_loss 1277.413330078125\n",
      "epoch 96, val_loss 455.05853271484375\n",
      "epoch 97, train_loss 1277.4132080078125\n",
      "epoch 97, val_loss 455.05853271484375\n",
      "epoch 98, train_loss 1277.4130859375\n",
      "epoch 98, val_loss 455.0584716796875\n",
      "epoch 99, train_loss 1277.4130859375\n",
      "epoch 99, val_loss 455.0583801269531\n",
      "Parameter containing:\n",
      "tensor([4.8622e-18], requires_grad=True)\n",
      "iter 90, train_loss_regularization 0.7754276394844055\n",
      "iter 90, val_loss_regularization 0.7754276394844055\n",
      "epoch 0, train_loss 1277.4129638671875\n",
      "epoch 0, val_loss 455.05828857421875\n",
      "epoch 1, train_loss 1277.4129638671875\n",
      "epoch 1, val_loss 455.0581970214844\n",
      "epoch 2, train_loss 1277.4127197265625\n",
      "epoch 2, val_loss 455.05816650390625\n",
      "epoch 3, train_loss 1277.4127197265625\n",
      "epoch 3, val_loss 455.0580749511719\n",
      "epoch 4, train_loss 1277.4127197265625\n",
      "epoch 4, val_loss 455.0580139160156\n",
      "epoch 5, train_loss 1277.4124755859375\n",
      "epoch 5, val_loss 455.0579528808594\n",
      "epoch 6, train_loss 1277.41259765625\n",
      "epoch 6, val_loss 455.057861328125\n",
      "epoch 7, train_loss 1277.4124755859375\n",
      "epoch 7, val_loss 455.05780029296875\n",
      "epoch 8, train_loss 1277.412353515625\n",
      "epoch 8, val_loss 455.0577087402344\n",
      "epoch 9, train_loss 1277.4122314453125\n",
      "epoch 9, val_loss 455.05767822265625\n",
      "epoch 10, train_loss 1277.4122314453125\n",
      "epoch 10, val_loss 455.0575866699219\n",
      "epoch 11, train_loss 1277.4119873046875\n",
      "epoch 11, val_loss 455.0574951171875\n",
      "epoch 12, train_loss 1277.4119873046875\n",
      "epoch 12, val_loss 455.0574951171875\n",
      "epoch 13, train_loss 1277.411865234375\n",
      "epoch 13, val_loss 455.0573425292969\n",
      "epoch 14, train_loss 1277.411865234375\n",
      "epoch 14, val_loss 455.0572814941406\n",
      "epoch 15, train_loss 1277.41162109375\n",
      "epoch 15, val_loss 455.0572509765625\n",
      "epoch 16, train_loss 1277.41162109375\n",
      "epoch 16, val_loss 455.0571594238281\n",
      "epoch 17, train_loss 1277.4117431640625\n",
      "epoch 17, val_loss 455.05712890625\n",
      "epoch 18, train_loss 1277.41162109375\n",
      "epoch 18, val_loss 455.0570373535156\n",
      "epoch 19, train_loss 1277.411376953125\n",
      "epoch 19, val_loss 455.0570068359375\n",
      "epoch 20, train_loss 1277.411376953125\n",
      "epoch 20, val_loss 455.05682373046875\n",
      "epoch 21, train_loss 1277.4112548828125\n",
      "epoch 21, val_loss 455.0567626953125\n",
      "epoch 22, train_loss 1277.4111328125\n",
      "epoch 22, val_loss 455.05670166015625\n",
      "epoch 23, train_loss 1277.4111328125\n",
      "epoch 23, val_loss 455.0566711425781\n",
      "epoch 24, train_loss 1277.410888671875\n",
      "epoch 24, val_loss 455.0566101074219\n",
      "epoch 25, train_loss 1277.410888671875\n",
      "epoch 25, val_loss 455.05657958984375\n",
      "epoch 26, train_loss 1277.4107666015625\n",
      "epoch 26, val_loss 455.05645751953125\n",
      "epoch 27, train_loss 1277.410888671875\n",
      "epoch 27, val_loss 455.05633544921875\n",
      "epoch 28, train_loss 1277.41064453125\n",
      "epoch 28, val_loss 455.0563049316406\n",
      "epoch 29, train_loss 1277.4105224609375\n",
      "epoch 29, val_loss 455.05621337890625\n",
      "epoch 30, train_loss 1277.4105224609375\n",
      "epoch 30, val_loss 455.05615234375\n",
      "epoch 31, train_loss 1277.410400390625\n",
      "epoch 31, val_loss 455.05609130859375\n",
      "epoch 32, train_loss 1277.410400390625\n",
      "epoch 32, val_loss 455.0559997558594\n",
      "epoch 33, train_loss 1277.4102783203125\n",
      "epoch 33, val_loss 455.05596923828125\n",
      "epoch 34, train_loss 1277.41015625\n",
      "epoch 34, val_loss 455.0558776855469\n",
      "epoch 35, train_loss 1277.4100341796875\n",
      "epoch 35, val_loss 455.0558776855469\n",
      "epoch 36, train_loss 1277.4100341796875\n",
      "epoch 36, val_loss 455.0557556152344\n",
      "epoch 37, train_loss 1277.4097900390625\n",
      "epoch 37, val_loss 455.0556640625\n",
      "epoch 38, train_loss 1277.4097900390625\n",
      "epoch 38, val_loss 455.0555419921875\n",
      "epoch 39, train_loss 1277.40966796875\n",
      "epoch 39, val_loss 455.0555114746094\n",
      "epoch 40, train_loss 1277.4095458984375\n",
      "epoch 40, val_loss 455.0554504394531\n",
      "epoch 41, train_loss 1277.4095458984375\n",
      "epoch 41, val_loss 455.0553894042969\n",
      "epoch 42, train_loss 1277.4095458984375\n",
      "epoch 42, val_loss 455.0553894042969\n",
      "epoch 43, train_loss 1277.4093017578125\n",
      "epoch 43, val_loss 455.0552978515625\n",
      "epoch 44, train_loss 1277.4093017578125\n",
      "epoch 44, val_loss 455.05517578125\n",
      "epoch 45, train_loss 1277.4093017578125\n",
      "epoch 45, val_loss 455.0550537109375\n",
      "epoch 46, train_loss 1277.4091796875\n",
      "epoch 46, val_loss 455.05499267578125\n",
      "epoch 47, train_loss 1277.4090576171875\n",
      "epoch 47, val_loss 455.05499267578125\n",
      "epoch 48, train_loss 1277.408935546875\n",
      "epoch 48, val_loss 455.05487060546875\n",
      "epoch 49, train_loss 1277.408935546875\n",
      "epoch 49, val_loss 455.05487060546875\n",
      "epoch 50, train_loss 1277.4088134765625\n",
      "epoch 50, val_loss 455.05474853515625\n",
      "epoch 51, train_loss 1277.40869140625\n",
      "epoch 51, val_loss 455.0546569824219\n",
      "epoch 52, train_loss 1277.4085693359375\n",
      "epoch 52, val_loss 455.0545959472656\n",
      "epoch 53, train_loss 1277.408447265625\n",
      "epoch 53, val_loss 455.05450439453125\n",
      "epoch 54, train_loss 1277.408447265625\n",
      "epoch 54, val_loss 455.05450439453125\n",
      "epoch 55, train_loss 1277.408447265625\n",
      "epoch 55, val_loss 455.05438232421875\n",
      "epoch 56, train_loss 1277.4083251953125\n",
      "epoch 56, val_loss 455.0543212890625\n",
      "epoch 57, train_loss 1277.408203125\n",
      "epoch 57, val_loss 455.05426025390625\n",
      "epoch 58, train_loss 1277.408203125\n",
      "epoch 58, val_loss 455.05419921875\n",
      "epoch 59, train_loss 1277.407958984375\n",
      "epoch 59, val_loss 455.0541687011719\n",
      "epoch 60, train_loss 1277.4078369140625\n",
      "epoch 60, val_loss 455.0540771484375\n",
      "epoch 61, train_loss 1277.407958984375\n",
      "epoch 61, val_loss 455.053955078125\n",
      "epoch 62, train_loss 1277.40771484375\n",
      "epoch 62, val_loss 455.0539245605469\n",
      "epoch 63, train_loss 1277.40771484375\n",
      "epoch 63, val_loss 455.0538024902344\n",
      "epoch 64, train_loss 1277.4075927734375\n",
      "epoch 64, val_loss 455.0537414550781\n",
      "epoch 65, train_loss 1277.407470703125\n",
      "epoch 65, val_loss 455.0536804199219\n",
      "epoch 66, train_loss 1277.4073486328125\n",
      "epoch 66, val_loss 455.0535888671875\n",
      "epoch 67, train_loss 1277.4073486328125\n",
      "epoch 67, val_loss 455.0535888671875\n",
      "epoch 68, train_loss 1277.4072265625\n",
      "epoch 68, val_loss 455.0534973144531\n",
      "epoch 69, train_loss 1277.4072265625\n",
      "epoch 69, val_loss 455.0533752441406\n",
      "epoch 70, train_loss 1277.4072265625\n",
      "epoch 70, val_loss 455.0533447265625\n",
      "epoch 71, train_loss 1277.4072265625\n",
      "epoch 71, val_loss 455.05322265625\n",
      "epoch 72, train_loss 1277.406982421875\n",
      "epoch 72, val_loss 455.05322265625\n",
      "epoch 73, train_loss 1277.40673828125\n",
      "epoch 73, val_loss 455.0531311035156\n",
      "epoch 74, train_loss 1277.40673828125\n",
      "epoch 74, val_loss 455.0530700683594\n",
      "epoch 75, train_loss 1277.4066162109375\n",
      "epoch 75, val_loss 455.0530090332031\n",
      "epoch 76, train_loss 1277.40673828125\n",
      "epoch 76, val_loss 455.05291748046875\n",
      "epoch 77, train_loss 1277.4063720703125\n",
      "epoch 77, val_loss 455.0528259277344\n",
      "epoch 78, train_loss 1277.4063720703125\n",
      "epoch 78, val_loss 455.0527648925781\n",
      "epoch 79, train_loss 1277.40625\n",
      "epoch 79, val_loss 455.0527038574219\n",
      "epoch 80, train_loss 1277.40625\n",
      "epoch 80, val_loss 455.0526123046875\n",
      "epoch 81, train_loss 1277.4061279296875\n",
      "epoch 81, val_loss 455.05255126953125\n",
      "epoch 82, train_loss 1277.406005859375\n",
      "epoch 82, val_loss 455.052490234375\n",
      "epoch 83, train_loss 1277.406005859375\n",
      "epoch 83, val_loss 455.05242919921875\n",
      "epoch 84, train_loss 1277.406005859375\n",
      "epoch 84, val_loss 455.0523376464844\n",
      "epoch 85, train_loss 1277.406005859375\n",
      "epoch 85, val_loss 455.0522766113281\n",
      "epoch 86, train_loss 1277.40576171875\n",
      "epoch 86, val_loss 455.0522155761719\n",
      "epoch 87, train_loss 1277.40576171875\n",
      "epoch 87, val_loss 455.0521545410156\n",
      "epoch 88, train_loss 1277.40576171875\n",
      "epoch 88, val_loss 455.0520324707031\n",
      "epoch 89, train_loss 1277.405517578125\n",
      "epoch 89, val_loss 455.0519714355469\n",
      "epoch 90, train_loss 1277.4053955078125\n",
      "epoch 90, val_loss 455.0519104003906\n",
      "epoch 91, train_loss 1277.4052734375\n",
      "epoch 91, val_loss 455.0518798828125\n",
      "epoch 92, train_loss 1277.4052734375\n",
      "epoch 92, val_loss 455.0517578125\n",
      "epoch 93, train_loss 1277.4052734375\n",
      "epoch 93, val_loss 455.0516662597656\n",
      "epoch 94, train_loss 1277.4051513671875\n",
      "epoch 94, val_loss 455.0516662597656\n",
      "epoch 95, train_loss 1277.405029296875\n",
      "epoch 95, val_loss 455.05157470703125\n",
      "epoch 96, train_loss 1277.405029296875\n",
      "epoch 96, val_loss 455.05145263671875\n",
      "epoch 97, train_loss 1277.405029296875\n",
      "epoch 97, val_loss 455.0514221191406\n",
      "epoch 98, train_loss 1277.4049072265625\n",
      "epoch 98, val_loss 455.0513610839844\n",
      "epoch 99, train_loss 1277.4046630859375\n",
      "epoch 99, val_loss 455.05133056640625\n",
      "Parameter containing:\n",
      "tensor([3.3061e-18], requires_grad=True)\n",
      "iter 91, train_loss_regularization 0.7731609344482422\n",
      "iter 91, val_loss_regularization 0.7731609344482422\n",
      "epoch 0, train_loss 1277.40478515625\n",
      "epoch 0, val_loss 455.0512390136719\n",
      "epoch 1, train_loss 1277.4046630859375\n",
      "epoch 1, val_loss 455.05108642578125\n",
      "epoch 2, train_loss 1277.404296875\n",
      "epoch 2, val_loss 455.05108642578125\n",
      "epoch 3, train_loss 1277.4044189453125\n",
      "epoch 3, val_loss 455.05096435546875\n",
      "epoch 4, train_loss 1277.404296875\n",
      "epoch 4, val_loss 455.05096435546875\n",
      "epoch 5, train_loss 1277.404296875\n",
      "epoch 5, val_loss 455.0509033203125\n",
      "epoch 6, train_loss 1277.4041748046875\n",
      "epoch 6, val_loss 455.05078125\n",
      "epoch 7, train_loss 1277.4041748046875\n",
      "epoch 7, val_loss 455.05072021484375\n",
      "epoch 8, train_loss 1277.404052734375\n",
      "epoch 8, val_loss 455.0506286621094\n",
      "epoch 9, train_loss 1277.4039306640625\n",
      "epoch 9, val_loss 455.05059814453125\n",
      "epoch 10, train_loss 1277.4039306640625\n",
      "epoch 10, val_loss 455.050537109375\n",
      "epoch 11, train_loss 1277.4036865234375\n",
      "epoch 11, val_loss 455.0504455566406\n",
      "epoch 12, train_loss 1277.403564453125\n",
      "epoch 12, val_loss 455.0503845214844\n",
      "epoch 13, train_loss 1277.403564453125\n",
      "epoch 13, val_loss 455.0502624511719\n",
      "epoch 14, train_loss 1277.4034423828125\n",
      "epoch 14, val_loss 455.05029296875\n",
      "epoch 15, train_loss 1277.4033203125\n",
      "epoch 15, val_loss 455.0501708984375\n",
      "epoch 16, train_loss 1277.4033203125\n",
      "epoch 16, val_loss 455.0500793457031\n",
      "epoch 17, train_loss 1277.4031982421875\n",
      "epoch 17, val_loss 455.04998779296875\n",
      "epoch 18, train_loss 1277.4031982421875\n",
      "epoch 18, val_loss 455.0499267578125\n",
      "epoch 19, train_loss 1277.4029541015625\n",
      "epoch 19, val_loss 455.04986572265625\n",
      "epoch 20, train_loss 1277.4029541015625\n",
      "epoch 20, val_loss 455.0498352050781\n",
      "epoch 21, train_loss 1277.40283203125\n",
      "epoch 21, val_loss 455.0497131347656\n",
      "epoch 22, train_loss 1277.4027099609375\n",
      "epoch 22, val_loss 455.0496520996094\n",
      "epoch 23, train_loss 1277.4027099609375\n",
      "epoch 23, val_loss 455.0495910644531\n",
      "epoch 24, train_loss 1277.4027099609375\n",
      "epoch 24, val_loss 455.0495910644531\n",
      "epoch 25, train_loss 1277.4024658203125\n",
      "epoch 25, val_loss 455.04949951171875\n",
      "epoch 26, train_loss 1277.4024658203125\n",
      "epoch 26, val_loss 455.0492858886719\n",
      "epoch 27, train_loss 1277.4024658203125\n",
      "epoch 27, val_loss 455.0492858886719\n",
      "epoch 28, train_loss 1277.4022216796875\n",
      "epoch 28, val_loss 455.0491943359375\n",
      "epoch 29, train_loss 1277.4022216796875\n",
      "epoch 29, val_loss 455.0491638183594\n",
      "epoch 30, train_loss 1277.402099609375\n",
      "epoch 30, val_loss 455.0491638183594\n",
      "epoch 31, train_loss 1277.4019775390625\n",
      "epoch 31, val_loss 455.04901123046875\n",
      "epoch 32, train_loss 1277.4019775390625\n",
      "epoch 32, val_loss 455.04901123046875\n",
      "epoch 33, train_loss 1277.4019775390625\n",
      "epoch 33, val_loss 455.048828125\n",
      "epoch 34, train_loss 1277.40185546875\n",
      "epoch 34, val_loss 455.0487976074219\n",
      "epoch 35, train_loss 1277.401611328125\n",
      "epoch 35, val_loss 455.0487976074219\n",
      "epoch 36, train_loss 1277.401611328125\n",
      "epoch 36, val_loss 455.0486755371094\n",
      "epoch 37, train_loss 1277.401611328125\n",
      "epoch 37, val_loss 455.0486145019531\n",
      "epoch 38, train_loss 1277.4013671875\n",
      "epoch 38, val_loss 455.0485534667969\n",
      "epoch 39, train_loss 1277.4013671875\n",
      "epoch 39, val_loss 455.0484619140625\n",
      "epoch 40, train_loss 1277.401123046875\n",
      "epoch 40, val_loss 455.0483703613281\n",
      "epoch 41, train_loss 1277.401123046875\n",
      "epoch 41, val_loss 455.04833984375\n",
      "epoch 42, train_loss 1277.401123046875\n",
      "epoch 42, val_loss 455.0482482910156\n",
      "epoch 43, train_loss 1277.401123046875\n",
      "epoch 43, val_loss 455.04815673828125\n",
      "epoch 44, train_loss 1277.40087890625\n",
      "epoch 44, val_loss 455.0481262207031\n",
      "epoch 45, train_loss 1277.40087890625\n",
      "epoch 45, val_loss 455.04803466796875\n",
      "epoch 46, train_loss 1277.40087890625\n",
      "epoch 46, val_loss 455.0480041503906\n",
      "epoch 47, train_loss 1277.40087890625\n",
      "epoch 47, val_loss 455.04791259765625\n",
      "epoch 48, train_loss 1277.400634765625\n",
      "epoch 48, val_loss 455.0478820800781\n",
      "epoch 49, train_loss 1277.4005126953125\n",
      "epoch 49, val_loss 455.04779052734375\n",
      "epoch 50, train_loss 1277.400390625\n",
      "epoch 50, val_loss 455.0477600097656\n",
      "epoch 51, train_loss 1277.4002685546875\n",
      "epoch 51, val_loss 455.0475769042969\n",
      "epoch 52, train_loss 1277.400146484375\n",
      "epoch 52, val_loss 455.0474853515625\n",
      "epoch 53, train_loss 1277.4000244140625\n",
      "epoch 53, val_loss 455.0474548339844\n",
      "epoch 54, train_loss 1277.4000244140625\n",
      "epoch 54, val_loss 455.04742431640625\n",
      "epoch 55, train_loss 1277.400146484375\n",
      "epoch 55, val_loss 455.04736328125\n",
      "epoch 56, train_loss 1277.400146484375\n",
      "epoch 56, val_loss 455.04730224609375\n",
      "epoch 57, train_loss 1277.39990234375\n",
      "epoch 57, val_loss 455.0472106933594\n",
      "epoch 58, train_loss 1277.39990234375\n",
      "epoch 58, val_loss 455.04718017578125\n",
      "epoch 59, train_loss 1277.3997802734375\n",
      "epoch 59, val_loss 455.0469970703125\n",
      "epoch 60, train_loss 1277.3997802734375\n",
      "epoch 60, val_loss 455.0470275878906\n",
      "epoch 61, train_loss 1277.3995361328125\n",
      "epoch 61, val_loss 455.0469055175781\n",
      "epoch 62, train_loss 1277.3995361328125\n",
      "epoch 62, val_loss 455.046875\n",
      "epoch 63, train_loss 1277.3994140625\n",
      "epoch 63, val_loss 455.0467834472656\n",
      "epoch 64, train_loss 1277.3994140625\n",
      "epoch 64, val_loss 455.0467224121094\n",
      "epoch 65, train_loss 1277.3992919921875\n",
      "epoch 65, val_loss 455.0466613769531\n",
      "epoch 66, train_loss 1277.3992919921875\n",
      "epoch 66, val_loss 455.04656982421875\n",
      "epoch 67, train_loss 1277.399169921875\n",
      "epoch 67, val_loss 455.0465087890625\n",
      "epoch 68, train_loss 1277.3990478515625\n",
      "epoch 68, val_loss 455.04644775390625\n",
      "epoch 69, train_loss 1277.3990478515625\n",
      "epoch 69, val_loss 455.04632568359375\n",
      "epoch 70, train_loss 1277.3988037109375\n",
      "epoch 70, val_loss 455.0462951660156\n",
      "epoch 71, train_loss 1277.3988037109375\n",
      "epoch 71, val_loss 455.0462646484375\n",
      "epoch 72, train_loss 1277.398681640625\n",
      "epoch 72, val_loss 455.0461120605469\n",
      "epoch 73, train_loss 1277.398681640625\n",
      "epoch 73, val_loss 455.0461120605469\n",
      "epoch 74, train_loss 1277.3984375\n",
      "epoch 74, val_loss 455.0460510253906\n",
      "epoch 75, train_loss 1277.3984375\n",
      "epoch 75, val_loss 455.0459289550781\n",
      "epoch 76, train_loss 1277.3983154296875\n",
      "epoch 76, val_loss 455.0459289550781\n",
      "epoch 77, train_loss 1277.398193359375\n",
      "epoch 77, val_loss 455.0457458496094\n",
      "epoch 78, train_loss 1277.398193359375\n",
      "epoch 78, val_loss 455.0457458496094\n",
      "epoch 79, train_loss 1277.398193359375\n",
      "epoch 79, val_loss 455.045654296875\n",
      "epoch 80, train_loss 1277.39794921875\n",
      "epoch 80, val_loss 455.04559326171875\n",
      "epoch 81, train_loss 1277.39794921875\n",
      "epoch 81, val_loss 455.04559326171875\n",
      "epoch 82, train_loss 1277.39794921875\n",
      "epoch 82, val_loss 455.0455017089844\n",
      "epoch 83, train_loss 1277.397705078125\n",
      "epoch 83, val_loss 455.0453186035156\n",
      "epoch 84, train_loss 1277.397705078125\n",
      "epoch 84, val_loss 455.0452880859375\n",
      "epoch 85, train_loss 1277.397705078125\n",
      "epoch 85, val_loss 455.0452575683594\n",
      "epoch 86, train_loss 1277.397705078125\n",
      "epoch 86, val_loss 455.0451965332031\n",
      "epoch 87, train_loss 1277.3973388671875\n",
      "epoch 87, val_loss 455.0451354980469\n",
      "epoch 88, train_loss 1277.397216796875\n",
      "epoch 88, val_loss 455.0450744628906\n",
      "epoch 89, train_loss 1277.397216796875\n",
      "epoch 89, val_loss 455.0449523925781\n",
      "epoch 90, train_loss 1277.397216796875\n",
      "epoch 90, val_loss 455.044921875\n",
      "epoch 91, train_loss 1277.397216796875\n",
      "epoch 91, val_loss 455.0448303222656\n",
      "epoch 92, train_loss 1277.39697265625\n",
      "epoch 92, val_loss 455.04473876953125\n",
      "epoch 93, train_loss 1277.3968505859375\n",
      "epoch 93, val_loss 455.0447082519531\n",
      "epoch 94, train_loss 1277.39697265625\n",
      "epoch 94, val_loss 455.04461669921875\n",
      "epoch 95, train_loss 1277.39697265625\n",
      "epoch 95, val_loss 455.0445861816406\n",
      "epoch 96, train_loss 1277.396484375\n",
      "epoch 96, val_loss 455.0444641113281\n",
      "epoch 97, train_loss 1277.3966064453125\n",
      "epoch 97, val_loss 455.0444030761719\n",
      "epoch 98, train_loss 1277.396484375\n",
      "epoch 98, val_loss 455.0443420410156\n",
      "epoch 99, train_loss 1277.396484375\n",
      "epoch 99, val_loss 455.04425048828125\n",
      "Parameter containing:\n",
      "tensor([2.2496e-18], requires_grad=True)\n",
      "iter 92, train_loss_regularization 0.770936131477356\n",
      "iter 92, val_loss_regularization 0.770936131477356\n",
      "epoch 0, train_loss 1277.3963623046875\n",
      "epoch 0, val_loss 455.04425048828125\n",
      "epoch 1, train_loss 1277.396240234375\n",
      "epoch 1, val_loss 455.0441589355469\n",
      "epoch 2, train_loss 1277.3961181640625\n",
      "epoch 2, val_loss 455.0440368652344\n",
      "epoch 3, train_loss 1277.3961181640625\n",
      "epoch 3, val_loss 455.04400634765625\n",
      "epoch 4, train_loss 1277.3961181640625\n",
      "epoch 4, val_loss 455.0439147949219\n",
      "epoch 5, train_loss 1277.39599609375\n",
      "epoch 5, val_loss 455.04388427734375\n",
      "epoch 6, train_loss 1277.395751953125\n",
      "epoch 6, val_loss 455.0438232421875\n",
      "epoch 7, train_loss 1277.395751953125\n",
      "epoch 7, val_loss 455.04376220703125\n",
      "epoch 8, train_loss 1277.395751953125\n",
      "epoch 8, val_loss 455.0435791015625\n",
      "epoch 9, train_loss 1277.3955078125\n",
      "epoch 9, val_loss 455.0435485839844\n",
      "epoch 10, train_loss 1277.3953857421875\n",
      "epoch 10, val_loss 455.04345703125\n",
      "epoch 11, train_loss 1277.3953857421875\n",
      "epoch 11, val_loss 455.04345703125\n",
      "epoch 12, train_loss 1277.3953857421875\n",
      "epoch 12, val_loss 455.0433654785156\n",
      "epoch 13, train_loss 1277.395263671875\n",
      "epoch 13, val_loss 455.0433349609375\n",
      "epoch 14, train_loss 1277.3951416015625\n",
      "epoch 14, val_loss 455.0433044433594\n",
      "epoch 15, train_loss 1277.3951416015625\n",
      "epoch 15, val_loss 455.0431213378906\n",
      "epoch 16, train_loss 1277.39501953125\n",
      "epoch 16, val_loss 455.0430908203125\n",
      "epoch 17, train_loss 1277.394775390625\n",
      "epoch 17, val_loss 455.0429992675781\n",
      "epoch 18, train_loss 1277.394775390625\n",
      "epoch 18, val_loss 455.04296875\n",
      "epoch 19, train_loss 1277.394775390625\n",
      "epoch 19, val_loss 455.04290771484375\n",
      "epoch 20, train_loss 1277.394775390625\n",
      "epoch 20, val_loss 455.0428466796875\n",
      "epoch 21, train_loss 1277.394775390625\n",
      "epoch 21, val_loss 455.04278564453125\n",
      "epoch 22, train_loss 1277.39453125\n",
      "epoch 22, val_loss 455.04266357421875\n",
      "epoch 23, train_loss 1277.394287109375\n",
      "epoch 23, val_loss 455.0426330566406\n",
      "epoch 24, train_loss 1277.3944091796875\n",
      "epoch 24, val_loss 455.04254150390625\n",
      "epoch 25, train_loss 1277.394287109375\n",
      "epoch 25, val_loss 455.0424499511719\n",
      "epoch 26, train_loss 1277.39404296875\n",
      "epoch 26, val_loss 455.04241943359375\n",
      "epoch 27, train_loss 1277.39404296875\n",
      "epoch 27, val_loss 455.0423278808594\n",
      "epoch 28, train_loss 1277.39404296875\n",
      "epoch 28, val_loss 455.042236328125\n",
      "epoch 29, train_loss 1277.39404296875\n",
      "epoch 29, val_loss 455.04217529296875\n",
      "epoch 30, train_loss 1277.3939208984375\n",
      "epoch 30, val_loss 455.0421142578125\n",
      "epoch 31, train_loss 1277.3939208984375\n",
      "epoch 31, val_loss 455.04205322265625\n",
      "epoch 32, train_loss 1277.3936767578125\n",
      "epoch 32, val_loss 455.0419616699219\n",
      "epoch 33, train_loss 1277.393798828125\n",
      "epoch 33, val_loss 455.04193115234375\n",
      "epoch 34, train_loss 1277.3935546875\n",
      "epoch 34, val_loss 455.0417785644531\n",
      "epoch 35, train_loss 1277.3935546875\n",
      "epoch 35, val_loss 455.041748046875\n",
      "epoch 36, train_loss 1277.393310546875\n",
      "epoch 36, val_loss 455.0417175292969\n",
      "epoch 37, train_loss 1277.393310546875\n",
      "epoch 37, val_loss 455.0416564941406\n",
      "epoch 38, train_loss 1277.3934326171875\n",
      "epoch 38, val_loss 455.0415954589844\n",
      "epoch 39, train_loss 1277.3931884765625\n",
      "epoch 39, val_loss 455.0415344238281\n",
      "epoch 40, train_loss 1277.3931884765625\n",
      "epoch 40, val_loss 455.0414733886719\n",
      "epoch 41, train_loss 1277.3929443359375\n",
      "epoch 41, val_loss 455.04132080078125\n",
      "epoch 42, train_loss 1277.39306640625\n",
      "epoch 42, val_loss 455.04132080078125\n",
      "epoch 43, train_loss 1277.3929443359375\n",
      "epoch 43, val_loss 455.04119873046875\n",
      "epoch 44, train_loss 1277.3929443359375\n",
      "epoch 44, val_loss 455.0411682128906\n",
      "epoch 45, train_loss 1277.392822265625\n",
      "epoch 45, val_loss 455.04107666015625\n",
      "epoch 46, train_loss 1277.392578125\n",
      "epoch 46, val_loss 455.0409851074219\n",
      "epoch 47, train_loss 1277.392578125\n",
      "epoch 47, val_loss 455.04095458984375\n",
      "epoch 48, train_loss 1277.3924560546875\n",
      "epoch 48, val_loss 455.0409240722656\n",
      "epoch 49, train_loss 1277.3924560546875\n",
      "epoch 49, val_loss 455.04083251953125\n",
      "epoch 50, train_loss 1277.3922119140625\n",
      "epoch 50, val_loss 455.04071044921875\n",
      "epoch 51, train_loss 1277.3922119140625\n",
      "epoch 51, val_loss 455.0406799316406\n",
      "epoch 52, train_loss 1277.39208984375\n",
      "epoch 52, val_loss 455.0406188964844\n",
      "epoch 53, train_loss 1277.3919677734375\n",
      "epoch 53, val_loss 455.0404968261719\n",
      "epoch 54, train_loss 1277.3919677734375\n",
      "epoch 54, val_loss 455.04046630859375\n",
      "epoch 55, train_loss 1277.3919677734375\n",
      "epoch 55, val_loss 455.0403747558594\n",
      "epoch 56, train_loss 1277.3917236328125\n",
      "epoch 56, val_loss 455.04034423828125\n",
      "epoch 57, train_loss 1277.3917236328125\n",
      "epoch 57, val_loss 455.040283203125\n",
      "epoch 58, train_loss 1277.391845703125\n",
      "epoch 58, val_loss 455.0401611328125\n",
      "epoch 59, train_loss 1277.3916015625\n",
      "epoch 59, val_loss 455.0401611328125\n",
      "epoch 60, train_loss 1277.391357421875\n",
      "epoch 60, val_loss 455.0400390625\n",
      "epoch 61, train_loss 1277.391357421875\n",
      "epoch 61, val_loss 455.0399475097656\n",
      "epoch 62, train_loss 1277.391357421875\n",
      "epoch 62, val_loss 455.0399475097656\n",
      "epoch 63, train_loss 1277.39111328125\n",
      "epoch 63, val_loss 455.0398864746094\n",
      "epoch 64, train_loss 1277.39111328125\n",
      "epoch 64, val_loss 455.0397644042969\n",
      "epoch 65, train_loss 1277.39111328125\n",
      "epoch 65, val_loss 455.0396728515625\n",
      "epoch 66, train_loss 1277.3909912109375\n",
      "epoch 66, val_loss 455.0395812988281\n",
      "epoch 67, train_loss 1277.390869140625\n",
      "epoch 67, val_loss 455.03955078125\n",
      "epoch 68, train_loss 1277.3909912109375\n",
      "epoch 68, val_loss 455.03948974609375\n",
      "epoch 69, train_loss 1277.3907470703125\n",
      "epoch 69, val_loss 455.0394287109375\n",
      "epoch 70, train_loss 1277.390625\n",
      "epoch 70, val_loss 455.03936767578125\n",
      "epoch 71, train_loss 1277.390625\n",
      "epoch 71, val_loss 455.03924560546875\n",
      "epoch 72, train_loss 1277.3905029296875\n",
      "epoch 72, val_loss 455.0392150878906\n",
      "epoch 73, train_loss 1277.390380859375\n",
      "epoch 73, val_loss 455.0391540527344\n",
      "epoch 74, train_loss 1277.390380859375\n",
      "epoch 74, val_loss 455.0390930175781\n",
      "epoch 75, train_loss 1277.39013671875\n",
      "epoch 75, val_loss 455.03900146484375\n",
      "epoch 76, train_loss 1277.39013671875\n",
      "epoch 76, val_loss 455.0389099121094\n",
      "epoch 77, train_loss 1277.39013671875\n",
      "epoch 77, val_loss 455.03887939453125\n",
      "epoch 78, train_loss 1277.39013671875\n",
      "epoch 78, val_loss 455.038818359375\n",
      "epoch 79, train_loss 1277.3900146484375\n",
      "epoch 79, val_loss 455.0386962890625\n",
      "epoch 80, train_loss 1277.389892578125\n",
      "epoch 80, val_loss 455.0386657714844\n",
      "epoch 81, train_loss 1277.389892578125\n",
      "epoch 81, val_loss 455.03857421875\n",
      "epoch 82, train_loss 1277.3896484375\n",
      "epoch 82, val_loss 455.0385437011719\n",
      "epoch 83, train_loss 1277.3895263671875\n",
      "epoch 83, val_loss 455.0384216308594\n",
      "epoch 84, train_loss 1277.3895263671875\n",
      "epoch 84, val_loss 455.0384216308594\n",
      "epoch 85, train_loss 1277.389404296875\n",
      "epoch 85, val_loss 455.0382995605469\n",
      "epoch 86, train_loss 1277.3892822265625\n",
      "epoch 86, val_loss 455.0382080078125\n",
      "epoch 87, train_loss 1277.3892822265625\n",
      "epoch 87, val_loss 455.0381774902344\n",
      "epoch 88, train_loss 1277.3892822265625\n",
      "epoch 88, val_loss 455.0380859375\n",
      "epoch 89, train_loss 1277.3890380859375\n",
      "epoch 89, val_loss 455.0380859375\n",
      "epoch 90, train_loss 1277.3887939453125\n",
      "epoch 90, val_loss 455.0379943847656\n",
      "epoch 91, train_loss 1277.388916015625\n",
      "epoch 91, val_loss 455.037841796875\n",
      "epoch 92, train_loss 1277.388916015625\n",
      "epoch 92, val_loss 455.03778076171875\n",
      "epoch 93, train_loss 1277.3887939453125\n",
      "epoch 93, val_loss 455.0377502441406\n",
      "epoch 94, train_loss 1277.3887939453125\n",
      "epoch 94, val_loss 455.0377197265625\n",
      "epoch 95, train_loss 1277.388671875\n",
      "epoch 95, val_loss 455.03765869140625\n",
      "epoch 96, train_loss 1277.3885498046875\n",
      "epoch 96, val_loss 455.03753662109375\n",
      "epoch 97, train_loss 1277.3885498046875\n",
      "epoch 97, val_loss 455.0375061035156\n",
      "epoch 98, train_loss 1277.3885498046875\n",
      "epoch 98, val_loss 455.03741455078125\n",
      "epoch 99, train_loss 1277.3883056640625\n",
      "epoch 99, val_loss 455.0373840332031\n",
      "Parameter containing:\n",
      "tensor([1.5317e-18], requires_grad=True)\n",
      "iter 93, train_loss_regularization 0.7687525749206543\n",
      "iter 93, val_loss_regularization 0.7687525749206543\n",
      "epoch 0, train_loss 1277.3880615234375\n",
      "epoch 0, val_loss 455.0372619628906\n",
      "epoch 1, train_loss 1277.3880615234375\n",
      "epoch 1, val_loss 455.0372009277344\n",
      "epoch 2, train_loss 1277.3880615234375\n",
      "epoch 2, val_loss 455.03717041015625\n",
      "epoch 3, train_loss 1277.3880615234375\n",
      "epoch 3, val_loss 455.0370788574219\n",
      "epoch 4, train_loss 1277.387939453125\n",
      "epoch 4, val_loss 455.0369873046875\n",
      "epoch 5, train_loss 1277.387939453125\n",
      "epoch 5, val_loss 455.0369567871094\n",
      "epoch 6, train_loss 1277.3876953125\n",
      "epoch 6, val_loss 455.036865234375\n",
      "epoch 7, train_loss 1277.3878173828125\n",
      "epoch 7, val_loss 455.03680419921875\n",
      "epoch 8, train_loss 1277.3876953125\n",
      "epoch 8, val_loss 455.0367431640625\n",
      "epoch 9, train_loss 1277.387451171875\n",
      "epoch 9, val_loss 455.03662109375\n",
      "epoch 10, train_loss 1277.3875732421875\n",
      "epoch 10, val_loss 455.0365905761719\n",
      "epoch 11, train_loss 1277.387451171875\n",
      "epoch 11, val_loss 455.0365295410156\n",
      "epoch 12, train_loss 1277.38720703125\n",
      "epoch 12, val_loss 455.0364074707031\n",
      "epoch 13, train_loss 1277.38720703125\n",
      "epoch 13, val_loss 455.0364685058594\n",
      "epoch 14, train_loss 1277.38720703125\n",
      "epoch 14, val_loss 455.0363464355469\n",
      "epoch 15, train_loss 1277.3870849609375\n",
      "epoch 15, val_loss 455.0362854003906\n",
      "epoch 16, train_loss 1277.386962890625\n",
      "epoch 16, val_loss 455.0362548828125\n",
      "epoch 17, train_loss 1277.386962890625\n",
      "epoch 17, val_loss 455.03607177734375\n",
      "epoch 18, train_loss 1277.386962890625\n",
      "epoch 18, val_loss 455.0360412597656\n",
      "epoch 19, train_loss 1277.38671875\n",
      "epoch 19, val_loss 455.03594970703125\n",
      "epoch 20, train_loss 1277.3865966796875\n",
      "epoch 20, val_loss 455.03594970703125\n",
      "epoch 21, train_loss 1277.38671875\n",
      "epoch 21, val_loss 455.035888671875\n",
      "epoch 22, train_loss 1277.386474609375\n",
      "epoch 22, val_loss 455.0357971191406\n",
      "epoch 23, train_loss 1277.386474609375\n",
      "epoch 23, val_loss 455.03570556640625\n",
      "epoch 24, train_loss 1277.3863525390625\n",
      "epoch 24, val_loss 455.03558349609375\n",
      "epoch 25, train_loss 1277.38623046875\n",
      "epoch 25, val_loss 455.0354919433594\n",
      "epoch 26, train_loss 1277.3861083984375\n",
      "epoch 26, val_loss 455.0354919433594\n",
      "epoch 27, train_loss 1277.385986328125\n",
      "epoch 27, val_loss 455.0354309082031\n",
      "epoch 28, train_loss 1277.3861083984375\n",
      "epoch 28, val_loss 455.0353698730469\n",
      "epoch 29, train_loss 1277.3858642578125\n",
      "epoch 29, val_loss 455.03533935546875\n",
      "epoch 30, train_loss 1277.3858642578125\n",
      "epoch 30, val_loss 455.03515625\n",
      "epoch 31, train_loss 1277.3857421875\n",
      "epoch 31, val_loss 455.03521728515625\n",
      "epoch 32, train_loss 1277.3857421875\n",
      "epoch 32, val_loss 455.0351257324219\n",
      "epoch 33, train_loss 1277.3856201171875\n",
      "epoch 33, val_loss 455.0350036621094\n",
      "epoch 34, train_loss 1277.385498046875\n",
      "epoch 34, val_loss 455.03497314453125\n",
      "epoch 35, train_loss 1277.3853759765625\n",
      "epoch 35, val_loss 455.0348205566406\n",
      "epoch 36, train_loss 1277.3853759765625\n",
      "epoch 36, val_loss 455.0348205566406\n",
      "epoch 37, train_loss 1277.38525390625\n",
      "epoch 37, val_loss 455.0346984863281\n",
      "epoch 38, train_loss 1277.3851318359375\n",
      "epoch 38, val_loss 455.0346374511719\n",
      "epoch 39, train_loss 1277.38525390625\n",
      "epoch 39, val_loss 455.0346374511719\n",
      "epoch 40, train_loss 1277.385009765625\n",
      "epoch 40, val_loss 455.0345458984375\n",
      "epoch 41, train_loss 1277.385009765625\n",
      "epoch 41, val_loss 455.03448486328125\n",
      "epoch 42, train_loss 1277.3848876953125\n",
      "epoch 42, val_loss 455.034423828125\n",
      "epoch 43, train_loss 1277.384765625\n",
      "epoch 43, val_loss 455.0343017578125\n",
      "epoch 44, train_loss 1277.3846435546875\n",
      "epoch 44, val_loss 455.03424072265625\n",
      "epoch 45, train_loss 1277.3846435546875\n",
      "epoch 45, val_loss 455.0341796875\n",
      "epoch 46, train_loss 1277.3846435546875\n",
      "epoch 46, val_loss 455.0341796875\n",
      "epoch 47, train_loss 1277.384521484375\n",
      "epoch 47, val_loss 455.0340881347656\n",
      "epoch 48, train_loss 1277.3843994140625\n",
      "epoch 48, val_loss 455.03399658203125\n",
      "epoch 49, train_loss 1277.3841552734375\n",
      "epoch 49, val_loss 455.03399658203125\n",
      "epoch 50, train_loss 1277.3841552734375\n",
      "epoch 50, val_loss 455.0337829589844\n",
      "epoch 51, train_loss 1277.3841552734375\n",
      "epoch 51, val_loss 455.0337829589844\n",
      "epoch 52, train_loss 1277.384033203125\n",
      "epoch 52, val_loss 455.03375244140625\n",
      "epoch 53, train_loss 1277.384033203125\n",
      "epoch 53, val_loss 455.0336608886719\n",
      "epoch 54, train_loss 1277.3839111328125\n",
      "epoch 54, val_loss 455.0335693359375\n",
      "epoch 55, train_loss 1277.3837890625\n",
      "epoch 55, val_loss 455.0335388183594\n",
      "epoch 56, train_loss 1277.3837890625\n",
      "epoch 56, val_loss 455.0334167480469\n",
      "epoch 57, train_loss 1277.3837890625\n",
      "epoch 57, val_loss 455.03338623046875\n",
      "epoch 58, train_loss 1277.3834228515625\n",
      "epoch 58, val_loss 455.0333251953125\n",
      "epoch 59, train_loss 1277.383544921875\n",
      "epoch 59, val_loss 455.033203125\n",
      "epoch 60, train_loss 1277.3834228515625\n",
      "epoch 60, val_loss 455.0331726074219\n",
      "epoch 61, train_loss 1277.3834228515625\n",
      "epoch 61, val_loss 455.0331115722656\n",
      "epoch 62, train_loss 1277.38330078125\n",
      "epoch 62, val_loss 455.0329895019531\n",
      "epoch 63, train_loss 1277.383056640625\n",
      "epoch 63, val_loss 455.032958984375\n",
      "epoch 64, train_loss 1277.383056640625\n",
      "epoch 64, val_loss 455.0328674316406\n",
      "epoch 65, train_loss 1277.383056640625\n",
      "epoch 65, val_loss 455.0328674316406\n",
      "epoch 66, train_loss 1277.383056640625\n",
      "epoch 66, val_loss 455.0328063964844\n",
      "epoch 67, train_loss 1277.3829345703125\n",
      "epoch 67, val_loss 455.03271484375\n",
      "epoch 68, train_loss 1277.3829345703125\n",
      "epoch 68, val_loss 455.0326232910156\n",
      "epoch 69, train_loss 1277.3826904296875\n",
      "epoch 69, val_loss 455.03253173828125\n",
      "epoch 70, train_loss 1277.3826904296875\n",
      "epoch 70, val_loss 455.032470703125\n",
      "epoch 71, train_loss 1277.3826904296875\n",
      "epoch 71, val_loss 455.03240966796875\n",
      "epoch 72, train_loss 1277.3824462890625\n",
      "epoch 72, val_loss 455.0323791503906\n",
      "epoch 73, train_loss 1277.3822021484375\n",
      "epoch 73, val_loss 455.03228759765625\n",
      "epoch 74, train_loss 1277.3824462890625\n",
      "epoch 74, val_loss 455.03228759765625\n",
      "epoch 75, train_loss 1277.38232421875\n",
      "epoch 75, val_loss 455.03216552734375\n",
      "epoch 76, train_loss 1277.3822021484375\n",
      "epoch 76, val_loss 455.03204345703125\n",
      "epoch 77, train_loss 1277.382080078125\n",
      "epoch 77, val_loss 455.0320129394531\n",
      "epoch 78, train_loss 1277.3819580078125\n",
      "epoch 78, val_loss 455.0319519042969\n",
      "epoch 79, train_loss 1277.3819580078125\n",
      "epoch 79, val_loss 455.0318603515625\n",
      "epoch 80, train_loss 1277.3819580078125\n",
      "epoch 80, val_loss 455.0318298339844\n",
      "epoch 81, train_loss 1277.3818359375\n",
      "epoch 81, val_loss 455.03173828125\n",
      "epoch 82, train_loss 1277.3817138671875\n",
      "epoch 82, val_loss 455.0317077636719\n",
      "epoch 83, train_loss 1277.381591796875\n",
      "epoch 83, val_loss 455.0316162109375\n",
      "epoch 84, train_loss 1277.3817138671875\n",
      "epoch 84, val_loss 455.031494140625\n",
      "epoch 85, train_loss 1277.3814697265625\n",
      "epoch 85, val_loss 455.0314636230469\n",
      "epoch 86, train_loss 1277.3812255859375\n",
      "epoch 86, val_loss 455.0313720703125\n",
      "epoch 87, train_loss 1277.3812255859375\n",
      "epoch 87, val_loss 455.0312805175781\n",
      "epoch 88, train_loss 1277.3812255859375\n",
      "epoch 88, val_loss 455.0312194824219\n",
      "epoch 89, train_loss 1277.3812255859375\n",
      "epoch 89, val_loss 455.0312194824219\n",
      "epoch 90, train_loss 1277.381103515625\n",
      "epoch 90, val_loss 455.0311279296875\n",
      "epoch 91, train_loss 1277.381103515625\n",
      "epoch 91, val_loss 455.0310363769531\n",
      "epoch 92, train_loss 1277.3809814453125\n",
      "epoch 92, val_loss 455.0309143066406\n",
      "epoch 93, train_loss 1277.380859375\n",
      "epoch 93, val_loss 455.0309143066406\n",
      "epoch 94, train_loss 1277.380615234375\n",
      "epoch 94, val_loss 455.03082275390625\n",
      "epoch 95, train_loss 1277.3804931640625\n",
      "epoch 95, val_loss 455.03076171875\n",
      "epoch 96, train_loss 1277.380615234375\n",
      "epoch 96, val_loss 455.0306701660156\n",
      "epoch 97, train_loss 1277.3804931640625\n",
      "epoch 97, val_loss 455.0306396484375\n",
      "epoch 98, train_loss 1277.3804931640625\n",
      "epoch 98, val_loss 455.03057861328125\n",
      "epoch 99, train_loss 1277.3802490234375\n",
      "epoch 99, val_loss 455.0304870605469\n",
      "Parameter containing:\n",
      "tensor([1.0436e-18], requires_grad=True)\n",
      "iter 94, train_loss_regularization 0.7666096687316895\n",
      "iter 94, val_loss_regularization 0.7666096687316895\n",
      "epoch 0, train_loss 1277.38037109375\n",
      "epoch 0, val_loss 455.0304260253906\n",
      "epoch 1, train_loss 1277.3802490234375\n",
      "epoch 1, val_loss 455.0303039550781\n",
      "epoch 2, train_loss 1277.380126953125\n",
      "epoch 2, val_loss 455.0303039550781\n",
      "epoch 3, train_loss 1277.3800048828125\n",
      "epoch 3, val_loss 455.0302429199219\n",
      "epoch 4, train_loss 1277.3800048828125\n",
      "epoch 4, val_loss 455.0301818847656\n",
      "epoch 5, train_loss 1277.3800048828125\n",
      "epoch 5, val_loss 455.0301208496094\n",
      "epoch 6, train_loss 1277.3798828125\n",
      "epoch 6, val_loss 455.03009033203125\n",
      "epoch 7, train_loss 1277.3797607421875\n",
      "epoch 7, val_loss 455.02996826171875\n",
      "epoch 8, train_loss 1277.3795166015625\n",
      "epoch 8, val_loss 455.0298767089844\n",
      "epoch 9, train_loss 1277.3795166015625\n",
      "epoch 9, val_loss 455.02984619140625\n",
      "epoch 10, train_loss 1277.3795166015625\n",
      "epoch 10, val_loss 455.0297546386719\n",
      "epoch 11, train_loss 1277.3795166015625\n",
      "epoch 11, val_loss 455.0296630859375\n",
      "epoch 12, train_loss 1277.379150390625\n",
      "epoch 12, val_loss 455.0296325683594\n",
      "epoch 13, train_loss 1277.379150390625\n",
      "epoch 13, val_loss 455.0295715332031\n",
      "epoch 14, train_loss 1277.379150390625\n",
      "epoch 14, val_loss 455.0294494628906\n",
      "epoch 15, train_loss 1277.379150390625\n",
      "epoch 15, val_loss 455.0294494628906\n",
      "epoch 16, train_loss 1277.3790283203125\n",
      "epoch 16, val_loss 455.0293884277344\n",
      "epoch 17, train_loss 1277.3787841796875\n",
      "epoch 17, val_loss 455.02923583984375\n",
      "epoch 18, train_loss 1277.3787841796875\n",
      "epoch 18, val_loss 455.0292053222656\n",
      "epoch 19, train_loss 1277.3787841796875\n",
      "epoch 19, val_loss 455.0291748046875\n",
      "epoch 20, train_loss 1277.3787841796875\n",
      "epoch 20, val_loss 455.029052734375\n",
      "epoch 21, train_loss 1277.378662109375\n",
      "epoch 21, val_loss 455.02899169921875\n",
      "epoch 22, train_loss 1277.37841796875\n",
      "epoch 22, val_loss 455.0289611816406\n",
      "epoch 23, train_loss 1277.3782958984375\n",
      "epoch 23, val_loss 455.0288391113281\n",
      "epoch 24, train_loss 1277.3782958984375\n",
      "epoch 24, val_loss 455.0288391113281\n",
      "epoch 25, train_loss 1277.3782958984375\n",
      "epoch 25, val_loss 455.0287170410156\n",
      "epoch 26, train_loss 1277.378173828125\n",
      "epoch 26, val_loss 455.0286560058594\n",
      "epoch 27, train_loss 1277.3780517578125\n",
      "epoch 27, val_loss 455.0285339355469\n",
      "epoch 28, train_loss 1277.3780517578125\n",
      "epoch 28, val_loss 455.02850341796875\n",
      "epoch 29, train_loss 1277.3780517578125\n",
      "epoch 29, val_loss 455.0284729003906\n",
      "epoch 30, train_loss 1277.3779296875\n",
      "epoch 30, val_loss 455.02838134765625\n",
      "epoch 31, train_loss 1277.3778076171875\n",
      "epoch 31, val_loss 455.02838134765625\n",
      "epoch 32, train_loss 1277.3778076171875\n",
      "epoch 32, val_loss 455.0283203125\n",
      "epoch 33, train_loss 1277.377685546875\n",
      "epoch 33, val_loss 455.02825927734375\n",
      "epoch 34, train_loss 1277.37744140625\n",
      "epoch 34, val_loss 455.028076171875\n",
      "epoch 35, train_loss 1277.37744140625\n",
      "epoch 35, val_loss 455.0280456542969\n",
      "epoch 36, train_loss 1277.37744140625\n",
      "epoch 36, val_loss 455.0279541015625\n",
      "epoch 37, train_loss 1277.377197265625\n",
      "epoch 37, val_loss 455.0279541015625\n",
      "epoch 38, train_loss 1277.377197265625\n",
      "epoch 38, val_loss 455.02783203125\n",
      "epoch 39, train_loss 1277.377197265625\n",
      "epoch 39, val_loss 455.02783203125\n",
      "epoch 40, train_loss 1277.3770751953125\n",
      "epoch 40, val_loss 455.0277404785156\n",
      "epoch 41, train_loss 1277.377197265625\n",
      "epoch 41, val_loss 455.0276794433594\n",
      "epoch 42, train_loss 1277.376953125\n",
      "epoch 42, val_loss 455.027587890625\n",
      "epoch 43, train_loss 1277.376953125\n",
      "epoch 43, val_loss 455.0274963378906\n",
      "epoch 44, train_loss 1277.3768310546875\n",
      "epoch 44, val_loss 455.02740478515625\n",
      "epoch 45, train_loss 1277.376708984375\n",
      "epoch 45, val_loss 455.02740478515625\n",
      "epoch 46, train_loss 1277.376708984375\n",
      "epoch 46, val_loss 455.02734375\n",
      "epoch 47, train_loss 1277.37646484375\n",
      "epoch 47, val_loss 455.0272521972656\n",
      "epoch 48, train_loss 1277.37646484375\n",
      "epoch 48, val_loss 455.02716064453125\n",
      "epoch 49, train_loss 1277.3763427734375\n",
      "epoch 49, val_loss 455.0271301269531\n",
      "epoch 50, train_loss 1277.3763427734375\n",
      "epoch 50, val_loss 455.02703857421875\n",
      "epoch 51, train_loss 1277.37646484375\n",
      "epoch 51, val_loss 455.0270080566406\n",
      "epoch 52, train_loss 1277.376220703125\n",
      "epoch 52, val_loss 455.02691650390625\n",
      "epoch 53, train_loss 1277.376220703125\n",
      "epoch 53, val_loss 455.0268249511719\n",
      "epoch 54, train_loss 1277.3760986328125\n",
      "epoch 54, val_loss 455.02679443359375\n",
      "epoch 55, train_loss 1277.3760986328125\n",
      "epoch 55, val_loss 455.0267028808594\n",
      "epoch 56, train_loss 1277.3758544921875\n",
      "epoch 56, val_loss 455.026611328125\n",
      "epoch 57, train_loss 1277.375732421875\n",
      "epoch 57, val_loss 455.0265808105469\n",
      "epoch 58, train_loss 1277.3756103515625\n",
      "epoch 58, val_loss 455.02655029296875\n",
      "epoch 59, train_loss 1277.375732421875\n",
      "epoch 59, val_loss 455.0264587402344\n",
      "epoch 60, train_loss 1277.37548828125\n",
      "epoch 60, val_loss 455.0263671875\n",
      "epoch 61, train_loss 1277.37548828125\n",
      "epoch 61, val_loss 455.0262451171875\n",
      "epoch 62, train_loss 1277.3753662109375\n",
      "epoch 62, val_loss 455.0262145996094\n",
      "epoch 63, train_loss 1277.3751220703125\n",
      "epoch 63, val_loss 455.0261535644531\n",
      "epoch 64, train_loss 1277.3751220703125\n",
      "epoch 64, val_loss 455.0260925292969\n",
      "epoch 65, train_loss 1277.3751220703125\n",
      "epoch 65, val_loss 455.0260314941406\n",
      "epoch 66, train_loss 1277.3751220703125\n",
      "epoch 66, val_loss 455.0259704589844\n",
      "epoch 67, train_loss 1277.375\n",
      "epoch 67, val_loss 455.0259094238281\n",
      "epoch 68, train_loss 1277.3748779296875\n",
      "epoch 68, val_loss 455.0258483886719\n",
      "epoch 69, train_loss 1277.374755859375\n",
      "epoch 69, val_loss 455.0257568359375\n",
      "epoch 70, train_loss 1277.374755859375\n",
      "epoch 70, val_loss 455.0256652832031\n",
      "epoch 71, train_loss 1277.374755859375\n",
      "epoch 71, val_loss 455.025634765625\n",
      "epoch 72, train_loss 1277.3746337890625\n",
      "epoch 72, val_loss 455.0255432128906\n",
      "epoch 73, train_loss 1277.37451171875\n",
      "epoch 73, val_loss 455.02545166015625\n",
      "epoch 74, train_loss 1277.3743896484375\n",
      "epoch 74, val_loss 455.0253601074219\n",
      "epoch 75, train_loss 1277.3743896484375\n",
      "epoch 75, val_loss 455.0253601074219\n",
      "epoch 76, train_loss 1277.3743896484375\n",
      "epoch 76, val_loss 455.0252990722656\n",
      "epoch 77, train_loss 1277.374267578125\n",
      "epoch 77, val_loss 455.0252380371094\n",
      "epoch 78, train_loss 1277.3740234375\n",
      "epoch 78, val_loss 455.0251159667969\n",
      "epoch 79, train_loss 1277.3740234375\n",
      "epoch 79, val_loss 455.0250549316406\n",
      "epoch 80, train_loss 1277.3740234375\n",
      "epoch 80, val_loss 455.0249938964844\n",
      "epoch 81, train_loss 1277.3739013671875\n",
      "epoch 81, val_loss 455.02496337890625\n",
      "epoch 82, train_loss 1277.373779296875\n",
      "epoch 82, val_loss 455.0248718261719\n",
      "epoch 83, train_loss 1277.37353515625\n",
      "epoch 83, val_loss 455.02484130859375\n",
      "epoch 84, train_loss 1277.3736572265625\n",
      "epoch 84, val_loss 455.0247802734375\n",
      "epoch 85, train_loss 1277.3736572265625\n",
      "epoch 85, val_loss 455.024658203125\n",
      "epoch 86, train_loss 1277.37353515625\n",
      "epoch 86, val_loss 455.02459716796875\n",
      "epoch 87, train_loss 1277.373291015625\n",
      "epoch 87, val_loss 455.0245361328125\n",
      "epoch 88, train_loss 1277.373291015625\n",
      "epoch 88, val_loss 455.0245056152344\n",
      "epoch 89, train_loss 1277.373291015625\n",
      "epoch 89, val_loss 455.0244140625\n",
      "epoch 90, train_loss 1277.3731689453125\n",
      "epoch 90, val_loss 455.0243835449219\n",
      "epoch 91, train_loss 1277.373046875\n",
      "epoch 91, val_loss 455.0242919921875\n",
      "epoch 92, train_loss 1277.373046875\n",
      "epoch 92, val_loss 455.0242004394531\n",
      "epoch 93, train_loss 1277.3729248046875\n",
      "epoch 93, val_loss 455.024169921875\n",
      "epoch 94, train_loss 1277.372802734375\n",
      "epoch 94, val_loss 455.0240783691406\n",
      "epoch 95, train_loss 1277.372802734375\n",
      "epoch 95, val_loss 455.0239562988281\n",
      "epoch 96, train_loss 1277.3726806640625\n",
      "epoch 96, val_loss 455.0239562988281\n",
      "epoch 97, train_loss 1277.37255859375\n",
      "epoch 97, val_loss 455.02386474609375\n",
      "epoch 98, train_loss 1277.37255859375\n",
      "epoch 98, val_loss 455.02374267578125\n",
      "epoch 99, train_loss 1277.37255859375\n",
      "epoch 99, val_loss 455.0237121582031\n",
      "Parameter containing:\n",
      "tensor([7.1156e-19], requires_grad=True)\n",
      "iter 95, train_loss_regularization 0.7645069360733032\n",
      "iter 95, val_loss_regularization 0.7645069360733032\n",
      "epoch 0, train_loss 1277.3721923828125\n",
      "epoch 0, val_loss 455.023681640625\n",
      "epoch 1, train_loss 1277.3721923828125\n",
      "epoch 1, val_loss 455.02362060546875\n",
      "epoch 2, train_loss 1277.372314453125\n",
      "epoch 2, val_loss 455.0235290527344\n",
      "epoch 3, train_loss 1277.372314453125\n",
      "epoch 3, val_loss 455.02349853515625\n",
      "epoch 4, train_loss 1277.3720703125\n",
      "epoch 4, val_loss 455.02337646484375\n",
      "epoch 5, train_loss 1277.3719482421875\n",
      "epoch 5, val_loss 455.0233459472656\n",
      "epoch 6, train_loss 1277.371826171875\n",
      "epoch 6, val_loss 455.02325439453125\n",
      "epoch 7, train_loss 1277.371826171875\n",
      "epoch 7, val_loss 455.023193359375\n",
      "epoch 8, train_loss 1277.371826171875\n",
      "epoch 8, val_loss 455.02313232421875\n",
      "epoch 9, train_loss 1277.37158203125\n",
      "epoch 9, val_loss 455.0230712890625\n",
      "epoch 10, train_loss 1277.3717041015625\n",
      "epoch 10, val_loss 455.02301025390625\n",
      "epoch 11, train_loss 1277.3717041015625\n",
      "epoch 11, val_loss 455.0229187011719\n",
      "epoch 12, train_loss 1277.37158203125\n",
      "epoch 12, val_loss 455.0228271484375\n",
      "epoch 13, train_loss 1277.3712158203125\n",
      "epoch 13, val_loss 455.0227355957031\n",
      "epoch 14, train_loss 1277.371337890625\n",
      "epoch 14, val_loss 455.0226745605469\n",
      "epoch 15, train_loss 1277.371337890625\n",
      "epoch 15, val_loss 455.022705078125\n",
      "epoch 16, train_loss 1277.371337890625\n",
      "epoch 16, val_loss 455.0226135253906\n",
      "epoch 17, train_loss 1277.3712158203125\n",
      "epoch 17, val_loss 455.0225524902344\n",
      "epoch 18, train_loss 1277.37109375\n",
      "epoch 18, val_loss 455.0224609375\n",
      "epoch 19, train_loss 1277.37109375\n",
      "epoch 19, val_loss 455.0223388671875\n",
      "epoch 20, train_loss 1277.3709716796875\n",
      "epoch 20, val_loss 455.0223388671875\n",
      "epoch 21, train_loss 1277.370849609375\n",
      "epoch 21, val_loss 455.0222473144531\n",
      "epoch 22, train_loss 1277.3707275390625\n",
      "epoch 22, val_loss 455.02215576171875\n",
      "epoch 23, train_loss 1277.37060546875\n",
      "epoch 23, val_loss 455.0221252441406\n",
      "epoch 24, train_loss 1277.3704833984375\n",
      "epoch 24, val_loss 455.02203369140625\n",
      "epoch 25, train_loss 1277.37060546875\n",
      "epoch 25, val_loss 455.02197265625\n",
      "epoch 26, train_loss 1277.370361328125\n",
      "epoch 26, val_loss 455.02197265625\n",
      "epoch 27, train_loss 1277.3701171875\n",
      "epoch 27, val_loss 455.02191162109375\n",
      "epoch 28, train_loss 1277.3702392578125\n",
      "epoch 28, val_loss 455.0218200683594\n",
      "epoch 29, train_loss 1277.3701171875\n",
      "epoch 29, val_loss 455.02166748046875\n",
      "epoch 30, train_loss 1277.3701171875\n",
      "epoch 30, val_loss 455.02166748046875\n",
      "epoch 31, train_loss 1277.369873046875\n",
      "epoch 31, val_loss 455.0215759277344\n",
      "epoch 32, train_loss 1277.369873046875\n",
      "epoch 32, val_loss 455.0215148925781\n",
      "epoch 33, train_loss 1277.369873046875\n",
      "epoch 33, val_loss 455.0214538574219\n",
      "epoch 34, train_loss 1277.369873046875\n",
      "epoch 34, val_loss 455.0213623046875\n",
      "epoch 35, train_loss 1277.3697509765625\n",
      "epoch 35, val_loss 455.02130126953125\n",
      "epoch 36, train_loss 1277.36962890625\n",
      "epoch 36, val_loss 455.02130126953125\n",
      "epoch 37, train_loss 1277.369384765625\n",
      "epoch 37, val_loss 455.02117919921875\n",
      "epoch 38, train_loss 1277.369384765625\n",
      "epoch 38, val_loss 455.0210876464844\n",
      "epoch 39, train_loss 1277.369384765625\n",
      "epoch 39, val_loss 455.0210876464844\n",
      "epoch 40, train_loss 1277.369384765625\n",
      "epoch 40, val_loss 455.0209655761719\n",
      "epoch 41, train_loss 1277.3692626953125\n",
      "epoch 41, val_loss 455.0209045410156\n",
      "epoch 42, train_loss 1277.369140625\n",
      "epoch 42, val_loss 455.0208435058594\n",
      "epoch 43, train_loss 1277.369140625\n",
      "epoch 43, val_loss 455.0207824707031\n",
      "epoch 44, train_loss 1277.368896484375\n",
      "epoch 44, val_loss 455.020751953125\n",
      "epoch 45, train_loss 1277.3687744140625\n",
      "epoch 45, val_loss 455.02056884765625\n",
      "epoch 46, train_loss 1277.368896484375\n",
      "epoch 46, val_loss 455.02056884765625\n",
      "epoch 47, train_loss 1277.36865234375\n",
      "epoch 47, val_loss 455.02044677734375\n",
      "epoch 48, train_loss 1277.36865234375\n",
      "epoch 48, val_loss 455.0204162597656\n",
      "epoch 49, train_loss 1277.36865234375\n",
      "epoch 49, val_loss 455.0204162597656\n",
      "epoch 50, train_loss 1277.3685302734375\n",
      "epoch 50, val_loss 455.020263671875\n",
      "epoch 51, train_loss 1277.3682861328125\n",
      "epoch 51, val_loss 455.020263671875\n",
      "epoch 52, train_loss 1277.3682861328125\n",
      "epoch 52, val_loss 455.02020263671875\n",
      "epoch 53, train_loss 1277.3682861328125\n",
      "epoch 53, val_loss 455.0201110839844\n",
      "epoch 54, train_loss 1277.3681640625\n",
      "epoch 54, val_loss 455.0200500488281\n",
      "epoch 55, train_loss 1277.3680419921875\n",
      "epoch 55, val_loss 455.01995849609375\n",
      "epoch 56, train_loss 1277.3681640625\n",
      "epoch 56, val_loss 455.0198669433594\n",
      "epoch 57, train_loss 1277.3680419921875\n",
      "epoch 57, val_loss 455.0198669433594\n",
      "epoch 58, train_loss 1277.3677978515625\n",
      "epoch 58, val_loss 455.0197448730469\n",
      "epoch 59, train_loss 1277.3677978515625\n",
      "epoch 59, val_loss 455.01971435546875\n",
      "epoch 60, train_loss 1277.3677978515625\n",
      "epoch 60, val_loss 455.01971435546875\n",
      "epoch 61, train_loss 1277.36767578125\n",
      "epoch 61, val_loss 455.01959228515625\n",
      "epoch 62, train_loss 1277.3675537109375\n",
      "epoch 62, val_loss 455.01953125\n",
      "epoch 63, train_loss 1277.3675537109375\n",
      "epoch 63, val_loss 455.0194091796875\n",
      "epoch 64, train_loss 1277.367431640625\n",
      "epoch 64, val_loss 455.0193786621094\n",
      "epoch 65, train_loss 1277.367431640625\n",
      "epoch 65, val_loss 455.019287109375\n",
      "epoch 66, train_loss 1277.3673095703125\n",
      "epoch 66, val_loss 455.0191955566406\n",
      "epoch 67, train_loss 1277.3670654296875\n",
      "epoch 67, val_loss 455.0191650390625\n",
      "epoch 68, train_loss 1277.366943359375\n",
      "epoch 68, val_loss 455.0191650390625\n",
      "epoch 69, train_loss 1277.366943359375\n",
      "epoch 69, val_loss 455.01904296875\n",
      "epoch 70, train_loss 1277.366943359375\n",
      "epoch 70, val_loss 455.0190124511719\n",
      "epoch 71, train_loss 1277.366943359375\n",
      "epoch 71, val_loss 455.0189208984375\n",
      "epoch 72, train_loss 1277.36669921875\n",
      "epoch 72, val_loss 455.0188293457031\n",
      "epoch 73, train_loss 1277.36669921875\n",
      "epoch 73, val_loss 455.01873779296875\n",
      "epoch 74, train_loss 1277.3665771484375\n",
      "epoch 74, val_loss 455.0186767578125\n",
      "epoch 75, train_loss 1277.366455078125\n",
      "epoch 75, val_loss 455.01861572265625\n",
      "epoch 76, train_loss 1277.3665771484375\n",
      "epoch 76, val_loss 455.0185852050781\n",
      "epoch 77, train_loss 1277.366455078125\n",
      "epoch 77, val_loss 455.01849365234375\n",
      "epoch 78, train_loss 1277.366455078125\n",
      "epoch 78, val_loss 455.0184631347656\n",
      "epoch 79, train_loss 1277.3663330078125\n",
      "epoch 79, val_loss 455.01837158203125\n",
      "epoch 80, train_loss 1277.3663330078125\n",
      "epoch 80, val_loss 455.0183410644531\n",
      "epoch 81, train_loss 1277.3662109375\n",
      "epoch 81, val_loss 455.01824951171875\n",
      "epoch 82, train_loss 1277.3662109375\n",
      "epoch 82, val_loss 455.0182189941406\n",
      "epoch 83, train_loss 1277.365966796875\n",
      "epoch 83, val_loss 455.0180969238281\n",
      "epoch 84, train_loss 1277.365966796875\n",
      "epoch 84, val_loss 455.0179443359375\n",
      "epoch 85, train_loss 1277.3658447265625\n",
      "epoch 85, val_loss 455.01800537109375\n",
      "epoch 86, train_loss 1277.3658447265625\n",
      "epoch 86, val_loss 455.0179138183594\n",
      "epoch 87, train_loss 1277.36572265625\n",
      "epoch 87, val_loss 455.017822265625\n",
      "epoch 88, train_loss 1277.36572265625\n",
      "epoch 88, val_loss 455.01776123046875\n",
      "epoch 89, train_loss 1277.365478515625\n",
      "epoch 89, val_loss 455.0177001953125\n",
      "epoch 90, train_loss 1277.3653564453125\n",
      "epoch 90, val_loss 455.017578125\n",
      "epoch 91, train_loss 1277.3656005859375\n",
      "epoch 91, val_loss 455.017578125\n",
      "epoch 92, train_loss 1277.365234375\n",
      "epoch 92, val_loss 455.0174865722656\n",
      "epoch 93, train_loss 1277.3651123046875\n",
      "epoch 93, val_loss 455.0174865722656\n",
      "epoch 94, train_loss 1277.3651123046875\n",
      "epoch 94, val_loss 455.0173645019531\n",
      "epoch 95, train_loss 1277.3651123046875\n",
      "epoch 95, val_loss 455.017333984375\n",
      "epoch 96, train_loss 1277.3651123046875\n",
      "epoch 96, val_loss 455.0172119140625\n",
      "epoch 97, train_loss 1277.364990234375\n",
      "epoch 97, val_loss 455.0171813964844\n",
      "epoch 98, train_loss 1277.36474609375\n",
      "epoch 98, val_loss 455.01708984375\n",
      "epoch 99, train_loss 1277.36474609375\n",
      "epoch 99, val_loss 455.0169982910156\n",
      "Parameter containing:\n",
      "tensor([4.8546e-19], requires_grad=True)\n",
      "iter 96, train_loss_regularization 0.7624437808990479\n",
      "iter 96, val_loss_regularization 0.7624437808990479\n",
      "epoch 0, train_loss 1277.3646240234375\n",
      "epoch 0, val_loss 455.0169677734375\n",
      "epoch 1, train_loss 1277.3646240234375\n",
      "epoch 1, val_loss 455.01690673828125\n",
      "epoch 2, train_loss 1277.3643798828125\n",
      "epoch 2, val_loss 455.0168762207031\n",
      "epoch 3, train_loss 1277.3643798828125\n",
      "epoch 3, val_loss 455.016845703125\n",
      "epoch 4, train_loss 1277.3643798828125\n",
      "epoch 4, val_loss 455.01666259765625\n",
      "epoch 5, train_loss 1277.3643798828125\n",
      "epoch 5, val_loss 455.01666259765625\n",
      "epoch 6, train_loss 1277.3642578125\n",
      "epoch 6, val_loss 455.01654052734375\n",
      "epoch 7, train_loss 1277.3641357421875\n",
      "epoch 7, val_loss 455.0165100097656\n",
      "epoch 8, train_loss 1277.3641357421875\n",
      "epoch 8, val_loss 455.0164489746094\n",
      "epoch 9, train_loss 1277.364013671875\n",
      "epoch 9, val_loss 455.0163879394531\n",
      "epoch 10, train_loss 1277.364013671875\n",
      "epoch 10, val_loss 455.01629638671875\n",
      "epoch 11, train_loss 1277.36376953125\n",
      "epoch 11, val_loss 455.0162048339844\n",
      "epoch 12, train_loss 1277.363525390625\n",
      "epoch 12, val_loss 455.0162048339844\n",
      "epoch 13, train_loss 1277.3636474609375\n",
      "epoch 13, val_loss 455.01611328125\n",
      "epoch 14, train_loss 1277.363525390625\n",
      "epoch 14, val_loss 455.01605224609375\n",
      "epoch 15, train_loss 1277.363525390625\n",
      "epoch 15, val_loss 455.0159606933594\n",
      "epoch 16, train_loss 1277.363525390625\n",
      "epoch 16, val_loss 455.0158386230469\n",
      "epoch 17, train_loss 1277.36328125\n",
      "epoch 17, val_loss 455.0158386230469\n",
      "epoch 18, train_loss 1277.36328125\n",
      "epoch 18, val_loss 455.0157775878906\n",
      "epoch 19, train_loss 1277.3631591796875\n",
      "epoch 19, val_loss 455.0157165527344\n",
      "epoch 20, train_loss 1277.363037109375\n",
      "epoch 20, val_loss 455.0156555175781\n",
      "epoch 21, train_loss 1277.363037109375\n",
      "epoch 21, val_loss 455.015625\n",
      "epoch 22, train_loss 1277.36279296875\n",
      "epoch 22, val_loss 455.0155944824219\n",
      "epoch 23, train_loss 1277.3629150390625\n",
      "epoch 23, val_loss 455.0154113769531\n",
      "epoch 24, train_loss 1277.36279296875\n",
      "epoch 24, val_loss 455.01531982421875\n",
      "epoch 25, train_loss 1277.36279296875\n",
      "epoch 25, val_loss 455.01531982421875\n",
      "epoch 26, train_loss 1277.3626708984375\n",
      "epoch 26, val_loss 455.0152587890625\n",
      "epoch 27, train_loss 1277.362548828125\n",
      "epoch 27, val_loss 455.01519775390625\n",
      "epoch 28, train_loss 1277.362548828125\n",
      "epoch 28, val_loss 455.01513671875\n",
      "epoch 29, train_loss 1277.3623046875\n",
      "epoch 29, val_loss 455.0150451660156\n",
      "epoch 30, train_loss 1277.3623046875\n",
      "epoch 30, val_loss 455.01495361328125\n",
      "epoch 31, train_loss 1277.3623046875\n",
      "epoch 31, val_loss 455.01495361328125\n",
      "epoch 32, train_loss 1277.3623046875\n",
      "epoch 32, val_loss 455.01483154296875\n",
      "epoch 33, train_loss 1277.362060546875\n",
      "epoch 33, val_loss 455.0147399902344\n",
      "epoch 34, train_loss 1277.362060546875\n",
      "epoch 34, val_loss 455.01470947265625\n",
      "epoch 35, train_loss 1277.362060546875\n",
      "epoch 35, val_loss 455.0146179199219\n",
      "epoch 36, train_loss 1277.3619384765625\n",
      "epoch 36, val_loss 455.0146789550781\n",
      "epoch 37, train_loss 1277.36181640625\n",
      "epoch 37, val_loss 455.0144958496094\n",
      "epoch 38, train_loss 1277.3616943359375\n",
      "epoch 38, val_loss 455.014404296875\n",
      "epoch 39, train_loss 1277.36181640625\n",
      "epoch 39, val_loss 455.014404296875\n",
      "epoch 40, train_loss 1277.3616943359375\n",
      "epoch 40, val_loss 455.01434326171875\n",
      "epoch 41, train_loss 1277.3616943359375\n",
      "epoch 41, val_loss 455.0142517089844\n",
      "epoch 42, train_loss 1277.361572265625\n",
      "epoch 42, val_loss 455.0141296386719\n",
      "epoch 43, train_loss 1277.3612060546875\n",
      "epoch 43, val_loss 455.0141296386719\n",
      "epoch 44, train_loss 1277.361328125\n",
      "epoch 44, val_loss 455.0140380859375\n",
      "epoch 45, train_loss 1277.361328125\n",
      "epoch 45, val_loss 455.0140075683594\n",
      "epoch 46, train_loss 1277.3612060546875\n",
      "epoch 46, val_loss 455.0139465332031\n",
      "epoch 47, train_loss 1277.3609619140625\n",
      "epoch 47, val_loss 455.013916015625\n",
      "epoch 48, train_loss 1277.361083984375\n",
      "epoch 48, val_loss 455.0137939453125\n",
      "epoch 49, train_loss 1277.3609619140625\n",
      "epoch 49, val_loss 455.0137634277344\n",
      "epoch 50, train_loss 1277.3609619140625\n",
      "epoch 50, val_loss 455.0135803222656\n",
      "epoch 51, train_loss 1277.3607177734375\n",
      "epoch 51, val_loss 455.0135803222656\n",
      "epoch 52, train_loss 1277.3607177734375\n",
      "epoch 52, val_loss 455.0135498046875\n",
      "epoch 53, train_loss 1277.360595703125\n",
      "epoch 53, val_loss 455.0134582519531\n",
      "epoch 54, train_loss 1277.3607177734375\n",
      "epoch 54, val_loss 455.01336669921875\n",
      "epoch 55, train_loss 1277.360595703125\n",
      "epoch 55, val_loss 455.01336669921875\n",
      "epoch 56, train_loss 1277.3604736328125\n",
      "epoch 56, val_loss 455.01324462890625\n",
      "epoch 57, train_loss 1277.3602294921875\n",
      "epoch 57, val_loss 455.0132141113281\n",
      "epoch 58, train_loss 1277.360107421875\n",
      "epoch 58, val_loss 455.0131530761719\n",
      "epoch 59, train_loss 1277.3602294921875\n",
      "epoch 59, val_loss 455.0130310058594\n",
      "epoch 60, train_loss 1277.360107421875\n",
      "epoch 60, val_loss 455.01300048828125\n",
      "epoch 61, train_loss 1277.360107421875\n",
      "epoch 61, val_loss 455.0129699707031\n",
      "epoch 62, train_loss 1277.3599853515625\n",
      "epoch 62, val_loss 455.01287841796875\n",
      "epoch 63, train_loss 1277.35986328125\n",
      "epoch 63, val_loss 455.0127868652344\n",
      "epoch 64, train_loss 1277.3597412109375\n",
      "epoch 64, val_loss 455.0126953125\n",
      "epoch 65, train_loss 1277.359619140625\n",
      "epoch 65, val_loss 455.0126647949219\n",
      "epoch 66, train_loss 1277.359619140625\n",
      "epoch 66, val_loss 455.0125732421875\n",
      "epoch 67, train_loss 1277.359619140625\n",
      "epoch 67, val_loss 455.0125427246094\n",
      "epoch 68, train_loss 1277.359375\n",
      "epoch 68, val_loss 455.01251220703125\n",
      "epoch 69, train_loss 1277.3594970703125\n",
      "epoch 69, val_loss 455.0124206542969\n",
      "epoch 70, train_loss 1277.359375\n",
      "epoch 70, val_loss 455.0123291015625\n",
      "epoch 71, train_loss 1277.3592529296875\n",
      "epoch 71, val_loss 455.0122375488281\n",
      "epoch 72, train_loss 1277.3592529296875\n",
      "epoch 72, val_loss 455.01220703125\n",
      "epoch 73, train_loss 1277.359130859375\n",
      "epoch 73, val_loss 455.0121765136719\n",
      "epoch 74, train_loss 1277.35888671875\n",
      "epoch 74, val_loss 455.0120544433594\n",
      "epoch 75, train_loss 1277.35888671875\n",
      "epoch 75, val_loss 455.0120544433594\n",
      "epoch 76, train_loss 1277.35888671875\n",
      "epoch 76, val_loss 455.011962890625\n",
      "epoch 77, train_loss 1277.3587646484375\n",
      "epoch 77, val_loss 455.01177978515625\n",
      "epoch 78, train_loss 1277.358642578125\n",
      "epoch 78, val_loss 455.0118408203125\n",
      "epoch 79, train_loss 1277.358642578125\n",
      "epoch 79, val_loss 455.01171875\n",
      "epoch 80, train_loss 1277.358642578125\n",
      "epoch 80, val_loss 455.01165771484375\n",
      "epoch 81, train_loss 1277.3583984375\n",
      "epoch 81, val_loss 455.01165771484375\n",
      "epoch 82, train_loss 1277.3583984375\n",
      "epoch 82, val_loss 455.01153564453125\n",
      "epoch 83, train_loss 1277.3582763671875\n",
      "epoch 83, val_loss 455.0115051269531\n",
      "epoch 84, train_loss 1277.3582763671875\n",
      "epoch 84, val_loss 455.01141357421875\n",
      "epoch 85, train_loss 1277.358154296875\n",
      "epoch 85, val_loss 455.0113830566406\n",
      "epoch 86, train_loss 1277.358154296875\n",
      "epoch 86, val_loss 455.0112609863281\n",
      "epoch 87, train_loss 1277.3580322265625\n",
      "epoch 87, val_loss 455.01116943359375\n",
      "epoch 88, train_loss 1277.35791015625\n",
      "epoch 88, val_loss 455.01116943359375\n",
      "epoch 89, train_loss 1277.3577880859375\n",
      "epoch 89, val_loss 455.0110778808594\n",
      "epoch 90, train_loss 1277.3577880859375\n",
      "epoch 90, val_loss 455.010986328125\n",
      "epoch 91, train_loss 1277.357666015625\n",
      "epoch 91, val_loss 455.0109558105469\n",
      "epoch 92, train_loss 1277.357666015625\n",
      "epoch 92, val_loss 455.01092529296875\n",
      "epoch 93, train_loss 1277.357666015625\n",
      "epoch 93, val_loss 455.0107421875\n",
      "epoch 94, train_loss 1277.3575439453125\n",
      "epoch 94, val_loss 455.0107421875\n",
      "epoch 95, train_loss 1277.357421875\n",
      "epoch 95, val_loss 455.0107116699219\n",
      "epoch 96, train_loss 1277.3572998046875\n",
      "epoch 96, val_loss 455.0105895996094\n",
      "epoch 97, train_loss 1277.3572998046875\n",
      "epoch 97, val_loss 455.0105285644531\n",
      "epoch 98, train_loss 1277.3570556640625\n",
      "epoch 98, val_loss 455.010498046875\n",
      "epoch 99, train_loss 1277.3570556640625\n",
      "epoch 99, val_loss 455.0104064941406\n",
      "Parameter containing:\n",
      "tensor([3.3142e-19], requires_grad=True)\n",
      "iter 97, train_loss_regularization 0.760419487953186\n",
      "iter 97, val_loss_regularization 0.760419487953186\n",
      "epoch 0, train_loss 1277.3570556640625\n",
      "epoch 0, val_loss 455.0104064941406\n",
      "epoch 1, train_loss 1277.3570556640625\n",
      "epoch 1, val_loss 455.0103454589844\n",
      "epoch 2, train_loss 1277.35693359375\n",
      "epoch 2, val_loss 455.0101623535156\n",
      "epoch 3, train_loss 1277.3568115234375\n",
      "epoch 3, val_loss 455.0101318359375\n",
      "epoch 4, train_loss 1277.356689453125\n",
      "epoch 4, val_loss 455.0100402832031\n",
      "epoch 5, train_loss 1277.356689453125\n",
      "epoch 5, val_loss 455.0100402832031\n",
      "epoch 6, train_loss 1277.356689453125\n",
      "epoch 6, val_loss 455.00994873046875\n",
      "epoch 7, train_loss 1277.356689453125\n",
      "epoch 7, val_loss 455.0099182128906\n",
      "epoch 8, train_loss 1277.356689453125\n",
      "epoch 8, val_loss 455.0098876953125\n",
      "epoch 9, train_loss 1277.3563232421875\n",
      "epoch 9, val_loss 455.0097351074219\n",
      "epoch 10, train_loss 1277.3563232421875\n",
      "epoch 10, val_loss 455.0096740722656\n",
      "epoch 11, train_loss 1277.3563232421875\n",
      "epoch 11, val_loss 455.00958251953125\n",
      "epoch 12, train_loss 1277.356201171875\n",
      "epoch 12, val_loss 455.0095520019531\n",
      "epoch 13, train_loss 1277.356201171875\n",
      "epoch 13, val_loss 455.0095520019531\n",
      "epoch 14, train_loss 1277.35595703125\n",
      "epoch 14, val_loss 455.0094299316406\n",
      "epoch 15, train_loss 1277.3558349609375\n",
      "epoch 15, val_loss 455.0093688964844\n",
      "epoch 16, train_loss 1277.3558349609375\n",
      "epoch 16, val_loss 455.00927734375\n",
      "epoch 17, train_loss 1277.35595703125\n",
      "epoch 17, val_loss 455.00921630859375\n",
      "epoch 18, train_loss 1277.35595703125\n",
      "epoch 18, val_loss 455.00921630859375\n",
      "epoch 19, train_loss 1277.355712890625\n",
      "epoch 19, val_loss 455.00909423828125\n",
      "epoch 20, train_loss 1277.35546875\n",
      "epoch 20, val_loss 455.009033203125\n",
      "epoch 21, train_loss 1277.3555908203125\n",
      "epoch 21, val_loss 455.0090026855469\n",
      "epoch 22, train_loss 1277.3555908203125\n",
      "epoch 22, val_loss 455.0089111328125\n",
      "epoch 23, train_loss 1277.355224609375\n",
      "epoch 23, val_loss 455.0088195800781\n",
      "epoch 24, train_loss 1277.355224609375\n",
      "epoch 24, val_loss 455.0087585449219\n",
      "epoch 25, train_loss 1277.355224609375\n",
      "epoch 25, val_loss 455.0087585449219\n",
      "epoch 26, train_loss 1277.355224609375\n",
      "epoch 26, val_loss 455.0086669921875\n",
      "epoch 27, train_loss 1277.35498046875\n",
      "epoch 27, val_loss 455.0085754394531\n",
      "epoch 28, train_loss 1277.3548583984375\n",
      "epoch 28, val_loss 455.008544921875\n",
      "epoch 29, train_loss 1277.354736328125\n",
      "epoch 29, val_loss 455.0084533691406\n",
      "epoch 30, train_loss 1277.3548583984375\n",
      "epoch 30, val_loss 455.00836181640625\n",
      "epoch 31, train_loss 1277.3548583984375\n",
      "epoch 31, val_loss 455.0083312988281\n",
      "epoch 32, train_loss 1277.3546142578125\n",
      "epoch 32, val_loss 455.00823974609375\n",
      "epoch 33, train_loss 1277.3546142578125\n",
      "epoch 33, val_loss 455.0081787109375\n",
      "epoch 34, train_loss 1277.3546142578125\n",
      "epoch 34, val_loss 455.0081787109375\n",
      "epoch 35, train_loss 1277.3544921875\n",
      "epoch 35, val_loss 455.00811767578125\n",
      "epoch 36, train_loss 1277.3543701171875\n",
      "epoch 36, val_loss 455.0079650878906\n",
      "epoch 37, train_loss 1277.3541259765625\n",
      "epoch 37, val_loss 455.0079040527344\n",
      "epoch 38, train_loss 1277.3541259765625\n",
      "epoch 38, val_loss 455.0078430175781\n",
      "epoch 39, train_loss 1277.3541259765625\n",
      "epoch 39, val_loss 455.0077819824219\n",
      "epoch 40, train_loss 1277.3541259765625\n",
      "epoch 40, val_loss 455.00775146484375\n",
      "epoch 41, train_loss 1277.3541259765625\n",
      "epoch 41, val_loss 455.0077209472656\n",
      "epoch 42, train_loss 1277.3538818359375\n",
      "epoch 42, val_loss 455.00762939453125\n",
      "epoch 43, train_loss 1277.3538818359375\n",
      "epoch 43, val_loss 455.00750732421875\n",
      "epoch 44, train_loss 1277.3538818359375\n",
      "epoch 44, val_loss 455.00750732421875\n",
      "epoch 45, train_loss 1277.353759765625\n",
      "epoch 45, val_loss 455.0074157714844\n",
      "epoch 46, train_loss 1277.3533935546875\n",
      "epoch 46, val_loss 455.0072937011719\n",
      "epoch 47, train_loss 1277.353515625\n",
      "epoch 47, val_loss 455.00726318359375\n",
      "epoch 48, train_loss 1277.353515625\n",
      "epoch 48, val_loss 455.0071716308594\n",
      "epoch 49, train_loss 1277.353515625\n",
      "epoch 49, val_loss 455.0071105957031\n",
      "epoch 50, train_loss 1277.3533935546875\n",
      "epoch 50, val_loss 455.0071105957031\n",
      "epoch 51, train_loss 1277.35302734375\n",
      "epoch 51, val_loss 455.0069580078125\n",
      "epoch 52, train_loss 1277.35302734375\n",
      "epoch 52, val_loss 455.0069274902344\n",
      "epoch 53, train_loss 1277.35302734375\n",
      "epoch 53, val_loss 455.0068664550781\n",
      "epoch 54, train_loss 1277.35302734375\n",
      "epoch 54, val_loss 455.0068359375\n",
      "epoch 55, train_loss 1277.35302734375\n",
      "epoch 55, val_loss 455.0067443847656\n",
      "epoch 56, train_loss 1277.3529052734375\n",
      "epoch 56, val_loss 455.0067443847656\n",
      "epoch 57, train_loss 1277.352783203125\n",
      "epoch 57, val_loss 455.006591796875\n",
      "epoch 58, train_loss 1277.352783203125\n",
      "epoch 58, val_loss 455.00653076171875\n",
      "epoch 59, train_loss 1277.3525390625\n",
      "epoch 59, val_loss 455.0065002441406\n",
      "epoch 60, train_loss 1277.352294921875\n",
      "epoch 60, val_loss 455.00640869140625\n",
      "epoch 61, train_loss 1277.3525390625\n",
      "epoch 61, val_loss 455.0063781738281\n",
      "epoch 62, train_loss 1277.352294921875\n",
      "epoch 62, val_loss 455.0063781738281\n",
      "epoch 63, train_loss 1277.352294921875\n",
      "epoch 63, val_loss 455.00616455078125\n",
      "epoch 64, train_loss 1277.352294921875\n",
      "epoch 64, val_loss 455.0061340332031\n",
      "epoch 65, train_loss 1277.35205078125\n",
      "epoch 65, val_loss 455.0060729980469\n",
      "epoch 66, train_loss 1277.3521728515625\n",
      "epoch 66, val_loss 455.0060119628906\n",
      "epoch 67, train_loss 1277.35205078125\n",
      "epoch 67, val_loss 455.0059509277344\n",
      "epoch 68, train_loss 1277.35205078125\n",
      "epoch 68, val_loss 455.0059509277344\n",
      "epoch 69, train_loss 1277.35205078125\n",
      "epoch 69, val_loss 455.0058898925781\n",
      "epoch 70, train_loss 1277.351806640625\n",
      "epoch 70, val_loss 455.0057373046875\n",
      "epoch 71, train_loss 1277.351806640625\n",
      "epoch 71, val_loss 455.0057373046875\n",
      "epoch 72, train_loss 1277.351806640625\n",
      "epoch 72, val_loss 455.00567626953125\n",
      "epoch 73, train_loss 1277.351806640625\n",
      "epoch 73, val_loss 455.0055847167969\n",
      "epoch 74, train_loss 1277.3516845703125\n",
      "epoch 74, val_loss 455.0054931640625\n",
      "epoch 75, train_loss 1277.3514404296875\n",
      "epoch 75, val_loss 455.0054016113281\n",
      "epoch 76, train_loss 1277.351318359375\n",
      "epoch 76, val_loss 455.0053405761719\n",
      "epoch 77, train_loss 1277.351318359375\n",
      "epoch 77, val_loss 455.0052795410156\n",
      "epoch 78, train_loss 1277.3511962890625\n",
      "epoch 78, val_loss 455.0052185058594\n",
      "epoch 79, train_loss 1277.3511962890625\n",
      "epoch 79, val_loss 455.0052185058594\n",
      "epoch 80, train_loss 1277.35107421875\n",
      "epoch 80, val_loss 455.0051574707031\n",
      "epoch 81, train_loss 1277.35107421875\n",
      "epoch 81, val_loss 455.0050964355469\n",
      "epoch 82, train_loss 1277.3509521484375\n",
      "epoch 82, val_loss 455.0050048828125\n",
      "epoch 83, train_loss 1277.3509521484375\n",
      "epoch 83, val_loss 455.0049133300781\n",
      "epoch 84, train_loss 1277.3507080078125\n",
      "epoch 84, val_loss 455.00482177734375\n",
      "epoch 85, train_loss 1277.3507080078125\n",
      "epoch 85, val_loss 455.0047912597656\n",
      "epoch 86, train_loss 1277.3507080078125\n",
      "epoch 86, val_loss 455.00469970703125\n",
      "epoch 87, train_loss 1277.3505859375\n",
      "epoch 87, val_loss 455.0046691894531\n",
      "epoch 88, train_loss 1277.3504638671875\n",
      "epoch 88, val_loss 455.004638671875\n",
      "epoch 89, train_loss 1277.350341796875\n",
      "epoch 89, val_loss 455.00457763671875\n",
      "epoch 90, train_loss 1277.350341796875\n",
      "epoch 90, val_loss 455.0044250488281\n",
      "epoch 91, train_loss 1277.350341796875\n",
      "epoch 91, val_loss 455.00433349609375\n",
      "epoch 92, train_loss 1277.3502197265625\n",
      "epoch 92, val_loss 455.0043029785156\n",
      "epoch 93, train_loss 1277.3502197265625\n",
      "epoch 93, val_loss 455.0043029785156\n",
      "epoch 94, train_loss 1277.35009765625\n",
      "epoch 94, val_loss 455.00421142578125\n",
      "epoch 95, train_loss 1277.35009765625\n",
      "epoch 95, val_loss 455.0041809082031\n",
      "epoch 96, train_loss 1277.3499755859375\n",
      "epoch 96, val_loss 455.0041198730469\n",
      "epoch 97, train_loss 1277.3497314453125\n",
      "epoch 97, val_loss 455.0039978027344\n",
      "epoch 98, train_loss 1277.349609375\n",
      "epoch 98, val_loss 455.00396728515625\n",
      "epoch 99, train_loss 1277.349609375\n",
      "epoch 99, val_loss 455.00384521484375\n",
      "Parameter containing:\n",
      "tensor([2.2640e-19], requires_grad=True)\n",
      "iter 98, train_loss_regularization 0.7584335207939148\n",
      "iter 98, val_loss_regularization 0.7584335207939148\n",
      "epoch 0, train_loss 1277.349609375\n",
      "epoch 0, val_loss 455.0037841796875\n",
      "epoch 1, train_loss 1277.3494873046875\n",
      "epoch 1, val_loss 455.0037536621094\n",
      "epoch 2, train_loss 1277.349365234375\n",
      "epoch 2, val_loss 455.003662109375\n",
      "epoch 3, train_loss 1277.3492431640625\n",
      "epoch 3, val_loss 455.0036315917969\n",
      "epoch 4, train_loss 1277.3492431640625\n",
      "epoch 4, val_loss 455.0035705566406\n",
      "epoch 5, train_loss 1277.3492431640625\n",
      "epoch 5, val_loss 455.0034484863281\n",
      "epoch 6, train_loss 1277.34912109375\n",
      "epoch 6, val_loss 455.0034484863281\n",
      "epoch 7, train_loss 1277.34912109375\n",
      "epoch 7, val_loss 455.0033264160156\n",
      "epoch 8, train_loss 1277.3489990234375\n",
      "epoch 8, val_loss 455.0032958984375\n",
      "epoch 9, train_loss 1277.348876953125\n",
      "epoch 9, val_loss 455.0032958984375\n",
      "epoch 10, train_loss 1277.348876953125\n",
      "epoch 10, val_loss 455.00311279296875\n",
      "epoch 11, train_loss 1277.348876953125\n",
      "epoch 11, val_loss 455.0030822753906\n",
      "epoch 12, train_loss 1277.3486328125\n",
      "epoch 12, val_loss 455.0030822753906\n",
      "epoch 13, train_loss 1277.3485107421875\n",
      "epoch 13, val_loss 455.0029602050781\n",
      "epoch 14, train_loss 1277.3485107421875\n",
      "epoch 14, val_loss 455.0029602050781\n",
      "epoch 15, train_loss 1277.348388671875\n",
      "epoch 15, val_loss 455.00286865234375\n",
      "epoch 16, train_loss 1277.3482666015625\n",
      "epoch 16, val_loss 455.0028381347656\n",
      "epoch 17, train_loss 1277.3482666015625\n",
      "epoch 17, val_loss 455.0027160644531\n",
      "epoch 18, train_loss 1277.348388671875\n",
      "epoch 18, val_loss 455.00262451171875\n",
      "epoch 19, train_loss 1277.34814453125\n",
      "epoch 19, val_loss 455.0025939941406\n",
      "epoch 20, train_loss 1277.3480224609375\n",
      "epoch 20, val_loss 455.0025329589844\n",
      "epoch 21, train_loss 1277.3480224609375\n",
      "epoch 21, val_loss 455.0024719238281\n",
      "epoch 22, train_loss 1277.347900390625\n",
      "epoch 22, val_loss 455.0024108886719\n",
      "epoch 23, train_loss 1277.3477783203125\n",
      "epoch 23, val_loss 455.00238037109375\n",
      "epoch 24, train_loss 1277.3477783203125\n",
      "epoch 24, val_loss 455.00225830078125\n",
      "epoch 25, train_loss 1277.3477783203125\n",
      "epoch 25, val_loss 455.0021667480469\n",
      "epoch 26, train_loss 1277.3477783203125\n",
      "epoch 26, val_loss 455.0020751953125\n",
      "epoch 27, train_loss 1277.3475341796875\n",
      "epoch 27, val_loss 455.0020751953125\n",
      "epoch 28, train_loss 1277.3472900390625\n",
      "epoch 28, val_loss 455.00201416015625\n",
      "epoch 29, train_loss 1277.347412109375\n",
      "epoch 29, val_loss 455.00201416015625\n",
      "epoch 30, train_loss 1277.347412109375\n",
      "epoch 30, val_loss 455.0018310546875\n",
      "epoch 31, train_loss 1277.3472900390625\n",
      "epoch 31, val_loss 455.0018005371094\n",
      "epoch 32, train_loss 1277.3472900390625\n",
      "epoch 32, val_loss 455.001708984375\n",
      "epoch 33, train_loss 1277.3472900390625\n",
      "epoch 33, val_loss 455.001708984375\n",
      "epoch 34, train_loss 1277.34716796875\n",
      "epoch 34, val_loss 455.0015869140625\n",
      "epoch 35, train_loss 1277.34716796875\n",
      "epoch 35, val_loss 455.0014953613281\n",
      "epoch 36, train_loss 1277.3470458984375\n",
      "epoch 36, val_loss 455.0014953613281\n",
      "epoch 37, train_loss 1277.3466796875\n",
      "epoch 37, val_loss 455.0013427734375\n",
      "epoch 38, train_loss 1277.3468017578125\n",
      "epoch 38, val_loss 455.0013427734375\n",
      "epoch 39, train_loss 1277.3466796875\n",
      "epoch 39, val_loss 455.0013427734375\n",
      "epoch 40, train_loss 1277.3466796875\n",
      "epoch 40, val_loss 455.001220703125\n",
      "epoch 41, train_loss 1277.3465576171875\n",
      "epoch 41, val_loss 455.001220703125\n",
      "epoch 42, train_loss 1277.3465576171875\n",
      "epoch 42, val_loss 455.0010681152344\n",
      "epoch 43, train_loss 1277.3465576171875\n",
      "epoch 43, val_loss 455.00103759765625\n",
      "epoch 44, train_loss 1277.3463134765625\n",
      "epoch 44, val_loss 455.00091552734375\n",
      "epoch 45, train_loss 1277.3463134765625\n",
      "epoch 45, val_loss 455.0008850097656\n",
      "epoch 46, train_loss 1277.34619140625\n",
      "epoch 46, val_loss 455.0008239746094\n",
      "epoch 47, train_loss 1277.3460693359375\n",
      "epoch 47, val_loss 455.0007629394531\n",
      "epoch 48, train_loss 1277.345947265625\n",
      "epoch 48, val_loss 455.0007019042969\n",
      "epoch 49, train_loss 1277.345947265625\n",
      "epoch 49, val_loss 455.00067138671875\n",
      "epoch 50, train_loss 1277.3458251953125\n",
      "epoch 50, val_loss 455.0005798339844\n",
      "epoch 51, train_loss 1277.345703125\n",
      "epoch 51, val_loss 455.00054931640625\n",
      "epoch 52, train_loss 1277.345703125\n",
      "epoch 52, val_loss 455.00042724609375\n",
      "epoch 53, train_loss 1277.3455810546875\n",
      "epoch 53, val_loss 455.0003356933594\n",
      "epoch 54, train_loss 1277.3455810546875\n",
      "epoch 54, val_loss 455.00030517578125\n",
      "epoch 55, train_loss 1277.3453369140625\n",
      "epoch 55, val_loss 455.00030517578125\n",
      "epoch 56, train_loss 1277.345458984375\n",
      "epoch 56, val_loss 455.0002136230469\n",
      "epoch 57, train_loss 1277.345458984375\n",
      "epoch 57, val_loss 455.0001220703125\n",
      "epoch 58, train_loss 1277.34521484375\n",
      "epoch 58, val_loss 455.0000305175781\n",
      "epoch 59, train_loss 1277.3453369140625\n",
      "epoch 59, val_loss 454.9999694824219\n",
      "epoch 60, train_loss 1277.3450927734375\n",
      "epoch 60, val_loss 454.9999084472656\n",
      "epoch 61, train_loss 1277.3450927734375\n",
      "epoch 61, val_loss 454.9998474121094\n",
      "epoch 62, train_loss 1277.344970703125\n",
      "epoch 62, val_loss 454.9997863769531\n",
      "epoch 63, train_loss 1277.344970703125\n",
      "epoch 63, val_loss 454.99969482421875\n",
      "epoch 64, train_loss 1277.344970703125\n",
      "epoch 64, val_loss 454.9996643066406\n",
      "epoch 65, train_loss 1277.3448486328125\n",
      "epoch 65, val_loss 454.99957275390625\n",
      "epoch 66, train_loss 1277.3447265625\n",
      "epoch 66, val_loss 454.99957275390625\n",
      "epoch 67, train_loss 1277.3446044921875\n",
      "epoch 67, val_loss 454.99951171875\n",
      "epoch 68, train_loss 1277.344482421875\n",
      "epoch 68, val_loss 454.99945068359375\n",
      "epoch 69, train_loss 1277.3443603515625\n",
      "epoch 69, val_loss 454.9992370605469\n",
      "epoch 70, train_loss 1277.3443603515625\n",
      "epoch 70, val_loss 454.9992980957031\n",
      "epoch 71, train_loss 1277.3443603515625\n",
      "epoch 71, val_loss 454.99920654296875\n",
      "epoch 72, train_loss 1277.3443603515625\n",
      "epoch 72, val_loss 454.99908447265625\n",
      "epoch 73, train_loss 1277.34423828125\n",
      "epoch 73, val_loss 454.9991149902344\n",
      "epoch 74, train_loss 1277.343994140625\n",
      "epoch 74, val_loss 454.9990539550781\n",
      "epoch 75, train_loss 1277.3441162109375\n",
      "epoch 75, val_loss 454.99896240234375\n",
      "epoch 76, train_loss 1277.3438720703125\n",
      "epoch 76, val_loss 454.99896240234375\n",
      "epoch 77, train_loss 1277.3438720703125\n",
      "epoch 77, val_loss 454.99884033203125\n",
      "epoch 78, train_loss 1277.3438720703125\n",
      "epoch 78, val_loss 454.9987487792969\n",
      "epoch 79, train_loss 1277.3436279296875\n",
      "epoch 79, val_loss 454.9986572265625\n",
      "epoch 80, train_loss 1277.34375\n",
      "epoch 80, val_loss 454.99859619140625\n",
      "epoch 81, train_loss 1277.3436279296875\n",
      "epoch 81, val_loss 454.99859619140625\n",
      "epoch 82, train_loss 1277.3436279296875\n",
      "epoch 82, val_loss 454.99853515625\n",
      "epoch 83, train_loss 1277.343505859375\n",
      "epoch 83, val_loss 454.9984436035156\n",
      "epoch 84, train_loss 1277.3431396484375\n",
      "epoch 84, val_loss 454.9984130859375\n",
      "epoch 85, train_loss 1277.34326171875\n",
      "epoch 85, val_loss 454.9983215332031\n",
      "epoch 86, train_loss 1277.3433837890625\n",
      "epoch 86, val_loss 454.9982604980469\n",
      "epoch 87, train_loss 1277.3431396484375\n",
      "epoch 87, val_loss 454.9981689453125\n",
      "epoch 88, train_loss 1277.3431396484375\n",
      "epoch 88, val_loss 454.9981384277344\n",
      "epoch 89, train_loss 1277.3428955078125\n",
      "epoch 89, val_loss 454.998046875\n",
      "epoch 90, train_loss 1277.3428955078125\n",
      "epoch 90, val_loss 454.998046875\n",
      "epoch 91, train_loss 1277.3427734375\n",
      "epoch 91, val_loss 454.99786376953125\n",
      "epoch 92, train_loss 1277.3427734375\n",
      "epoch 92, val_loss 454.9978332519531\n",
      "epoch 93, train_loss 1277.3427734375\n",
      "epoch 93, val_loss 454.9978332519531\n",
      "epoch 94, train_loss 1277.3426513671875\n",
      "epoch 94, val_loss 454.9977111816406\n",
      "epoch 95, train_loss 1277.3426513671875\n",
      "epoch 95, val_loss 454.9977111816406\n",
      "epoch 96, train_loss 1277.3426513671875\n",
      "epoch 96, val_loss 454.9976806640625\n",
      "epoch 97, train_loss 1277.3424072265625\n",
      "epoch 97, val_loss 454.9975280761719\n",
      "epoch 98, train_loss 1277.34228515625\n",
      "epoch 98, val_loss 454.9974670410156\n",
      "epoch 99, train_loss 1277.34228515625\n",
      "epoch 99, val_loss 454.99737548828125\n",
      "Parameter containing:\n",
      "tensor([1.5475e-19], requires_grad=True)\n",
      "iter 99, train_loss_regularization 0.7564852237701416\n",
      "iter 99, val_loss_regularization 0.7564852237701416\n",
      "epoch 0, train_loss 1277.34228515625\n",
      "epoch 0, val_loss 454.9973449707031\n",
      "epoch 1, train_loss 1277.3421630859375\n",
      "epoch 1, val_loss 454.9973449707031\n",
      "epoch 2, train_loss 1277.3421630859375\n",
      "epoch 2, val_loss 454.9972229003906\n",
      "epoch 3, train_loss 1277.342041015625\n",
      "epoch 3, val_loss 454.9971618652344\n",
      "epoch 4, train_loss 1277.3419189453125\n",
      "epoch 4, val_loss 454.9971618652344\n",
      "epoch 5, train_loss 1277.341796875\n",
      "epoch 5, val_loss 454.99700927734375\n",
      "epoch 6, train_loss 1277.341796875\n",
      "epoch 6, val_loss 454.9969177246094\n",
      "epoch 7, train_loss 1277.3419189453125\n",
      "epoch 7, val_loss 454.99688720703125\n",
      "epoch 8, train_loss 1277.3416748046875\n",
      "epoch 8, val_loss 454.996826171875\n",
      "epoch 9, train_loss 1277.3416748046875\n",
      "epoch 9, val_loss 454.9967956542969\n",
      "epoch 10, train_loss 1277.341552734375\n",
      "epoch 10, val_loss 454.99676513671875\n",
      "epoch 11, train_loss 1277.3414306640625\n",
      "epoch 11, val_loss 454.9966125488281\n",
      "epoch 12, train_loss 1277.3414306640625\n",
      "epoch 12, val_loss 454.9965515136719\n",
      "epoch 13, train_loss 1277.34130859375\n",
      "epoch 13, val_loss 454.9964904785156\n",
      "epoch 14, train_loss 1277.34130859375\n",
      "epoch 14, val_loss 454.9964599609375\n",
      "epoch 15, train_loss 1277.3409423828125\n",
      "epoch 15, val_loss 454.996337890625\n",
      "epoch 16, train_loss 1277.3409423828125\n",
      "epoch 16, val_loss 454.996337890625\n",
      "epoch 17, train_loss 1277.3409423828125\n",
      "epoch 17, val_loss 454.9962463378906\n",
      "epoch 18, train_loss 1277.3408203125\n",
      "epoch 18, val_loss 454.99615478515625\n",
      "epoch 19, train_loss 1277.3409423828125\n",
      "epoch 19, val_loss 454.9961242675781\n",
      "epoch 20, train_loss 1277.3408203125\n",
      "epoch 20, val_loss 454.99609375\n",
      "epoch 21, train_loss 1277.3408203125\n",
      "epoch 21, val_loss 454.9960021972656\n",
      "epoch 22, train_loss 1277.3406982421875\n",
      "epoch 22, val_loss 454.9959716796875\n",
      "epoch 23, train_loss 1277.3404541015625\n",
      "epoch 23, val_loss 454.9958190917969\n",
      "epoch 24, train_loss 1277.3404541015625\n",
      "epoch 24, val_loss 454.9958190917969\n",
      "epoch 25, train_loss 1277.34033203125\n",
      "epoch 25, val_loss 454.9956970214844\n",
      "epoch 26, train_loss 1277.3402099609375\n",
      "epoch 26, val_loss 454.9956359863281\n",
      "epoch 27, train_loss 1277.3402099609375\n",
      "epoch 27, val_loss 454.9956359863281\n",
      "epoch 28, train_loss 1277.3402099609375\n",
      "epoch 28, val_loss 454.99554443359375\n",
      "epoch 29, train_loss 1277.3399658203125\n",
      "epoch 29, val_loss 454.9955139160156\n",
      "epoch 30, train_loss 1277.3399658203125\n",
      "epoch 30, val_loss 454.9954528808594\n",
      "epoch 31, train_loss 1277.3399658203125\n",
      "epoch 31, val_loss 454.99542236328125\n",
      "epoch 32, train_loss 1277.33984375\n",
      "epoch 32, val_loss 454.9952392578125\n",
      "epoch 33, train_loss 1277.3397216796875\n",
      "epoch 33, val_loss 454.9952087402344\n",
      "epoch 34, train_loss 1277.3397216796875\n",
      "epoch 34, val_loss 454.9951171875\n",
      "epoch 35, train_loss 1277.3397216796875\n",
      "epoch 35, val_loss 454.9951171875\n",
      "epoch 36, train_loss 1277.3394775390625\n",
      "epoch 36, val_loss 454.9950866699219\n",
      "epoch 37, train_loss 1277.3394775390625\n",
      "epoch 37, val_loss 454.9949645996094\n",
      "epoch 38, train_loss 1277.3394775390625\n",
      "epoch 38, val_loss 454.9949951171875\n",
      "epoch 39, train_loss 1277.33935546875\n",
      "epoch 39, val_loss 454.994873046875\n",
      "epoch 40, train_loss 1277.3392333984375\n",
      "epoch 40, val_loss 454.9947509765625\n",
      "epoch 41, train_loss 1277.339111328125\n",
      "epoch 41, val_loss 454.9947509765625\n",
      "epoch 42, train_loss 1277.339111328125\n",
      "epoch 42, val_loss 454.99462890625\n",
      "epoch 43, train_loss 1277.3388671875\n",
      "epoch 43, val_loss 454.9945983886719\n",
      "epoch 44, train_loss 1277.339111328125\n",
      "epoch 44, val_loss 454.9945373535156\n",
      "epoch 45, train_loss 1277.339111328125\n",
      "epoch 45, val_loss 454.9945068359375\n",
      "epoch 46, train_loss 1277.3389892578125\n",
      "epoch 46, val_loss 454.994384765625\n",
      "epoch 47, train_loss 1277.3387451171875\n",
      "epoch 47, val_loss 454.994384765625\n",
      "epoch 48, train_loss 1277.3387451171875\n",
      "epoch 48, val_loss 454.9942932128906\n",
      "epoch 49, train_loss 1277.338623046875\n",
      "epoch 49, val_loss 454.9942626953125\n",
      "epoch 50, train_loss 1277.338623046875\n",
      "epoch 50, val_loss 454.9941711425781\n",
      "epoch 51, train_loss 1277.3385009765625\n",
      "epoch 51, val_loss 454.99407958984375\n",
      "epoch 52, train_loss 1277.33837890625\n",
      "epoch 52, val_loss 454.9939880371094\n",
      "epoch 53, train_loss 1277.33837890625\n",
      "epoch 53, val_loss 454.9939270019531\n",
      "epoch 54, train_loss 1277.33837890625\n",
      "epoch 54, val_loss 454.9938659667969\n",
      "epoch 55, train_loss 1277.3382568359375\n",
      "epoch 55, val_loss 454.99383544921875\n",
      "epoch 56, train_loss 1277.338134765625\n",
      "epoch 56, val_loss 454.9937438964844\n",
      "epoch 57, train_loss 1277.337890625\n",
      "epoch 57, val_loss 454.9937438964844\n",
      "epoch 58, train_loss 1277.3380126953125\n",
      "epoch 58, val_loss 454.99365234375\n",
      "epoch 59, train_loss 1277.3380126953125\n",
      "epoch 59, val_loss 454.99359130859375\n",
      "epoch 60, train_loss 1277.338134765625\n",
      "epoch 60, val_loss 454.9934997558594\n",
      "epoch 61, train_loss 1277.337890625\n",
      "epoch 61, val_loss 454.993408203125\n",
      "epoch 62, train_loss 1277.3377685546875\n",
      "epoch 62, val_loss 454.9933776855469\n",
      "epoch 63, train_loss 1277.3375244140625\n",
      "epoch 63, val_loss 454.99334716796875\n",
      "epoch 64, train_loss 1277.3375244140625\n",
      "epoch 64, val_loss 454.9932556152344\n",
      "epoch 65, train_loss 1277.3375244140625\n",
      "epoch 65, val_loss 454.9931945800781\n",
      "epoch 66, train_loss 1277.33740234375\n",
      "epoch 66, val_loss 454.9931335449219\n",
      "epoch 67, train_loss 1277.33740234375\n",
      "epoch 67, val_loss 454.9930419921875\n",
      "epoch 68, train_loss 1277.3372802734375\n",
      "epoch 68, val_loss 454.9930114746094\n",
      "epoch 69, train_loss 1277.3372802734375\n",
      "epoch 69, val_loss 454.992919921875\n",
      "epoch 70, train_loss 1277.3372802734375\n",
      "epoch 70, val_loss 454.9928283691406\n",
      "epoch 71, train_loss 1277.3370361328125\n",
      "epoch 71, val_loss 454.9928283691406\n",
      "epoch 72, train_loss 1277.3369140625\n",
      "epoch 72, val_loss 454.99273681640625\n",
      "epoch 73, train_loss 1277.3369140625\n",
      "epoch 73, val_loss 454.99267578125\n",
      "epoch 74, train_loss 1277.3367919921875\n",
      "epoch 74, val_loss 454.9925842285156\n",
      "epoch 75, train_loss 1277.336669921875\n",
      "epoch 75, val_loss 454.9925842285156\n",
      "epoch 76, train_loss 1277.336669921875\n",
      "epoch 76, val_loss 454.99249267578125\n",
      "epoch 77, train_loss 1277.336669921875\n",
      "epoch 77, val_loss 454.992431640625\n",
      "epoch 78, train_loss 1277.3365478515625\n",
      "epoch 78, val_loss 454.9923400878906\n",
      "epoch 79, train_loss 1277.3365478515625\n",
      "epoch 79, val_loss 454.9922790527344\n",
      "epoch 80, train_loss 1277.3363037109375\n",
      "epoch 80, val_loss 454.9922180175781\n",
      "epoch 81, train_loss 1277.3363037109375\n",
      "epoch 81, val_loss 454.9921569824219\n",
      "epoch 82, train_loss 1277.3363037109375\n",
      "epoch 82, val_loss 454.99212646484375\n",
      "epoch 83, train_loss 1277.3363037109375\n",
      "epoch 83, val_loss 454.9920349121094\n",
      "epoch 84, train_loss 1277.3363037109375\n",
      "epoch 84, val_loss 454.9920349121094\n",
      "epoch 85, train_loss 1277.3360595703125\n",
      "epoch 85, val_loss 454.991943359375\n",
      "epoch 86, train_loss 1277.3359375\n",
      "epoch 86, val_loss 454.9919128417969\n",
      "epoch 87, train_loss 1277.3359375\n",
      "epoch 87, val_loss 454.99169921875\n",
      "epoch 88, train_loss 1277.3360595703125\n",
      "epoch 88, val_loss 454.99169921875\n",
      "epoch 89, train_loss 1277.335693359375\n",
      "epoch 89, val_loss 454.99163818359375\n",
      "epoch 90, train_loss 1277.3355712890625\n",
      "epoch 90, val_loss 454.9915771484375\n",
      "epoch 91, train_loss 1277.335693359375\n",
      "epoch 91, val_loss 454.9915466308594\n",
      "epoch 92, train_loss 1277.335693359375\n",
      "epoch 92, val_loss 454.9914855957031\n",
      "epoch 93, train_loss 1277.33544921875\n",
      "epoch 93, val_loss 454.9914245605469\n",
      "epoch 94, train_loss 1277.33544921875\n",
      "epoch 94, val_loss 454.9913024902344\n",
      "epoch 95, train_loss 1277.335205078125\n",
      "epoch 95, val_loss 454.9912414550781\n",
      "epoch 96, train_loss 1277.335205078125\n",
      "epoch 96, val_loss 454.9912109375\n",
      "epoch 97, train_loss 1277.335205078125\n",
      "epoch 97, val_loss 454.9911193847656\n",
      "epoch 98, train_loss 1277.335205078125\n",
      "epoch 98, val_loss 454.9910888671875\n",
      "epoch 99, train_loss 1277.3350830078125\n",
      "epoch 99, val_loss 454.99102783203125\n",
      "Parameter containing:\n",
      "tensor([1.0584e-19], requires_grad=True)\n",
      "iter 100, train_loss_regularization 0.7545742392539978\n",
      "iter 100, val_loss_regularization 0.7545742392539978\n",
      "epoch 0, train_loss 1277.3348388671875\n",
      "epoch 0, val_loss 454.990966796875\n",
      "epoch 1, train_loss 1277.3349609375\n",
      "epoch 1, val_loss 454.99090576171875\n",
      "epoch 2, train_loss 1277.334716796875\n",
      "epoch 2, val_loss 454.99078369140625\n",
      "epoch 3, train_loss 1277.334716796875\n",
      "epoch 3, val_loss 454.99078369140625\n",
      "epoch 4, train_loss 1277.3345947265625\n",
      "epoch 4, val_loss 454.9907531738281\n",
      "epoch 5, train_loss 1277.33447265625\n",
      "epoch 5, val_loss 454.9905700683594\n",
      "epoch 6, train_loss 1277.33447265625\n",
      "epoch 6, val_loss 454.9905700683594\n",
      "epoch 7, train_loss 1277.33447265625\n",
      "epoch 7, val_loss 454.9905700683594\n",
      "epoch 8, train_loss 1277.33447265625\n",
      "epoch 8, val_loss 454.99041748046875\n",
      "epoch 9, train_loss 1277.3343505859375\n",
      "epoch 9, val_loss 454.9903869628906\n",
      "epoch 10, train_loss 1277.334228515625\n",
      "epoch 10, val_loss 454.9903259277344\n",
      "epoch 11, train_loss 1277.3341064453125\n",
      "epoch 11, val_loss 454.9902648925781\n",
      "epoch 12, train_loss 1277.3341064453125\n",
      "epoch 12, val_loss 454.9902038574219\n",
      "epoch 13, train_loss 1277.3341064453125\n",
      "epoch 13, val_loss 454.99017333984375\n",
      "epoch 14, train_loss 1277.333740234375\n",
      "epoch 14, val_loss 454.989990234375\n",
      "epoch 15, train_loss 1277.3338623046875\n",
      "epoch 15, val_loss 454.9899597167969\n",
      "epoch 16, train_loss 1277.3338623046875\n",
      "epoch 16, val_loss 454.98992919921875\n",
      "epoch 17, train_loss 1277.3338623046875\n",
      "epoch 17, val_loss 454.9898681640625\n",
      "epoch 18, train_loss 1277.3338623046875\n",
      "epoch 18, val_loss 454.9898376464844\n",
      "epoch 19, train_loss 1277.3336181640625\n",
      "epoch 19, val_loss 454.9897766113281\n",
      "epoch 20, train_loss 1277.3336181640625\n",
      "epoch 20, val_loss 454.9897155761719\n",
      "epoch 21, train_loss 1277.33349609375\n",
      "epoch 21, val_loss 454.9895935058594\n",
      "epoch 22, train_loss 1277.3333740234375\n",
      "epoch 22, val_loss 454.9895935058594\n",
      "epoch 23, train_loss 1277.3333740234375\n",
      "epoch 23, val_loss 454.989501953125\n",
      "epoch 24, train_loss 1277.3333740234375\n",
      "epoch 24, val_loss 454.9894104003906\n",
      "epoch 25, train_loss 1277.3331298828125\n",
      "epoch 25, val_loss 454.9893798828125\n",
      "epoch 26, train_loss 1277.3330078125\n",
      "epoch 26, val_loss 454.98931884765625\n",
      "epoch 27, train_loss 1277.3330078125\n",
      "epoch 27, val_loss 454.9892578125\n",
      "epoch 28, train_loss 1277.3331298828125\n",
      "epoch 28, val_loss 454.9891357421875\n",
      "epoch 29, train_loss 1277.3328857421875\n",
      "epoch 29, val_loss 454.9891357421875\n",
      "epoch 30, train_loss 1277.3328857421875\n",
      "epoch 30, val_loss 454.9890441894531\n",
      "epoch 31, train_loss 1277.3326416015625\n",
      "epoch 31, val_loss 454.989013671875\n",
      "epoch 32, train_loss 1277.3326416015625\n",
      "epoch 32, val_loss 454.9889221191406\n",
      "epoch 33, train_loss 1277.3326416015625\n",
      "epoch 33, val_loss 454.98883056640625\n",
      "epoch 34, train_loss 1277.3326416015625\n",
      "epoch 34, val_loss 454.98883056640625\n",
      "epoch 35, train_loss 1277.33251953125\n",
      "epoch 35, val_loss 454.98870849609375\n",
      "epoch 36, train_loss 1277.332275390625\n",
      "epoch 36, val_loss 454.9886779785156\n",
      "epoch 37, train_loss 1277.332275390625\n",
      "epoch 37, val_loss 454.9886169433594\n",
      "epoch 38, train_loss 1277.332275390625\n",
      "epoch 38, val_loss 454.98858642578125\n",
      "epoch 39, train_loss 1277.3321533203125\n",
      "epoch 39, val_loss 454.9884948730469\n",
      "epoch 40, train_loss 1277.3321533203125\n",
      "epoch 40, val_loss 454.9884033203125\n",
      "epoch 41, train_loss 1277.33203125\n",
      "epoch 41, val_loss 454.9883728027344\n",
      "epoch 42, train_loss 1277.33203125\n",
      "epoch 42, val_loss 454.9882507324219\n",
      "epoch 43, train_loss 1277.331787109375\n",
      "epoch 43, val_loss 454.9881591796875\n",
      "epoch 44, train_loss 1277.331787109375\n",
      "epoch 44, val_loss 454.9881591796875\n",
      "epoch 45, train_loss 1277.3316650390625\n",
      "epoch 45, val_loss 454.98809814453125\n",
      "epoch 46, train_loss 1277.33154296875\n",
      "epoch 46, val_loss 454.988037109375\n",
      "epoch 47, train_loss 1277.3316650390625\n",
      "epoch 47, val_loss 454.988037109375\n",
      "epoch 48, train_loss 1277.33154296875\n",
      "epoch 48, val_loss 454.9879150390625\n",
      "epoch 49, train_loss 1277.33154296875\n",
      "epoch 49, val_loss 454.9878234863281\n",
      "epoch 50, train_loss 1277.3314208984375\n",
      "epoch 50, val_loss 454.9877624511719\n",
      "epoch 51, train_loss 1277.331298828125\n",
      "epoch 51, val_loss 454.9877014160156\n",
      "epoch 52, train_loss 1277.331298828125\n",
      "epoch 52, val_loss 454.98760986328125\n",
      "epoch 53, train_loss 1277.3311767578125\n",
      "epoch 53, val_loss 454.98760986328125\n",
      "epoch 54, train_loss 1277.3310546875\n",
      "epoch 54, val_loss 454.987548828125\n",
      "epoch 55, train_loss 1277.3310546875\n",
      "epoch 55, val_loss 454.98748779296875\n",
      "epoch 56, train_loss 1277.3310546875\n",
      "epoch 56, val_loss 454.9874267578125\n",
      "epoch 57, train_loss 1277.330810546875\n",
      "epoch 57, val_loss 454.9873046875\n",
      "epoch 58, train_loss 1277.330810546875\n",
      "epoch 58, val_loss 454.9873046875\n",
      "epoch 59, train_loss 1277.33056640625\n",
      "epoch 59, val_loss 454.98724365234375\n",
      "epoch 60, train_loss 1277.33056640625\n",
      "epoch 60, val_loss 454.9871520996094\n",
      "epoch 61, train_loss 1277.33056640625\n",
      "epoch 61, val_loss 454.9870910644531\n",
      "epoch 62, train_loss 1277.3304443359375\n",
      "epoch 62, val_loss 454.98699951171875\n",
      "epoch 63, train_loss 1277.3304443359375\n",
      "epoch 63, val_loss 454.9869689941406\n",
      "epoch 64, train_loss 1277.3304443359375\n",
      "epoch 64, val_loss 454.9869079589844\n",
      "epoch 65, train_loss 1277.330322265625\n",
      "epoch 65, val_loss 454.9868469238281\n",
      "epoch 66, train_loss 1277.330322265625\n",
      "epoch 66, val_loss 454.9867858886719\n",
      "epoch 67, train_loss 1277.3302001953125\n",
      "epoch 67, val_loss 454.98675537109375\n",
      "epoch 68, train_loss 1277.3299560546875\n",
      "epoch 68, val_loss 454.986572265625\n",
      "epoch 69, train_loss 1277.3299560546875\n",
      "epoch 69, val_loss 454.98663330078125\n",
      "epoch 70, train_loss 1277.3299560546875\n",
      "epoch 70, val_loss 454.9865417480469\n",
      "epoch 71, train_loss 1277.3299560546875\n",
      "epoch 71, val_loss 454.9864501953125\n",
      "epoch 72, train_loss 1277.3299560546875\n",
      "epoch 72, val_loss 454.98638916015625\n",
      "epoch 73, train_loss 1277.3297119140625\n",
      "epoch 73, val_loss 454.986328125\n",
      "epoch 74, train_loss 1277.3297119140625\n",
      "epoch 74, val_loss 454.9862976074219\n",
      "epoch 75, train_loss 1277.32958984375\n",
      "epoch 75, val_loss 454.9862365722656\n",
      "epoch 76, train_loss 1277.32958984375\n",
      "epoch 76, val_loss 454.9861145019531\n",
      "epoch 77, train_loss 1277.3297119140625\n",
      "epoch 77, val_loss 454.986083984375\n",
      "epoch 78, train_loss 1277.329345703125\n",
      "epoch 78, val_loss 454.9859924316406\n",
      "epoch 79, train_loss 1277.329345703125\n",
      "epoch 79, val_loss 454.9859313964844\n",
      "epoch 80, train_loss 1277.329345703125\n",
      "epoch 80, val_loss 454.9858703613281\n",
      "epoch 81, train_loss 1277.3292236328125\n",
      "epoch 81, val_loss 454.98583984375\n",
      "epoch 82, train_loss 1277.3292236328125\n",
      "epoch 82, val_loss 454.98577880859375\n",
      "epoch 83, train_loss 1277.3289794921875\n",
      "epoch 83, val_loss 454.9857177734375\n",
      "epoch 84, train_loss 1277.328857421875\n",
      "epoch 84, val_loss 454.9856262207031\n",
      "epoch 85, train_loss 1277.328857421875\n",
      "epoch 85, val_loss 454.985595703125\n",
      "epoch 86, train_loss 1277.328857421875\n",
      "epoch 86, val_loss 454.9855041503906\n",
      "epoch 87, train_loss 1277.328857421875\n",
      "epoch 87, val_loss 454.98541259765625\n",
      "epoch 88, train_loss 1277.32861328125\n",
      "epoch 88, val_loss 454.98541259765625\n",
      "epoch 89, train_loss 1277.32861328125\n",
      "epoch 89, val_loss 454.9853210449219\n",
      "epoch 90, train_loss 1277.32861328125\n",
      "epoch 90, val_loss 454.9851989746094\n",
      "epoch 91, train_loss 1277.32861328125\n",
      "epoch 91, val_loss 454.9851989746094\n",
      "epoch 92, train_loss 1277.328369140625\n",
      "epoch 92, val_loss 454.9851379394531\n",
      "epoch 93, train_loss 1277.3282470703125\n",
      "epoch 93, val_loss 454.98504638671875\n",
      "epoch 94, train_loss 1277.328125\n",
      "epoch 94, val_loss 454.9850769042969\n",
      "epoch 95, train_loss 1277.3282470703125\n",
      "epoch 95, val_loss 454.9849548339844\n",
      "epoch 96, train_loss 1277.328125\n",
      "epoch 96, val_loss 454.98492431640625\n",
      "epoch 97, train_loss 1277.3280029296875\n",
      "epoch 97, val_loss 454.98480224609375\n",
      "epoch 98, train_loss 1277.328125\n",
      "epoch 98, val_loss 454.9847412109375\n",
      "epoch 99, train_loss 1277.327880859375\n",
      "epoch 99, val_loss 454.98468017578125\n",
      "Parameter containing:\n",
      "tensor([7.2430e-20], requires_grad=True)\n",
      "iter 101, train_loss_regularization 0.7527000904083252\n",
      "iter 101, val_loss_regularization 0.7527000904083252\n",
      "epoch 0, train_loss 1277.327880859375\n",
      "epoch 0, val_loss 454.98468017578125\n",
      "epoch 1, train_loss 1277.3277587890625\n",
      "epoch 1, val_loss 454.9845886230469\n",
      "epoch 2, train_loss 1277.32763671875\n",
      "epoch 2, val_loss 454.9845275878906\n",
      "epoch 3, train_loss 1277.32763671875\n",
      "epoch 3, val_loss 454.9844970703125\n",
      "epoch 4, train_loss 1277.3275146484375\n",
      "epoch 4, val_loss 454.984375\n",
      "epoch 5, train_loss 1277.32763671875\n",
      "epoch 5, val_loss 454.9842834472656\n",
      "epoch 6, train_loss 1277.327392578125\n",
      "epoch 6, val_loss 454.9842834472656\n",
      "epoch 7, train_loss 1277.3272705078125\n",
      "epoch 7, val_loss 454.9841613769531\n",
      "epoch 8, train_loss 1277.3272705078125\n",
      "epoch 8, val_loss 454.984130859375\n",
      "epoch 9, train_loss 1277.3272705078125\n",
      "epoch 9, val_loss 454.9840393066406\n",
      "epoch 10, train_loss 1277.3271484375\n",
      "epoch 10, val_loss 454.9840393066406\n",
      "epoch 11, train_loss 1277.3270263671875\n",
      "epoch 11, val_loss 454.98394775390625\n",
      "epoch 12, train_loss 1277.3270263671875\n",
      "epoch 12, val_loss 454.98388671875\n",
      "epoch 13, train_loss 1277.3270263671875\n",
      "epoch 13, val_loss 454.98382568359375\n",
      "epoch 14, train_loss 1277.3270263671875\n",
      "epoch 14, val_loss 454.9837646484375\n",
      "epoch 15, train_loss 1277.326904296875\n",
      "epoch 15, val_loss 454.9836730957031\n",
      "epoch 16, train_loss 1277.32666015625\n",
      "epoch 16, val_loss 454.9836730957031\n",
      "epoch 17, train_loss 1277.3265380859375\n",
      "epoch 17, val_loss 454.9836120605469\n",
      "epoch 18, train_loss 1277.3265380859375\n",
      "epoch 18, val_loss 454.98345947265625\n",
      "epoch 19, train_loss 1277.3265380859375\n",
      "epoch 19, val_loss 454.9834289550781\n",
      "epoch 20, train_loss 1277.326416015625\n",
      "epoch 20, val_loss 454.9833679199219\n",
      "epoch 21, train_loss 1277.3262939453125\n",
      "epoch 21, val_loss 454.98333740234375\n",
      "epoch 22, train_loss 1277.326171875\n",
      "epoch 22, val_loss 454.98333740234375\n",
      "epoch 23, train_loss 1277.3262939453125\n",
      "epoch 23, val_loss 454.983154296875\n",
      "epoch 24, train_loss 1277.3262939453125\n",
      "epoch 24, val_loss 454.9831237792969\n",
      "epoch 25, train_loss 1277.3260498046875\n",
      "epoch 25, val_loss 454.9830322265625\n",
      "epoch 26, train_loss 1277.3260498046875\n",
      "epoch 26, val_loss 454.9830017089844\n",
      "epoch 27, train_loss 1277.3258056640625\n",
      "epoch 27, val_loss 454.98291015625\n",
      "epoch 28, train_loss 1277.3258056640625\n",
      "epoch 28, val_loss 454.9828186035156\n",
      "epoch 29, train_loss 1277.3258056640625\n",
      "epoch 29, val_loss 454.9828186035156\n",
      "epoch 30, train_loss 1277.3258056640625\n",
      "epoch 30, val_loss 454.9827880859375\n",
      "epoch 31, train_loss 1277.32568359375\n",
      "epoch 31, val_loss 454.9826965332031\n",
      "epoch 32, train_loss 1277.325439453125\n",
      "epoch 32, val_loss 454.982666015625\n",
      "epoch 33, train_loss 1277.325439453125\n",
      "epoch 33, val_loss 454.9825744628906\n",
      "epoch 34, train_loss 1277.325439453125\n",
      "epoch 34, val_loss 454.9824523925781\n",
      "epoch 35, train_loss 1277.325439453125\n",
      "epoch 35, val_loss 454.9824523925781\n",
      "epoch 36, train_loss 1277.3253173828125\n",
      "epoch 36, val_loss 454.982421875\n",
      "epoch 37, train_loss 1277.3251953125\n",
      "epoch 37, val_loss 454.9823303222656\n",
      "epoch 38, train_loss 1277.3251953125\n",
      "epoch 38, val_loss 454.98223876953125\n",
      "epoch 39, train_loss 1277.3251953125\n",
      "epoch 39, val_loss 454.9822082519531\n",
      "epoch 40, train_loss 1277.3250732421875\n",
      "epoch 40, val_loss 454.9820861816406\n",
      "epoch 41, train_loss 1277.324951171875\n",
      "epoch 41, val_loss 454.9820861816406\n",
      "epoch 42, train_loss 1277.324951171875\n",
      "epoch 42, val_loss 454.9819030761719\n",
      "epoch 43, train_loss 1277.32470703125\n",
      "epoch 43, val_loss 454.9819030761719\n",
      "epoch 44, train_loss 1277.32470703125\n",
      "epoch 44, val_loss 454.9819030761719\n",
      "epoch 45, train_loss 1277.32470703125\n",
      "epoch 45, val_loss 454.9817810058594\n",
      "epoch 46, train_loss 1277.32470703125\n",
      "epoch 46, val_loss 454.98175048828125\n",
      "epoch 47, train_loss 1277.3245849609375\n",
      "epoch 47, val_loss 454.9816589355469\n",
      "epoch 48, train_loss 1277.324462890625\n",
      "epoch 48, val_loss 454.98162841796875\n",
      "epoch 49, train_loss 1277.324462890625\n",
      "epoch 49, val_loss 454.9815979003906\n",
      "epoch 50, train_loss 1277.324462890625\n",
      "epoch 50, val_loss 454.98150634765625\n",
      "epoch 51, train_loss 1277.32421875\n",
      "epoch 51, val_loss 454.9814453125\n",
      "epoch 52, train_loss 1277.32421875\n",
      "epoch 52, val_loss 454.9813232421875\n",
      "epoch 53, train_loss 1277.32421875\n",
      "epoch 53, val_loss 454.98126220703125\n",
      "epoch 54, train_loss 1277.3240966796875\n",
      "epoch 54, val_loss 454.98126220703125\n",
      "epoch 55, train_loss 1277.323974609375\n",
      "epoch 55, val_loss 454.981201171875\n",
      "epoch 56, train_loss 1277.323974609375\n",
      "epoch 56, val_loss 454.98114013671875\n",
      "epoch 57, train_loss 1277.323974609375\n",
      "epoch 57, val_loss 454.98114013671875\n",
      "epoch 58, train_loss 1277.3236083984375\n",
      "epoch 58, val_loss 454.9810485839844\n",
      "epoch 59, train_loss 1277.3236083984375\n",
      "epoch 59, val_loss 454.9809265136719\n",
      "epoch 60, train_loss 1277.3236083984375\n",
      "epoch 60, val_loss 454.9808349609375\n",
      "epoch 61, train_loss 1277.323486328125\n",
      "epoch 61, val_loss 454.9808349609375\n",
      "epoch 62, train_loss 1277.3236083984375\n",
      "epoch 62, val_loss 454.9807434082031\n",
      "epoch 63, train_loss 1277.323486328125\n",
      "epoch 63, val_loss 454.98065185546875\n",
      "epoch 64, train_loss 1277.323486328125\n",
      "epoch 64, val_loss 454.9806213378906\n",
      "epoch 65, train_loss 1277.3232421875\n",
      "epoch 65, val_loss 454.9805908203125\n",
      "epoch 66, train_loss 1277.3231201171875\n",
      "epoch 66, val_loss 454.98052978515625\n",
      "epoch 67, train_loss 1277.3231201171875\n",
      "epoch 67, val_loss 454.98040771484375\n",
      "epoch 68, train_loss 1277.3231201171875\n",
      "epoch 68, val_loss 454.9803466796875\n",
      "epoch 69, train_loss 1277.3228759765625\n",
      "epoch 69, val_loss 454.9803466796875\n",
      "epoch 70, train_loss 1277.3228759765625\n",
      "epoch 70, val_loss 454.9801940917969\n",
      "epoch 71, train_loss 1277.3228759765625\n",
      "epoch 71, val_loss 454.9801940917969\n",
      "epoch 72, train_loss 1277.3226318359375\n",
      "epoch 72, val_loss 454.9801330566406\n",
      "epoch 73, train_loss 1277.3226318359375\n",
      "epoch 73, val_loss 454.9800720214844\n",
      "epoch 74, train_loss 1277.322509765625\n",
      "epoch 74, val_loss 454.9800109863281\n",
      "epoch 75, train_loss 1277.322509765625\n",
      "epoch 75, val_loss 454.97991943359375\n",
      "epoch 76, train_loss 1277.3223876953125\n",
      "epoch 76, val_loss 454.9798889160156\n",
      "epoch 77, train_loss 1277.3223876953125\n",
      "epoch 77, val_loss 454.9798278808594\n",
      "epoch 78, train_loss 1277.3223876953125\n",
      "epoch 78, val_loss 454.979736328125\n",
      "epoch 79, train_loss 1277.3223876953125\n",
      "epoch 79, val_loss 454.97967529296875\n",
      "epoch 80, train_loss 1277.3221435546875\n",
      "epoch 80, val_loss 454.9796142578125\n",
      "epoch 81, train_loss 1277.3221435546875\n",
      "epoch 81, val_loss 454.9794921875\n",
      "epoch 82, train_loss 1277.322021484375\n",
      "epoch 82, val_loss 454.9794921875\n",
      "epoch 83, train_loss 1277.322021484375\n",
      "epoch 83, val_loss 454.9794616699219\n",
      "epoch 84, train_loss 1277.3218994140625\n",
      "epoch 84, val_loss 454.9793701171875\n",
      "epoch 85, train_loss 1277.32177734375\n",
      "epoch 85, val_loss 454.9793395996094\n",
      "epoch 86, train_loss 1277.32177734375\n",
      "epoch 86, val_loss 454.9792785644531\n",
      "epoch 87, train_loss 1277.321533203125\n",
      "epoch 87, val_loss 454.9792175292969\n",
      "epoch 88, train_loss 1277.32177734375\n",
      "epoch 88, val_loss 454.9790954589844\n",
      "epoch 89, train_loss 1277.32177734375\n",
      "epoch 89, val_loss 454.97900390625\n",
      "epoch 90, train_loss 1277.321533203125\n",
      "epoch 90, val_loss 454.97900390625\n",
      "epoch 91, train_loss 1277.3214111328125\n",
      "epoch 91, val_loss 454.9789123535156\n",
      "epoch 92, train_loss 1277.3214111328125\n",
      "epoch 92, val_loss 454.9788818359375\n",
      "epoch 93, train_loss 1277.3212890625\n",
      "epoch 93, val_loss 454.97882080078125\n",
      "epoch 94, train_loss 1277.3212890625\n",
      "epoch 94, val_loss 454.978759765625\n",
      "epoch 95, train_loss 1277.3211669921875\n",
      "epoch 95, val_loss 454.9786682128906\n",
      "epoch 96, train_loss 1277.321044921875\n",
      "epoch 96, val_loss 454.9786376953125\n",
      "epoch 97, train_loss 1277.3211669921875\n",
      "epoch 97, val_loss 454.97857666015625\n",
      "epoch 98, train_loss 1277.3209228515625\n",
      "epoch 98, val_loss 454.9784851074219\n",
      "epoch 99, train_loss 1277.3209228515625\n",
      "epoch 99, val_loss 454.97845458984375\n",
      "Parameter containing:\n",
      "tensor([4.9596e-20], requires_grad=True)\n",
      "iter 102, train_loss_regularization 0.7508618235588074\n",
      "iter 102, val_loss_regularization 0.7508618235588074\n",
      "epoch 0, train_loss 1277.32080078125\n",
      "epoch 0, val_loss 454.9784240722656\n",
      "epoch 1, train_loss 1277.3206787109375\n",
      "epoch 1, val_loss 454.9783020019531\n",
      "epoch 2, train_loss 1277.32080078125\n",
      "epoch 2, val_loss 454.9782409667969\n",
      "epoch 3, train_loss 1277.32080078125\n",
      "epoch 3, val_loss 454.97821044921875\n",
      "epoch 4, train_loss 1277.3206787109375\n",
      "epoch 4, val_loss 454.9781799316406\n",
      "epoch 5, train_loss 1277.320556640625\n",
      "epoch 5, val_loss 454.97808837890625\n",
      "epoch 6, train_loss 1277.3204345703125\n",
      "epoch 6, val_loss 454.9779968261719\n",
      "epoch 7, train_loss 1277.3203125\n",
      "epoch 7, val_loss 454.9779052734375\n",
      "epoch 8, train_loss 1277.3203125\n",
      "epoch 8, val_loss 454.9779052734375\n",
      "epoch 9, train_loss 1277.3201904296875\n",
      "epoch 9, val_loss 454.977783203125\n",
      "epoch 10, train_loss 1277.3201904296875\n",
      "epoch 10, val_loss 454.9777526855469\n",
      "epoch 11, train_loss 1277.320068359375\n",
      "epoch 11, val_loss 454.97772216796875\n",
      "epoch 12, train_loss 1277.3199462890625\n",
      "epoch 12, val_loss 454.9776306152344\n",
      "epoch 13, train_loss 1277.3199462890625\n",
      "epoch 13, val_loss 454.9775695800781\n",
      "epoch 14, train_loss 1277.3199462890625\n",
      "epoch 14, val_loss 454.9775390625\n",
      "epoch 15, train_loss 1277.31982421875\n",
      "epoch 15, val_loss 454.9775085449219\n",
      "epoch 16, train_loss 1277.3197021484375\n",
      "epoch 16, val_loss 454.9773864746094\n",
      "epoch 17, train_loss 1277.3197021484375\n",
      "epoch 17, val_loss 454.977294921875\n",
      "epoch 18, train_loss 1277.3197021484375\n",
      "epoch 18, val_loss 454.9772644042969\n",
      "epoch 19, train_loss 1277.319580078125\n",
      "epoch 19, val_loss 454.9771728515625\n",
      "epoch 20, train_loss 1277.3193359375\n",
      "epoch 20, val_loss 454.9771728515625\n",
      "epoch 21, train_loss 1277.3193359375\n",
      "epoch 21, val_loss 454.97711181640625\n",
      "epoch 22, train_loss 1277.3192138671875\n",
      "epoch 22, val_loss 454.97705078125\n",
      "epoch 23, train_loss 1277.3192138671875\n",
      "epoch 23, val_loss 454.9769287109375\n",
      "epoch 24, train_loss 1277.3192138671875\n",
      "epoch 24, val_loss 454.9769287109375\n",
      "epoch 25, train_loss 1277.3189697265625\n",
      "epoch 25, val_loss 454.9768371582031\n",
      "epoch 26, train_loss 1277.319091796875\n",
      "epoch 26, val_loss 454.97674560546875\n",
      "epoch 27, train_loss 1277.3189697265625\n",
      "epoch 27, val_loss 454.9767150878906\n",
      "epoch 28, train_loss 1277.3189697265625\n",
      "epoch 28, val_loss 454.97662353515625\n",
      "epoch 29, train_loss 1277.3189697265625\n",
      "epoch 29, val_loss 454.9765930175781\n",
      "epoch 30, train_loss 1277.3187255859375\n",
      "epoch 30, val_loss 454.97650146484375\n",
      "epoch 31, train_loss 1277.318603515625\n",
      "epoch 31, val_loss 454.9764709472656\n",
      "epoch 32, train_loss 1277.318603515625\n",
      "epoch 32, val_loss 454.97637939453125\n",
      "epoch 33, train_loss 1277.3184814453125\n",
      "epoch 33, val_loss 454.9764099121094\n",
      "epoch 34, train_loss 1277.318359375\n",
      "epoch 34, val_loss 454.97625732421875\n",
      "epoch 35, train_loss 1277.3182373046875\n",
      "epoch 35, val_loss 454.9761962890625\n",
      "epoch 36, train_loss 1277.3182373046875\n",
      "epoch 36, val_loss 454.9761962890625\n",
      "epoch 37, train_loss 1277.318359375\n",
      "epoch 37, val_loss 454.97607421875\n",
      "epoch 38, train_loss 1277.318115234375\n",
      "epoch 38, val_loss 454.97601318359375\n",
      "epoch 39, train_loss 1277.318115234375\n",
      "epoch 39, val_loss 454.9760437011719\n",
      "epoch 40, train_loss 1277.3179931640625\n",
      "epoch 40, val_loss 454.9759216308594\n",
      "epoch 41, train_loss 1277.31787109375\n",
      "epoch 41, val_loss 454.9758605957031\n",
      "epoch 42, train_loss 1277.31787109375\n",
      "epoch 42, val_loss 454.9757995605469\n",
      "epoch 43, train_loss 1277.31787109375\n",
      "epoch 43, val_loss 454.9756774902344\n",
      "epoch 44, train_loss 1277.3177490234375\n",
      "epoch 44, val_loss 454.9756774902344\n",
      "epoch 45, train_loss 1277.317626953125\n",
      "epoch 45, val_loss 454.9755859375\n",
      "epoch 46, train_loss 1277.317626953125\n",
      "epoch 46, val_loss 454.9755554199219\n",
      "epoch 47, train_loss 1277.317626953125\n",
      "epoch 47, val_loss 454.9754943847656\n",
      "epoch 48, train_loss 1277.317626953125\n",
      "epoch 48, val_loss 454.97540283203125\n",
      "epoch 49, train_loss 1277.3175048828125\n",
      "epoch 49, val_loss 454.9753723144531\n",
      "epoch 50, train_loss 1277.3172607421875\n",
      "epoch 50, val_loss 454.975341796875\n",
      "epoch 51, train_loss 1277.3173828125\n",
      "epoch 51, val_loss 454.9752197265625\n",
      "epoch 52, train_loss 1277.3172607421875\n",
      "epoch 52, val_loss 454.97515869140625\n",
      "epoch 53, train_loss 1277.3172607421875\n",
      "epoch 53, val_loss 454.97509765625\n",
      "epoch 54, train_loss 1277.3172607421875\n",
      "epoch 54, val_loss 454.9750061035156\n",
      "epoch 55, train_loss 1277.317138671875\n",
      "epoch 55, val_loss 454.9750061035156\n",
      "epoch 56, train_loss 1277.31689453125\n",
      "epoch 56, val_loss 454.97491455078125\n",
      "epoch 57, train_loss 1277.31689453125\n",
      "epoch 57, val_loss 454.9748840332031\n",
      "epoch 58, train_loss 1277.3167724609375\n",
      "epoch 58, val_loss 454.97479248046875\n",
      "epoch 59, train_loss 1277.3167724609375\n",
      "epoch 59, val_loss 454.9747619628906\n",
      "epoch 60, train_loss 1277.3165283203125\n",
      "epoch 60, val_loss 454.9747009277344\n",
      "epoch 61, train_loss 1277.3165283203125\n",
      "epoch 61, val_loss 454.97467041015625\n",
      "epoch 62, train_loss 1277.3165283203125\n",
      "epoch 62, val_loss 454.97454833984375\n",
      "epoch 63, train_loss 1277.3165283203125\n",
      "epoch 63, val_loss 454.9744567871094\n",
      "epoch 64, train_loss 1277.31640625\n",
      "epoch 64, val_loss 454.97442626953125\n",
      "epoch 65, train_loss 1277.3162841796875\n",
      "epoch 65, val_loss 454.97430419921875\n",
      "epoch 66, train_loss 1277.3162841796875\n",
      "epoch 66, val_loss 454.97430419921875\n",
      "epoch 67, train_loss 1277.3162841796875\n",
      "epoch 67, val_loss 454.9742126464844\n",
      "epoch 68, train_loss 1277.316162109375\n",
      "epoch 68, val_loss 454.9741516113281\n",
      "epoch 69, train_loss 1277.316162109375\n",
      "epoch 69, val_loss 454.97412109375\n",
      "epoch 70, train_loss 1277.3160400390625\n",
      "epoch 70, val_loss 454.9739990234375\n",
      "epoch 71, train_loss 1277.31591796875\n",
      "epoch 71, val_loss 454.9739990234375\n",
      "epoch 72, train_loss 1277.315673828125\n",
      "epoch 72, val_loss 454.9739074707031\n",
      "epoch 73, train_loss 1277.3157958984375\n",
      "epoch 73, val_loss 454.9738464355469\n",
      "epoch 74, train_loss 1277.3157958984375\n",
      "epoch 74, val_loss 454.9738464355469\n",
      "epoch 75, train_loss 1277.315673828125\n",
      "epoch 75, val_loss 454.9737854003906\n",
      "epoch 76, train_loss 1277.3155517578125\n",
      "epoch 76, val_loss 454.9736633300781\n",
      "epoch 77, train_loss 1277.3155517578125\n",
      "epoch 77, val_loss 454.9736633300781\n",
      "epoch 78, train_loss 1277.3155517578125\n",
      "epoch 78, val_loss 454.9736328125\n",
      "epoch 79, train_loss 1277.3154296875\n",
      "epoch 79, val_loss 454.97344970703125\n",
      "epoch 80, train_loss 1277.3153076171875\n",
      "epoch 80, val_loss 454.9734191894531\n",
      "epoch 81, train_loss 1277.315185546875\n",
      "epoch 81, val_loss 454.9732971191406\n",
      "epoch 82, train_loss 1277.31494140625\n",
      "epoch 82, val_loss 454.9732971191406\n",
      "epoch 83, train_loss 1277.3150634765625\n",
      "epoch 83, val_loss 454.9732360839844\n",
      "epoch 84, train_loss 1277.3150634765625\n",
      "epoch 84, val_loss 454.9731140136719\n",
      "epoch 85, train_loss 1277.31494140625\n",
      "epoch 85, val_loss 454.9731750488281\n",
      "epoch 86, train_loss 1277.31494140625\n",
      "epoch 86, val_loss 454.97308349609375\n",
      "epoch 87, train_loss 1277.314697265625\n",
      "epoch 87, val_loss 454.9729919433594\n",
      "epoch 88, train_loss 1277.314697265625\n",
      "epoch 88, val_loss 454.97296142578125\n",
      "epoch 89, train_loss 1277.314697265625\n",
      "epoch 89, val_loss 454.9729309082031\n",
      "epoch 90, train_loss 1277.3145751953125\n",
      "epoch 90, val_loss 454.97283935546875\n",
      "epoch 91, train_loss 1277.314453125\n",
      "epoch 91, val_loss 454.9727783203125\n",
      "epoch 92, train_loss 1277.314453125\n",
      "epoch 92, val_loss 454.97271728515625\n",
      "epoch 93, train_loss 1277.3143310546875\n",
      "epoch 93, val_loss 454.9726257324219\n",
      "epoch 94, train_loss 1277.3143310546875\n",
      "epoch 94, val_loss 454.9725341796875\n",
      "epoch 95, train_loss 1277.3143310546875\n",
      "epoch 95, val_loss 454.9725036621094\n",
      "epoch 96, train_loss 1277.314208984375\n",
      "epoch 96, val_loss 454.9725036621094\n",
      "epoch 97, train_loss 1277.3140869140625\n",
      "epoch 97, val_loss 454.97247314453125\n",
      "epoch 98, train_loss 1277.31396484375\n",
      "epoch 98, val_loss 454.9723205566406\n",
      "epoch 99, train_loss 1277.31396484375\n",
      "epoch 99, val_loss 454.9722595214844\n",
      "Parameter containing:\n",
      "tensor([3.3980e-20], requires_grad=True)\n",
      "iter 103, train_loss_regularization 0.7490594983100891\n",
      "iter 103, val_loss_regularization 0.7490594983100891\n",
      "epoch 0, train_loss 1277.3138427734375\n",
      "epoch 0, val_loss 454.97216796875\n",
      "epoch 1, train_loss 1277.3138427734375\n",
      "epoch 1, val_loss 454.9721374511719\n",
      "epoch 2, train_loss 1277.3138427734375\n",
      "epoch 2, val_loss 454.9720458984375\n",
      "epoch 3, train_loss 1277.3138427734375\n",
      "epoch 3, val_loss 454.9720458984375\n",
      "epoch 4, train_loss 1277.3138427734375\n",
      "epoch 4, val_loss 454.971923828125\n",
      "epoch 5, train_loss 1277.3135986328125\n",
      "epoch 5, val_loss 454.971923828125\n",
      "epoch 6, train_loss 1277.3134765625\n",
      "epoch 6, val_loss 454.9718322753906\n",
      "epoch 7, train_loss 1277.3134765625\n",
      "epoch 7, val_loss 454.97174072265625\n",
      "epoch 8, train_loss 1277.3133544921875\n",
      "epoch 8, val_loss 454.9716796875\n",
      "epoch 9, train_loss 1277.3133544921875\n",
      "epoch 9, val_loss 454.9716796875\n",
      "epoch 10, train_loss 1277.3133544921875\n",
      "epoch 10, val_loss 454.9715270996094\n",
      "epoch 11, train_loss 1277.3131103515625\n",
      "epoch 11, val_loss 454.9715270996094\n",
      "epoch 12, train_loss 1277.3131103515625\n",
      "epoch 12, val_loss 454.97149658203125\n",
      "epoch 13, train_loss 1277.3131103515625\n",
      "epoch 13, val_loss 454.97137451171875\n",
      "epoch 14, train_loss 1277.31298828125\n",
      "epoch 14, val_loss 454.9713439941406\n",
      "epoch 15, train_loss 1277.312744140625\n",
      "epoch 15, val_loss 454.9712829589844\n",
      "epoch 16, train_loss 1277.312744140625\n",
      "epoch 16, val_loss 454.9712219238281\n",
      "epoch 17, train_loss 1277.3128662109375\n",
      "epoch 17, val_loss 454.9711608886719\n",
      "epoch 18, train_loss 1277.3128662109375\n",
      "epoch 18, val_loss 454.9710693359375\n",
      "epoch 19, train_loss 1277.312744140625\n",
      "epoch 19, val_loss 454.9710388183594\n",
      "epoch 20, train_loss 1277.3125\n",
      "epoch 20, val_loss 454.97100830078125\n",
      "epoch 21, train_loss 1277.3125\n",
      "epoch 21, val_loss 454.9709167480469\n",
      "epoch 22, train_loss 1277.3125\n",
      "epoch 22, val_loss 454.9708251953125\n",
      "epoch 23, train_loss 1277.3123779296875\n",
      "epoch 23, val_loss 454.9707946777344\n",
      "epoch 24, train_loss 1277.3123779296875\n",
      "epoch 24, val_loss 454.970703125\n",
      "epoch 25, train_loss 1277.3121337890625\n",
      "epoch 25, val_loss 454.970703125\n",
      "epoch 26, train_loss 1277.3121337890625\n",
      "epoch 26, val_loss 454.9705810546875\n",
      "epoch 27, train_loss 1277.3121337890625\n",
      "epoch 27, val_loss 454.9705810546875\n",
      "epoch 28, train_loss 1277.3121337890625\n",
      "epoch 28, val_loss 454.970458984375\n",
      "epoch 29, train_loss 1277.3121337890625\n",
      "epoch 29, val_loss 454.9703674316406\n",
      "epoch 30, train_loss 1277.3118896484375\n",
      "epoch 30, val_loss 454.9703369140625\n",
      "epoch 31, train_loss 1277.311767578125\n",
      "epoch 31, val_loss 454.9703063964844\n",
      "epoch 32, train_loss 1277.311767578125\n",
      "epoch 32, val_loss 454.97021484375\n",
      "epoch 33, train_loss 1277.311767578125\n",
      "epoch 33, val_loss 454.97021484375\n",
      "epoch 34, train_loss 1277.3116455078125\n",
      "epoch 34, val_loss 454.97015380859375\n",
      "epoch 35, train_loss 1277.3115234375\n",
      "epoch 35, val_loss 454.97003173828125\n",
      "epoch 36, train_loss 1277.3114013671875\n",
      "epoch 36, val_loss 454.9700012207031\n",
      "epoch 37, train_loss 1277.3114013671875\n",
      "epoch 37, val_loss 454.9698791503906\n",
      "epoch 38, train_loss 1277.311279296875\n",
      "epoch 38, val_loss 454.9698791503906\n",
      "epoch 39, train_loss 1277.311279296875\n",
      "epoch 39, val_loss 454.9698181152344\n",
      "epoch 40, train_loss 1277.31103515625\n",
      "epoch 40, val_loss 454.96978759765625\n",
      "epoch 41, train_loss 1277.3109130859375\n",
      "epoch 41, val_loss 454.96966552734375\n",
      "epoch 42, train_loss 1277.31103515625\n",
      "epoch 42, val_loss 454.96966552734375\n",
      "epoch 43, train_loss 1277.31103515625\n",
      "epoch 43, val_loss 454.96954345703125\n",
      "epoch 44, train_loss 1277.31103515625\n",
      "epoch 44, val_loss 454.9695129394531\n",
      "epoch 45, train_loss 1277.310791015625\n",
      "epoch 45, val_loss 454.9695129394531\n",
      "epoch 46, train_loss 1277.310791015625\n",
      "epoch 46, val_loss 454.9693298339844\n",
      "epoch 47, train_loss 1277.310791015625\n",
      "epoch 47, val_loss 454.9693298339844\n",
      "epoch 48, train_loss 1277.310546875\n",
      "epoch 48, val_loss 454.96929931640625\n",
      "epoch 49, train_loss 1277.310546875\n",
      "epoch 49, val_loss 454.9692077636719\n",
      "epoch 50, train_loss 1277.3104248046875\n",
      "epoch 50, val_loss 454.9691162109375\n",
      "epoch 51, train_loss 1277.3104248046875\n",
      "epoch 51, val_loss 454.9691162109375\n",
      "epoch 52, train_loss 1277.310302734375\n",
      "epoch 52, val_loss 454.968994140625\n",
      "epoch 53, train_loss 1277.310302734375\n",
      "epoch 53, val_loss 454.968994140625\n",
      "epoch 54, train_loss 1277.3101806640625\n",
      "epoch 54, val_loss 454.9689025878906\n",
      "epoch 55, train_loss 1277.310302734375\n",
      "epoch 55, val_loss 454.9688415527344\n",
      "epoch 56, train_loss 1277.3101806640625\n",
      "epoch 56, val_loss 454.96875\n",
      "epoch 57, train_loss 1277.3101806640625\n",
      "epoch 57, val_loss 454.9686584472656\n",
      "epoch 58, train_loss 1277.31005859375\n",
      "epoch 58, val_loss 454.9686279296875\n",
      "epoch 59, train_loss 1277.31005859375\n",
      "epoch 59, val_loss 454.9685363769531\n",
      "epoch 60, train_loss 1277.3096923828125\n",
      "epoch 60, val_loss 454.9685363769531\n",
      "epoch 61, train_loss 1277.3096923828125\n",
      "epoch 61, val_loss 454.968505859375\n",
      "epoch 62, train_loss 1277.3096923828125\n",
      "epoch 62, val_loss 454.96844482421875\n",
      "epoch 63, train_loss 1277.3095703125\n",
      "epoch 63, val_loss 454.96832275390625\n",
      "epoch 64, train_loss 1277.3094482421875\n",
      "epoch 64, val_loss 454.96826171875\n",
      "epoch 65, train_loss 1277.3094482421875\n",
      "epoch 65, val_loss 454.96820068359375\n",
      "epoch 66, train_loss 1277.3094482421875\n",
      "epoch 66, val_loss 454.9681396484375\n",
      "epoch 67, train_loss 1277.309326171875\n",
      "epoch 67, val_loss 454.9681396484375\n",
      "epoch 68, train_loss 1277.309326171875\n",
      "epoch 68, val_loss 454.9680480957031\n",
      "epoch 69, train_loss 1277.3092041015625\n",
      "epoch 69, val_loss 454.9679870605469\n",
      "epoch 70, train_loss 1277.3092041015625\n",
      "epoch 70, val_loss 454.96795654296875\n",
      "epoch 71, train_loss 1277.3092041015625\n",
      "epoch 71, val_loss 454.9678649902344\n",
      "epoch 72, train_loss 1277.3092041015625\n",
      "epoch 72, val_loss 454.9678039550781\n",
      "epoch 73, train_loss 1277.3089599609375\n",
      "epoch 73, val_loss 454.9678039550781\n",
      "epoch 74, train_loss 1277.308837890625\n",
      "epoch 74, val_loss 454.9676818847656\n",
      "epoch 75, train_loss 1277.308837890625\n",
      "epoch 75, val_loss 454.96759033203125\n",
      "epoch 76, train_loss 1277.3087158203125\n",
      "epoch 76, val_loss 454.96759033203125\n",
      "epoch 77, train_loss 1277.3087158203125\n",
      "epoch 77, val_loss 454.9674987792969\n",
      "epoch 78, train_loss 1277.3087158203125\n",
      "epoch 78, val_loss 454.9674072265625\n",
      "epoch 79, train_loss 1277.3087158203125\n",
      "epoch 79, val_loss 454.9673767089844\n",
      "epoch 80, train_loss 1277.308349609375\n",
      "epoch 80, val_loss 454.9672546386719\n",
      "epoch 81, train_loss 1277.308349609375\n",
      "epoch 81, val_loss 454.9672546386719\n",
      "epoch 82, train_loss 1277.3084716796875\n",
      "epoch 82, val_loss 454.9671936035156\n",
      "epoch 83, train_loss 1277.3082275390625\n",
      "epoch 83, val_loss 454.9671630859375\n",
      "epoch 84, train_loss 1277.3082275390625\n",
      "epoch 84, val_loss 454.967041015625\n",
      "epoch 85, train_loss 1277.3082275390625\n",
      "epoch 85, val_loss 454.9670104980469\n",
      "epoch 86, train_loss 1277.30810546875\n",
      "epoch 86, val_loss 454.9669189453125\n",
      "epoch 87, train_loss 1277.30810546875\n",
      "epoch 87, val_loss 454.9669189453125\n",
      "epoch 88, train_loss 1277.30810546875\n",
      "epoch 88, val_loss 454.966796875\n",
      "epoch 89, train_loss 1277.307861328125\n",
      "epoch 89, val_loss 454.966796875\n",
      "epoch 90, train_loss 1277.307861328125\n",
      "epoch 90, val_loss 454.96673583984375\n",
      "epoch 91, train_loss 1277.3076171875\n",
      "epoch 91, val_loss 454.96661376953125\n",
      "epoch 92, train_loss 1277.3076171875\n",
      "epoch 92, val_loss 454.96661376953125\n",
      "epoch 93, train_loss 1277.3076171875\n",
      "epoch 93, val_loss 454.96649169921875\n",
      "epoch 94, train_loss 1277.3076171875\n",
      "epoch 94, val_loss 454.96636962890625\n",
      "epoch 95, train_loss 1277.3074951171875\n",
      "epoch 95, val_loss 454.96636962890625\n",
      "epoch 96, train_loss 1277.307373046875\n",
      "epoch 96, val_loss 454.9663391113281\n",
      "epoch 97, train_loss 1277.3072509765625\n",
      "epoch 97, val_loss 454.96624755859375\n",
      "epoch 98, train_loss 1277.3072509765625\n",
      "epoch 98, val_loss 454.96624755859375\n",
      "epoch 99, train_loss 1277.3072509765625\n",
      "epoch 99, val_loss 454.9662170410156\n",
      "Parameter containing:\n",
      "tensor([2.3294e-20], requires_grad=True)\n",
      "iter 104, train_loss_regularization 0.7472922801971436\n",
      "iter 104, val_loss_regularization 0.7472922801971436\n",
      "epoch 0, train_loss 1277.30712890625\n",
      "epoch 0, val_loss 454.9660339355469\n",
      "epoch 1, train_loss 1277.306884765625\n",
      "epoch 1, val_loss 454.9660339355469\n",
      "epoch 2, train_loss 1277.306884765625\n",
      "epoch 2, val_loss 454.96600341796875\n",
      "epoch 3, train_loss 1277.306884765625\n",
      "epoch 3, val_loss 454.9659118652344\n",
      "epoch 4, train_loss 1277.306884765625\n",
      "epoch 4, val_loss 454.9658203125\n",
      "epoch 5, train_loss 1277.306884765625\n",
      "epoch 5, val_loss 454.96588134765625\n",
      "epoch 6, train_loss 1277.306640625\n",
      "epoch 6, val_loss 454.9656982421875\n",
      "epoch 7, train_loss 1277.306396484375\n",
      "epoch 7, val_loss 454.9656677246094\n",
      "epoch 8, train_loss 1277.306640625\n",
      "epoch 8, val_loss 454.96563720703125\n",
      "epoch 9, train_loss 1277.3065185546875\n",
      "epoch 9, val_loss 454.9655456542969\n",
      "epoch 10, train_loss 1277.3065185546875\n",
      "epoch 10, val_loss 454.9655456542969\n",
      "epoch 11, train_loss 1277.3065185546875\n",
      "epoch 11, val_loss 454.9654541015625\n",
      "epoch 12, train_loss 1277.306396484375\n",
      "epoch 12, val_loss 454.96533203125\n",
      "epoch 13, train_loss 1277.3062744140625\n",
      "epoch 13, val_loss 454.96533203125\n",
      "epoch 14, train_loss 1277.30615234375\n",
      "epoch 14, val_loss 454.9651794433594\n",
      "epoch 15, train_loss 1277.30615234375\n",
      "epoch 15, val_loss 454.9651794433594\n",
      "epoch 16, train_loss 1277.3060302734375\n",
      "epoch 16, val_loss 454.9651794433594\n",
      "epoch 17, train_loss 1277.3060302734375\n",
      "epoch 17, val_loss 454.96502685546875\n",
      "epoch 18, train_loss 1277.305908203125\n",
      "epoch 18, val_loss 454.9649963378906\n",
      "epoch 19, train_loss 1277.3057861328125\n",
      "epoch 19, val_loss 454.9649963378906\n",
      "epoch 20, train_loss 1277.3057861328125\n",
      "epoch 20, val_loss 454.9648742675781\n",
      "epoch 21, train_loss 1277.3057861328125\n",
      "epoch 21, val_loss 454.96484375\n",
      "epoch 22, train_loss 1277.3057861328125\n",
      "epoch 22, val_loss 454.9647521972656\n",
      "epoch 23, train_loss 1277.3056640625\n",
      "epoch 23, val_loss 454.96466064453125\n",
      "epoch 24, train_loss 1277.3055419921875\n",
      "epoch 24, val_loss 454.96466064453125\n",
      "epoch 25, train_loss 1277.3055419921875\n",
      "epoch 25, val_loss 454.96453857421875\n",
      "epoch 26, train_loss 1277.305419921875\n",
      "epoch 26, val_loss 454.96453857421875\n",
      "epoch 27, train_loss 1277.3052978515625\n",
      "epoch 27, val_loss 454.9645080566406\n",
      "epoch 28, train_loss 1277.3052978515625\n",
      "epoch 28, val_loss 454.9643859863281\n",
      "epoch 29, train_loss 1277.3052978515625\n",
      "epoch 29, val_loss 454.9643249511719\n",
      "epoch 30, train_loss 1277.3050537109375\n",
      "epoch 30, val_loss 454.96429443359375\n",
      "epoch 31, train_loss 1277.3050537109375\n",
      "epoch 31, val_loss 454.9642028808594\n",
      "epoch 32, train_loss 1277.304931640625\n",
      "epoch 32, val_loss 454.964111328125\n",
      "epoch 33, train_loss 1277.304931640625\n",
      "epoch 33, val_loss 454.9640808105469\n",
      "epoch 34, train_loss 1277.3050537109375\n",
      "epoch 34, val_loss 454.96405029296875\n",
      "epoch 35, train_loss 1277.304931640625\n",
      "epoch 35, val_loss 454.9639587402344\n",
      "epoch 36, train_loss 1277.3048095703125\n",
      "epoch 36, val_loss 454.9638671875\n",
      "epoch 37, train_loss 1277.3046875\n",
      "epoch 37, val_loss 454.9638671875\n",
      "epoch 38, train_loss 1277.3046875\n",
      "epoch 38, val_loss 454.96380615234375\n",
      "epoch 39, train_loss 1277.3046875\n",
      "epoch 39, val_loss 454.96380615234375\n",
      "epoch 40, train_loss 1277.3045654296875\n",
      "epoch 40, val_loss 454.9636535644531\n",
      "epoch 41, train_loss 1277.3043212890625\n",
      "epoch 41, val_loss 454.9635925292969\n",
      "epoch 42, train_loss 1277.30419921875\n",
      "epoch 42, val_loss 454.9635314941406\n",
      "epoch 43, train_loss 1277.30419921875\n",
      "epoch 43, val_loss 454.9634094238281\n",
      "epoch 44, train_loss 1277.30419921875\n",
      "epoch 44, val_loss 454.9634094238281\n",
      "epoch 45, train_loss 1277.3040771484375\n",
      "epoch 45, val_loss 454.9633483886719\n",
      "epoch 46, train_loss 1277.303955078125\n",
      "epoch 46, val_loss 454.9632873535156\n",
      "epoch 47, train_loss 1277.303955078125\n",
      "epoch 47, val_loss 454.9632873535156\n",
      "epoch 48, train_loss 1277.303955078125\n",
      "epoch 48, val_loss 454.96319580078125\n",
      "epoch 49, train_loss 1277.3038330078125\n",
      "epoch 49, val_loss 454.963134765625\n",
      "epoch 50, train_loss 1277.3038330078125\n",
      "epoch 50, val_loss 454.96307373046875\n",
      "epoch 51, train_loss 1277.3037109375\n",
      "epoch 51, val_loss 454.96295166015625\n",
      "epoch 52, train_loss 1277.303466796875\n",
      "epoch 52, val_loss 454.96295166015625\n",
      "epoch 53, train_loss 1277.303466796875\n",
      "epoch 53, val_loss 454.9628601074219\n",
      "epoch 54, train_loss 1277.3035888671875\n",
      "epoch 54, val_loss 454.9627990722656\n",
      "epoch 55, train_loss 1277.3035888671875\n",
      "epoch 55, val_loss 454.9627990722656\n",
      "epoch 56, train_loss 1277.3033447265625\n",
      "epoch 56, val_loss 454.9626770019531\n",
      "epoch 57, train_loss 1277.3031005859375\n",
      "epoch 57, val_loss 454.9626770019531\n",
      "epoch 58, train_loss 1277.30322265625\n",
      "epoch 58, val_loss 454.96258544921875\n",
      "epoch 59, train_loss 1277.3033447265625\n",
      "epoch 59, val_loss 454.9624938964844\n",
      "epoch 60, train_loss 1277.30322265625\n",
      "epoch 60, val_loss 454.96246337890625\n",
      "epoch 61, train_loss 1277.30322265625\n",
      "epoch 61, val_loss 454.96240234375\n",
      "epoch 62, train_loss 1277.3028564453125\n",
      "epoch 62, val_loss 454.9622802734375\n",
      "epoch 63, train_loss 1277.3028564453125\n",
      "epoch 63, val_loss 454.96234130859375\n",
      "epoch 64, train_loss 1277.3028564453125\n",
      "epoch 64, val_loss 454.9622497558594\n",
      "epoch 65, train_loss 1277.3028564453125\n",
      "epoch 65, val_loss 454.9621276855469\n",
      "epoch 66, train_loss 1277.3026123046875\n",
      "epoch 66, val_loss 454.96209716796875\n",
      "epoch 67, train_loss 1277.3026123046875\n",
      "epoch 67, val_loss 454.9620361328125\n",
      "epoch 68, train_loss 1277.3026123046875\n",
      "epoch 68, val_loss 454.9620056152344\n",
      "epoch 69, train_loss 1277.302490234375\n",
      "epoch 69, val_loss 454.9619445800781\n",
      "epoch 70, train_loss 1277.3026123046875\n",
      "epoch 70, val_loss 454.9618835449219\n",
      "epoch 71, train_loss 1277.3023681640625\n",
      "epoch 71, val_loss 454.9617919921875\n",
      "epoch 72, train_loss 1277.3023681640625\n",
      "epoch 72, val_loss 454.9617614746094\n",
      "epoch 73, train_loss 1277.3023681640625\n",
      "epoch 73, val_loss 454.961669921875\n",
      "epoch 74, train_loss 1277.30224609375\n",
      "epoch 74, val_loss 454.9616394042969\n",
      "epoch 75, train_loss 1277.3021240234375\n",
      "epoch 75, val_loss 454.9615783691406\n",
      "epoch 76, train_loss 1277.3021240234375\n",
      "epoch 76, val_loss 454.96148681640625\n",
      "epoch 77, train_loss 1277.302001953125\n",
      "epoch 77, val_loss 454.9614562988281\n",
      "epoch 78, train_loss 1277.3018798828125\n",
      "epoch 78, val_loss 454.9613342285156\n",
      "epoch 79, train_loss 1277.3018798828125\n",
      "epoch 79, val_loss 454.9613037109375\n",
      "epoch 80, train_loss 1277.3017578125\n",
      "epoch 80, val_loss 454.9612121582031\n",
      "epoch 81, train_loss 1277.3018798828125\n",
      "epoch 81, val_loss 454.961181640625\n",
      "epoch 82, train_loss 1277.3016357421875\n",
      "epoch 82, val_loss 454.96112060546875\n",
      "epoch 83, train_loss 1277.3016357421875\n",
      "epoch 83, val_loss 454.96112060546875\n",
      "epoch 84, train_loss 1277.3016357421875\n",
      "epoch 84, val_loss 454.9610290527344\n",
      "epoch 85, train_loss 1277.3013916015625\n",
      "epoch 85, val_loss 454.9609680175781\n",
      "epoch 86, train_loss 1277.3013916015625\n",
      "epoch 86, val_loss 454.96087646484375\n",
      "epoch 87, train_loss 1277.3013916015625\n",
      "epoch 87, val_loss 454.96087646484375\n",
      "epoch 88, train_loss 1277.3013916015625\n",
      "epoch 88, val_loss 454.96075439453125\n",
      "epoch 89, train_loss 1277.3013916015625\n",
      "epoch 89, val_loss 454.960693359375\n",
      "epoch 90, train_loss 1277.3011474609375\n",
      "epoch 90, val_loss 454.9606628417969\n",
      "epoch 91, train_loss 1277.301025390625\n",
      "epoch 91, val_loss 454.9605712890625\n",
      "epoch 92, train_loss 1277.301025390625\n",
      "epoch 92, val_loss 454.96051025390625\n",
      "epoch 93, train_loss 1277.301025390625\n",
      "epoch 93, val_loss 454.96044921875\n",
      "epoch 94, train_loss 1277.3009033203125\n",
      "epoch 94, val_loss 454.9604187011719\n",
      "epoch 95, train_loss 1277.3006591796875\n",
      "epoch 95, val_loss 454.96038818359375\n",
      "epoch 96, train_loss 1277.30078125\n",
      "epoch 96, val_loss 454.9602966308594\n",
      "epoch 97, train_loss 1277.300537109375\n",
      "epoch 97, val_loss 454.9602355957031\n",
      "epoch 98, train_loss 1277.300537109375\n",
      "epoch 98, val_loss 454.960205078125\n",
      "epoch 99, train_loss 1277.300537109375\n",
      "epoch 99, val_loss 454.9600524902344\n",
      "Parameter containing:\n",
      "tensor([1.5977e-20], requires_grad=True)\n",
      "iter 105, train_loss_regularization 0.745559573173523\n",
      "iter 105, val_loss_regularization 0.745559573173523\n",
      "epoch 0, train_loss 1277.300537109375\n",
      "epoch 0, val_loss 454.9600524902344\n",
      "epoch 1, train_loss 1277.30029296875\n",
      "epoch 1, val_loss 454.9599914550781\n",
      "epoch 2, train_loss 1277.30029296875\n",
      "epoch 2, val_loss 454.9599304199219\n",
      "epoch 3, train_loss 1277.30029296875\n",
      "epoch 3, val_loss 454.9598693847656\n",
      "epoch 4, train_loss 1277.30029296875\n",
      "epoch 4, val_loss 454.95977783203125\n",
      "epoch 5, train_loss 1277.3001708984375\n",
      "epoch 5, val_loss 454.95977783203125\n",
      "epoch 6, train_loss 1277.300048828125\n",
      "epoch 6, val_loss 454.959716796875\n",
      "epoch 7, train_loss 1277.300048828125\n",
      "epoch 7, val_loss 454.9595947265625\n",
      "epoch 8, train_loss 1277.2999267578125\n",
      "epoch 8, val_loss 454.95953369140625\n",
      "epoch 9, train_loss 1277.2998046875\n",
      "epoch 9, val_loss 454.95947265625\n",
      "epoch 10, train_loss 1277.2998046875\n",
      "epoch 10, val_loss 454.95947265625\n",
      "epoch 11, train_loss 1277.2998046875\n",
      "epoch 11, val_loss 454.95941162109375\n",
      "epoch 12, train_loss 1277.2996826171875\n",
      "epoch 12, val_loss 454.9593200683594\n",
      "epoch 13, train_loss 1277.299560546875\n",
      "epoch 13, val_loss 454.95928955078125\n",
      "epoch 14, train_loss 1277.299560546875\n",
      "epoch 14, val_loss 454.9592590332031\n",
      "epoch 15, train_loss 1277.299560546875\n",
      "epoch 15, val_loss 454.9591369628906\n",
      "epoch 16, train_loss 1277.29931640625\n",
      "epoch 16, val_loss 454.9590759277344\n",
      "epoch 17, train_loss 1277.29931640625\n",
      "epoch 17, val_loss 454.9590148925781\n",
      "epoch 18, train_loss 1277.2991943359375\n",
      "epoch 18, val_loss 454.9590148925781\n",
      "epoch 19, train_loss 1277.2991943359375\n",
      "epoch 19, val_loss 454.95892333984375\n",
      "epoch 20, train_loss 1277.299072265625\n",
      "epoch 20, val_loss 454.9588317871094\n",
      "epoch 21, train_loss 1277.299072265625\n",
      "epoch 21, val_loss 454.95880126953125\n",
      "epoch 22, train_loss 1277.2989501953125\n",
      "epoch 22, val_loss 454.9586181640625\n",
      "epoch 23, train_loss 1277.2989501953125\n",
      "epoch 23, val_loss 454.95867919921875\n",
      "epoch 24, train_loss 1277.2987060546875\n",
      "epoch 24, val_loss 454.95867919921875\n",
      "epoch 25, train_loss 1277.2987060546875\n",
      "epoch 25, val_loss 454.9585876464844\n",
      "epoch 26, train_loss 1277.2987060546875\n",
      "epoch 26, val_loss 454.95849609375\n",
      "epoch 27, train_loss 1277.2987060546875\n",
      "epoch 27, val_loss 454.9584655761719\n",
      "epoch 28, train_loss 1277.2987060546875\n",
      "epoch 28, val_loss 454.9583435058594\n",
      "epoch 29, train_loss 1277.298583984375\n",
      "epoch 29, val_loss 454.9582824707031\n",
      "epoch 30, train_loss 1277.2984619140625\n",
      "epoch 30, val_loss 454.9582824707031\n",
      "epoch 31, train_loss 1277.29833984375\n",
      "epoch 31, val_loss 454.9582214355469\n",
      "epoch 32, train_loss 1277.29833984375\n",
      "epoch 32, val_loss 454.9581298828125\n",
      "epoch 33, train_loss 1277.29833984375\n",
      "epoch 33, val_loss 454.95806884765625\n",
      "epoch 34, train_loss 1277.2982177734375\n",
      "epoch 34, val_loss 454.9580383300781\n",
      "epoch 35, train_loss 1277.2979736328125\n",
      "epoch 35, val_loss 454.9580078125\n",
      "epoch 36, train_loss 1277.2979736328125\n",
      "epoch 36, val_loss 454.95782470703125\n",
      "epoch 37, train_loss 1277.298095703125\n",
      "epoch 37, val_loss 454.95782470703125\n",
      "epoch 38, train_loss 1277.2982177734375\n",
      "epoch 38, val_loss 454.957763671875\n",
      "epoch 39, train_loss 1277.2979736328125\n",
      "epoch 39, val_loss 454.9576721191406\n",
      "epoch 40, train_loss 1277.2978515625\n",
      "epoch 40, val_loss 454.9576110839844\n",
      "epoch 41, train_loss 1277.2978515625\n",
      "epoch 41, val_loss 454.9576110839844\n",
      "epoch 42, train_loss 1277.2978515625\n",
      "epoch 42, val_loss 454.9575500488281\n",
      "epoch 43, train_loss 1277.297607421875\n",
      "epoch 43, val_loss 454.9574890136719\n",
      "epoch 44, train_loss 1277.297607421875\n",
      "epoch 44, val_loss 454.9574279785156\n",
      "epoch 45, train_loss 1277.29736328125\n",
      "epoch 45, val_loss 454.95733642578125\n",
      "epoch 46, train_loss 1277.29736328125\n",
      "epoch 46, val_loss 454.9573059082031\n",
      "epoch 47, train_loss 1277.29736328125\n",
      "epoch 47, val_loss 454.9572448730469\n",
      "epoch 48, train_loss 1277.29736328125\n",
      "epoch 48, val_loss 454.9571533203125\n",
      "epoch 49, train_loss 1277.29736328125\n",
      "epoch 49, val_loss 454.9571228027344\n",
      "epoch 50, train_loss 1277.297119140625\n",
      "epoch 50, val_loss 454.95703125\n",
      "epoch 51, train_loss 1277.2969970703125\n",
      "epoch 51, val_loss 454.95697021484375\n",
      "epoch 52, train_loss 1277.2969970703125\n",
      "epoch 52, val_loss 454.9569091796875\n",
      "epoch 53, train_loss 1277.296875\n",
      "epoch 53, val_loss 454.9568786621094\n",
      "epoch 54, train_loss 1277.296875\n",
      "epoch 54, val_loss 454.9568786621094\n",
      "epoch 55, train_loss 1277.2967529296875\n",
      "epoch 55, val_loss 454.9567565917969\n",
      "epoch 56, train_loss 1277.2967529296875\n",
      "epoch 56, val_loss 454.9567565917969\n",
      "epoch 57, train_loss 1277.2967529296875\n",
      "epoch 57, val_loss 454.9565734863281\n",
      "epoch 58, train_loss 1277.296630859375\n",
      "epoch 58, val_loss 454.9565124511719\n",
      "epoch 59, train_loss 1277.29638671875\n",
      "epoch 59, val_loss 454.95654296875\n",
      "epoch 60, train_loss 1277.29638671875\n",
      "epoch 60, val_loss 454.9564514160156\n",
      "epoch 61, train_loss 1277.29638671875\n",
      "epoch 61, val_loss 454.9564514160156\n",
      "epoch 62, train_loss 1277.2962646484375\n",
      "epoch 62, val_loss 454.9563293457031\n",
      "epoch 63, train_loss 1277.29638671875\n",
      "epoch 63, val_loss 454.956298828125\n",
      "epoch 64, train_loss 1277.2962646484375\n",
      "epoch 64, val_loss 454.956298828125\n",
      "epoch 65, train_loss 1277.296142578125\n",
      "epoch 65, val_loss 454.9560852050781\n",
      "epoch 66, train_loss 1277.296142578125\n",
      "epoch 66, val_loss 454.9560852050781\n",
      "epoch 67, train_loss 1277.296142578125\n",
      "epoch 67, val_loss 454.9560546875\n",
      "epoch 68, train_loss 1277.2960205078125\n",
      "epoch 68, val_loss 454.9559631347656\n",
      "epoch 69, train_loss 1277.2957763671875\n",
      "epoch 69, val_loss 454.9559020996094\n",
      "epoch 70, train_loss 1277.2957763671875\n",
      "epoch 70, val_loss 454.9559020996094\n",
      "epoch 71, train_loss 1277.2957763671875\n",
      "epoch 71, val_loss 454.95574951171875\n",
      "epoch 72, train_loss 1277.2957763671875\n",
      "epoch 72, val_loss 454.95574951171875\n",
      "epoch 73, train_loss 1277.2957763671875\n",
      "epoch 73, val_loss 454.9557189941406\n",
      "epoch 74, train_loss 1277.2955322265625\n",
      "epoch 74, val_loss 454.9555969238281\n",
      "epoch 75, train_loss 1277.2955322265625\n",
      "epoch 75, val_loss 454.9555358886719\n",
      "epoch 76, train_loss 1277.2955322265625\n",
      "epoch 76, val_loss 454.9555358886719\n",
      "epoch 77, train_loss 1277.2952880859375\n",
      "epoch 77, val_loss 454.9554443359375\n",
      "epoch 78, train_loss 1277.2952880859375\n",
      "epoch 78, val_loss 454.95538330078125\n",
      "epoch 79, train_loss 1277.295166015625\n",
      "epoch 79, val_loss 454.955322265625\n",
      "epoch 80, train_loss 1277.295166015625\n",
      "epoch 80, val_loss 454.95526123046875\n",
      "epoch 81, train_loss 1277.295166015625\n",
      "epoch 81, val_loss 454.9551696777344\n",
      "epoch 82, train_loss 1277.295166015625\n",
      "epoch 82, val_loss 454.95513916015625\n",
      "epoch 83, train_loss 1277.2950439453125\n",
      "epoch 83, val_loss 454.95513916015625\n",
      "epoch 84, train_loss 1277.2947998046875\n",
      "epoch 84, val_loss 454.9549865722656\n",
      "epoch 85, train_loss 1277.294677734375\n",
      "epoch 85, val_loss 454.9549865722656\n",
      "epoch 86, train_loss 1277.2947998046875\n",
      "epoch 86, val_loss 454.9548645019531\n",
      "epoch 87, train_loss 1277.2947998046875\n",
      "epoch 87, val_loss 454.954833984375\n",
      "epoch 88, train_loss 1277.294677734375\n",
      "epoch 88, val_loss 454.9547424316406\n",
      "epoch 89, train_loss 1277.294677734375\n",
      "epoch 89, val_loss 454.9547119140625\n",
      "epoch 90, train_loss 1277.2945556640625\n",
      "epoch 90, val_loss 454.9547119140625\n",
      "epoch 91, train_loss 1277.2945556640625\n",
      "epoch 91, val_loss 454.9546203613281\n",
      "epoch 92, train_loss 1277.2945556640625\n",
      "epoch 92, val_loss 454.95458984375\n",
      "epoch 93, train_loss 1277.2943115234375\n",
      "epoch 93, val_loss 454.9544982910156\n",
      "epoch 94, train_loss 1277.2945556640625\n",
      "epoch 94, val_loss 454.9543762207031\n",
      "epoch 95, train_loss 1277.294189453125\n",
      "epoch 95, val_loss 454.954345703125\n",
      "epoch 96, train_loss 1277.294189453125\n",
      "epoch 96, val_loss 454.954345703125\n",
      "epoch 97, train_loss 1277.2940673828125\n",
      "epoch 97, val_loss 454.9541931152344\n",
      "epoch 98, train_loss 1277.2940673828125\n",
      "epoch 98, val_loss 454.9541931152344\n",
      "epoch 99, train_loss 1277.2939453125\n",
      "epoch 99, val_loss 454.95416259765625\n",
      "Parameter containing:\n",
      "tensor([1.0964e-20], requires_grad=True)\n",
      "iter 106, train_loss_regularization 0.7438610792160034\n",
      "iter 106, val_loss_regularization 0.7438610792160034\n",
      "epoch 0, train_loss 1277.2938232421875\n",
      "epoch 0, val_loss 454.95404052734375\n",
      "epoch 1, train_loss 1277.293701171875\n",
      "epoch 1, val_loss 454.9540100097656\n",
      "epoch 2, train_loss 1277.293701171875\n",
      "epoch 2, val_loss 454.9539489746094\n",
      "epoch 3, train_loss 1277.293701171875\n",
      "epoch 3, val_loss 454.95391845703125\n",
      "epoch 4, train_loss 1277.293701171875\n",
      "epoch 4, val_loss 454.9538269042969\n",
      "epoch 5, train_loss 1277.2935791015625\n",
      "epoch 5, val_loss 454.9537353515625\n",
      "epoch 6, train_loss 1277.29345703125\n",
      "epoch 6, val_loss 454.9537048339844\n",
      "epoch 7, train_loss 1277.29345703125\n",
      "epoch 7, val_loss 454.95367431640625\n",
      "epoch 8, train_loss 1277.2933349609375\n",
      "epoch 8, val_loss 454.9535827636719\n",
      "epoch 9, train_loss 1277.2933349609375\n",
      "epoch 9, val_loss 454.9535827636719\n",
      "epoch 10, train_loss 1277.2933349609375\n",
      "epoch 10, val_loss 454.9534912109375\n",
      "epoch 11, train_loss 1277.2930908203125\n",
      "epoch 11, val_loss 454.95343017578125\n",
      "epoch 12, train_loss 1277.2930908203125\n",
      "epoch 12, val_loss 454.953369140625\n",
      "epoch 13, train_loss 1277.29296875\n",
      "epoch 13, val_loss 454.9533386230469\n",
      "epoch 14, train_loss 1277.2928466796875\n",
      "epoch 14, val_loss 454.9532165527344\n",
      "epoch 15, train_loss 1277.2928466796875\n",
      "epoch 15, val_loss 454.9532165527344\n",
      "epoch 16, train_loss 1277.29296875\n",
      "epoch 16, val_loss 454.953125\n",
      "epoch 17, train_loss 1277.29296875\n",
      "epoch 17, val_loss 454.9530029296875\n",
      "epoch 18, train_loss 1277.292724609375\n",
      "epoch 18, val_loss 454.9530334472656\n",
      "epoch 19, train_loss 1277.292724609375\n",
      "epoch 19, val_loss 454.9529724121094\n",
      "epoch 20, train_loss 1277.292724609375\n",
      "epoch 20, val_loss 454.952880859375\n",
      "epoch 21, train_loss 1277.29248046875\n",
      "epoch 21, val_loss 454.9529113769531\n",
      "epoch 22, train_loss 1277.2923583984375\n",
      "epoch 22, val_loss 454.9527893066406\n",
      "epoch 23, train_loss 1277.2923583984375\n",
      "epoch 23, val_loss 454.9526672363281\n",
      "epoch 24, train_loss 1277.292236328125\n",
      "epoch 24, val_loss 454.95263671875\n",
      "epoch 25, train_loss 1277.2923583984375\n",
      "epoch 25, val_loss 454.95257568359375\n",
      "epoch 26, train_loss 1277.2921142578125\n",
      "epoch 26, val_loss 454.9525146484375\n",
      "epoch 27, train_loss 1277.2921142578125\n",
      "epoch 27, val_loss 454.9525146484375\n",
      "epoch 28, train_loss 1277.2921142578125\n",
      "epoch 28, val_loss 454.95245361328125\n",
      "epoch 29, train_loss 1277.2918701171875\n",
      "epoch 29, val_loss 454.95233154296875\n",
      "epoch 30, train_loss 1277.2918701171875\n",
      "epoch 30, val_loss 454.9523010253906\n",
      "epoch 31, train_loss 1277.291748046875\n",
      "epoch 31, val_loss 454.95220947265625\n",
      "epoch 32, train_loss 1277.2918701171875\n",
      "epoch 32, val_loss 454.9521789550781\n",
      "epoch 33, train_loss 1277.2916259765625\n",
      "epoch 33, val_loss 454.95208740234375\n",
      "epoch 34, train_loss 1277.2916259765625\n",
      "epoch 34, val_loss 454.95208740234375\n",
      "epoch 35, train_loss 1277.2916259765625\n",
      "epoch 35, val_loss 454.9519958496094\n",
      "epoch 36, train_loss 1277.2916259765625\n",
      "epoch 36, val_loss 454.95196533203125\n",
      "epoch 37, train_loss 1277.2913818359375\n",
      "epoch 37, val_loss 454.9518737792969\n",
      "epoch 38, train_loss 1277.2913818359375\n",
      "epoch 38, val_loss 454.9517822265625\n",
      "epoch 39, train_loss 1277.2913818359375\n",
      "epoch 39, val_loss 454.9517822265625\n",
      "epoch 40, train_loss 1277.2913818359375\n",
      "epoch 40, val_loss 454.95172119140625\n",
      "epoch 41, train_loss 1277.2911376953125\n",
      "epoch 41, val_loss 454.95166015625\n",
      "epoch 42, train_loss 1277.291259765625\n",
      "epoch 42, val_loss 454.9516296386719\n",
      "epoch 43, train_loss 1277.2911376953125\n",
      "epoch 43, val_loss 454.9515075683594\n",
      "epoch 44, train_loss 1277.2911376953125\n",
      "epoch 44, val_loss 454.9514465332031\n",
      "epoch 45, train_loss 1277.2911376953125\n",
      "epoch 45, val_loss 454.9513854980469\n",
      "epoch 46, train_loss 1277.2908935546875\n",
      "epoch 46, val_loss 454.9513244628906\n",
      "epoch 47, train_loss 1277.290771484375\n",
      "epoch 47, val_loss 454.9513244628906\n",
      "epoch 48, train_loss 1277.290771484375\n",
      "epoch 48, val_loss 454.9512023925781\n",
      "epoch 49, train_loss 1277.2906494140625\n",
      "epoch 49, val_loss 454.9512023925781\n",
      "epoch 50, train_loss 1277.290771484375\n",
      "epoch 50, val_loss 454.951171875\n",
      "epoch 51, train_loss 1277.29052734375\n",
      "epoch 51, val_loss 454.95111083984375\n",
      "epoch 52, train_loss 1277.29052734375\n",
      "epoch 52, val_loss 454.95098876953125\n",
      "epoch 53, train_loss 1277.2904052734375\n",
      "epoch 53, val_loss 454.950927734375\n",
      "epoch 54, train_loss 1277.2904052734375\n",
      "epoch 54, val_loss 454.9508361816406\n",
      "epoch 55, train_loss 1277.2904052734375\n",
      "epoch 55, val_loss 454.9508361816406\n",
      "epoch 56, train_loss 1277.290283203125\n",
      "epoch 56, val_loss 454.95074462890625\n",
      "epoch 57, train_loss 1277.2901611328125\n",
      "epoch 57, val_loss 454.9507141113281\n",
      "epoch 58, train_loss 1277.2900390625\n",
      "epoch 58, val_loss 454.9506530761719\n",
      "epoch 59, train_loss 1277.2900390625\n",
      "epoch 59, val_loss 454.9505920410156\n",
      "epoch 60, train_loss 1277.2900390625\n",
      "epoch 60, val_loss 454.95050048828125\n",
      "epoch 61, train_loss 1277.2900390625\n",
      "epoch 61, val_loss 454.95050048828125\n",
      "epoch 62, train_loss 1277.2899169921875\n",
      "epoch 62, val_loss 454.95037841796875\n",
      "epoch 63, train_loss 1277.289794921875\n",
      "epoch 63, val_loss 454.9502868652344\n",
      "epoch 64, train_loss 1277.2896728515625\n",
      "epoch 64, val_loss 454.9502868652344\n",
      "epoch 65, train_loss 1277.2896728515625\n",
      "epoch 65, val_loss 454.95025634765625\n",
      "epoch 66, train_loss 1277.28955078125\n",
      "epoch 66, val_loss 454.9501647949219\n",
      "epoch 67, train_loss 1277.28955078125\n",
      "epoch 67, val_loss 454.9500732421875\n",
      "epoch 68, train_loss 1277.289306640625\n",
      "epoch 68, val_loss 454.9500427246094\n",
      "epoch 69, train_loss 1277.2894287109375\n",
      "epoch 69, val_loss 454.95001220703125\n",
      "epoch 70, train_loss 1277.2894287109375\n",
      "epoch 70, val_loss 454.9499206542969\n",
      "epoch 71, train_loss 1277.2894287109375\n",
      "epoch 71, val_loss 454.949951171875\n",
      "epoch 72, train_loss 1277.2891845703125\n",
      "epoch 72, val_loss 454.9497985839844\n",
      "epoch 73, train_loss 1277.2890625\n",
      "epoch 73, val_loss 454.9497985839844\n",
      "epoch 74, train_loss 1277.2889404296875\n",
      "epoch 74, val_loss 454.94970703125\n",
      "epoch 75, train_loss 1277.2889404296875\n",
      "epoch 75, val_loss 454.9496154785156\n",
      "epoch 76, train_loss 1277.2889404296875\n",
      "epoch 76, val_loss 454.9495849609375\n",
      "epoch 77, train_loss 1277.2889404296875\n",
      "epoch 77, val_loss 454.9495849609375\n",
      "epoch 78, train_loss 1277.288818359375\n",
      "epoch 78, val_loss 454.949462890625\n",
      "epoch 79, train_loss 1277.2886962890625\n",
      "epoch 79, val_loss 454.94940185546875\n",
      "epoch 80, train_loss 1277.2886962890625\n",
      "epoch 80, val_loss 454.9493713378906\n",
      "epoch 81, train_loss 1277.2886962890625\n",
      "epoch 81, val_loss 454.9492492675781\n",
      "epoch 82, train_loss 1277.28857421875\n",
      "epoch 82, val_loss 454.94921875\n",
      "epoch 83, train_loss 1277.2884521484375\n",
      "epoch 83, val_loss 454.9491271972656\n",
      "epoch 84, train_loss 1277.2884521484375\n",
      "epoch 84, val_loss 454.9491271972656\n",
      "epoch 85, train_loss 1277.288330078125\n",
      "epoch 85, val_loss 454.9490966796875\n",
      "epoch 86, train_loss 1277.2884521484375\n",
      "epoch 86, val_loss 454.9490051269531\n",
      "epoch 87, train_loss 1277.2882080078125\n",
      "epoch 87, val_loss 454.9489440917969\n",
      "epoch 88, train_loss 1277.2880859375\n",
      "epoch 88, val_loss 454.9488830566406\n",
      "epoch 89, train_loss 1277.2879638671875\n",
      "epoch 89, val_loss 454.94879150390625\n",
      "epoch 90, train_loss 1277.2879638671875\n",
      "epoch 90, val_loss 454.9487609863281\n",
      "epoch 91, train_loss 1277.2879638671875\n",
      "epoch 91, val_loss 454.94866943359375\n",
      "epoch 92, train_loss 1277.287841796875\n",
      "epoch 92, val_loss 454.9485778808594\n",
      "epoch 93, train_loss 1277.2877197265625\n",
      "epoch 93, val_loss 454.9485778808594\n",
      "epoch 94, train_loss 1277.2877197265625\n",
      "epoch 94, val_loss 454.948486328125\n",
      "epoch 95, train_loss 1277.287841796875\n",
      "epoch 95, val_loss 454.9484558105469\n",
      "epoch 96, train_loss 1277.2877197265625\n",
      "epoch 96, val_loss 454.9483642578125\n",
      "epoch 97, train_loss 1277.2877197265625\n",
      "epoch 97, val_loss 454.9483642578125\n",
      "epoch 98, train_loss 1277.2874755859375\n",
      "epoch 98, val_loss 454.9482421875\n",
      "epoch 99, train_loss 1277.2874755859375\n",
      "epoch 99, val_loss 454.9482421875\n",
      "Parameter containing:\n",
      "tensor([7.5282e-21], requires_grad=True)\n",
      "iter 107, train_loss_regularization 0.7421962022781372\n",
      "iter 107, val_loss_regularization 0.7421962022781372\n",
      "epoch 0, train_loss 1277.2874755859375\n",
      "epoch 0, val_loss 454.94818115234375\n",
      "epoch 1, train_loss 1277.2874755859375\n",
      "epoch 1, val_loss 454.9480895996094\n",
      "epoch 2, train_loss 1277.2872314453125\n",
      "epoch 2, val_loss 454.9480895996094\n",
      "epoch 3, train_loss 1277.2872314453125\n",
      "epoch 3, val_loss 454.947998046875\n",
      "epoch 4, train_loss 1277.287109375\n",
      "epoch 4, val_loss 454.9478759765625\n",
      "epoch 5, train_loss 1277.287109375\n",
      "epoch 5, val_loss 454.9478454589844\n",
      "epoch 6, train_loss 1277.287109375\n",
      "epoch 6, val_loss 454.9478454589844\n",
      "epoch 7, train_loss 1277.287109375\n",
      "epoch 7, val_loss 454.9477233886719\n",
      "epoch 8, train_loss 1277.2869873046875\n",
      "epoch 8, val_loss 454.9477233886719\n",
      "epoch 9, train_loss 1277.286865234375\n",
      "epoch 9, val_loss 454.9476623535156\n",
      "epoch 10, train_loss 1277.2867431640625\n",
      "epoch 10, val_loss 454.9476318359375\n",
      "epoch 11, train_loss 1277.2867431640625\n",
      "epoch 11, val_loss 454.94744873046875\n",
      "epoch 12, train_loss 1277.2867431640625\n",
      "epoch 12, val_loss 454.947509765625\n",
      "epoch 13, train_loss 1277.28662109375\n",
      "epoch 13, val_loss 454.9474182128906\n",
      "epoch 14, train_loss 1277.2864990234375\n",
      "epoch 14, val_loss 454.94732666015625\n",
      "epoch 15, train_loss 1277.2864990234375\n",
      "epoch 15, val_loss 454.9472961425781\n",
      "epoch 16, train_loss 1277.286376953125\n",
      "epoch 16, val_loss 454.94720458984375\n",
      "epoch 17, train_loss 1277.286376953125\n",
      "epoch 17, val_loss 454.94720458984375\n",
      "epoch 18, train_loss 1277.2862548828125\n",
      "epoch 18, val_loss 454.9470520019531\n",
      "epoch 19, train_loss 1277.2861328125\n",
      "epoch 19, val_loss 454.9470520019531\n",
      "epoch 20, train_loss 1277.2861328125\n",
      "epoch 20, val_loss 454.9470520019531\n",
      "epoch 21, train_loss 1277.2861328125\n",
      "epoch 21, val_loss 454.9469299316406\n",
      "epoch 22, train_loss 1277.2861328125\n",
      "epoch 22, val_loss 454.9468688964844\n",
      "epoch 23, train_loss 1277.2860107421875\n",
      "epoch 23, val_loss 454.94683837890625\n",
      "epoch 24, train_loss 1277.2860107421875\n",
      "epoch 24, val_loss 454.9467468261719\n",
      "epoch 25, train_loss 1277.2857666015625\n",
      "epoch 25, val_loss 454.94671630859375\n",
      "epoch 26, train_loss 1277.28564453125\n",
      "epoch 26, val_loss 454.9466247558594\n",
      "epoch 27, train_loss 1277.28564453125\n",
      "epoch 27, val_loss 454.946533203125\n",
      "epoch 28, train_loss 1277.2855224609375\n",
      "epoch 28, val_loss 454.946533203125\n",
      "epoch 29, train_loss 1277.2855224609375\n",
      "epoch 29, val_loss 454.9465026855469\n",
      "epoch 30, train_loss 1277.285400390625\n",
      "epoch 30, val_loss 454.9464111328125\n",
      "epoch 31, train_loss 1277.285400390625\n",
      "epoch 31, val_loss 454.9463195800781\n",
      "epoch 32, train_loss 1277.285400390625\n",
      "epoch 32, val_loss 454.9462890625\n",
      "epoch 33, train_loss 1277.2852783203125\n",
      "epoch 33, val_loss 454.9461975097656\n",
      "epoch 34, train_loss 1277.28515625\n",
      "epoch 34, val_loss 454.9461364746094\n",
      "epoch 35, train_loss 1277.28515625\n",
      "epoch 35, val_loss 454.9461669921875\n",
      "epoch 36, train_loss 1277.2850341796875\n",
      "epoch 36, val_loss 454.9460754394531\n",
      "epoch 37, train_loss 1277.2850341796875\n",
      "epoch 37, val_loss 454.9460144042969\n",
      "epoch 38, train_loss 1277.284912109375\n",
      "epoch 38, val_loss 454.9460144042969\n",
      "epoch 39, train_loss 1277.284912109375\n",
      "epoch 39, val_loss 454.9459228515625\n",
      "epoch 40, train_loss 1277.2847900390625\n",
      "epoch 40, val_loss 454.9458312988281\n",
      "epoch 41, train_loss 1277.28466796875\n",
      "epoch 41, val_loss 454.94573974609375\n",
      "epoch 42, train_loss 1277.28466796875\n",
      "epoch 42, val_loss 454.9457092285156\n",
      "epoch 43, train_loss 1277.2845458984375\n",
      "epoch 43, val_loss 454.9456787109375\n",
      "epoch 44, train_loss 1277.28466796875\n",
      "epoch 44, val_loss 454.945556640625\n",
      "epoch 45, train_loss 1277.2845458984375\n",
      "epoch 45, val_loss 454.945556640625\n",
      "epoch 46, train_loss 1277.2845458984375\n",
      "epoch 46, val_loss 454.94549560546875\n",
      "epoch 47, train_loss 1277.2845458984375\n",
      "epoch 47, val_loss 454.9454040527344\n",
      "epoch 48, train_loss 1277.284423828125\n",
      "epoch 48, val_loss 454.9453430175781\n",
      "epoch 49, train_loss 1277.2843017578125\n",
      "epoch 49, val_loss 454.9453430175781\n",
      "epoch 50, train_loss 1277.2843017578125\n",
      "epoch 50, val_loss 454.94525146484375\n",
      "epoch 51, train_loss 1277.2841796875\n",
      "epoch 51, val_loss 454.9451599121094\n",
      "epoch 52, train_loss 1277.2840576171875\n",
      "epoch 52, val_loss 454.94512939453125\n",
      "epoch 53, train_loss 1277.283935546875\n",
      "epoch 53, val_loss 454.9450378417969\n",
      "epoch 54, train_loss 1277.2840576171875\n",
      "epoch 54, val_loss 454.9449462890625\n",
      "epoch 55, train_loss 1277.283935546875\n",
      "epoch 55, val_loss 454.9449462890625\n",
      "epoch 56, train_loss 1277.28369140625\n",
      "epoch 56, val_loss 454.94488525390625\n",
      "epoch 57, train_loss 1277.28369140625\n",
      "epoch 57, val_loss 454.94482421875\n",
      "epoch 58, train_loss 1277.28369140625\n",
      "epoch 58, val_loss 454.94482421875\n",
      "epoch 59, train_loss 1277.28369140625\n",
      "epoch 59, val_loss 454.9446716308594\n",
      "epoch 60, train_loss 1277.2835693359375\n",
      "epoch 60, val_loss 454.9446716308594\n",
      "epoch 61, train_loss 1277.283447265625\n",
      "epoch 61, val_loss 454.9446105957031\n",
      "epoch 62, train_loss 1277.283447265625\n",
      "epoch 62, val_loss 454.9444885253906\n",
      "epoch 63, train_loss 1277.283447265625\n",
      "epoch 63, val_loss 454.9444885253906\n",
      "epoch 64, train_loss 1277.2833251953125\n",
      "epoch 64, val_loss 454.9444274902344\n",
      "epoch 65, train_loss 1277.2833251953125\n",
      "epoch 65, val_loss 454.9443664550781\n",
      "epoch 66, train_loss 1277.283203125\n",
      "epoch 66, val_loss 454.9443054199219\n",
      "epoch 67, train_loss 1277.2830810546875\n",
      "epoch 67, val_loss 454.9443054199219\n",
      "epoch 68, train_loss 1277.282958984375\n",
      "epoch 68, val_loss 454.9442138671875\n",
      "epoch 69, train_loss 1277.282958984375\n",
      "epoch 69, val_loss 454.94415283203125\n",
      "epoch 70, train_loss 1277.282958984375\n",
      "epoch 70, val_loss 454.94403076171875\n",
      "epoch 71, train_loss 1277.2828369140625\n",
      "epoch 71, val_loss 454.9440002441406\n",
      "epoch 72, train_loss 1277.28271484375\n",
      "epoch 72, val_loss 454.9439697265625\n",
      "epoch 73, train_loss 1277.28271484375\n",
      "epoch 73, val_loss 454.94390869140625\n",
      "epoch 74, train_loss 1277.28271484375\n",
      "epoch 74, val_loss 454.94384765625\n",
      "epoch 75, train_loss 1277.28271484375\n",
      "epoch 75, val_loss 454.94378662109375\n",
      "epoch 76, train_loss 1277.2825927734375\n",
      "epoch 76, val_loss 454.94378662109375\n",
      "epoch 77, train_loss 1277.2825927734375\n",
      "epoch 77, val_loss 454.94366455078125\n",
      "epoch 78, train_loss 1277.282470703125\n",
      "epoch 78, val_loss 454.9436340332031\n",
      "epoch 79, train_loss 1277.2822265625\n",
      "epoch 79, val_loss 454.9435729980469\n",
      "epoch 80, train_loss 1277.2822265625\n",
      "epoch 80, val_loss 454.9434509277344\n",
      "epoch 81, train_loss 1277.2822265625\n",
      "epoch 81, val_loss 454.94342041015625\n",
      "epoch 82, train_loss 1277.2822265625\n",
      "epoch 82, val_loss 454.94342041015625\n",
      "epoch 83, train_loss 1277.281982421875\n",
      "epoch 83, val_loss 454.94329833984375\n",
      "epoch 84, train_loss 1277.281982421875\n",
      "epoch 84, val_loss 454.9432373046875\n",
      "epoch 85, train_loss 1277.281982421875\n",
      "epoch 85, val_loss 454.9432067871094\n",
      "epoch 86, train_loss 1277.281982421875\n",
      "epoch 86, val_loss 454.94317626953125\n",
      "epoch 87, train_loss 1277.2818603515625\n",
      "epoch 87, val_loss 454.9430847167969\n",
      "epoch 88, train_loss 1277.28173828125\n",
      "epoch 88, val_loss 454.94305419921875\n",
      "epoch 89, train_loss 1277.2816162109375\n",
      "epoch 89, val_loss 454.9429931640625\n",
      "epoch 90, train_loss 1277.2816162109375\n",
      "epoch 90, val_loss 454.94287109375\n",
      "epoch 91, train_loss 1277.2816162109375\n",
      "epoch 91, val_loss 454.9428405761719\n",
      "epoch 92, train_loss 1277.2816162109375\n",
      "epoch 92, val_loss 454.9427490234375\n",
      "epoch 93, train_loss 1277.2813720703125\n",
      "epoch 93, val_loss 454.9427490234375\n",
      "epoch 94, train_loss 1277.2813720703125\n",
      "epoch 94, val_loss 454.942626953125\n",
      "epoch 95, train_loss 1277.2811279296875\n",
      "epoch 95, val_loss 454.942626953125\n",
      "epoch 96, train_loss 1277.2813720703125\n",
      "epoch 96, val_loss 454.9425964355469\n",
      "epoch 97, train_loss 1277.28125\n",
      "epoch 97, val_loss 454.9425048828125\n",
      "epoch 98, train_loss 1277.2811279296875\n",
      "epoch 98, val_loss 454.9425048828125\n",
      "epoch 99, train_loss 1277.2808837890625\n",
      "epoch 99, val_loss 454.9424133300781\n",
      "Parameter containing:\n",
      "tensor([5.1716e-21], requires_grad=True)\n",
      "iter 108, train_loss_regularization 0.7405645251274109\n",
      "iter 108, val_loss_regularization 0.7405645251274109\n",
      "epoch 0, train_loss 1277.2811279296875\n",
      "epoch 0, val_loss 454.94232177734375\n",
      "epoch 1, train_loss 1277.2811279296875\n",
      "epoch 1, val_loss 454.9422607421875\n",
      "epoch 2, train_loss 1277.2811279296875\n",
      "epoch 2, val_loss 454.9421691894531\n",
      "epoch 3, train_loss 1277.281005859375\n",
      "epoch 3, val_loss 454.942138671875\n",
      "epoch 4, train_loss 1277.2806396484375\n",
      "epoch 4, val_loss 454.94207763671875\n",
      "epoch 5, train_loss 1277.28076171875\n",
      "epoch 5, val_loss 454.9420471191406\n",
      "epoch 6, train_loss 1277.2806396484375\n",
      "epoch 6, val_loss 454.94195556640625\n",
      "epoch 7, train_loss 1277.2806396484375\n",
      "epoch 7, val_loss 454.9419250488281\n",
      "epoch 8, train_loss 1277.2806396484375\n",
      "epoch 8, val_loss 454.9418640136719\n",
      "epoch 9, train_loss 1277.280517578125\n",
      "epoch 9, val_loss 454.9417419433594\n",
      "epoch 10, train_loss 1277.2802734375\n",
      "epoch 10, val_loss 454.9418029785156\n",
      "epoch 11, train_loss 1277.2802734375\n",
      "epoch 11, val_loss 454.94171142578125\n",
      "epoch 12, train_loss 1277.2802734375\n",
      "epoch 12, val_loss 454.9416198730469\n",
      "epoch 13, train_loss 1277.2803955078125\n",
      "epoch 13, val_loss 454.9415283203125\n",
      "epoch 14, train_loss 1277.280029296875\n",
      "epoch 14, val_loss 454.9415283203125\n",
      "epoch 15, train_loss 1277.280029296875\n",
      "epoch 15, val_loss 454.94140625\n",
      "epoch 16, train_loss 1277.280029296875\n",
      "epoch 16, val_loss 454.9413757324219\n",
      "epoch 17, train_loss 1277.280029296875\n",
      "epoch 17, val_loss 454.94134521484375\n",
      "epoch 18, train_loss 1277.2799072265625\n",
      "epoch 18, val_loss 454.9412841796875\n",
      "epoch 19, train_loss 1277.27978515625\n",
      "epoch 19, val_loss 454.9412536621094\n",
      "epoch 20, train_loss 1277.27978515625\n",
      "epoch 20, val_loss 454.9411315917969\n",
      "epoch 21, train_loss 1277.27978515625\n",
      "epoch 21, val_loss 454.9411315917969\n",
      "epoch 22, train_loss 1277.279541015625\n",
      "epoch 22, val_loss 454.9410095214844\n",
      "epoch 23, train_loss 1277.279541015625\n",
      "epoch 23, val_loss 454.9410095214844\n",
      "epoch 24, train_loss 1277.279541015625\n",
      "epoch 24, val_loss 454.94091796875\n",
      "epoch 25, train_loss 1277.279296875\n",
      "epoch 25, val_loss 454.94091796875\n",
      "epoch 26, train_loss 1277.279541015625\n",
      "epoch 26, val_loss 454.9408264160156\n",
      "epoch 27, train_loss 1277.279296875\n",
      "epoch 27, val_loss 454.94073486328125\n",
      "epoch 28, train_loss 1277.2794189453125\n",
      "epoch 28, val_loss 454.94073486328125\n",
      "epoch 29, train_loss 1277.279296875\n",
      "epoch 29, val_loss 454.9405822753906\n",
      "epoch 30, train_loss 1277.279052734375\n",
      "epoch 30, val_loss 454.9405517578125\n",
      "epoch 31, train_loss 1277.279052734375\n",
      "epoch 31, val_loss 454.94049072265625\n",
      "epoch 32, train_loss 1277.279052734375\n",
      "epoch 32, val_loss 454.9404296875\n",
      "epoch 33, train_loss 1277.2789306640625\n",
      "epoch 33, val_loss 454.9404296875\n",
      "epoch 34, train_loss 1277.27880859375\n",
      "epoch 34, val_loss 454.94036865234375\n",
      "epoch 35, train_loss 1277.27880859375\n",
      "epoch 35, val_loss 454.9403381347656\n",
      "epoch 36, train_loss 1277.27880859375\n",
      "epoch 36, val_loss 454.9402160644531\n",
      "epoch 37, train_loss 1277.27880859375\n",
      "epoch 37, val_loss 454.94024658203125\n",
      "epoch 38, train_loss 1277.27880859375\n",
      "epoch 38, val_loss 454.94012451171875\n",
      "epoch 39, train_loss 1277.2784423828125\n",
      "epoch 39, val_loss 454.9400329589844\n",
      "epoch 40, train_loss 1277.2784423828125\n",
      "epoch 40, val_loss 454.94000244140625\n",
      "epoch 41, train_loss 1277.2784423828125\n",
      "epoch 41, val_loss 454.9399108886719\n",
      "epoch 42, train_loss 1277.2784423828125\n",
      "epoch 42, val_loss 454.93988037109375\n",
      "epoch 43, train_loss 1277.2784423828125\n",
      "epoch 43, val_loss 454.9398193359375\n",
      "epoch 44, train_loss 1277.2781982421875\n",
      "epoch 44, val_loss 454.93975830078125\n",
      "epoch 45, train_loss 1277.2781982421875\n",
      "epoch 45, val_loss 454.939697265625\n",
      "epoch 46, train_loss 1277.278076171875\n",
      "epoch 46, val_loss 454.9396667480469\n",
      "epoch 47, train_loss 1277.278076171875\n",
      "epoch 47, val_loss 454.93963623046875\n",
      "epoch 48, train_loss 1277.2779541015625\n",
      "epoch 48, val_loss 454.9395751953125\n",
      "epoch 49, train_loss 1277.2779541015625\n",
      "epoch 49, val_loss 454.93951416015625\n",
      "epoch 50, train_loss 1277.27783203125\n",
      "epoch 50, val_loss 454.9394226074219\n",
      "epoch 51, train_loss 1277.2777099609375\n",
      "epoch 51, val_loss 454.9393615722656\n",
      "epoch 52, train_loss 1277.2777099609375\n",
      "epoch 52, val_loss 454.9393005371094\n",
      "epoch 53, train_loss 1277.2777099609375\n",
      "epoch 53, val_loss 454.939208984375\n",
      "epoch 54, train_loss 1277.2777099609375\n",
      "epoch 54, val_loss 454.939208984375\n",
      "epoch 55, train_loss 1277.2777099609375\n",
      "epoch 55, val_loss 454.9391784667969\n",
      "epoch 56, train_loss 1277.2774658203125\n",
      "epoch 56, val_loss 454.9390869140625\n",
      "epoch 57, train_loss 1277.2774658203125\n",
      "epoch 57, val_loss 454.9390563964844\n",
      "epoch 58, train_loss 1277.27734375\n",
      "epoch 58, val_loss 454.9390563964844\n",
      "epoch 59, train_loss 1277.27734375\n",
      "epoch 59, val_loss 454.9388427734375\n",
      "epoch 60, train_loss 1277.27734375\n",
      "epoch 60, val_loss 454.9388732910156\n",
      "epoch 61, train_loss 1277.2772216796875\n",
      "epoch 61, val_loss 454.93878173828125\n",
      "epoch 62, train_loss 1277.277099609375\n",
      "epoch 62, val_loss 454.938720703125\n",
      "epoch 63, train_loss 1277.277099609375\n",
      "epoch 63, val_loss 454.93865966796875\n",
      "epoch 64, train_loss 1277.2769775390625\n",
      "epoch 64, val_loss 454.93865966796875\n",
      "epoch 65, train_loss 1277.2769775390625\n",
      "epoch 65, val_loss 454.9385681152344\n",
      "epoch 66, train_loss 1277.277099609375\n",
      "epoch 66, val_loss 454.9385070800781\n",
      "epoch 67, train_loss 1277.27685546875\n",
      "epoch 67, val_loss 454.9384460449219\n",
      "epoch 68, train_loss 1277.27685546875\n",
      "epoch 68, val_loss 454.9383850097656\n",
      "epoch 69, train_loss 1277.2767333984375\n",
      "epoch 69, val_loss 454.93829345703125\n",
      "epoch 70, train_loss 1277.276611328125\n",
      "epoch 70, val_loss 454.93829345703125\n",
      "epoch 71, train_loss 1277.276611328125\n",
      "epoch 71, val_loss 454.93817138671875\n",
      "epoch 72, train_loss 1277.276611328125\n",
      "epoch 72, val_loss 454.93817138671875\n",
      "epoch 73, train_loss 1277.2763671875\n",
      "epoch 73, val_loss 454.9381103515625\n",
      "epoch 74, train_loss 1277.2763671875\n",
      "epoch 74, val_loss 454.93798828125\n",
      "epoch 75, train_loss 1277.2763671875\n",
      "epoch 75, val_loss 454.93798828125\n",
      "epoch 76, train_loss 1277.2762451171875\n",
      "epoch 76, val_loss 454.9379577636719\n",
      "epoch 77, train_loss 1277.276123046875\n",
      "epoch 77, val_loss 454.9378662109375\n",
      "epoch 78, train_loss 1277.2762451171875\n",
      "epoch 78, val_loss 454.9378356933594\n",
      "epoch 79, train_loss 1277.276123046875\n",
      "epoch 79, val_loss 454.937744140625\n",
      "epoch 80, train_loss 1277.2760009765625\n",
      "epoch 80, val_loss 454.9376525878906\n",
      "epoch 81, train_loss 1277.2760009765625\n",
      "epoch 81, val_loss 454.9376220703125\n",
      "epoch 82, train_loss 1277.2760009765625\n",
      "epoch 82, val_loss 454.9375915527344\n",
      "epoch 83, train_loss 1277.27587890625\n",
      "epoch 83, val_loss 454.9375\n",
      "epoch 84, train_loss 1277.275634765625\n",
      "epoch 84, val_loss 454.9375\n",
      "epoch 85, train_loss 1277.275634765625\n",
      "epoch 85, val_loss 454.9374084472656\n",
      "epoch 86, train_loss 1277.275634765625\n",
      "epoch 86, val_loss 454.9373779296875\n",
      "epoch 87, train_loss 1277.275634765625\n",
      "epoch 87, val_loss 454.9373474121094\n",
      "epoch 88, train_loss 1277.2755126953125\n",
      "epoch 88, val_loss 454.937255859375\n",
      "epoch 89, train_loss 1277.2755126953125\n",
      "epoch 89, val_loss 454.9371643066406\n",
      "epoch 90, train_loss 1277.275390625\n",
      "epoch 90, val_loss 454.93707275390625\n",
      "epoch 91, train_loss 1277.275390625\n",
      "epoch 91, val_loss 454.93707275390625\n",
      "epoch 92, train_loss 1277.275390625\n",
      "epoch 92, val_loss 454.93701171875\n",
      "epoch 93, train_loss 1277.2752685546875\n",
      "epoch 93, val_loss 454.93695068359375\n",
      "epoch 94, train_loss 1277.275146484375\n",
      "epoch 94, val_loss 454.9369201660156\n",
      "epoch 95, train_loss 1277.2750244140625\n",
      "epoch 95, val_loss 454.9367980957031\n",
      "epoch 96, train_loss 1277.2750244140625\n",
      "epoch 96, val_loss 454.9367370605469\n",
      "epoch 97, train_loss 1277.2750244140625\n",
      "epoch 97, val_loss 454.93670654296875\n",
      "epoch 98, train_loss 1277.27490234375\n",
      "epoch 98, val_loss 454.9366760253906\n",
      "epoch 99, train_loss 1277.2747802734375\n",
      "epoch 99, val_loss 454.93658447265625\n",
      "Parameter containing:\n",
      "tensor([3.5546e-21], requires_grad=True)\n",
      "iter 109, train_loss_regularization 0.7389655113220215\n",
      "iter 109, val_loss_regularization 0.7389655113220215\n",
      "epoch 0, train_loss 1277.2747802734375\n",
      "epoch 0, val_loss 454.9365539550781\n",
      "epoch 1, train_loss 1277.274658203125\n",
      "epoch 1, val_loss 454.9364929199219\n",
      "epoch 2, train_loss 1277.274658203125\n",
      "epoch 2, val_loss 454.93646240234375\n",
      "epoch 3, train_loss 1277.2745361328125\n",
      "epoch 3, val_loss 454.93634033203125\n",
      "epoch 4, train_loss 1277.2744140625\n",
      "epoch 4, val_loss 454.936279296875\n",
      "epoch 5, train_loss 1277.2745361328125\n",
      "epoch 5, val_loss 454.9362487792969\n",
      "epoch 6, train_loss 1277.2744140625\n",
      "epoch 6, val_loss 454.93621826171875\n",
      "epoch 7, train_loss 1277.2745361328125\n",
      "epoch 7, val_loss 454.9361572265625\n",
      "epoch 8, train_loss 1277.2742919921875\n",
      "epoch 8, val_loss 454.9361267089844\n",
      "epoch 9, train_loss 1277.2742919921875\n",
      "epoch 9, val_loss 454.9360046386719\n",
      "epoch 10, train_loss 1277.2742919921875\n",
      "epoch 10, val_loss 454.9359436035156\n",
      "epoch 11, train_loss 1277.274169921875\n",
      "epoch 11, val_loss 454.9359436035156\n",
      "epoch 12, train_loss 1277.2740478515625\n",
      "epoch 12, val_loss 454.9358215332031\n",
      "epoch 13, train_loss 1277.2740478515625\n",
      "epoch 13, val_loss 454.935791015625\n",
      "epoch 14, train_loss 1277.2740478515625\n",
      "epoch 14, val_loss 454.935791015625\n",
      "epoch 15, train_loss 1277.2738037109375\n",
      "epoch 15, val_loss 454.9356689453125\n",
      "epoch 16, train_loss 1277.2738037109375\n",
      "epoch 16, val_loss 454.9356689453125\n",
      "epoch 17, train_loss 1277.2738037109375\n",
      "epoch 17, val_loss 454.9355773925781\n",
      "epoch 18, train_loss 1277.2738037109375\n",
      "epoch 18, val_loss 454.93548583984375\n",
      "epoch 19, train_loss 1277.2738037109375\n",
      "epoch 19, val_loss 454.9354553222656\n",
      "epoch 20, train_loss 1277.2735595703125\n",
      "epoch 20, val_loss 454.93536376953125\n",
      "epoch 21, train_loss 1277.2734375\n",
      "epoch 21, val_loss 454.935302734375\n",
      "epoch 22, train_loss 1277.2734375\n",
      "epoch 22, val_loss 454.935302734375\n",
      "epoch 23, train_loss 1277.2733154296875\n",
      "epoch 23, val_loss 454.9352111816406\n",
      "epoch 24, train_loss 1277.273193359375\n",
      "epoch 24, val_loss 454.9351806640625\n",
      "epoch 25, train_loss 1277.2733154296875\n",
      "epoch 25, val_loss 454.9350891113281\n",
      "epoch 26, train_loss 1277.273193359375\n",
      "epoch 26, val_loss 454.9350891113281\n",
      "epoch 27, train_loss 1277.2730712890625\n",
      "epoch 27, val_loss 454.93499755859375\n",
      "epoch 28, train_loss 1277.2730712890625\n",
      "epoch 28, val_loss 454.9349670410156\n",
      "epoch 29, train_loss 1277.2730712890625\n",
      "epoch 29, val_loss 454.93487548828125\n",
      "epoch 30, train_loss 1277.2730712890625\n",
      "epoch 30, val_loss 454.9347839355469\n",
      "epoch 31, train_loss 1277.272705078125\n",
      "epoch 31, val_loss 454.93475341796875\n",
      "epoch 32, train_loss 1277.272705078125\n",
      "epoch 32, val_loss 454.93475341796875\n",
      "epoch 33, train_loss 1277.272705078125\n",
      "epoch 33, val_loss 454.9346618652344\n",
      "epoch 34, train_loss 1277.272705078125\n",
      "epoch 34, val_loss 454.9345703125\n",
      "epoch 35, train_loss 1277.272705078125\n",
      "epoch 35, val_loss 454.9345397949219\n",
      "epoch 36, train_loss 1277.2724609375\n",
      "epoch 36, val_loss 454.9344482421875\n",
      "epoch 37, train_loss 1277.2724609375\n",
      "epoch 37, val_loss 454.9344482421875\n",
      "epoch 38, train_loss 1277.2724609375\n",
      "epoch 38, val_loss 454.9344177246094\n",
      "epoch 39, train_loss 1277.272216796875\n",
      "epoch 39, val_loss 454.934326171875\n",
      "epoch 40, train_loss 1277.272216796875\n",
      "epoch 40, val_loss 454.93426513671875\n",
      "epoch 41, train_loss 1277.272216796875\n",
      "epoch 41, val_loss 454.9341735839844\n",
      "epoch 42, train_loss 1277.2720947265625\n",
      "epoch 42, val_loss 454.93408203125\n",
      "epoch 43, train_loss 1277.27197265625\n",
      "epoch 43, val_loss 454.93408203125\n",
      "epoch 44, train_loss 1277.2720947265625\n",
      "epoch 44, val_loss 454.9340515136719\n",
      "epoch 45, train_loss 1277.2720947265625\n",
      "epoch 45, val_loss 454.9339599609375\n",
      "epoch 46, train_loss 1277.2720947265625\n",
      "epoch 46, val_loss 454.9339294433594\n",
      "epoch 47, train_loss 1277.2718505859375\n",
      "epoch 47, val_loss 454.9339294433594\n",
      "epoch 48, train_loss 1277.271728515625\n",
      "epoch 48, val_loss 454.9337463378906\n",
      "epoch 49, train_loss 1277.271728515625\n",
      "epoch 49, val_loss 454.9337463378906\n",
      "epoch 50, train_loss 1277.271728515625\n",
      "epoch 50, val_loss 454.9337158203125\n",
      "epoch 51, train_loss 1277.2716064453125\n",
      "epoch 51, val_loss 454.93359375\n",
      "epoch 52, train_loss 1277.271484375\n",
      "epoch 52, val_loss 454.93359375\n",
      "epoch 53, train_loss 1277.271484375\n",
      "epoch 53, val_loss 454.93353271484375\n",
      "epoch 54, train_loss 1277.2713623046875\n",
      "epoch 54, val_loss 454.9334716796875\n",
      "epoch 55, train_loss 1277.2713623046875\n",
      "epoch 55, val_loss 454.9333801269531\n",
      "epoch 56, train_loss 1277.2713623046875\n",
      "epoch 56, val_loss 454.9333801269531\n",
      "epoch 57, train_loss 1277.2711181640625\n",
      "epoch 57, val_loss 454.9332580566406\n",
      "epoch 58, train_loss 1277.2711181640625\n",
      "epoch 58, val_loss 454.9332580566406\n",
      "epoch 59, train_loss 1277.271240234375\n",
      "epoch 59, val_loss 454.93316650390625\n",
      "epoch 60, train_loss 1277.2711181640625\n",
      "epoch 60, val_loss 454.9330749511719\n",
      "epoch 61, train_loss 1277.2711181640625\n",
      "epoch 61, val_loss 454.9330749511719\n",
      "epoch 62, train_loss 1277.27099609375\n",
      "epoch 62, val_loss 454.9330139160156\n",
      "epoch 63, train_loss 1277.2708740234375\n",
      "epoch 63, val_loss 454.93292236328125\n",
      "epoch 64, train_loss 1277.2708740234375\n",
      "epoch 64, val_loss 454.93292236328125\n",
      "epoch 65, train_loss 1277.2708740234375\n",
      "epoch 65, val_loss 454.9328308105469\n",
      "epoch 66, train_loss 1277.270751953125\n",
      "epoch 66, val_loss 454.93280029296875\n",
      "epoch 67, train_loss 1277.2706298828125\n",
      "epoch 67, val_loss 454.9327087402344\n",
      "epoch 68, train_loss 1277.2705078125\n",
      "epoch 68, val_loss 454.93267822265625\n",
      "epoch 69, train_loss 1277.2705078125\n",
      "epoch 69, val_loss 454.9325866699219\n",
      "epoch 70, train_loss 1277.2706298828125\n",
      "epoch 70, val_loss 454.93255615234375\n",
      "epoch 71, train_loss 1277.2705078125\n",
      "epoch 71, val_loss 454.9324645996094\n",
      "epoch 72, train_loss 1277.2703857421875\n",
      "epoch 72, val_loss 454.9324035644531\n",
      "epoch 73, train_loss 1277.2703857421875\n",
      "epoch 73, val_loss 454.932373046875\n",
      "epoch 74, train_loss 1277.270263671875\n",
      "epoch 74, val_loss 454.9322814941406\n",
      "epoch 75, train_loss 1277.2701416015625\n",
      "epoch 75, val_loss 454.9322814941406\n",
      "epoch 76, train_loss 1277.2701416015625\n",
      "epoch 76, val_loss 454.9322204589844\n",
      "epoch 77, train_loss 1277.2701416015625\n",
      "epoch 77, val_loss 454.9322204589844\n",
      "epoch 78, train_loss 1277.269775390625\n",
      "epoch 78, val_loss 454.9320983886719\n",
      "epoch 79, train_loss 1277.269775390625\n",
      "epoch 79, val_loss 454.9320068359375\n",
      "epoch 80, train_loss 1277.269775390625\n",
      "epoch 80, val_loss 454.93194580078125\n",
      "epoch 81, train_loss 1277.269775390625\n",
      "epoch 81, val_loss 454.931884765625\n",
      "epoch 82, train_loss 1277.2696533203125\n",
      "epoch 82, val_loss 454.93182373046875\n",
      "epoch 83, train_loss 1277.26953125\n",
      "epoch 83, val_loss 454.9317932128906\n",
      "epoch 84, train_loss 1277.26953125\n",
      "epoch 84, val_loss 454.9317626953125\n",
      "epoch 85, train_loss 1277.26953125\n",
      "epoch 85, val_loss 454.9316711425781\n",
      "epoch 86, train_loss 1277.2694091796875\n",
      "epoch 86, val_loss 454.9316711425781\n",
      "epoch 87, train_loss 1277.26953125\n",
      "epoch 87, val_loss 454.93157958984375\n",
      "epoch 88, train_loss 1277.26953125\n",
      "epoch 88, val_loss 454.9315490722656\n",
      "epoch 89, train_loss 1277.269287109375\n",
      "epoch 89, val_loss 454.9314880371094\n",
      "epoch 90, train_loss 1277.2691650390625\n",
      "epoch 90, val_loss 454.9314270019531\n",
      "epoch 91, train_loss 1277.2691650390625\n",
      "epoch 91, val_loss 454.9313049316406\n",
      "epoch 92, train_loss 1277.2691650390625\n",
      "epoch 92, val_loss 454.9312438964844\n",
      "epoch 93, train_loss 1277.26904296875\n",
      "epoch 93, val_loss 454.93115234375\n",
      "epoch 94, train_loss 1277.26904296875\n",
      "epoch 94, val_loss 454.93115234375\n",
      "epoch 95, train_loss 1277.268798828125\n",
      "epoch 95, val_loss 454.9310302734375\n",
      "epoch 96, train_loss 1277.268798828125\n",
      "epoch 96, val_loss 454.93109130859375\n",
      "epoch 97, train_loss 1277.268798828125\n",
      "epoch 97, val_loss 454.9310302734375\n",
      "epoch 98, train_loss 1277.2686767578125\n",
      "epoch 98, val_loss 454.93096923828125\n",
      "epoch 99, train_loss 1277.268798828125\n",
      "epoch 99, val_loss 454.930908203125\n",
      "Parameter containing:\n",
      "tensor([2.4443e-21], requires_grad=True)\n",
      "iter 110, train_loss_regularization 0.7373932003974915\n",
      "iter 110, val_loss_regularization 0.7373932003974915\n",
      "epoch 0, train_loss 1277.2686767578125\n",
      "epoch 0, val_loss 454.9308776855469\n",
      "epoch 1, train_loss 1277.2685546875\n",
      "epoch 1, val_loss 454.9307556152344\n",
      "epoch 2, train_loss 1277.2685546875\n",
      "epoch 2, val_loss 454.9306945800781\n",
      "epoch 3, train_loss 1277.2684326171875\n",
      "epoch 3, val_loss 454.9306640625\n",
      "epoch 4, train_loss 1277.268310546875\n",
      "epoch 4, val_loss 454.9305725097656\n",
      "epoch 5, train_loss 1277.268310546875\n",
      "epoch 5, val_loss 454.9305419921875\n",
      "epoch 6, train_loss 1277.268310546875\n",
      "epoch 6, val_loss 454.9305114746094\n",
      "epoch 7, train_loss 1277.26806640625\n",
      "epoch 7, val_loss 454.930419921875\n",
      "epoch 8, train_loss 1277.26806640625\n",
      "epoch 8, val_loss 454.930419921875\n",
      "epoch 9, train_loss 1277.26806640625\n",
      "epoch 9, val_loss 454.9303283691406\n",
      "epoch 10, train_loss 1277.2679443359375\n",
      "epoch 10, val_loss 454.93023681640625\n",
      "epoch 11, train_loss 1277.2679443359375\n",
      "epoch 11, val_loss 454.93017578125\n",
      "epoch 12, train_loss 1277.2679443359375\n",
      "epoch 12, val_loss 454.93017578125\n",
      "epoch 13, train_loss 1277.267822265625\n",
      "epoch 13, val_loss 454.9300842285156\n",
      "epoch 14, train_loss 1277.2677001953125\n",
      "epoch 14, val_loss 454.9300537109375\n",
      "epoch 15, train_loss 1277.2677001953125\n",
      "epoch 15, val_loss 454.9300537109375\n",
      "epoch 16, train_loss 1277.2677001953125\n",
      "epoch 16, val_loss 454.929931640625\n",
      "epoch 17, train_loss 1277.2677001953125\n",
      "epoch 17, val_loss 454.92987060546875\n",
      "epoch 18, train_loss 1277.2674560546875\n",
      "epoch 18, val_loss 454.9297790527344\n",
      "epoch 19, train_loss 1277.2674560546875\n",
      "epoch 19, val_loss 454.92974853515625\n",
      "epoch 20, train_loss 1277.2674560546875\n",
      "epoch 20, val_loss 454.9296569824219\n",
      "epoch 21, train_loss 1277.2674560546875\n",
      "epoch 21, val_loss 454.92962646484375\n",
      "epoch 22, train_loss 1277.2672119140625\n",
      "epoch 22, val_loss 454.9295959472656\n",
      "epoch 23, train_loss 1277.2672119140625\n",
      "epoch 23, val_loss 454.92950439453125\n",
      "epoch 24, train_loss 1277.26708984375\n",
      "epoch 24, val_loss 454.929443359375\n",
      "epoch 25, train_loss 1277.26708984375\n",
      "epoch 25, val_loss 454.92938232421875\n",
      "epoch 26, train_loss 1277.26708984375\n",
      "epoch 26, val_loss 454.92938232421875\n",
      "epoch 27, train_loss 1277.2669677734375\n",
      "epoch 27, val_loss 454.9293212890625\n",
      "epoch 28, train_loss 1277.2669677734375\n",
      "epoch 28, val_loss 454.9292907714844\n",
      "epoch 29, train_loss 1277.266845703125\n",
      "epoch 29, val_loss 454.92919921875\n",
      "epoch 30, train_loss 1277.2667236328125\n",
      "epoch 30, val_loss 454.9291687011719\n",
      "epoch 31, train_loss 1277.2667236328125\n",
      "epoch 31, val_loss 454.9289855957031\n",
      "epoch 32, train_loss 1277.2667236328125\n",
      "epoch 32, val_loss 454.9289855957031\n",
      "epoch 33, train_loss 1277.2666015625\n",
      "epoch 33, val_loss 454.9289245605469\n",
      "epoch 34, train_loss 1277.2664794921875\n",
      "epoch 34, val_loss 454.9288635253906\n",
      "epoch 35, train_loss 1277.2664794921875\n",
      "epoch 35, val_loss 454.9288330078125\n",
      "epoch 36, train_loss 1277.2664794921875\n",
      "epoch 36, val_loss 454.9288635253906\n",
      "epoch 37, train_loss 1277.266357421875\n",
      "epoch 37, val_loss 454.9287414550781\n",
      "epoch 38, train_loss 1277.2662353515625\n",
      "epoch 38, val_loss 454.9286804199219\n",
      "epoch 39, train_loss 1277.26611328125\n",
      "epoch 39, val_loss 454.9286804199219\n",
      "epoch 40, train_loss 1277.2662353515625\n",
      "epoch 40, val_loss 454.92852783203125\n",
      "epoch 41, train_loss 1277.26611328125\n",
      "epoch 41, val_loss 454.9284973144531\n",
      "epoch 42, train_loss 1277.2659912109375\n",
      "epoch 42, val_loss 454.928466796875\n",
      "epoch 43, train_loss 1277.2659912109375\n",
      "epoch 43, val_loss 454.9283752441406\n",
      "epoch 44, train_loss 1277.2659912109375\n",
      "epoch 44, val_loss 454.9283447265625\n",
      "epoch 45, train_loss 1277.265869140625\n",
      "epoch 45, val_loss 454.9282531738281\n",
      "epoch 46, train_loss 1277.265869140625\n",
      "epoch 46, val_loss 454.9282531738281\n",
      "epoch 47, train_loss 1277.2657470703125\n",
      "epoch 47, val_loss 454.92822265625\n",
      "epoch 48, train_loss 1277.265625\n",
      "epoch 48, val_loss 454.9281311035156\n",
      "epoch 49, train_loss 1277.265625\n",
      "epoch 49, val_loss 454.9280700683594\n",
      "epoch 50, train_loss 1277.265380859375\n",
      "epoch 50, val_loss 454.9280090332031\n",
      "epoch 51, train_loss 1277.265625\n",
      "epoch 51, val_loss 454.9279479980469\n",
      "epoch 52, train_loss 1277.265625\n",
      "epoch 52, val_loss 454.9278259277344\n",
      "epoch 53, train_loss 1277.265380859375\n",
      "epoch 53, val_loss 454.9278869628906\n",
      "epoch 54, train_loss 1277.265380859375\n",
      "epoch 54, val_loss 454.9277648925781\n",
      "epoch 55, train_loss 1277.2652587890625\n",
      "epoch 55, val_loss 454.9277038574219\n",
      "epoch 56, train_loss 1277.2652587890625\n",
      "epoch 56, val_loss 454.92767333984375\n",
      "epoch 57, train_loss 1277.2652587890625\n",
      "epoch 57, val_loss 454.9276123046875\n",
      "epoch 58, train_loss 1277.26513671875\n",
      "epoch 58, val_loss 454.92755126953125\n",
      "epoch 59, train_loss 1277.2650146484375\n",
      "epoch 59, val_loss 454.927490234375\n",
      "epoch 60, train_loss 1277.264892578125\n",
      "epoch 60, val_loss 454.927490234375\n",
      "epoch 61, train_loss 1277.2647705078125\n",
      "epoch 61, val_loss 454.9273376464844\n",
      "epoch 62, train_loss 1277.2647705078125\n",
      "epoch 62, val_loss 454.9272766113281\n",
      "epoch 63, train_loss 1277.264892578125\n",
      "epoch 63, val_loss 454.92724609375\n",
      "epoch 64, train_loss 1277.2650146484375\n",
      "epoch 64, val_loss 454.9272155761719\n",
      "epoch 65, train_loss 1277.2646484375\n",
      "epoch 65, val_loss 454.9271240234375\n",
      "epoch 66, train_loss 1277.2645263671875\n",
      "epoch 66, val_loss 454.9271240234375\n",
      "epoch 67, train_loss 1277.2645263671875\n",
      "epoch 67, val_loss 454.9270324707031\n",
      "epoch 68, train_loss 1277.2645263671875\n",
      "epoch 68, val_loss 454.927001953125\n",
      "epoch 69, train_loss 1277.2645263671875\n",
      "epoch 69, val_loss 454.9269104003906\n",
      "epoch 70, train_loss 1277.264404296875\n",
      "epoch 70, val_loss 454.9267883300781\n",
      "epoch 71, train_loss 1277.2642822265625\n",
      "epoch 71, val_loss 454.9267883300781\n",
      "epoch 72, train_loss 1277.2642822265625\n",
      "epoch 72, val_loss 454.9267578125\n",
      "epoch 73, train_loss 1277.2642822265625\n",
      "epoch 73, val_loss 454.9266662597656\n",
      "epoch 74, train_loss 1277.26416015625\n",
      "epoch 74, val_loss 454.9266662597656\n",
      "epoch 75, train_loss 1277.26416015625\n",
      "epoch 75, val_loss 454.9266357421875\n",
      "epoch 76, train_loss 1277.2637939453125\n",
      "epoch 76, val_loss 454.9265441894531\n",
      "epoch 77, train_loss 1277.2637939453125\n",
      "epoch 77, val_loss 454.92645263671875\n",
      "epoch 78, train_loss 1277.263916015625\n",
      "epoch 78, val_loss 454.9264221191406\n",
      "epoch 79, train_loss 1277.2637939453125\n",
      "epoch 79, val_loss 454.92633056640625\n",
      "epoch 80, train_loss 1277.263916015625\n",
      "epoch 80, val_loss 454.92633056640625\n",
      "epoch 81, train_loss 1277.2637939453125\n",
      "epoch 81, val_loss 454.9262390136719\n",
      "epoch 82, train_loss 1277.2637939453125\n",
      "epoch 82, val_loss 454.9261779785156\n",
      "epoch 83, train_loss 1277.2635498046875\n",
      "epoch 83, val_loss 454.9261169433594\n",
      "epoch 84, train_loss 1277.2635498046875\n",
      "epoch 84, val_loss 454.9260559082031\n",
      "epoch 85, train_loss 1277.2635498046875\n",
      "epoch 85, val_loss 454.9259948730469\n",
      "epoch 86, train_loss 1277.263427734375\n",
      "epoch 86, val_loss 454.92596435546875\n",
      "epoch 87, train_loss 1277.2633056640625\n",
      "epoch 87, val_loss 454.92596435546875\n",
      "epoch 88, train_loss 1277.2633056640625\n",
      "epoch 88, val_loss 454.92584228515625\n",
      "epoch 89, train_loss 1277.26318359375\n",
      "epoch 89, val_loss 454.92584228515625\n",
      "epoch 90, train_loss 1277.26318359375\n",
      "epoch 90, val_loss 454.9257507324219\n",
      "epoch 91, train_loss 1277.26318359375\n",
      "epoch 91, val_loss 454.92572021484375\n",
      "epoch 92, train_loss 1277.26318359375\n",
      "epoch 92, val_loss 454.92559814453125\n",
      "epoch 93, train_loss 1277.262939453125\n",
      "epoch 93, val_loss 454.92559814453125\n",
      "epoch 94, train_loss 1277.262939453125\n",
      "epoch 94, val_loss 454.9255065917969\n",
      "epoch 95, train_loss 1277.2628173828125\n",
      "epoch 95, val_loss 454.9254150390625\n",
      "epoch 96, train_loss 1277.2628173828125\n",
      "epoch 96, val_loss 454.9254150390625\n",
      "epoch 97, train_loss 1277.2626953125\n",
      "epoch 97, val_loss 454.9253234863281\n",
      "epoch 98, train_loss 1277.2625732421875\n",
      "epoch 98, val_loss 454.92529296875\n",
      "epoch 99, train_loss 1277.2625732421875\n",
      "epoch 99, val_loss 454.9252624511719\n",
      "Parameter containing:\n",
      "tensor([1.6817e-21], requires_grad=True)\n",
      "iter 111, train_loss_regularization 0.7358496189117432\n",
      "iter 111, val_loss_regularization 0.7358496189117432\n",
      "epoch 0, train_loss 1277.2626953125\n",
      "epoch 0, val_loss 454.9251708984375\n",
      "epoch 1, train_loss 1277.262451171875\n",
      "epoch 1, val_loss 454.9250793457031\n",
      "epoch 2, train_loss 1277.262451171875\n",
      "epoch 2, val_loss 454.925048828125\n",
      "epoch 3, train_loss 1277.2623291015625\n",
      "epoch 3, val_loss 454.925048828125\n",
      "epoch 4, train_loss 1277.2623291015625\n",
      "epoch 4, val_loss 454.9249572753906\n",
      "epoch 5, train_loss 1277.26220703125\n",
      "epoch 5, val_loss 454.92486572265625\n",
      "epoch 6, train_loss 1277.26220703125\n",
      "epoch 6, val_loss 454.92486572265625\n",
      "epoch 7, train_loss 1277.26220703125\n",
      "epoch 7, val_loss 454.92474365234375\n",
      "epoch 8, train_loss 1277.2620849609375\n",
      "epoch 8, val_loss 454.92474365234375\n",
      "epoch 9, train_loss 1277.2620849609375\n",
      "epoch 9, val_loss 454.9246520996094\n",
      "epoch 10, train_loss 1277.261962890625\n",
      "epoch 10, val_loss 454.92462158203125\n",
      "epoch 11, train_loss 1277.2620849609375\n",
      "epoch 11, val_loss 454.9245300292969\n",
      "epoch 12, train_loss 1277.2618408203125\n",
      "epoch 12, val_loss 454.9244689941406\n",
      "epoch 13, train_loss 1277.26171875\n",
      "epoch 13, val_loss 454.9244689941406\n",
      "epoch 14, train_loss 1277.26171875\n",
      "epoch 14, val_loss 454.92437744140625\n",
      "epoch 15, train_loss 1277.26171875\n",
      "epoch 15, val_loss 454.9243469238281\n",
      "epoch 16, train_loss 1277.2615966796875\n",
      "epoch 16, val_loss 454.9242858886719\n",
      "epoch 17, train_loss 1277.261474609375\n",
      "epoch 17, val_loss 454.92425537109375\n",
      "epoch 18, train_loss 1277.261474609375\n",
      "epoch 18, val_loss 454.9241638183594\n",
      "epoch 19, train_loss 1277.261474609375\n",
      "epoch 19, val_loss 454.92413330078125\n",
      "epoch 20, train_loss 1277.261474609375\n",
      "epoch 20, val_loss 454.92413330078125\n",
      "epoch 21, train_loss 1277.261474609375\n",
      "epoch 21, val_loss 454.92401123046875\n",
      "epoch 22, train_loss 1277.2611083984375\n",
      "epoch 22, val_loss 454.9239196777344\n",
      "epoch 23, train_loss 1277.2613525390625\n",
      "epoch 23, val_loss 454.92388916015625\n",
      "epoch 24, train_loss 1277.2611083984375\n",
      "epoch 24, val_loss 454.923828125\n",
      "epoch 25, train_loss 1277.2611083984375\n",
      "epoch 25, val_loss 454.9237976074219\n",
      "epoch 26, train_loss 1277.260986328125\n",
      "epoch 26, val_loss 454.9237365722656\n",
      "epoch 27, train_loss 1277.260986328125\n",
      "epoch 27, val_loss 454.9236755371094\n",
      "epoch 28, train_loss 1277.2608642578125\n",
      "epoch 28, val_loss 454.9236145019531\n",
      "epoch 29, train_loss 1277.2607421875\n",
      "epoch 29, val_loss 454.923583984375\n",
      "epoch 30, train_loss 1277.2606201171875\n",
      "epoch 30, val_loss 454.9235534667969\n",
      "epoch 31, train_loss 1277.2606201171875\n",
      "epoch 31, val_loss 454.9234313964844\n",
      "epoch 32, train_loss 1277.2607421875\n",
      "epoch 32, val_loss 454.92333984375\n",
      "epoch 33, train_loss 1277.2606201171875\n",
      "epoch 33, val_loss 454.92333984375\n",
      "epoch 34, train_loss 1277.2606201171875\n",
      "epoch 34, val_loss 454.9232482910156\n",
      "epoch 35, train_loss 1277.2603759765625\n",
      "epoch 35, val_loss 454.9232177734375\n",
      "epoch 36, train_loss 1277.2603759765625\n",
      "epoch 36, val_loss 454.92315673828125\n",
      "epoch 37, train_loss 1277.2603759765625\n",
      "epoch 37, val_loss 454.9231262207031\n",
      "epoch 38, train_loss 1277.26025390625\n",
      "epoch 38, val_loss 454.92303466796875\n",
      "epoch 39, train_loss 1277.2601318359375\n",
      "epoch 39, val_loss 454.92303466796875\n",
      "epoch 40, train_loss 1277.2601318359375\n",
      "epoch 40, val_loss 454.9229431152344\n",
      "epoch 41, train_loss 1277.260009765625\n",
      "epoch 41, val_loss 454.92291259765625\n",
      "epoch 42, train_loss 1277.2598876953125\n",
      "epoch 42, val_loss 454.9228210449219\n",
      "epoch 43, train_loss 1277.260009765625\n",
      "epoch 43, val_loss 454.9227600097656\n",
      "epoch 44, train_loss 1277.2598876953125\n",
      "epoch 44, val_loss 454.9226989746094\n",
      "epoch 45, train_loss 1277.2598876953125\n",
      "epoch 45, val_loss 454.92266845703125\n",
      "epoch 46, train_loss 1277.2598876953125\n",
      "epoch 46, val_loss 454.9225769042969\n",
      "epoch 47, train_loss 1277.259765625\n",
      "epoch 47, val_loss 454.92254638671875\n",
      "epoch 48, train_loss 1277.2598876953125\n",
      "epoch 48, val_loss 454.9224853515625\n",
      "epoch 49, train_loss 1277.2596435546875\n",
      "epoch 49, val_loss 454.92242431640625\n",
      "epoch 50, train_loss 1277.259521484375\n",
      "epoch 50, val_loss 454.92242431640625\n",
      "epoch 51, train_loss 1277.259521484375\n",
      "epoch 51, val_loss 454.92236328125\n",
      "epoch 52, train_loss 1277.259521484375\n",
      "epoch 52, val_loss 454.9223327636719\n",
      "epoch 53, train_loss 1277.25927734375\n",
      "epoch 53, val_loss 454.9222106933594\n",
      "epoch 54, train_loss 1277.25927734375\n",
      "epoch 54, val_loss 454.92218017578125\n",
      "epoch 55, train_loss 1277.25927734375\n",
      "epoch 55, val_loss 454.9220275878906\n",
      "epoch 56, train_loss 1277.25927734375\n",
      "epoch 56, val_loss 454.9220886230469\n",
      "epoch 57, train_loss 1277.2591552734375\n",
      "epoch 57, val_loss 454.9219970703125\n",
      "epoch 58, train_loss 1277.259033203125\n",
      "epoch 58, val_loss 454.9219055175781\n",
      "epoch 59, train_loss 1277.2589111328125\n",
      "epoch 59, val_loss 454.9219055175781\n",
      "epoch 60, train_loss 1277.2589111328125\n",
      "epoch 60, val_loss 454.9217834472656\n",
      "epoch 61, train_loss 1277.2587890625\n",
      "epoch 61, val_loss 454.9217529296875\n",
      "epoch 62, train_loss 1277.2589111328125\n",
      "epoch 62, val_loss 454.9217224121094\n",
      "epoch 63, train_loss 1277.2587890625\n",
      "epoch 63, val_loss 454.9216613769531\n",
      "epoch 64, train_loss 1277.2587890625\n",
      "epoch 64, val_loss 454.92156982421875\n",
      "epoch 65, train_loss 1277.258544921875\n",
      "epoch 65, val_loss 454.92156982421875\n",
      "epoch 66, train_loss 1277.258544921875\n",
      "epoch 66, val_loss 454.9215087890625\n",
      "epoch 67, train_loss 1277.2586669921875\n",
      "epoch 67, val_loss 454.9214172363281\n",
      "epoch 68, train_loss 1277.258544921875\n",
      "epoch 68, val_loss 454.9214172363281\n",
      "epoch 69, train_loss 1277.25830078125\n",
      "epoch 69, val_loss 454.9212951660156\n",
      "epoch 70, train_loss 1277.25830078125\n",
      "epoch 70, val_loss 454.9212951660156\n",
      "epoch 71, train_loss 1277.25830078125\n",
      "epoch 71, val_loss 454.9212646484375\n",
      "epoch 72, train_loss 1277.25830078125\n",
      "epoch 72, val_loss 454.9211730957031\n",
      "epoch 73, train_loss 1277.25830078125\n",
      "epoch 73, val_loss 454.9211120605469\n",
      "epoch 74, train_loss 1277.2581787109375\n",
      "epoch 74, val_loss 454.9210510253906\n",
      "epoch 75, train_loss 1277.258056640625\n",
      "epoch 75, val_loss 454.9210510253906\n",
      "epoch 76, train_loss 1277.258056640625\n",
      "epoch 76, val_loss 454.9208679199219\n",
      "epoch 77, train_loss 1277.2579345703125\n",
      "epoch 77, val_loss 454.9208679199219\n",
      "epoch 78, train_loss 1277.2578125\n",
      "epoch 78, val_loss 454.9208679199219\n",
      "epoch 79, train_loss 1277.2578125\n",
      "epoch 79, val_loss 454.9207458496094\n",
      "epoch 80, train_loss 1277.2576904296875\n",
      "epoch 80, val_loss 454.92071533203125\n",
      "epoch 81, train_loss 1277.257568359375\n",
      "epoch 81, val_loss 454.920654296875\n",
      "epoch 82, train_loss 1277.2576904296875\n",
      "epoch 82, val_loss 454.9206237792969\n",
      "epoch 83, train_loss 1277.257568359375\n",
      "epoch 83, val_loss 454.92059326171875\n",
      "epoch 84, train_loss 1277.257568359375\n",
      "epoch 84, val_loss 454.92047119140625\n",
      "epoch 85, train_loss 1277.25732421875\n",
      "epoch 85, val_loss 454.92041015625\n",
      "epoch 86, train_loss 1277.2574462890625\n",
      "epoch 86, val_loss 454.9203186035156\n",
      "epoch 87, train_loss 1277.2574462890625\n",
      "epoch 87, val_loss 454.9203796386719\n",
      "epoch 88, train_loss 1277.25732421875\n",
      "epoch 88, val_loss 454.9202575683594\n",
      "epoch 89, train_loss 1277.2572021484375\n",
      "epoch 89, val_loss 454.9201965332031\n",
      "epoch 90, train_loss 1277.2572021484375\n",
      "epoch 90, val_loss 454.9201965332031\n",
      "epoch 91, train_loss 1277.2569580078125\n",
      "epoch 91, val_loss 454.920166015625\n",
      "epoch 92, train_loss 1277.2569580078125\n",
      "epoch 92, val_loss 454.9200134277344\n",
      "epoch 93, train_loss 1277.257080078125\n",
      "epoch 93, val_loss 454.9200134277344\n",
      "epoch 94, train_loss 1277.2569580078125\n",
      "epoch 94, val_loss 454.919921875\n",
      "epoch 95, train_loss 1277.2568359375\n",
      "epoch 95, val_loss 454.9198303222656\n",
      "epoch 96, train_loss 1277.2567138671875\n",
      "epoch 96, val_loss 454.9198303222656\n",
      "epoch 97, train_loss 1277.256591796875\n",
      "epoch 97, val_loss 454.9197998046875\n",
      "epoch 98, train_loss 1277.256591796875\n",
      "epoch 98, val_loss 454.9197082519531\n",
      "epoch 99, train_loss 1277.2567138671875\n",
      "epoch 99, val_loss 454.919677734375\n",
      "Parameter containing:\n",
      "tensor([1.1575e-21], requires_grad=True)\n",
      "iter 112, train_loss_regularization 0.7343372702598572\n",
      "iter 112, val_loss_regularization 0.7343372702598572\n",
      "epoch 0, train_loss 1277.2567138671875\n",
      "epoch 0, val_loss 454.9195861816406\n",
      "epoch 1, train_loss 1277.2564697265625\n",
      "epoch 1, val_loss 454.9195556640625\n",
      "epoch 2, train_loss 1277.2564697265625\n",
      "epoch 2, val_loss 454.91949462890625\n",
      "epoch 3, train_loss 1277.2564697265625\n",
      "epoch 3, val_loss 454.9194641113281\n",
      "epoch 4, train_loss 1277.25634765625\n",
      "epoch 4, val_loss 454.9194030761719\n",
      "epoch 5, train_loss 1277.25634765625\n",
      "epoch 5, val_loss 454.9192810058594\n",
      "epoch 6, train_loss 1277.2562255859375\n",
      "epoch 6, val_loss 454.9191589355469\n",
      "epoch 7, train_loss 1277.2559814453125\n",
      "epoch 7, val_loss 454.9191589355469\n",
      "epoch 8, train_loss 1277.2559814453125\n",
      "epoch 8, val_loss 454.9191589355469\n",
      "epoch 9, train_loss 1277.2559814453125\n",
      "epoch 9, val_loss 454.9190979003906\n",
      "epoch 10, train_loss 1277.2559814453125\n",
      "epoch 10, val_loss 454.9190368652344\n",
      "epoch 11, train_loss 1277.256103515625\n",
      "epoch 11, val_loss 454.9190368652344\n",
      "epoch 12, train_loss 1277.2559814453125\n",
      "epoch 12, val_loss 454.9189453125\n",
      "epoch 13, train_loss 1277.2557373046875\n",
      "epoch 13, val_loss 454.9189147949219\n",
      "epoch 14, train_loss 1277.255615234375\n",
      "epoch 14, val_loss 454.9187927246094\n",
      "epoch 15, train_loss 1277.2557373046875\n",
      "epoch 15, val_loss 454.918701171875\n",
      "epoch 16, train_loss 1277.2557373046875\n",
      "epoch 16, val_loss 454.918701171875\n",
      "epoch 17, train_loss 1277.255615234375\n",
      "epoch 17, val_loss 454.91864013671875\n",
      "epoch 18, train_loss 1277.25537109375\n",
      "epoch 18, val_loss 454.91864013671875\n",
      "epoch 19, train_loss 1277.25537109375\n",
      "epoch 19, val_loss 454.9184875488281\n",
      "epoch 20, train_loss 1277.25537109375\n",
      "epoch 20, val_loss 454.9184875488281\n",
      "epoch 21, train_loss 1277.2554931640625\n",
      "epoch 21, val_loss 454.91845703125\n",
      "epoch 22, train_loss 1277.25537109375\n",
      "epoch 22, val_loss 454.9183654785156\n",
      "epoch 23, train_loss 1277.255126953125\n",
      "epoch 23, val_loss 454.9183044433594\n",
      "epoch 24, train_loss 1277.255126953125\n",
      "epoch 24, val_loss 454.9183349609375\n",
      "epoch 25, train_loss 1277.255126953125\n",
      "epoch 25, val_loss 454.91815185546875\n",
      "epoch 26, train_loss 1277.255126953125\n",
      "epoch 26, val_loss 454.91815185546875\n",
      "epoch 27, train_loss 1277.2550048828125\n",
      "epoch 27, val_loss 454.9181213378906\n",
      "epoch 28, train_loss 1277.2550048828125\n",
      "epoch 28, val_loss 454.91802978515625\n",
      "epoch 29, train_loss 1277.2548828125\n",
      "epoch 29, val_loss 454.9179992675781\n",
      "epoch 30, train_loss 1277.2547607421875\n",
      "epoch 30, val_loss 454.91790771484375\n",
      "epoch 31, train_loss 1277.2548828125\n",
      "epoch 31, val_loss 454.9178771972656\n",
      "epoch 32, train_loss 1277.2547607421875\n",
      "epoch 32, val_loss 454.91778564453125\n",
      "epoch 33, train_loss 1277.254638671875\n",
      "epoch 33, val_loss 454.91778564453125\n",
      "epoch 34, train_loss 1277.2545166015625\n",
      "epoch 34, val_loss 454.9177551269531\n",
      "epoch 35, train_loss 1277.2545166015625\n",
      "epoch 35, val_loss 454.91766357421875\n",
      "epoch 36, train_loss 1277.25439453125\n",
      "epoch 36, val_loss 454.9176330566406\n",
      "epoch 37, train_loss 1277.2542724609375\n",
      "epoch 37, val_loss 454.9175720214844\n",
      "epoch 38, train_loss 1277.2542724609375\n",
      "epoch 38, val_loss 454.9175109863281\n",
      "epoch 39, train_loss 1277.2542724609375\n",
      "epoch 39, val_loss 454.91741943359375\n",
      "epoch 40, train_loss 1277.2542724609375\n",
      "epoch 40, val_loss 454.9173278808594\n",
      "epoch 41, train_loss 1277.2542724609375\n",
      "epoch 41, val_loss 454.9173278808594\n",
      "epoch 42, train_loss 1277.254150390625\n",
      "epoch 42, val_loss 454.91729736328125\n",
      "epoch 43, train_loss 1277.25390625\n",
      "epoch 43, val_loss 454.9172058105469\n",
      "epoch 44, train_loss 1277.2540283203125\n",
      "epoch 44, val_loss 454.9172058105469\n",
      "epoch 45, train_loss 1277.2537841796875\n",
      "epoch 45, val_loss 454.91717529296875\n",
      "epoch 46, train_loss 1277.2537841796875\n",
      "epoch 46, val_loss 454.9169616699219\n",
      "epoch 47, train_loss 1277.2537841796875\n",
      "epoch 47, val_loss 454.9169921875\n",
      "epoch 48, train_loss 1277.253662109375\n",
      "epoch 48, val_loss 454.91693115234375\n",
      "epoch 49, train_loss 1277.253662109375\n",
      "epoch 49, val_loss 454.9168395996094\n",
      "epoch 50, train_loss 1277.2535400390625\n",
      "epoch 50, val_loss 454.9168701171875\n",
      "epoch 51, train_loss 1277.253662109375\n",
      "epoch 51, val_loss 454.9168395996094\n",
      "epoch 52, train_loss 1277.25341796875\n",
      "epoch 52, val_loss 454.9167175292969\n",
      "epoch 53, train_loss 1277.2535400390625\n",
      "epoch 53, val_loss 454.9166564941406\n",
      "epoch 54, train_loss 1277.2532958984375\n",
      "epoch 54, val_loss 454.9166259765625\n",
      "epoch 55, train_loss 1277.2532958984375\n",
      "epoch 55, val_loss 454.9165954589844\n",
      "epoch 56, train_loss 1277.2532958984375\n",
      "epoch 56, val_loss 454.9164733886719\n",
      "epoch 57, train_loss 1277.2532958984375\n",
      "epoch 57, val_loss 454.9164733886719\n",
      "epoch 58, train_loss 1277.253173828125\n",
      "epoch 58, val_loss 454.9163818359375\n",
      "epoch 59, train_loss 1277.2530517578125\n",
      "epoch 59, val_loss 454.91632080078125\n",
      "epoch 60, train_loss 1277.2529296875\n",
      "epoch 60, val_loss 454.9162902832031\n",
      "epoch 61, train_loss 1277.2530517578125\n",
      "epoch 61, val_loss 454.9161682128906\n",
      "epoch 62, train_loss 1277.2528076171875\n",
      "epoch 62, val_loss 454.9161682128906\n",
      "epoch 63, train_loss 1277.2528076171875\n",
      "epoch 63, val_loss 454.9161376953125\n",
      "epoch 64, train_loss 1277.2528076171875\n",
      "epoch 64, val_loss 454.9160461425781\n",
      "epoch 65, train_loss 1277.2528076171875\n",
      "epoch 65, val_loss 454.9160461425781\n",
      "epoch 66, train_loss 1277.2528076171875\n",
      "epoch 66, val_loss 454.9159851074219\n",
      "epoch 67, train_loss 1277.252685546875\n",
      "epoch 67, val_loss 454.9158630371094\n",
      "epoch 68, train_loss 1277.252685546875\n",
      "epoch 68, val_loss 454.91583251953125\n",
      "epoch 69, train_loss 1277.2525634765625\n",
      "epoch 69, val_loss 454.9157409667969\n",
      "epoch 70, train_loss 1277.2523193359375\n",
      "epoch 70, val_loss 454.91571044921875\n",
      "epoch 71, train_loss 1277.25244140625\n",
      "epoch 71, val_loss 454.9156799316406\n",
      "epoch 72, train_loss 1277.2523193359375\n",
      "epoch 72, val_loss 454.9156188964844\n",
      "epoch 73, train_loss 1277.2523193359375\n",
      "epoch 73, val_loss 454.9154968261719\n",
      "epoch 74, train_loss 1277.2523193359375\n",
      "epoch 74, val_loss 454.91552734375\n",
      "epoch 75, train_loss 1277.2520751953125\n",
      "epoch 75, val_loss 454.9154968261719\n",
      "epoch 76, train_loss 1277.2520751953125\n",
      "epoch 76, val_loss 454.91534423828125\n",
      "epoch 77, train_loss 1277.252197265625\n",
      "epoch 77, val_loss 454.915283203125\n",
      "epoch 78, train_loss 1277.251953125\n",
      "epoch 78, val_loss 454.915283203125\n",
      "epoch 79, train_loss 1277.251953125\n",
      "epoch 79, val_loss 454.91522216796875\n",
      "epoch 80, train_loss 1277.2518310546875\n",
      "epoch 80, val_loss 454.9151611328125\n",
      "epoch 81, train_loss 1277.2518310546875\n",
      "epoch 81, val_loss 454.9151306152344\n",
      "epoch 82, train_loss 1277.251708984375\n",
      "epoch 82, val_loss 454.9150390625\n",
      "epoch 83, train_loss 1277.2518310546875\n",
      "epoch 83, val_loss 454.9150390625\n",
      "epoch 84, train_loss 1277.251708984375\n",
      "epoch 84, val_loss 454.9149475097656\n",
      "epoch 85, train_loss 1277.251708984375\n",
      "epoch 85, val_loss 454.9149169921875\n",
      "epoch 86, train_loss 1277.2515869140625\n",
      "epoch 86, val_loss 454.9148254394531\n",
      "epoch 87, train_loss 1277.25146484375\n",
      "epoch 87, val_loss 454.9147644042969\n",
      "epoch 88, train_loss 1277.25146484375\n",
      "epoch 88, val_loss 454.914794921875\n",
      "epoch 89, train_loss 1277.25146484375\n",
      "epoch 89, val_loss 454.9146728515625\n",
      "epoch 90, train_loss 1277.2510986328125\n",
      "epoch 90, val_loss 454.9146728515625\n",
      "epoch 91, train_loss 1277.2510986328125\n",
      "epoch 91, val_loss 454.9145812988281\n",
      "epoch 92, train_loss 1277.2510986328125\n",
      "epoch 92, val_loss 454.9144592285156\n",
      "epoch 93, train_loss 1277.2510986328125\n",
      "epoch 93, val_loss 454.9144592285156\n",
      "epoch 94, train_loss 1277.251220703125\n",
      "epoch 94, val_loss 454.9144287109375\n",
      "epoch 95, train_loss 1277.251220703125\n",
      "epoch 95, val_loss 454.91436767578125\n",
      "epoch 96, train_loss 1277.2509765625\n",
      "epoch 96, val_loss 454.9143371582031\n",
      "epoch 97, train_loss 1277.2509765625\n",
      "epoch 97, val_loss 454.91424560546875\n",
      "epoch 98, train_loss 1277.2509765625\n",
      "epoch 98, val_loss 454.9141540527344\n",
      "epoch 99, train_loss 1277.250732421875\n",
      "epoch 99, val_loss 454.91412353515625\n",
      "Parameter containing:\n",
      "tensor([7.9713e-22], requires_grad=True)\n",
      "iter 113, train_loss_regularization 0.7328558564186096\n",
      "iter 113, val_loss_regularization 0.7328558564186096\n",
      "epoch 0, train_loss 1277.2506103515625\n",
      "epoch 0, val_loss 454.9140319824219\n",
      "epoch 1, train_loss 1277.2506103515625\n",
      "epoch 1, val_loss 454.91400146484375\n",
      "epoch 2, train_loss 1277.2503662109375\n",
      "epoch 2, val_loss 454.91400146484375\n",
      "epoch 3, train_loss 1277.2503662109375\n",
      "epoch 3, val_loss 454.9139099121094\n",
      "epoch 4, train_loss 1277.2503662109375\n",
      "epoch 4, val_loss 454.9137878417969\n",
      "epoch 5, train_loss 1277.2503662109375\n",
      "epoch 5, val_loss 454.913818359375\n",
      "epoch 6, train_loss 1277.2503662109375\n",
      "epoch 6, val_loss 454.91375732421875\n",
      "epoch 7, train_loss 1277.2503662109375\n",
      "epoch 7, val_loss 454.9136657714844\n",
      "epoch 8, train_loss 1277.250244140625\n",
      "epoch 8, val_loss 454.91357421875\n",
      "epoch 9, train_loss 1277.2501220703125\n",
      "epoch 9, val_loss 454.91357421875\n",
      "epoch 10, train_loss 1277.250244140625\n",
      "epoch 10, val_loss 454.91351318359375\n",
      "epoch 11, train_loss 1277.2501220703125\n",
      "epoch 11, val_loss 454.9134216308594\n",
      "epoch 12, train_loss 1277.2498779296875\n",
      "epoch 12, val_loss 454.9134521484375\n",
      "epoch 13, train_loss 1277.2498779296875\n",
      "epoch 13, val_loss 454.9133605957031\n",
      "epoch 14, train_loss 1277.2498779296875\n",
      "epoch 14, val_loss 454.913330078125\n",
      "epoch 15, train_loss 1277.249755859375\n",
      "epoch 15, val_loss 454.9132385253906\n",
      "epoch 16, train_loss 1277.2498779296875\n",
      "epoch 16, val_loss 454.9131774902344\n",
      "epoch 17, train_loss 1277.2498779296875\n",
      "epoch 17, val_loss 454.9131774902344\n",
      "epoch 18, train_loss 1277.24951171875\n",
      "epoch 18, val_loss 454.9130859375\n",
      "epoch 19, train_loss 1277.2496337890625\n",
      "epoch 19, val_loss 454.9129943847656\n",
      "epoch 20, train_loss 1277.2496337890625\n",
      "epoch 20, val_loss 454.9129638671875\n",
      "epoch 21, train_loss 1277.24951171875\n",
      "epoch 21, val_loss 454.9129638671875\n",
      "epoch 22, train_loss 1277.24951171875\n",
      "epoch 22, val_loss 454.912841796875\n",
      "epoch 23, train_loss 1277.2493896484375\n",
      "epoch 23, val_loss 454.9127502441406\n",
      "epoch 24, train_loss 1277.2493896484375\n",
      "epoch 24, val_loss 454.9127502441406\n",
      "epoch 25, train_loss 1277.2493896484375\n",
      "epoch 25, val_loss 454.9127502441406\n",
      "epoch 26, train_loss 1277.2493896484375\n",
      "epoch 26, val_loss 454.91265869140625\n",
      "epoch 27, train_loss 1277.2490234375\n",
      "epoch 27, val_loss 454.9126281738281\n",
      "epoch 28, train_loss 1277.2490234375\n",
      "epoch 28, val_loss 454.91259765625\n",
      "epoch 29, train_loss 1277.2489013671875\n",
      "epoch 29, val_loss 454.9125061035156\n",
      "epoch 30, train_loss 1277.2489013671875\n",
      "epoch 30, val_loss 454.91241455078125\n",
      "epoch 31, train_loss 1277.2489013671875\n",
      "epoch 31, val_loss 454.9123840332031\n",
      "epoch 32, train_loss 1277.248779296875\n",
      "epoch 32, val_loss 454.91229248046875\n",
      "epoch 33, train_loss 1277.248779296875\n",
      "epoch 33, val_loss 454.9122619628906\n",
      "epoch 34, train_loss 1277.248779296875\n",
      "epoch 34, val_loss 454.9122009277344\n",
      "epoch 35, train_loss 1277.2486572265625\n",
      "epoch 35, val_loss 454.91217041015625\n",
      "epoch 36, train_loss 1277.2486572265625\n",
      "epoch 36, val_loss 454.9120788574219\n",
      "epoch 37, train_loss 1277.24853515625\n",
      "epoch 37, val_loss 454.9120788574219\n",
      "epoch 38, train_loss 1277.24853515625\n",
      "epoch 38, val_loss 454.9119873046875\n",
      "epoch 39, train_loss 1277.24853515625\n",
      "epoch 39, val_loss 454.91192626953125\n",
      "epoch 40, train_loss 1277.248291015625\n",
      "epoch 40, val_loss 454.9118347167969\n",
      "epoch 41, train_loss 1277.248291015625\n",
      "epoch 41, val_loss 454.9118347167969\n",
      "epoch 42, train_loss 1277.248291015625\n",
      "epoch 42, val_loss 454.9117431640625\n",
      "epoch 43, train_loss 1277.248291015625\n",
      "epoch 43, val_loss 454.9117431640625\n",
      "epoch 44, train_loss 1277.248046875\n",
      "epoch 44, val_loss 454.9116516113281\n",
      "epoch 45, train_loss 1277.248046875\n",
      "epoch 45, val_loss 454.91162109375\n",
      "epoch 46, train_loss 1277.248046875\n",
      "epoch 46, val_loss 454.9115905761719\n",
      "epoch 47, train_loss 1277.248046875\n",
      "epoch 47, val_loss 454.9114685058594\n",
      "epoch 48, train_loss 1277.248046875\n",
      "epoch 48, val_loss 454.9114685058594\n",
      "epoch 49, train_loss 1277.2479248046875\n",
      "epoch 49, val_loss 454.9114074707031\n",
      "epoch 50, train_loss 1277.2479248046875\n",
      "epoch 50, val_loss 454.9112854003906\n",
      "epoch 51, train_loss 1277.2476806640625\n",
      "epoch 51, val_loss 454.9112854003906\n",
      "epoch 52, train_loss 1277.24755859375\n",
      "epoch 52, val_loss 454.9112548828125\n",
      "epoch 53, train_loss 1277.2476806640625\n",
      "epoch 53, val_loss 454.9111633300781\n",
      "epoch 54, train_loss 1277.2476806640625\n",
      "epoch 54, val_loss 454.91107177734375\n",
      "epoch 55, train_loss 1277.247802734375\n",
      "epoch 55, val_loss 454.9110412597656\n",
      "epoch 56, train_loss 1277.2474365234375\n",
      "epoch 56, val_loss 454.9110107421875\n",
      "epoch 57, train_loss 1277.247314453125\n",
      "epoch 57, val_loss 454.91094970703125\n",
      "epoch 58, train_loss 1277.2471923828125\n",
      "epoch 58, val_loss 454.9109191894531\n",
      "epoch 59, train_loss 1277.2471923828125\n",
      "epoch 59, val_loss 454.91082763671875\n",
      "epoch 60, train_loss 1277.2471923828125\n",
      "epoch 60, val_loss 454.9107360839844\n",
      "epoch 61, train_loss 1277.2471923828125\n",
      "epoch 61, val_loss 454.9107360839844\n",
      "epoch 62, train_loss 1277.2471923828125\n",
      "epoch 62, val_loss 454.9106750488281\n",
      "epoch 63, train_loss 1277.2470703125\n",
      "epoch 63, val_loss 454.9106140136719\n",
      "epoch 64, train_loss 1277.2469482421875\n",
      "epoch 64, val_loss 454.9105529785156\n",
      "epoch 65, train_loss 1277.246826171875\n",
      "epoch 65, val_loss 454.9105529785156\n",
      "epoch 66, train_loss 1277.2469482421875\n",
      "epoch 66, val_loss 454.9104309082031\n",
      "epoch 67, train_loss 1277.2469482421875\n",
      "epoch 67, val_loss 454.91046142578125\n",
      "epoch 68, train_loss 1277.2467041015625\n",
      "epoch 68, val_loss 454.9104309082031\n",
      "epoch 69, train_loss 1277.2467041015625\n",
      "epoch 69, val_loss 454.9102783203125\n",
      "epoch 70, train_loss 1277.2467041015625\n",
      "epoch 70, val_loss 454.9102783203125\n",
      "epoch 71, train_loss 1277.2467041015625\n",
      "epoch 71, val_loss 454.9101257324219\n",
      "epoch 72, train_loss 1277.2467041015625\n",
      "epoch 72, val_loss 454.91009521484375\n",
      "epoch 73, train_loss 1277.2464599609375\n",
      "epoch 73, val_loss 454.91009521484375\n",
      "epoch 74, train_loss 1277.2464599609375\n",
      "epoch 74, val_loss 454.9100341796875\n",
      "epoch 75, train_loss 1277.2464599609375\n",
      "epoch 75, val_loss 454.90997314453125\n",
      "epoch 76, train_loss 1277.2462158203125\n",
      "epoch 76, val_loss 454.909912109375\n",
      "epoch 77, train_loss 1277.2462158203125\n",
      "epoch 77, val_loss 454.909912109375\n",
      "epoch 78, train_loss 1277.2462158203125\n",
      "epoch 78, val_loss 454.9097900390625\n",
      "epoch 79, train_loss 1277.2462158203125\n",
      "epoch 79, val_loss 454.9097595214844\n",
      "epoch 80, train_loss 1277.24609375\n",
      "epoch 80, val_loss 454.9097595214844\n",
      "epoch 81, train_loss 1277.2459716796875\n",
      "epoch 81, val_loss 454.9095764160156\n",
      "epoch 82, train_loss 1277.2459716796875\n",
      "epoch 82, val_loss 454.9095764160156\n",
      "epoch 83, train_loss 1277.2459716796875\n",
      "epoch 83, val_loss 454.9095458984375\n",
      "epoch 84, train_loss 1277.245849609375\n",
      "epoch 84, val_loss 454.90948486328125\n",
      "epoch 85, train_loss 1277.2457275390625\n",
      "epoch 85, val_loss 454.90936279296875\n",
      "epoch 86, train_loss 1277.2457275390625\n",
      "epoch 86, val_loss 454.9093322753906\n",
      "epoch 87, train_loss 1277.2457275390625\n",
      "epoch 87, val_loss 454.9093017578125\n",
      "epoch 88, train_loss 1277.2457275390625\n",
      "epoch 88, val_loss 454.9093017578125\n",
      "epoch 89, train_loss 1277.24560546875\n",
      "epoch 89, val_loss 454.90924072265625\n",
      "epoch 90, train_loss 1277.2454833984375\n",
      "epoch 90, val_loss 454.9091796875\n",
      "epoch 91, train_loss 1277.24560546875\n",
      "epoch 91, val_loss 454.9090881347656\n",
      "epoch 92, train_loss 1277.245361328125\n",
      "epoch 92, val_loss 454.9090270996094\n",
      "epoch 93, train_loss 1277.2451171875\n",
      "epoch 93, val_loss 454.9089050292969\n",
      "epoch 94, train_loss 1277.2452392578125\n",
      "epoch 94, val_loss 454.90887451171875\n",
      "epoch 95, train_loss 1277.2451171875\n",
      "epoch 95, val_loss 454.90887451171875\n",
      "epoch 96, train_loss 1277.2451171875\n",
      "epoch 96, val_loss 454.90887451171875\n",
      "epoch 97, train_loss 1277.2451171875\n",
      "epoch 97, val_loss 454.90875244140625\n",
      "epoch 98, train_loss 1277.2449951171875\n",
      "epoch 98, val_loss 454.9087219238281\n",
      "epoch 99, train_loss 1277.244873046875\n",
      "epoch 99, val_loss 454.9087219238281\n",
      "Parameter containing:\n",
      "tensor([5.4919e-22], requires_grad=True)\n",
      "iter 114, train_loss_regularization 0.7314047813415527\n",
      "iter 114, val_loss_regularization 0.7314047813415527\n",
      "epoch 0, train_loss 1277.244873046875\n",
      "epoch 0, val_loss 454.90863037109375\n",
      "epoch 1, train_loss 1277.2449951171875\n",
      "epoch 1, val_loss 454.9085693359375\n",
      "epoch 2, train_loss 1277.244873046875\n",
      "epoch 2, val_loss 454.90850830078125\n",
      "epoch 3, train_loss 1277.2447509765625\n",
      "epoch 3, val_loss 454.9084167480469\n",
      "epoch 4, train_loss 1277.2447509765625\n",
      "epoch 4, val_loss 454.9084167480469\n",
      "epoch 5, train_loss 1277.24462890625\n",
      "epoch 5, val_loss 454.9083251953125\n",
      "epoch 6, train_loss 1277.24462890625\n",
      "epoch 6, val_loss 454.90826416015625\n",
      "epoch 7, train_loss 1277.2445068359375\n",
      "epoch 7, val_loss 454.90826416015625\n",
      "epoch 8, train_loss 1277.2445068359375\n",
      "epoch 8, val_loss 454.908203125\n",
      "epoch 9, train_loss 1277.244384765625\n",
      "epoch 9, val_loss 454.9080810546875\n",
      "epoch 10, train_loss 1277.244384765625\n",
      "epoch 10, val_loss 454.9080810546875\n",
      "epoch 11, train_loss 1277.244384765625\n",
      "epoch 11, val_loss 454.9080505371094\n",
      "epoch 12, train_loss 1277.2442626953125\n",
      "epoch 12, val_loss 454.907958984375\n",
      "epoch 13, train_loss 1277.244140625\n",
      "epoch 13, val_loss 454.9078674316406\n",
      "epoch 14, train_loss 1277.243896484375\n",
      "epoch 14, val_loss 454.9078369140625\n",
      "epoch 15, train_loss 1277.2440185546875\n",
      "epoch 15, val_loss 454.9078063964844\n",
      "epoch 16, train_loss 1277.244140625\n",
      "epoch 16, val_loss 454.9078063964844\n",
      "epoch 17, train_loss 1277.2440185546875\n",
      "epoch 17, val_loss 454.90765380859375\n",
      "epoch 18, train_loss 1277.2437744140625\n",
      "epoch 18, val_loss 454.9076232910156\n",
      "epoch 19, train_loss 1277.2437744140625\n",
      "epoch 19, val_loss 454.9075927734375\n",
      "epoch 20, train_loss 1277.2437744140625\n",
      "epoch 20, val_loss 454.9075012207031\n",
      "epoch 21, train_loss 1277.2437744140625\n",
      "epoch 21, val_loss 454.9075012207031\n",
      "epoch 22, train_loss 1277.2437744140625\n",
      "epoch 22, val_loss 454.90740966796875\n",
      "epoch 23, train_loss 1277.2435302734375\n",
      "epoch 23, val_loss 454.9073181152344\n",
      "epoch 24, train_loss 1277.2435302734375\n",
      "epoch 24, val_loss 454.9073181152344\n",
      "epoch 25, train_loss 1277.243408203125\n",
      "epoch 25, val_loss 454.9072570800781\n",
      "epoch 26, train_loss 1277.243408203125\n",
      "epoch 26, val_loss 454.90716552734375\n",
      "epoch 27, train_loss 1277.243408203125\n",
      "epoch 27, val_loss 454.90716552734375\n",
      "epoch 28, train_loss 1277.2435302734375\n",
      "epoch 28, val_loss 454.9070739746094\n",
      "epoch 29, train_loss 1277.243408203125\n",
      "epoch 29, val_loss 454.90704345703125\n",
      "epoch 30, train_loss 1277.2431640625\n",
      "epoch 30, val_loss 454.9070129394531\n",
      "epoch 31, train_loss 1277.2431640625\n",
      "epoch 31, val_loss 454.9069519042969\n",
      "epoch 32, train_loss 1277.2430419921875\n",
      "epoch 32, val_loss 454.9068603515625\n",
      "epoch 33, train_loss 1277.2430419921875\n",
      "epoch 33, val_loss 454.9068298339844\n",
      "epoch 34, train_loss 1277.2430419921875\n",
      "epoch 34, val_loss 454.9067077636719\n",
      "epoch 35, train_loss 1277.2427978515625\n",
      "epoch 35, val_loss 454.9067077636719\n",
      "epoch 36, train_loss 1277.2427978515625\n",
      "epoch 36, val_loss 454.9066162109375\n",
      "epoch 37, train_loss 1277.2427978515625\n",
      "epoch 37, val_loss 454.9066162109375\n",
      "epoch 38, train_loss 1277.2427978515625\n",
      "epoch 38, val_loss 454.9065856933594\n",
      "epoch 39, train_loss 1277.2427978515625\n",
      "epoch 39, val_loss 454.90655517578125\n",
      "epoch 40, train_loss 1277.2427978515625\n",
      "epoch 40, val_loss 454.9064636230469\n",
      "epoch 41, train_loss 1277.2425537109375\n",
      "epoch 41, val_loss 454.9063720703125\n",
      "epoch 42, train_loss 1277.2425537109375\n",
      "epoch 42, val_loss 454.9063720703125\n",
      "epoch 43, train_loss 1277.2425537109375\n",
      "epoch 43, val_loss 454.9062805175781\n",
      "epoch 44, train_loss 1277.2425537109375\n",
      "epoch 44, val_loss 454.9061584472656\n",
      "epoch 45, train_loss 1277.2425537109375\n",
      "epoch 45, val_loss 454.9061584472656\n",
      "epoch 46, train_loss 1277.2423095703125\n",
      "epoch 46, val_loss 454.9060974121094\n",
      "epoch 47, train_loss 1277.2420654296875\n",
      "epoch 47, val_loss 454.9060974121094\n",
      "epoch 48, train_loss 1277.2420654296875\n",
      "epoch 48, val_loss 454.9060363769531\n",
      "epoch 49, train_loss 1277.2421875\n",
      "epoch 49, val_loss 454.9059143066406\n",
      "epoch 50, train_loss 1277.2421875\n",
      "epoch 50, val_loss 454.9058837890625\n",
      "epoch 51, train_loss 1277.241943359375\n",
      "epoch 51, val_loss 454.9058837890625\n",
      "epoch 52, train_loss 1277.2418212890625\n",
      "epoch 52, val_loss 454.9057922363281\n",
      "epoch 53, train_loss 1277.24169921875\n",
      "epoch 53, val_loss 454.90576171875\n",
      "epoch 54, train_loss 1277.24169921875\n",
      "epoch 54, val_loss 454.90570068359375\n",
      "epoch 55, train_loss 1277.24169921875\n",
      "epoch 55, val_loss 454.9056701660156\n",
      "epoch 56, train_loss 1277.2418212890625\n",
      "epoch 56, val_loss 454.90557861328125\n",
      "epoch 57, train_loss 1277.24169921875\n",
      "epoch 57, val_loss 454.9055480957031\n",
      "epoch 58, train_loss 1277.2415771484375\n",
      "epoch 58, val_loss 454.90545654296875\n",
      "epoch 59, train_loss 1277.24169921875\n",
      "epoch 59, val_loss 454.9054260253906\n",
      "epoch 60, train_loss 1277.241455078125\n",
      "epoch 60, val_loss 454.90533447265625\n",
      "epoch 61, train_loss 1277.2413330078125\n",
      "epoch 61, val_loss 454.90533447265625\n",
      "epoch 62, train_loss 1277.2413330078125\n",
      "epoch 62, val_loss 454.9053039550781\n",
      "epoch 63, train_loss 1277.2412109375\n",
      "epoch 63, val_loss 454.9052429199219\n",
      "epoch 64, train_loss 1277.2412109375\n",
      "epoch 64, val_loss 454.9051818847656\n",
      "epoch 65, train_loss 1277.2412109375\n",
      "epoch 65, val_loss 454.90509033203125\n",
      "epoch 66, train_loss 1277.2412109375\n",
      "epoch 66, val_loss 454.9049987792969\n",
      "epoch 67, train_loss 1277.2410888671875\n",
      "epoch 67, val_loss 454.9049987792969\n",
      "epoch 68, train_loss 1277.2410888671875\n",
      "epoch 68, val_loss 454.9048767089844\n",
      "epoch 69, train_loss 1277.240966796875\n",
      "epoch 69, val_loss 454.9048767089844\n",
      "epoch 70, train_loss 1277.240966796875\n",
      "epoch 70, val_loss 454.90484619140625\n",
      "epoch 71, train_loss 1277.240966796875\n",
      "epoch 71, val_loss 454.90478515625\n",
      "epoch 72, train_loss 1277.24072265625\n",
      "epoch 72, val_loss 454.9046936035156\n",
      "epoch 73, train_loss 1277.2408447265625\n",
      "epoch 73, val_loss 454.9046630859375\n",
      "epoch 74, train_loss 1277.24072265625\n",
      "epoch 74, val_loss 454.9045715332031\n",
      "epoch 75, train_loss 1277.24072265625\n",
      "epoch 75, val_loss 454.904541015625\n",
      "epoch 76, train_loss 1277.2406005859375\n",
      "epoch 76, val_loss 454.9045715332031\n",
      "epoch 77, train_loss 1277.2406005859375\n",
      "epoch 77, val_loss 454.9044189453125\n",
      "epoch 78, train_loss 1277.2406005859375\n",
      "epoch 78, val_loss 454.9044189453125\n",
      "epoch 79, train_loss 1277.240478515625\n",
      "epoch 79, val_loss 454.9043273925781\n",
      "epoch 80, train_loss 1277.2403564453125\n",
      "epoch 80, val_loss 454.90423583984375\n",
      "epoch 81, train_loss 1277.240234375\n",
      "epoch 81, val_loss 454.9042053222656\n",
      "epoch 82, train_loss 1277.240234375\n",
      "epoch 82, val_loss 454.9041748046875\n",
      "epoch 83, train_loss 1277.2401123046875\n",
      "epoch 83, val_loss 454.9041748046875\n",
      "epoch 84, train_loss 1277.2401123046875\n",
      "epoch 84, val_loss 454.9040832519531\n",
      "epoch 85, train_loss 1277.2401123046875\n",
      "epoch 85, val_loss 454.904052734375\n",
      "epoch 86, train_loss 1277.239990234375\n",
      "epoch 86, val_loss 454.904052734375\n",
      "epoch 87, train_loss 1277.239990234375\n",
      "epoch 87, val_loss 454.9039306640625\n",
      "epoch 88, train_loss 1277.2398681640625\n",
      "epoch 88, val_loss 454.9038391113281\n",
      "epoch 89, train_loss 1277.2398681640625\n",
      "epoch 89, val_loss 454.9038391113281\n",
      "epoch 90, train_loss 1277.2398681640625\n",
      "epoch 90, val_loss 454.9037170410156\n",
      "epoch 91, train_loss 1277.23974609375\n",
      "epoch 91, val_loss 454.9037170410156\n",
      "epoch 92, train_loss 1277.2396240234375\n",
      "epoch 92, val_loss 454.9036560058594\n",
      "epoch 93, train_loss 1277.2396240234375\n",
      "epoch 93, val_loss 454.9035949707031\n",
      "epoch 94, train_loss 1277.2396240234375\n",
      "epoch 94, val_loss 454.9035339355469\n",
      "epoch 95, train_loss 1277.2396240234375\n",
      "epoch 95, val_loss 454.90350341796875\n",
      "epoch 96, train_loss 1277.2393798828125\n",
      "epoch 96, val_loss 454.90338134765625\n",
      "epoch 97, train_loss 1277.239501953125\n",
      "epoch 97, val_loss 454.9034118652344\n",
      "epoch 98, train_loss 1277.2393798828125\n",
      "epoch 98, val_loss 454.9033203125\n",
      "epoch 99, train_loss 1277.2392578125\n",
      "epoch 99, val_loss 454.9031982421875\n",
      "Parameter containing:\n",
      "tensor([3.7855e-22], requires_grad=True)\n",
      "iter 115, train_loss_regularization 0.7299836277961731\n",
      "iter 115, val_loss_regularization 0.7299836277961731\n",
      "epoch 0, train_loss 1277.2391357421875\n",
      "epoch 0, val_loss 454.9031677246094\n",
      "epoch 1, train_loss 1277.2391357421875\n",
      "epoch 1, val_loss 454.9031677246094\n",
      "epoch 2, train_loss 1277.2391357421875\n",
      "epoch 2, val_loss 454.903076171875\n",
      "epoch 3, train_loss 1277.239013671875\n",
      "epoch 3, val_loss 454.903076171875\n",
      "epoch 4, train_loss 1277.239013671875\n",
      "epoch 4, val_loss 454.9029541015625\n",
      "epoch 5, train_loss 1277.2388916015625\n",
      "epoch 5, val_loss 454.9029541015625\n",
      "epoch 6, train_loss 1277.2388916015625\n",
      "epoch 6, val_loss 454.9028625488281\n",
      "epoch 7, train_loss 1277.2388916015625\n",
      "epoch 7, val_loss 454.9028625488281\n",
      "epoch 8, train_loss 1277.2388916015625\n",
      "epoch 8, val_loss 454.9027404785156\n",
      "epoch 9, train_loss 1277.238525390625\n",
      "epoch 9, val_loss 454.9027099609375\n",
      "epoch 10, train_loss 1277.2386474609375\n",
      "epoch 10, val_loss 454.9026794433594\n",
      "epoch 11, train_loss 1277.238525390625\n",
      "epoch 11, val_loss 454.9026794433594\n",
      "epoch 12, train_loss 1277.238525390625\n",
      "epoch 12, val_loss 454.90252685546875\n",
      "epoch 13, train_loss 1277.238525390625\n",
      "epoch 13, val_loss 454.9024963378906\n",
      "epoch 14, train_loss 1277.238525390625\n",
      "epoch 14, val_loss 454.9024658203125\n",
      "epoch 15, train_loss 1277.23828125\n",
      "epoch 15, val_loss 454.9023742675781\n",
      "epoch 16, train_loss 1277.2384033203125\n",
      "epoch 16, val_loss 454.90234375\n",
      "epoch 17, train_loss 1277.23828125\n",
      "epoch 17, val_loss 454.9023742675781\n",
      "epoch 18, train_loss 1277.23828125\n",
      "epoch 18, val_loss 454.90228271484375\n",
      "epoch 19, train_loss 1277.2381591796875\n",
      "epoch 19, val_loss 454.9022521972656\n",
      "epoch 20, train_loss 1277.2381591796875\n",
      "epoch 20, val_loss 454.9020690917969\n",
      "epoch 21, train_loss 1277.2379150390625\n",
      "epoch 21, val_loss 454.90203857421875\n",
      "epoch 22, train_loss 1277.2379150390625\n",
      "epoch 22, val_loss 454.9020080566406\n",
      "epoch 23, train_loss 1277.238037109375\n",
      "epoch 23, val_loss 454.9020080566406\n",
      "epoch 24, train_loss 1277.238037109375\n",
      "epoch 24, val_loss 454.90191650390625\n",
      "epoch 25, train_loss 1277.2379150390625\n",
      "epoch 25, val_loss 454.9018859863281\n",
      "epoch 26, train_loss 1277.2379150390625\n",
      "epoch 26, val_loss 454.9018859863281\n",
      "epoch 27, train_loss 1277.23779296875\n",
      "epoch 27, val_loss 454.9017639160156\n",
      "epoch 28, train_loss 1277.237548828125\n",
      "epoch 28, val_loss 454.9017028808594\n",
      "epoch 29, train_loss 1277.237548828125\n",
      "epoch 29, val_loss 454.9017028808594\n",
      "epoch 30, train_loss 1277.237548828125\n",
      "epoch 30, val_loss 454.90155029296875\n",
      "epoch 31, train_loss 1277.237548828125\n",
      "epoch 31, val_loss 454.90155029296875\n",
      "epoch 32, train_loss 1277.2374267578125\n",
      "epoch 32, val_loss 454.9014892578125\n",
      "epoch 33, train_loss 1277.2373046875\n",
      "epoch 33, val_loss 454.9014587402344\n",
      "epoch 34, train_loss 1277.2373046875\n",
      "epoch 34, val_loss 454.9013671875\n",
      "epoch 35, train_loss 1277.2371826171875\n",
      "epoch 35, val_loss 454.9013671875\n",
      "epoch 36, train_loss 1277.2373046875\n",
      "epoch 36, val_loss 454.9012451171875\n",
      "epoch 37, train_loss 1277.2373046875\n",
      "epoch 37, val_loss 454.9012145996094\n",
      "epoch 38, train_loss 1277.2371826171875\n",
      "epoch 38, val_loss 454.9012145996094\n",
      "epoch 39, train_loss 1277.237060546875\n",
      "epoch 39, val_loss 454.9011535644531\n",
      "epoch 40, train_loss 1277.237060546875\n",
      "epoch 40, val_loss 454.9010925292969\n",
      "epoch 41, train_loss 1277.237060546875\n",
      "epoch 41, val_loss 454.9010009765625\n",
      "epoch 42, train_loss 1277.2369384765625\n",
      "epoch 42, val_loss 454.9009704589844\n",
      "epoch 43, train_loss 1277.23681640625\n",
      "epoch 43, val_loss 454.9008483886719\n",
      "epoch 44, train_loss 1277.23681640625\n",
      "epoch 44, val_loss 454.9008483886719\n",
      "epoch 45, train_loss 1277.2366943359375\n",
      "epoch 45, val_loss 454.9007873535156\n",
      "epoch 46, train_loss 1277.236572265625\n",
      "epoch 46, val_loss 454.90069580078125\n",
      "epoch 47, train_loss 1277.2366943359375\n",
      "epoch 47, val_loss 454.90069580078125\n",
      "epoch 48, train_loss 1277.2364501953125\n",
      "epoch 48, val_loss 454.900634765625\n",
      "epoch 49, train_loss 1277.2364501953125\n",
      "epoch 49, val_loss 454.900634765625\n",
      "epoch 50, train_loss 1277.236328125\n",
      "epoch 50, val_loss 454.90057373046875\n",
      "epoch 51, train_loss 1277.236328125\n",
      "epoch 51, val_loss 454.9005126953125\n",
      "epoch 52, train_loss 1277.236328125\n",
      "epoch 52, val_loss 454.9003601074219\n",
      "epoch 53, train_loss 1277.236328125\n",
      "epoch 53, val_loss 454.9003601074219\n",
      "epoch 54, train_loss 1277.2362060546875\n",
      "epoch 54, val_loss 454.9002990722656\n",
      "epoch 55, train_loss 1277.236083984375\n",
      "epoch 55, val_loss 454.9002380371094\n",
      "epoch 56, train_loss 1277.2362060546875\n",
      "epoch 56, val_loss 454.90020751953125\n",
      "epoch 57, train_loss 1277.236083984375\n",
      "epoch 57, val_loss 454.9001770019531\n",
      "epoch 58, train_loss 1277.2359619140625\n",
      "epoch 58, val_loss 454.90008544921875\n",
      "epoch 59, train_loss 1277.2359619140625\n",
      "epoch 59, val_loss 454.9000549316406\n",
      "epoch 60, train_loss 1277.23583984375\n",
      "epoch 60, val_loss 454.8999938964844\n",
      "epoch 61, train_loss 1277.2357177734375\n",
      "epoch 61, val_loss 454.89996337890625\n",
      "epoch 62, train_loss 1277.2357177734375\n",
      "epoch 62, val_loss 454.8998718261719\n",
      "epoch 63, train_loss 1277.2357177734375\n",
      "epoch 63, val_loss 454.8997802734375\n",
      "epoch 64, train_loss 1277.2357177734375\n",
      "epoch 64, val_loss 454.8997802734375\n",
      "epoch 65, train_loss 1277.2357177734375\n",
      "epoch 65, val_loss 454.89971923828125\n",
      "epoch 66, train_loss 1277.2354736328125\n",
      "epoch 66, val_loss 454.89971923828125\n",
      "epoch 67, train_loss 1277.235595703125\n",
      "epoch 67, val_loss 454.8996276855469\n",
      "epoch 68, train_loss 1277.2354736328125\n",
      "epoch 68, val_loss 454.8995056152344\n",
      "epoch 69, train_loss 1277.2354736328125\n",
      "epoch 69, val_loss 454.8995361328125\n",
      "epoch 70, train_loss 1277.2353515625\n",
      "epoch 70, val_loss 454.8994445800781\n",
      "epoch 71, train_loss 1277.235107421875\n",
      "epoch 71, val_loss 454.8994140625\n",
      "epoch 72, train_loss 1277.235107421875\n",
      "epoch 72, val_loss 454.8993835449219\n",
      "epoch 73, train_loss 1277.235107421875\n",
      "epoch 73, val_loss 454.8993225097656\n",
      "epoch 74, train_loss 1277.2352294921875\n",
      "epoch 74, val_loss 454.8992614746094\n",
      "epoch 75, train_loss 1277.235107421875\n",
      "epoch 75, val_loss 454.8992004394531\n",
      "epoch 76, train_loss 1277.2349853515625\n",
      "epoch 76, val_loss 454.8991394042969\n",
      "epoch 77, train_loss 1277.23486328125\n",
      "epoch 77, val_loss 454.8990478515625\n",
      "epoch 78, train_loss 1277.23486328125\n",
      "epoch 78, val_loss 454.8990478515625\n",
      "epoch 79, train_loss 1277.23486328125\n",
      "epoch 79, val_loss 454.8990478515625\n",
      "epoch 80, train_loss 1277.2347412109375\n",
      "epoch 80, val_loss 454.8989562988281\n",
      "epoch 81, train_loss 1277.23486328125\n",
      "epoch 81, val_loss 454.89886474609375\n",
      "epoch 82, train_loss 1277.234619140625\n",
      "epoch 82, val_loss 454.8988342285156\n",
      "epoch 83, train_loss 1277.234619140625\n",
      "epoch 83, val_loss 454.89874267578125\n",
      "epoch 84, train_loss 1277.234375\n",
      "epoch 84, val_loss 454.898681640625\n",
      "epoch 85, train_loss 1277.2344970703125\n",
      "epoch 85, val_loss 454.898681640625\n",
      "epoch 86, train_loss 1277.234619140625\n",
      "epoch 86, val_loss 454.8985900878906\n",
      "epoch 87, train_loss 1277.2344970703125\n",
      "epoch 87, val_loss 454.8985290527344\n",
      "epoch 88, train_loss 1277.234130859375\n",
      "epoch 88, val_loss 454.89849853515625\n",
      "epoch 89, train_loss 1277.234130859375\n",
      "epoch 89, val_loss 454.8984680175781\n",
      "epoch 90, train_loss 1277.2342529296875\n",
      "epoch 90, val_loss 454.8984069824219\n",
      "epoch 91, train_loss 1277.2342529296875\n",
      "epoch 91, val_loss 454.8984069824219\n",
      "epoch 92, train_loss 1277.234130859375\n",
      "epoch 92, val_loss 454.8982849121094\n",
      "epoch 93, train_loss 1277.234130859375\n",
      "epoch 93, val_loss 454.89825439453125\n",
      "epoch 94, train_loss 1277.2340087890625\n",
      "epoch 94, val_loss 454.8981628417969\n",
      "epoch 95, train_loss 1277.2340087890625\n",
      "epoch 95, val_loss 454.8980712890625\n",
      "epoch 96, train_loss 1277.23388671875\n",
      "epoch 96, val_loss 454.8980407714844\n",
      "epoch 97, train_loss 1277.23388671875\n",
      "epoch 97, val_loss 454.8980407714844\n",
      "epoch 98, train_loss 1277.2337646484375\n",
      "epoch 98, val_loss 454.89801025390625\n",
      "epoch 99, train_loss 1277.233642578125\n",
      "epoch 99, val_loss 454.89794921875\n",
      "Parameter containing:\n",
      "tensor([2.6104e-22], requires_grad=True)\n",
      "iter 116, train_loss_regularization 0.728592038154602\n",
      "iter 116, val_loss_regularization 0.728592038154602\n",
      "epoch 0, train_loss 1277.233642578125\n",
      "epoch 0, val_loss 454.89788818359375\n",
      "epoch 1, train_loss 1277.2333984375\n",
      "epoch 1, val_loss 454.8978271484375\n",
      "epoch 2, train_loss 1277.2333984375\n",
      "epoch 2, val_loss 454.8977355957031\n",
      "epoch 3, train_loss 1277.2335205078125\n",
      "epoch 3, val_loss 454.897705078125\n",
      "epoch 4, train_loss 1277.2333984375\n",
      "epoch 4, val_loss 454.8976745605469\n",
      "epoch 5, train_loss 1277.2333984375\n",
      "epoch 5, val_loss 454.8976745605469\n",
      "epoch 6, train_loss 1277.2332763671875\n",
      "epoch 6, val_loss 454.8975524902344\n",
      "epoch 7, train_loss 1277.2332763671875\n",
      "epoch 7, val_loss 454.8974914550781\n",
      "epoch 8, train_loss 1277.2332763671875\n",
      "epoch 8, val_loss 454.8974304199219\n",
      "epoch 9, train_loss 1277.233154296875\n",
      "epoch 9, val_loss 454.8973388671875\n",
      "epoch 10, train_loss 1277.2330322265625\n",
      "epoch 10, val_loss 454.89727783203125\n",
      "epoch 11, train_loss 1277.2330322265625\n",
      "epoch 11, val_loss 454.89727783203125\n",
      "epoch 12, train_loss 1277.23291015625\n",
      "epoch 12, val_loss 454.8972473144531\n",
      "epoch 13, train_loss 1277.2327880859375\n",
      "epoch 13, val_loss 454.89715576171875\n",
      "epoch 14, train_loss 1277.2327880859375\n",
      "epoch 14, val_loss 454.897216796875\n",
      "epoch 15, train_loss 1277.2327880859375\n",
      "epoch 15, val_loss 454.8970947265625\n",
      "epoch 16, train_loss 1277.2327880859375\n",
      "epoch 16, val_loss 454.8970031738281\n",
      "epoch 17, train_loss 1277.232666015625\n",
      "epoch 17, val_loss 454.89697265625\n",
      "epoch 18, train_loss 1277.2325439453125\n",
      "epoch 18, val_loss 454.8968811035156\n",
      "epoch 19, train_loss 1277.2325439453125\n",
      "epoch 19, val_loss 454.8968200683594\n",
      "epoch 20, train_loss 1277.232666015625\n",
      "epoch 20, val_loss 454.8968200683594\n",
      "epoch 21, train_loss 1277.2325439453125\n",
      "epoch 21, val_loss 454.8968200683594\n",
      "epoch 22, train_loss 1277.2325439453125\n",
      "epoch 22, val_loss 454.89666748046875\n",
      "epoch 23, train_loss 1277.232421875\n",
      "epoch 23, val_loss 454.8966979980469\n",
      "epoch 24, train_loss 1277.232421875\n",
      "epoch 24, val_loss 454.8965759277344\n",
      "epoch 25, train_loss 1277.2322998046875\n",
      "epoch 25, val_loss 454.89654541015625\n",
      "epoch 26, train_loss 1277.2320556640625\n",
      "epoch 26, val_loss 454.8965148925781\n",
      "epoch 27, train_loss 1277.2320556640625\n",
      "epoch 27, val_loss 454.89642333984375\n",
      "epoch 28, train_loss 1277.2320556640625\n",
      "epoch 28, val_loss 454.8963623046875\n",
      "epoch 29, train_loss 1277.2320556640625\n",
      "epoch 29, val_loss 454.8963317871094\n",
      "epoch 30, train_loss 1277.2320556640625\n",
      "epoch 30, val_loss 454.89630126953125\n",
      "epoch 31, train_loss 1277.23193359375\n",
      "epoch 31, val_loss 454.89617919921875\n",
      "epoch 32, train_loss 1277.2318115234375\n",
      "epoch 32, val_loss 454.8961181640625\n",
      "epoch 33, train_loss 1277.2318115234375\n",
      "epoch 33, val_loss 454.8961181640625\n",
      "epoch 34, train_loss 1277.2318115234375\n",
      "epoch 34, val_loss 454.89599609375\n",
      "epoch 35, train_loss 1277.231689453125\n",
      "epoch 35, val_loss 454.8960876464844\n",
      "epoch 36, train_loss 1277.231689453125\n",
      "epoch 36, val_loss 454.89599609375\n",
      "epoch 37, train_loss 1277.231689453125\n",
      "epoch 37, val_loss 454.8959045410156\n",
      "epoch 38, train_loss 1277.2314453125\n",
      "epoch 38, val_loss 454.8958435058594\n",
      "epoch 39, train_loss 1277.2314453125\n",
      "epoch 39, val_loss 454.8957824707031\n",
      "epoch 40, train_loss 1277.2313232421875\n",
      "epoch 40, val_loss 454.8957214355469\n",
      "epoch 41, train_loss 1277.2314453125\n",
      "epoch 41, val_loss 454.8956604003906\n",
      "epoch 42, train_loss 1277.2313232421875\n",
      "epoch 42, val_loss 454.8956604003906\n",
      "epoch 43, train_loss 1277.231201171875\n",
      "epoch 43, val_loss 454.8956298828125\n",
      "epoch 44, train_loss 1277.231201171875\n",
      "epoch 44, val_loss 454.8955078125\n",
      "epoch 45, train_loss 1277.231201171875\n",
      "epoch 45, val_loss 454.8955078125\n",
      "epoch 46, train_loss 1277.23095703125\n",
      "epoch 46, val_loss 454.89544677734375\n",
      "epoch 47, train_loss 1277.23095703125\n",
      "epoch 47, val_loss 454.89532470703125\n",
      "epoch 48, train_loss 1277.23095703125\n",
      "epoch 48, val_loss 454.8952941894531\n",
      "epoch 49, train_loss 1277.23095703125\n",
      "epoch 49, val_loss 454.895263671875\n",
      "epoch 50, train_loss 1277.23095703125\n",
      "epoch 50, val_loss 454.8951721191406\n",
      "epoch 51, train_loss 1277.230712890625\n",
      "epoch 51, val_loss 454.8951110839844\n",
      "epoch 52, train_loss 1277.230712890625\n",
      "epoch 52, val_loss 454.8951721191406\n",
      "epoch 53, train_loss 1277.230712890625\n",
      "epoch 53, val_loss 454.89508056640625\n",
      "epoch 54, train_loss 1277.230712890625\n",
      "epoch 54, val_loss 454.8949890136719\n",
      "epoch 55, train_loss 1277.230712890625\n",
      "epoch 55, val_loss 454.89495849609375\n",
      "epoch 56, train_loss 1277.23046875\n",
      "epoch 56, val_loss 454.8948669433594\n",
      "epoch 57, train_loss 1277.23046875\n",
      "epoch 57, val_loss 454.89483642578125\n",
      "epoch 58, train_loss 1277.2303466796875\n",
      "epoch 58, val_loss 454.8948059082031\n",
      "epoch 59, train_loss 1277.23046875\n",
      "epoch 59, val_loss 454.89483642578125\n",
      "epoch 60, train_loss 1277.2303466796875\n",
      "epoch 60, val_loss 454.8946533203125\n",
      "epoch 61, train_loss 1277.230224609375\n",
      "epoch 61, val_loss 454.8946228027344\n",
      "epoch 62, train_loss 1277.230224609375\n",
      "epoch 62, val_loss 454.8946228027344\n",
      "epoch 63, train_loss 1277.230224609375\n",
      "epoch 63, val_loss 454.89453125\n",
      "epoch 64, train_loss 1277.2301025390625\n",
      "epoch 64, val_loss 454.89447021484375\n",
      "epoch 65, train_loss 1277.22998046875\n",
      "epoch 65, val_loss 454.8944091796875\n",
      "epoch 66, train_loss 1277.22998046875\n",
      "epoch 66, val_loss 454.8943786621094\n",
      "epoch 67, train_loss 1277.2301025390625\n",
      "epoch 67, val_loss 454.89434814453125\n",
      "epoch 68, train_loss 1277.2298583984375\n",
      "epoch 68, val_loss 454.89434814453125\n",
      "epoch 69, train_loss 1277.2298583984375\n",
      "epoch 69, val_loss 454.8941955566406\n",
      "epoch 70, train_loss 1277.2298583984375\n",
      "epoch 70, val_loss 454.8941650390625\n",
      "epoch 71, train_loss 1277.229736328125\n",
      "epoch 71, val_loss 454.8940734863281\n",
      "epoch 72, train_loss 1277.2296142578125\n",
      "epoch 72, val_loss 454.8939514160156\n",
      "epoch 73, train_loss 1277.229736328125\n",
      "epoch 73, val_loss 454.8939514160156\n",
      "epoch 74, train_loss 1277.2298583984375\n",
      "epoch 74, val_loss 454.8939514160156\n",
      "epoch 75, train_loss 1277.2296142578125\n",
      "epoch 75, val_loss 454.89385986328125\n",
      "epoch 76, train_loss 1277.2293701171875\n",
      "epoch 76, val_loss 454.8938293457031\n",
      "epoch 77, train_loss 1277.2293701171875\n",
      "epoch 77, val_loss 454.89385986328125\n",
      "epoch 78, train_loss 1277.2293701171875\n",
      "epoch 78, val_loss 454.89373779296875\n",
      "epoch 79, train_loss 1277.2293701171875\n",
      "epoch 79, val_loss 454.89373779296875\n",
      "epoch 80, train_loss 1277.2293701171875\n",
      "epoch 80, val_loss 454.8936767578125\n",
      "epoch 81, train_loss 1277.2291259765625\n",
      "epoch 81, val_loss 454.8935546875\n",
      "epoch 82, train_loss 1277.2291259765625\n",
      "epoch 82, val_loss 454.89349365234375\n",
      "epoch 83, train_loss 1277.2291259765625\n",
      "epoch 83, val_loss 454.8934020996094\n",
      "epoch 84, train_loss 1277.2291259765625\n",
      "epoch 84, val_loss 454.8934631347656\n",
      "epoch 85, train_loss 1277.2291259765625\n",
      "epoch 85, val_loss 454.8933410644531\n",
      "epoch 86, train_loss 1277.2291259765625\n",
      "epoch 86, val_loss 454.8933410644531\n",
      "epoch 87, train_loss 1277.2288818359375\n",
      "epoch 87, val_loss 454.8932800292969\n",
      "epoch 88, train_loss 1277.228759765625\n",
      "epoch 88, val_loss 454.8932189941406\n",
      "epoch 89, train_loss 1277.228759765625\n",
      "epoch 89, val_loss 454.8931579589844\n",
      "epoch 90, train_loss 1277.2286376953125\n",
      "epoch 90, val_loss 454.89312744140625\n",
      "epoch 91, train_loss 1277.228759765625\n",
      "epoch 91, val_loss 454.8930969238281\n",
      "epoch 92, train_loss 1277.2286376953125\n",
      "epoch 92, val_loss 454.89300537109375\n",
      "epoch 93, train_loss 1277.2286376953125\n",
      "epoch 93, val_loss 454.8929443359375\n",
      "epoch 94, train_loss 1277.2283935546875\n",
      "epoch 94, val_loss 454.89288330078125\n",
      "epoch 95, train_loss 1277.2283935546875\n",
      "epoch 95, val_loss 454.892822265625\n",
      "epoch 96, train_loss 1277.2283935546875\n",
      "epoch 96, val_loss 454.8927917480469\n",
      "epoch 97, train_loss 1277.2283935546875\n",
      "epoch 97, val_loss 454.8927001953125\n",
      "epoch 98, train_loss 1277.228271484375\n",
      "epoch 98, val_loss 454.8927001953125\n",
      "epoch 99, train_loss 1277.22802734375\n",
      "epoch 99, val_loss 454.89263916015625\n",
      "Parameter containing:\n",
      "tensor([1.8008e-22], requires_grad=True)\n",
      "iter 117, train_loss_regularization 0.7272295355796814\n",
      "iter 117, val_loss_regularization 0.7272295355796814\n",
      "epoch 0, train_loss 1277.2281494140625\n",
      "epoch 0, val_loss 454.8925476074219\n",
      "epoch 1, train_loss 1277.22802734375\n",
      "epoch 1, val_loss 454.8925476074219\n",
      "epoch 2, train_loss 1277.22802734375\n",
      "epoch 2, val_loss 454.8924865722656\n",
      "epoch 3, train_loss 1277.22802734375\n",
      "epoch 3, val_loss 454.8924255371094\n",
      "epoch 4, train_loss 1277.22802734375\n",
      "epoch 4, val_loss 454.892333984375\n",
      "epoch 5, train_loss 1277.2279052734375\n",
      "epoch 5, val_loss 454.8923034667969\n",
      "epoch 6, train_loss 1277.227783203125\n",
      "epoch 6, val_loss 454.8922424316406\n",
      "epoch 7, train_loss 1277.2276611328125\n",
      "epoch 7, val_loss 454.8921813964844\n",
      "epoch 8, train_loss 1277.227783203125\n",
      "epoch 8, val_loss 454.8921813964844\n",
      "epoch 9, train_loss 1277.2276611328125\n",
      "epoch 9, val_loss 454.8921203613281\n",
      "epoch 10, train_loss 1277.2275390625\n",
      "epoch 10, val_loss 454.89208984375\n",
      "epoch 11, train_loss 1277.2274169921875\n",
      "epoch 11, val_loss 454.89202880859375\n",
      "epoch 12, train_loss 1277.2274169921875\n",
      "epoch 12, val_loss 454.8919677734375\n",
      "epoch 13, train_loss 1277.2274169921875\n",
      "epoch 13, val_loss 454.8918762207031\n",
      "epoch 14, train_loss 1277.2275390625\n",
      "epoch 14, val_loss 454.891845703125\n",
      "epoch 15, train_loss 1277.2271728515625\n",
      "epoch 15, val_loss 454.89178466796875\n",
      "epoch 16, train_loss 1277.2271728515625\n",
      "epoch 16, val_loss 454.8916931152344\n",
      "epoch 17, train_loss 1277.227294921875\n",
      "epoch 17, val_loss 454.8916931152344\n",
      "epoch 18, train_loss 1277.227294921875\n",
      "epoch 18, val_loss 454.89166259765625\n",
      "epoch 19, train_loss 1277.2271728515625\n",
      "epoch 19, val_loss 454.8915710449219\n",
      "epoch 20, train_loss 1277.22705078125\n",
      "epoch 20, val_loss 454.8915100097656\n",
      "epoch 21, train_loss 1277.22705078125\n",
      "epoch 21, val_loss 454.8914489746094\n",
      "epoch 22, train_loss 1277.22705078125\n",
      "epoch 22, val_loss 454.8914489746094\n",
      "epoch 23, train_loss 1277.226806640625\n",
      "epoch 23, val_loss 454.8913879394531\n",
      "epoch 24, train_loss 1277.2269287109375\n",
      "epoch 24, val_loss 454.8913269042969\n",
      "epoch 25, train_loss 1277.226806640625\n",
      "epoch 25, val_loss 454.8912353515625\n",
      "epoch 26, train_loss 1277.2266845703125\n",
      "epoch 26, val_loss 454.8912048339844\n",
      "epoch 27, train_loss 1277.2264404296875\n",
      "epoch 27, val_loss 454.89111328125\n",
      "epoch 28, train_loss 1277.2264404296875\n",
      "epoch 28, val_loss 454.89111328125\n",
      "epoch 29, train_loss 1277.2265625\n",
      "epoch 29, val_loss 454.89105224609375\n",
      "epoch 30, train_loss 1277.2266845703125\n",
      "epoch 30, val_loss 454.89105224609375\n",
      "epoch 31, train_loss 1277.2265625\n",
      "epoch 31, val_loss 454.8909606933594\n",
      "epoch 32, train_loss 1277.2264404296875\n",
      "epoch 32, val_loss 454.89093017578125\n",
      "epoch 33, train_loss 1277.226318359375\n",
      "epoch 33, val_loss 454.890869140625\n",
      "epoch 34, train_loss 1277.226318359375\n",
      "epoch 34, val_loss 454.8908386230469\n",
      "epoch 35, train_loss 1277.226318359375\n",
      "epoch 35, val_loss 454.8907775878906\n",
      "epoch 36, train_loss 1277.2261962890625\n",
      "epoch 36, val_loss 454.8906555175781\n",
      "epoch 37, train_loss 1277.2261962890625\n",
      "epoch 37, val_loss 454.890625\n",
      "epoch 38, train_loss 1277.22607421875\n",
      "epoch 38, val_loss 454.8905944824219\n",
      "epoch 39, train_loss 1277.2259521484375\n",
      "epoch 39, val_loss 454.8905029296875\n",
      "epoch 40, train_loss 1277.225830078125\n",
      "epoch 40, val_loss 454.8905029296875\n",
      "epoch 41, train_loss 1277.2259521484375\n",
      "epoch 41, val_loss 454.8904724121094\n",
      "epoch 42, train_loss 1277.2259521484375\n",
      "epoch 42, val_loss 454.890380859375\n",
      "epoch 43, train_loss 1277.2259521484375\n",
      "epoch 43, val_loss 454.89031982421875\n",
      "epoch 44, train_loss 1277.225830078125\n",
      "epoch 44, val_loss 454.8902893066406\n",
      "epoch 45, train_loss 1277.2257080078125\n",
      "epoch 45, val_loss 454.89019775390625\n",
      "epoch 46, train_loss 1277.2257080078125\n",
      "epoch 46, val_loss 454.89019775390625\n",
      "epoch 47, train_loss 1277.2255859375\n",
      "epoch 47, val_loss 454.89007568359375\n",
      "epoch 48, train_loss 1277.2255859375\n",
      "epoch 48, val_loss 454.89007568359375\n",
      "epoch 49, train_loss 1277.2254638671875\n",
      "epoch 49, val_loss 454.8900146484375\n",
      "epoch 50, train_loss 1277.2252197265625\n",
      "epoch 50, val_loss 454.88995361328125\n",
      "epoch 51, train_loss 1277.225341796875\n",
      "epoch 51, val_loss 454.88995361328125\n",
      "epoch 52, train_loss 1277.225341796875\n",
      "epoch 52, val_loss 454.8898010253906\n",
      "epoch 53, train_loss 1277.2252197265625\n",
      "epoch 53, val_loss 454.8898010253906\n",
      "epoch 54, train_loss 1277.2252197265625\n",
      "epoch 54, val_loss 454.8897399902344\n",
      "epoch 55, train_loss 1277.22509765625\n",
      "epoch 55, val_loss 454.8896789550781\n",
      "epoch 56, train_loss 1277.2249755859375\n",
      "epoch 56, val_loss 454.8896789550781\n",
      "epoch 57, train_loss 1277.224853515625\n",
      "epoch 57, val_loss 454.8896789550781\n",
      "epoch 58, train_loss 1277.2249755859375\n",
      "epoch 58, val_loss 454.8895568847656\n",
      "epoch 59, train_loss 1277.2249755859375\n",
      "epoch 59, val_loss 454.8894958496094\n",
      "epoch 60, train_loss 1277.2249755859375\n",
      "epoch 60, val_loss 454.889404296875\n",
      "epoch 61, train_loss 1277.2249755859375\n",
      "epoch 61, val_loss 454.8893737792969\n",
      "epoch 62, train_loss 1277.2247314453125\n",
      "epoch 62, val_loss 454.8892822265625\n",
      "epoch 63, train_loss 1277.2247314453125\n",
      "epoch 63, val_loss 454.8892517089844\n",
      "epoch 64, train_loss 1277.2247314453125\n",
      "epoch 64, val_loss 454.88922119140625\n",
      "epoch 65, train_loss 1277.224609375\n",
      "epoch 65, val_loss 454.88916015625\n",
      "epoch 66, train_loss 1277.2244873046875\n",
      "epoch 66, val_loss 454.88916015625\n",
      "epoch 67, train_loss 1277.2244873046875\n",
      "epoch 67, val_loss 454.8891296386719\n",
      "epoch 68, train_loss 1277.224365234375\n",
      "epoch 68, val_loss 454.8890686035156\n",
      "epoch 69, train_loss 1277.224365234375\n",
      "epoch 69, val_loss 454.8889465332031\n",
      "epoch 70, train_loss 1277.2244873046875\n",
      "epoch 70, val_loss 454.888916015625\n",
      "epoch 71, train_loss 1277.224365234375\n",
      "epoch 71, val_loss 454.8888244628906\n",
      "epoch 72, train_loss 1277.224365234375\n",
      "epoch 72, val_loss 454.8887939453125\n",
      "epoch 73, train_loss 1277.224365234375\n",
      "epoch 73, val_loss 454.8887634277344\n",
      "epoch 74, train_loss 1277.22412109375\n",
      "epoch 74, val_loss 454.8887023925781\n",
      "epoch 75, train_loss 1277.223876953125\n",
      "epoch 75, val_loss 454.8887023925781\n",
      "epoch 76, train_loss 1277.223876953125\n",
      "epoch 76, val_loss 454.8885498046875\n",
      "epoch 77, train_loss 1277.223876953125\n",
      "epoch 77, val_loss 454.8885498046875\n",
      "epoch 78, train_loss 1277.223876953125\n",
      "epoch 78, val_loss 454.88848876953125\n",
      "epoch 79, train_loss 1277.223876953125\n",
      "epoch 79, val_loss 454.888427734375\n",
      "epoch 80, train_loss 1277.223876953125\n",
      "epoch 80, val_loss 454.8883361816406\n",
      "epoch 81, train_loss 1277.2237548828125\n",
      "epoch 81, val_loss 454.8883361816406\n",
      "epoch 82, train_loss 1277.2236328125\n",
      "epoch 82, val_loss 454.8883056640625\n",
      "epoch 83, train_loss 1277.2236328125\n",
      "epoch 83, val_loss 454.8882141113281\n",
      "epoch 84, train_loss 1277.2236328125\n",
      "epoch 84, val_loss 454.8881530761719\n",
      "epoch 85, train_loss 1277.2236328125\n",
      "epoch 85, val_loss 454.88812255859375\n",
      "epoch 86, train_loss 1277.2236328125\n",
      "epoch 86, val_loss 454.8880920410156\n",
      "epoch 87, train_loss 1277.223388671875\n",
      "epoch 87, val_loss 454.88800048828125\n",
      "epoch 88, train_loss 1277.2232666015625\n",
      "epoch 88, val_loss 454.88800048828125\n",
      "epoch 89, train_loss 1277.2232666015625\n",
      "epoch 89, val_loss 454.8879699707031\n",
      "epoch 90, train_loss 1277.223388671875\n",
      "epoch 90, val_loss 454.8879089355469\n",
      "epoch 91, train_loss 1277.2230224609375\n",
      "epoch 91, val_loss 454.8878479003906\n",
      "epoch 92, train_loss 1277.2230224609375\n",
      "epoch 92, val_loss 454.8877868652344\n",
      "epoch 93, train_loss 1277.2230224609375\n",
      "epoch 93, val_loss 454.8876647949219\n",
      "epoch 94, train_loss 1277.2230224609375\n",
      "epoch 94, val_loss 454.88763427734375\n",
      "epoch 95, train_loss 1277.2230224609375\n",
      "epoch 95, val_loss 454.88763427734375\n",
      "epoch 96, train_loss 1277.2227783203125\n",
      "epoch 96, val_loss 454.8875732421875\n",
      "epoch 97, train_loss 1277.2227783203125\n",
      "epoch 97, val_loss 454.88751220703125\n",
      "epoch 98, train_loss 1277.2227783203125\n",
      "epoch 98, val_loss 454.88751220703125\n",
      "epoch 99, train_loss 1277.2227783203125\n",
      "epoch 99, val_loss 454.887451171875\n",
      "Parameter containing:\n",
      "tensor([1.2429e-22], requires_grad=True)\n",
      "iter 118, train_loss_regularization 0.7258957028388977\n",
      "iter 118, val_loss_regularization 0.7258957028388977\n",
      "epoch 0, train_loss 1277.2227783203125\n",
      "epoch 0, val_loss 454.88739013671875\n",
      "epoch 1, train_loss 1277.2225341796875\n",
      "epoch 1, val_loss 454.8872985839844\n",
      "epoch 2, train_loss 1277.22265625\n",
      "epoch 2, val_loss 454.8872375488281\n",
      "epoch 3, train_loss 1277.2225341796875\n",
      "epoch 3, val_loss 454.8871765136719\n",
      "epoch 4, train_loss 1277.2225341796875\n",
      "epoch 4, val_loss 454.8871154785156\n",
      "epoch 5, train_loss 1277.2222900390625\n",
      "epoch 5, val_loss 454.8870849609375\n",
      "epoch 6, train_loss 1277.2222900390625\n",
      "epoch 6, val_loss 454.8870544433594\n",
      "epoch 7, train_loss 1277.2222900390625\n",
      "epoch 7, val_loss 454.8869934082031\n",
      "epoch 8, train_loss 1277.22216796875\n",
      "epoch 8, val_loss 454.8869934082031\n",
      "epoch 9, train_loss 1277.2220458984375\n",
      "epoch 9, val_loss 454.8868408203125\n",
      "epoch 10, train_loss 1277.2220458984375\n",
      "epoch 10, val_loss 454.8868713378906\n",
      "epoch 11, train_loss 1277.22216796875\n",
      "epoch 11, val_loss 454.88677978515625\n",
      "epoch 12, train_loss 1277.22216796875\n",
      "epoch 12, val_loss 454.88665771484375\n",
      "epoch 13, train_loss 1277.2220458984375\n",
      "epoch 13, val_loss 454.88665771484375\n",
      "epoch 14, train_loss 1277.2220458984375\n",
      "epoch 14, val_loss 454.8866271972656\n",
      "epoch 15, train_loss 1277.2218017578125\n",
      "epoch 15, val_loss 454.8865966796875\n",
      "epoch 16, train_loss 1277.2218017578125\n",
      "epoch 16, val_loss 454.8865051269531\n",
      "epoch 17, train_loss 1277.2218017578125\n",
      "epoch 17, val_loss 454.8864440917969\n",
      "epoch 18, train_loss 1277.2218017578125\n",
      "epoch 18, val_loss 454.88641357421875\n",
      "epoch 19, train_loss 1277.2218017578125\n",
      "epoch 19, val_loss 454.8863220214844\n",
      "epoch 20, train_loss 1277.2218017578125\n",
      "epoch 20, val_loss 454.8863220214844\n",
      "epoch 21, train_loss 1277.2215576171875\n",
      "epoch 21, val_loss 454.88629150390625\n",
      "epoch 22, train_loss 1277.221435546875\n",
      "epoch 22, val_loss 454.8861999511719\n",
      "epoch 23, train_loss 1277.2215576171875\n",
      "epoch 23, val_loss 454.8861999511719\n",
      "epoch 24, train_loss 1277.2215576171875\n",
      "epoch 24, val_loss 454.8861389160156\n",
      "epoch 25, train_loss 1277.2215576171875\n",
      "epoch 25, val_loss 454.885986328125\n",
      "epoch 26, train_loss 1277.2213134765625\n",
      "epoch 26, val_loss 454.8859558105469\n",
      "epoch 27, train_loss 1277.22119140625\n",
      "epoch 27, val_loss 454.88592529296875\n",
      "epoch 28, train_loss 1277.22119140625\n",
      "epoch 28, val_loss 454.88592529296875\n",
      "epoch 29, train_loss 1277.22119140625\n",
      "epoch 29, val_loss 454.8858337402344\n",
      "epoch 30, train_loss 1277.22119140625\n",
      "epoch 30, val_loss 454.8858337402344\n",
      "epoch 31, train_loss 1277.22119140625\n",
      "epoch 31, val_loss 454.88580322265625\n",
      "epoch 32, train_loss 1277.2210693359375\n",
      "epoch 32, val_loss 454.8857116699219\n",
      "epoch 33, train_loss 1277.220947265625\n",
      "epoch 33, val_loss 454.88568115234375\n",
      "epoch 34, train_loss 1277.220947265625\n",
      "epoch 34, val_loss 454.8855895996094\n",
      "epoch 35, train_loss 1277.220947265625\n",
      "epoch 35, val_loss 454.885498046875\n",
      "epoch 36, train_loss 1277.220703125\n",
      "epoch 36, val_loss 454.8854675292969\n",
      "epoch 37, train_loss 1277.2208251953125\n",
      "epoch 37, val_loss 454.8854064941406\n",
      "epoch 38, train_loss 1277.2205810546875\n",
      "epoch 38, val_loss 454.8853759765625\n",
      "epoch 39, train_loss 1277.220458984375\n",
      "epoch 39, val_loss 454.8853454589844\n",
      "epoch 40, train_loss 1277.2205810546875\n",
      "epoch 40, val_loss 454.88525390625\n",
      "epoch 41, train_loss 1277.2205810546875\n",
      "epoch 41, val_loss 454.8852844238281\n",
      "epoch 42, train_loss 1277.2205810546875\n",
      "epoch 42, val_loss 454.8851318359375\n",
      "epoch 43, train_loss 1277.220458984375\n",
      "epoch 43, val_loss 454.8851623535156\n",
      "epoch 44, train_loss 1277.2203369140625\n",
      "epoch 44, val_loss 454.88507080078125\n",
      "epoch 45, train_loss 1277.22021484375\n",
      "epoch 45, val_loss 454.8850402832031\n",
      "epoch 46, train_loss 1277.22021484375\n",
      "epoch 46, val_loss 454.885009765625\n",
      "epoch 47, train_loss 1277.22021484375\n",
      "epoch 47, val_loss 454.8849182128906\n",
      "epoch 48, train_loss 1277.22021484375\n",
      "epoch 48, val_loss 454.8848876953125\n",
      "epoch 49, train_loss 1277.2200927734375\n",
      "epoch 49, val_loss 454.88482666015625\n",
      "epoch 50, train_loss 1277.219970703125\n",
      "epoch 50, val_loss 454.8847351074219\n",
      "epoch 51, train_loss 1277.219970703125\n",
      "epoch 51, val_loss 454.8846740722656\n",
      "epoch 52, train_loss 1277.2198486328125\n",
      "epoch 52, val_loss 454.8846130371094\n",
      "epoch 53, train_loss 1277.2198486328125\n",
      "epoch 53, val_loss 454.8846130371094\n",
      "epoch 54, train_loss 1277.219970703125\n",
      "epoch 54, val_loss 454.8844909667969\n",
      "epoch 55, train_loss 1277.2198486328125\n",
      "epoch 55, val_loss 454.8845520019531\n",
      "epoch 56, train_loss 1277.2196044921875\n",
      "epoch 56, val_loss 454.8844909667969\n",
      "epoch 57, train_loss 1277.2197265625\n",
      "epoch 57, val_loss 454.8844299316406\n",
      "epoch 58, train_loss 1277.2196044921875\n",
      "epoch 58, val_loss 454.88433837890625\n",
      "epoch 59, train_loss 1277.2196044921875\n",
      "epoch 59, val_loss 454.88427734375\n",
      "epoch 60, train_loss 1277.2193603515625\n",
      "epoch 60, val_loss 454.88421630859375\n",
      "epoch 61, train_loss 1277.219482421875\n",
      "epoch 61, val_loss 454.8841552734375\n",
      "epoch 62, train_loss 1277.2193603515625\n",
      "epoch 62, val_loss 454.8841247558594\n",
      "epoch 63, train_loss 1277.2193603515625\n",
      "epoch 63, val_loss 454.8841247558594\n",
      "epoch 64, train_loss 1277.219482421875\n",
      "epoch 64, val_loss 454.88409423828125\n",
      "epoch 65, train_loss 1277.2193603515625\n",
      "epoch 65, val_loss 454.8840026855469\n",
      "epoch 66, train_loss 1277.2191162109375\n",
      "epoch 66, val_loss 454.88397216796875\n",
      "epoch 67, train_loss 1277.218994140625\n",
      "epoch 67, val_loss 454.8838806152344\n",
      "epoch 68, train_loss 1277.2191162109375\n",
      "epoch 68, val_loss 454.8838195800781\n",
      "epoch 69, train_loss 1277.218994140625\n",
      "epoch 69, val_loss 454.8837585449219\n",
      "epoch 70, train_loss 1277.218994140625\n",
      "epoch 70, val_loss 454.8837585449219\n",
      "epoch 71, train_loss 1277.218994140625\n",
      "epoch 71, val_loss 454.8836669921875\n",
      "epoch 72, train_loss 1277.218994140625\n",
      "epoch 72, val_loss 454.8836364746094\n",
      "epoch 73, train_loss 1277.2188720703125\n",
      "epoch 73, val_loss 454.8835754394531\n",
      "epoch 74, train_loss 1277.21875\n",
      "epoch 74, val_loss 454.8835144042969\n",
      "epoch 75, train_loss 1277.2188720703125\n",
      "epoch 75, val_loss 454.8834228515625\n",
      "epoch 76, train_loss 1277.2186279296875\n",
      "epoch 76, val_loss 454.8834533691406\n",
      "epoch 77, train_loss 1277.2186279296875\n",
      "epoch 77, val_loss 454.8833312988281\n",
      "epoch 78, train_loss 1277.218505859375\n",
      "epoch 78, val_loss 454.8833312988281\n",
      "epoch 79, train_loss 1277.218505859375\n",
      "epoch 79, val_loss 454.88323974609375\n",
      "epoch 80, train_loss 1277.218505859375\n",
      "epoch 80, val_loss 454.88323974609375\n",
      "epoch 81, train_loss 1277.218505859375\n",
      "epoch 81, val_loss 454.8831787109375\n",
      "epoch 82, train_loss 1277.2183837890625\n",
      "epoch 82, val_loss 454.88311767578125\n",
      "epoch 83, train_loss 1277.21826171875\n",
      "epoch 83, val_loss 454.88299560546875\n",
      "epoch 84, train_loss 1277.21826171875\n",
      "epoch 84, val_loss 454.8829650878906\n",
      "epoch 85, train_loss 1277.2181396484375\n",
      "epoch 85, val_loss 454.8829650878906\n",
      "epoch 86, train_loss 1277.218017578125\n",
      "epoch 86, val_loss 454.88287353515625\n",
      "epoch 87, train_loss 1277.218017578125\n",
      "epoch 87, val_loss 454.88287353515625\n",
      "epoch 88, train_loss 1277.2181396484375\n",
      "epoch 88, val_loss 454.8828430175781\n",
      "epoch 89, train_loss 1277.2178955078125\n",
      "epoch 89, val_loss 454.8827819824219\n",
      "epoch 90, train_loss 1277.2178955078125\n",
      "epoch 90, val_loss 454.8827209472656\n",
      "epoch 91, train_loss 1277.2177734375\n",
      "epoch 91, val_loss 454.8826599121094\n",
      "epoch 92, train_loss 1277.2177734375\n",
      "epoch 92, val_loss 454.882568359375\n",
      "epoch 93, train_loss 1277.2177734375\n",
      "epoch 93, val_loss 454.8825378417969\n",
      "epoch 94, train_loss 1277.2177734375\n",
      "epoch 94, val_loss 454.88250732421875\n",
      "epoch 95, train_loss 1277.2177734375\n",
      "epoch 95, val_loss 454.8824462890625\n",
      "epoch 96, train_loss 1277.217529296875\n",
      "epoch 96, val_loss 454.88238525390625\n",
      "epoch 97, train_loss 1277.217529296875\n",
      "epoch 97, val_loss 454.8822937011719\n",
      "epoch 98, train_loss 1277.217529296875\n",
      "epoch 98, val_loss 454.88232421875\n",
      "epoch 99, train_loss 1277.217529296875\n",
      "epoch 99, val_loss 454.88226318359375\n",
      "Parameter containing:\n",
      "tensor([8.5815e-23], requires_grad=True)\n",
      "iter 119, train_loss_regularization 0.7245901226997375\n",
      "iter 119, val_loss_regularization 0.7245901226997375\n",
      "epoch 0, train_loss 1277.21728515625\n",
      "epoch 0, val_loss 454.8821716308594\n",
      "epoch 1, train_loss 1277.21728515625\n",
      "epoch 1, val_loss 454.8821716308594\n",
      "epoch 2, train_loss 1277.21728515625\n",
      "epoch 2, val_loss 454.8821105957031\n",
      "epoch 3, train_loss 1277.2171630859375\n",
      "epoch 3, val_loss 454.8819885253906\n",
      "epoch 4, train_loss 1277.2171630859375\n",
      "epoch 4, val_loss 454.8819885253906\n",
      "epoch 5, train_loss 1277.217041015625\n",
      "epoch 5, val_loss 454.8819580078125\n",
      "epoch 6, train_loss 1277.217041015625\n",
      "epoch 6, val_loss 454.8818664550781\n",
      "epoch 7, train_loss 1277.217041015625\n",
      "epoch 7, val_loss 454.8818664550781\n",
      "epoch 8, train_loss 1277.216796875\n",
      "epoch 8, val_loss 454.8817443847656\n",
      "epoch 9, train_loss 1277.2169189453125\n",
      "epoch 9, val_loss 454.8817138671875\n",
      "epoch 10, train_loss 1277.2169189453125\n",
      "epoch 10, val_loss 454.88165283203125\n",
      "epoch 11, train_loss 1277.216796875\n",
      "epoch 11, val_loss 454.8816223144531\n",
      "epoch 12, train_loss 1277.216796875\n",
      "epoch 12, val_loss 454.881591796875\n",
      "epoch 13, train_loss 1277.216796875\n",
      "epoch 13, val_loss 454.88153076171875\n",
      "epoch 14, train_loss 1277.216796875\n",
      "epoch 14, val_loss 454.8814697265625\n",
      "epoch 15, train_loss 1277.2166748046875\n",
      "epoch 15, val_loss 454.88140869140625\n",
      "epoch 16, train_loss 1277.2166748046875\n",
      "epoch 16, val_loss 454.88134765625\n",
      "epoch 17, train_loss 1277.2164306640625\n",
      "epoch 17, val_loss 454.88128662109375\n",
      "epoch 18, train_loss 1277.2164306640625\n",
      "epoch 18, val_loss 454.8812561035156\n",
      "epoch 19, train_loss 1277.2164306640625\n",
      "epoch 19, val_loss 454.8811950683594\n",
      "epoch 20, train_loss 1277.21630859375\n",
      "epoch 20, val_loss 454.88116455078125\n",
      "epoch 21, train_loss 1277.2161865234375\n",
      "epoch 21, val_loss 454.8811340332031\n",
      "epoch 22, train_loss 1277.21630859375\n",
      "epoch 22, val_loss 454.8810729980469\n",
      "epoch 23, train_loss 1277.21630859375\n",
      "epoch 23, val_loss 454.8810119628906\n",
      "epoch 24, train_loss 1277.21630859375\n",
      "epoch 24, val_loss 454.88092041015625\n",
      "epoch 25, train_loss 1277.2161865234375\n",
      "epoch 25, val_loss 454.8808288574219\n",
      "epoch 26, train_loss 1277.2161865234375\n",
      "epoch 26, val_loss 454.8808288574219\n",
      "epoch 27, train_loss 1277.2159423828125\n",
      "epoch 27, val_loss 454.88079833984375\n",
      "epoch 28, train_loss 1277.2159423828125\n",
      "epoch 28, val_loss 454.88079833984375\n",
      "epoch 29, train_loss 1277.2158203125\n",
      "epoch 29, val_loss 454.8807067871094\n",
      "epoch 30, train_loss 1277.2156982421875\n",
      "epoch 30, val_loss 454.8807067871094\n",
      "epoch 31, train_loss 1277.2156982421875\n",
      "epoch 31, val_loss 454.880615234375\n",
      "epoch 32, train_loss 1277.2156982421875\n",
      "epoch 32, val_loss 454.88055419921875\n",
      "epoch 33, train_loss 1277.2156982421875\n",
      "epoch 33, val_loss 454.8804931640625\n",
      "epoch 34, train_loss 1277.2154541015625\n",
      "epoch 34, val_loss 454.8804626464844\n",
      "epoch 35, train_loss 1277.2154541015625\n",
      "epoch 35, val_loss 454.8803405761719\n",
      "epoch 36, train_loss 1277.215576171875\n",
      "epoch 36, val_loss 454.8803405761719\n",
      "epoch 37, train_loss 1277.2154541015625\n",
      "epoch 37, val_loss 454.8802795410156\n",
      "epoch 38, train_loss 1277.21533203125\n",
      "epoch 38, val_loss 454.8802185058594\n",
      "epoch 39, train_loss 1277.21533203125\n",
      "epoch 39, val_loss 454.880126953125\n",
      "epoch 40, train_loss 1277.2152099609375\n",
      "epoch 40, val_loss 454.8800964355469\n",
      "epoch 41, train_loss 1277.2152099609375\n",
      "epoch 41, val_loss 454.8800354003906\n",
      "epoch 42, train_loss 1277.2152099609375\n",
      "epoch 42, val_loss 454.8800048828125\n",
      "epoch 43, train_loss 1277.2149658203125\n",
      "epoch 43, val_loss 454.8800048828125\n",
      "epoch 44, train_loss 1277.2152099609375\n",
      "epoch 44, val_loss 454.8800048828125\n",
      "epoch 45, train_loss 1277.215087890625\n",
      "epoch 45, val_loss 454.8798828125\n",
      "epoch 46, train_loss 1277.2149658203125\n",
      "epoch 46, val_loss 454.8797912597656\n",
      "epoch 47, train_loss 1277.2147216796875\n",
      "epoch 47, val_loss 454.8797912597656\n",
      "epoch 48, train_loss 1277.2147216796875\n",
      "epoch 48, val_loss 454.87969970703125\n",
      "epoch 49, train_loss 1277.21484375\n",
      "epoch 49, val_loss 454.879638671875\n",
      "epoch 50, train_loss 1277.21484375\n",
      "epoch 50, val_loss 454.879638671875\n",
      "epoch 51, train_loss 1277.2147216796875\n",
      "epoch 51, val_loss 454.8795471191406\n",
      "epoch 52, train_loss 1277.214599609375\n",
      "epoch 52, val_loss 454.8794860839844\n",
      "epoch 53, train_loss 1277.214599609375\n",
      "epoch 53, val_loss 454.87945556640625\n",
      "epoch 54, train_loss 1277.214599609375\n",
      "epoch 54, val_loss 454.87945556640625\n",
      "epoch 55, train_loss 1277.214599609375\n",
      "epoch 55, val_loss 454.8793640136719\n",
      "epoch 56, train_loss 1277.2144775390625\n",
      "epoch 56, val_loss 454.87933349609375\n",
      "epoch 57, train_loss 1277.21435546875\n",
      "epoch 57, val_loss 454.8793029785156\n",
      "epoch 58, train_loss 1277.21435546875\n",
      "epoch 58, val_loss 454.8791809082031\n",
      "epoch 59, train_loss 1277.21435546875\n",
      "epoch 59, val_loss 454.8791809082031\n",
      "epoch 60, train_loss 1277.214111328125\n",
      "epoch 60, val_loss 454.8790283203125\n",
      "epoch 61, train_loss 1277.214111328125\n",
      "epoch 61, val_loss 454.8790283203125\n",
      "epoch 62, train_loss 1277.214111328125\n",
      "epoch 62, val_loss 454.8789978027344\n",
      "epoch 63, train_loss 1277.214111328125\n",
      "epoch 63, val_loss 454.87896728515625\n",
      "epoch 64, train_loss 1277.214111328125\n",
      "epoch 64, val_loss 454.8788757324219\n",
      "epoch 65, train_loss 1277.214111328125\n",
      "epoch 65, val_loss 454.87890625\n",
      "epoch 66, train_loss 1277.2138671875\n",
      "epoch 66, val_loss 454.8787841796875\n",
      "epoch 67, train_loss 1277.2138671875\n",
      "epoch 67, val_loss 454.8787536621094\n",
      "epoch 68, train_loss 1277.2138671875\n",
      "epoch 68, val_loss 454.878662109375\n",
      "epoch 69, train_loss 1277.213623046875\n",
      "epoch 69, val_loss 454.878662109375\n",
      "epoch 70, train_loss 1277.213623046875\n",
      "epoch 70, val_loss 454.8785705566406\n",
      "epoch 71, train_loss 1277.2137451171875\n",
      "epoch 71, val_loss 454.8785400390625\n",
      "epoch 72, train_loss 1277.213623046875\n",
      "epoch 72, val_loss 454.8785095214844\n",
      "epoch 73, train_loss 1277.213623046875\n",
      "epoch 73, val_loss 454.8785095214844\n",
      "epoch 74, train_loss 1277.21337890625\n",
      "epoch 74, val_loss 454.8783874511719\n",
      "epoch 75, train_loss 1277.21337890625\n",
      "epoch 75, val_loss 454.8782958984375\n",
      "epoch 76, train_loss 1277.21337890625\n",
      "epoch 76, val_loss 454.8783264160156\n",
      "epoch 77, train_loss 1277.21337890625\n",
      "epoch 77, val_loss 454.87823486328125\n",
      "epoch 78, train_loss 1277.21337890625\n",
      "epoch 78, val_loss 454.878173828125\n",
      "epoch 79, train_loss 1277.2132568359375\n",
      "epoch 79, val_loss 454.878173828125\n",
      "epoch 80, train_loss 1277.213134765625\n",
      "epoch 80, val_loss 454.878173828125\n",
      "epoch 81, train_loss 1277.213134765625\n",
      "epoch 81, val_loss 454.87799072265625\n",
      "epoch 82, train_loss 1277.2130126953125\n",
      "epoch 82, val_loss 454.8779602050781\n",
      "epoch 83, train_loss 1277.2130126953125\n",
      "epoch 83, val_loss 454.8779296875\n",
      "epoch 84, train_loss 1277.213134765625\n",
      "epoch 84, val_loss 454.8778381347656\n",
      "epoch 85, train_loss 1277.2130126953125\n",
      "epoch 85, val_loss 454.8778381347656\n",
      "epoch 86, train_loss 1277.2127685546875\n",
      "epoch 86, val_loss 454.8778381347656\n",
      "epoch 87, train_loss 1277.2127685546875\n",
      "epoch 87, val_loss 454.8777160644531\n",
      "epoch 88, train_loss 1277.2127685546875\n",
      "epoch 88, val_loss 454.8776550292969\n",
      "epoch 89, train_loss 1277.2127685546875\n",
      "epoch 89, val_loss 454.87762451171875\n",
      "epoch 90, train_loss 1277.212646484375\n",
      "epoch 90, val_loss 454.8775939941406\n",
      "epoch 91, train_loss 1277.212646484375\n",
      "epoch 91, val_loss 454.87750244140625\n",
      "epoch 92, train_loss 1277.2125244140625\n",
      "epoch 92, val_loss 454.8774108886719\n",
      "epoch 93, train_loss 1277.2125244140625\n",
      "epoch 93, val_loss 454.8774719238281\n",
      "epoch 94, train_loss 1277.2125244140625\n",
      "epoch 94, val_loss 454.87738037109375\n",
      "epoch 95, train_loss 1277.21240234375\n",
      "epoch 95, val_loss 454.8772888183594\n",
      "epoch 96, train_loss 1277.21240234375\n",
      "epoch 96, val_loss 454.8772888183594\n",
      "epoch 97, train_loss 1277.2122802734375\n",
      "epoch 97, val_loss 454.87725830078125\n",
      "epoch 98, train_loss 1277.212158203125\n",
      "epoch 98, val_loss 454.877197265625\n",
      "epoch 99, train_loss 1277.2120361328125\n",
      "epoch 99, val_loss 454.87713623046875\n",
      "Parameter containing:\n",
      "tensor([5.9276e-23], requires_grad=True)\n",
      "iter 120, train_loss_regularization 0.7233123779296875\n",
      "iter 120, val_loss_regularization 0.7233123779296875\n",
      "epoch 0, train_loss 1277.2120361328125\n",
      "epoch 0, val_loss 454.8770446777344\n",
      "epoch 1, train_loss 1277.2120361328125\n",
      "epoch 1, val_loss 454.87701416015625\n",
      "epoch 2, train_loss 1277.2120361328125\n",
      "epoch 2, val_loss 454.87701416015625\n",
      "epoch 3, train_loss 1277.2120361328125\n",
      "epoch 3, val_loss 454.8769226074219\n",
      "epoch 4, train_loss 1277.2117919921875\n",
      "epoch 4, val_loss 454.8768615722656\n",
      "epoch 5, train_loss 1277.2117919921875\n",
      "epoch 5, val_loss 454.8768310546875\n",
      "epoch 6, train_loss 1277.2119140625\n",
      "epoch 6, val_loss 454.8767395019531\n",
      "epoch 7, train_loss 1277.2120361328125\n",
      "epoch 7, val_loss 454.8766784667969\n",
      "epoch 8, train_loss 1277.2117919921875\n",
      "epoch 8, val_loss 454.8766784667969\n",
      "epoch 9, train_loss 1277.2117919921875\n",
      "epoch 9, val_loss 454.8766174316406\n",
      "epoch 10, train_loss 1277.2115478515625\n",
      "epoch 10, val_loss 454.8765563964844\n",
      "epoch 11, train_loss 1277.2115478515625\n",
      "epoch 11, val_loss 454.8765563964844\n",
      "epoch 12, train_loss 1277.2115478515625\n",
      "epoch 12, val_loss 454.87646484375\n",
      "epoch 13, train_loss 1277.2115478515625\n",
      "epoch 13, val_loss 454.87640380859375\n",
      "epoch 14, train_loss 1277.2113037109375\n",
      "epoch 14, val_loss 454.8763427734375\n",
      "epoch 15, train_loss 1277.21142578125\n",
      "epoch 15, val_loss 454.87628173828125\n",
      "epoch 16, train_loss 1277.2113037109375\n",
      "epoch 16, val_loss 454.876220703125\n",
      "epoch 17, train_loss 1277.211181640625\n",
      "epoch 17, val_loss 454.87615966796875\n",
      "epoch 18, train_loss 1277.2113037109375\n",
      "epoch 18, val_loss 454.8761291503906\n",
      "epoch 19, train_loss 1277.211181640625\n",
      "epoch 19, val_loss 454.8761291503906\n",
      "epoch 20, train_loss 1277.211181640625\n",
      "epoch 20, val_loss 454.8760681152344\n",
      "epoch 21, train_loss 1277.2110595703125\n",
      "epoch 21, val_loss 454.8760070800781\n",
      "epoch 22, train_loss 1277.2110595703125\n",
      "epoch 22, val_loss 454.8760070800781\n",
      "epoch 23, train_loss 1277.2110595703125\n",
      "epoch 23, val_loss 454.8758850097656\n",
      "epoch 24, train_loss 1277.2109375\n",
      "epoch 24, val_loss 454.8758850097656\n",
      "epoch 25, train_loss 1277.2109375\n",
      "epoch 25, val_loss 454.8758239746094\n",
      "epoch 26, train_loss 1277.210693359375\n",
      "epoch 26, val_loss 454.8757019042969\n",
      "epoch 27, train_loss 1277.210693359375\n",
      "epoch 27, val_loss 454.87567138671875\n",
      "epoch 28, train_loss 1277.210693359375\n",
      "epoch 28, val_loss 454.8756103515625\n",
      "epoch 29, train_loss 1277.210693359375\n",
      "epoch 29, val_loss 454.8756103515625\n",
      "epoch 30, train_loss 1277.21044921875\n",
      "epoch 30, val_loss 454.87554931640625\n",
      "epoch 31, train_loss 1277.21044921875\n",
      "epoch 31, val_loss 454.87548828125\n",
      "epoch 32, train_loss 1277.21044921875\n",
      "epoch 32, val_loss 454.8754577636719\n",
      "epoch 33, train_loss 1277.2105712890625\n",
      "epoch 33, val_loss 454.8753662109375\n",
      "epoch 34, train_loss 1277.2103271484375\n",
      "epoch 34, val_loss 454.8753356933594\n",
      "epoch 35, train_loss 1277.2103271484375\n",
      "epoch 35, val_loss 454.8753356933594\n",
      "epoch 36, train_loss 1277.210205078125\n",
      "epoch 36, val_loss 454.875244140625\n",
      "epoch 37, train_loss 1277.210205078125\n",
      "epoch 37, val_loss 454.8751525878906\n",
      "epoch 38, train_loss 1277.210205078125\n",
      "epoch 38, val_loss 454.8751220703125\n",
      "epoch 39, train_loss 1277.2100830078125\n",
      "epoch 39, val_loss 454.8751220703125\n",
      "epoch 40, train_loss 1277.2100830078125\n",
      "epoch 40, val_loss 454.8750305175781\n",
      "epoch 41, train_loss 1277.2099609375\n",
      "epoch 41, val_loss 454.875\n",
      "epoch 42, train_loss 1277.2099609375\n",
      "epoch 42, val_loss 454.8749694824219\n",
      "epoch 43, train_loss 1277.209716796875\n",
      "epoch 43, val_loss 454.8748779296875\n",
      "epoch 44, train_loss 1277.209716796875\n",
      "epoch 44, val_loss 454.8748474121094\n",
      "epoch 45, train_loss 1277.209716796875\n",
      "epoch 45, val_loss 454.8748474121094\n",
      "epoch 46, train_loss 1277.209716796875\n",
      "epoch 46, val_loss 454.874755859375\n",
      "epoch 47, train_loss 1277.209716796875\n",
      "epoch 47, val_loss 454.8746643066406\n",
      "epoch 48, train_loss 1277.209716796875\n",
      "epoch 48, val_loss 454.87457275390625\n",
      "epoch 49, train_loss 1277.209716796875\n",
      "epoch 49, val_loss 454.87451171875\n",
      "epoch 50, train_loss 1277.2095947265625\n",
      "epoch 50, val_loss 454.87451171875\n",
      "epoch 51, train_loss 1277.20947265625\n",
      "epoch 51, val_loss 454.87451171875\n",
      "epoch 52, train_loss 1277.20947265625\n",
      "epoch 52, val_loss 454.8744201660156\n",
      "epoch 53, train_loss 1277.20947265625\n",
      "epoch 53, val_loss 454.87432861328125\n",
      "epoch 54, train_loss 1277.209228515625\n",
      "epoch 54, val_loss 454.87432861328125\n",
      "epoch 55, train_loss 1277.2093505859375\n",
      "epoch 55, val_loss 454.8742980957031\n",
      "epoch 56, train_loss 1277.209228515625\n",
      "epoch 56, val_loss 454.8742370605469\n",
      "epoch 57, train_loss 1277.209228515625\n",
      "epoch 57, val_loss 454.87420654296875\n",
      "epoch 58, train_loss 1277.2091064453125\n",
      "epoch 58, val_loss 454.8741760253906\n",
      "epoch 59, train_loss 1277.2091064453125\n",
      "epoch 59, val_loss 454.87408447265625\n",
      "epoch 60, train_loss 1277.2091064453125\n",
      "epoch 60, val_loss 454.8740539550781\n",
      "epoch 61, train_loss 1277.2091064453125\n",
      "epoch 61, val_loss 454.87396240234375\n",
      "epoch 62, train_loss 1277.2088623046875\n",
      "epoch 62, val_loss 454.8739318847656\n",
      "epoch 63, train_loss 1277.2088623046875\n",
      "epoch 63, val_loss 454.8738708496094\n",
      "epoch 64, train_loss 1277.2088623046875\n",
      "epoch 64, val_loss 454.87384033203125\n",
      "epoch 65, train_loss 1277.208740234375\n",
      "epoch 65, val_loss 454.8737487792969\n",
      "epoch 66, train_loss 1277.208740234375\n",
      "epoch 66, val_loss 454.87371826171875\n",
      "epoch 67, train_loss 1277.2086181640625\n",
      "epoch 67, val_loss 454.8736572265625\n",
      "epoch 68, train_loss 1277.208740234375\n",
      "epoch 68, val_loss 454.8736267089844\n",
      "epoch 69, train_loss 1277.2086181640625\n",
      "epoch 69, val_loss 454.87359619140625\n",
      "epoch 70, train_loss 1277.2083740234375\n",
      "epoch 70, val_loss 454.8735046386719\n",
      "epoch 71, train_loss 1277.2083740234375\n",
      "epoch 71, val_loss 454.8735046386719\n",
      "epoch 72, train_loss 1277.2083740234375\n",
      "epoch 72, val_loss 454.8734130859375\n",
      "epoch 73, train_loss 1277.2083740234375\n",
      "epoch 73, val_loss 454.8733215332031\n",
      "epoch 74, train_loss 1277.2083740234375\n",
      "epoch 74, val_loss 454.873291015625\n",
      "epoch 75, train_loss 1277.2083740234375\n",
      "epoch 75, val_loss 454.873291015625\n",
      "epoch 76, train_loss 1277.2081298828125\n",
      "epoch 76, val_loss 454.8731689453125\n",
      "epoch 77, train_loss 1277.2081298828125\n",
      "epoch 77, val_loss 454.8731689453125\n",
      "epoch 78, train_loss 1277.2081298828125\n",
      "epoch 78, val_loss 454.8731689453125\n",
      "epoch 79, train_loss 1277.2080078125\n",
      "epoch 79, val_loss 454.873046875\n",
      "epoch 80, train_loss 1277.2081298828125\n",
      "epoch 80, val_loss 454.873046875\n",
      "epoch 81, train_loss 1277.2080078125\n",
      "epoch 81, val_loss 454.87298583984375\n",
      "epoch 82, train_loss 1277.2078857421875\n",
      "epoch 82, val_loss 454.8729248046875\n",
      "epoch 83, train_loss 1277.2076416015625\n",
      "epoch 83, val_loss 454.8728332519531\n",
      "epoch 84, train_loss 1277.207763671875\n",
      "epoch 84, val_loss 454.8728332519531\n",
      "epoch 85, train_loss 1277.207763671875\n",
      "epoch 85, val_loss 454.872802734375\n",
      "epoch 86, train_loss 1277.207763671875\n",
      "epoch 86, val_loss 454.8727111816406\n",
      "epoch 87, train_loss 1277.207763671875\n",
      "epoch 87, val_loss 454.8727111816406\n",
      "epoch 88, train_loss 1277.20751953125\n",
      "epoch 88, val_loss 454.87261962890625\n",
      "epoch 89, train_loss 1277.2073974609375\n",
      "epoch 89, val_loss 454.8725891113281\n",
      "epoch 90, train_loss 1277.2073974609375\n",
      "epoch 90, val_loss 454.87249755859375\n",
      "epoch 91, train_loss 1277.20751953125\n",
      "epoch 91, val_loss 454.87249755859375\n",
      "epoch 92, train_loss 1277.207275390625\n",
      "epoch 92, val_loss 454.8724060058594\n",
      "epoch 93, train_loss 1277.207275390625\n",
      "epoch 93, val_loss 454.8723449707031\n",
      "epoch 94, train_loss 1277.2071533203125\n",
      "epoch 94, val_loss 454.8723449707031\n",
      "epoch 95, train_loss 1277.2071533203125\n",
      "epoch 95, val_loss 454.87225341796875\n",
      "epoch 96, train_loss 1277.20703125\n",
      "epoch 96, val_loss 454.87225341796875\n",
      "epoch 97, train_loss 1277.2071533203125\n",
      "epoch 97, val_loss 454.8722229003906\n",
      "epoch 98, train_loss 1277.20703125\n",
      "epoch 98, val_loss 454.8720703125\n",
      "epoch 99, train_loss 1277.2071533203125\n",
      "epoch 99, val_loss 454.87200927734375\n",
      "Parameter containing:\n",
      "tensor([4.0960e-23], requires_grad=True)\n",
      "iter 121, train_loss_regularization 0.7220621109008789\n",
      "iter 121, val_loss_regularization 0.7220621109008789\n",
      "epoch 0, train_loss 1277.2069091796875\n",
      "epoch 0, val_loss 454.8720397949219\n",
      "epoch 1, train_loss 1277.2069091796875\n",
      "epoch 1, val_loss 454.8719482421875\n",
      "epoch 2, train_loss 1277.206787109375\n",
      "epoch 2, val_loss 454.87188720703125\n",
      "epoch 3, train_loss 1277.206787109375\n",
      "epoch 3, val_loss 454.87188720703125\n",
      "epoch 4, train_loss 1277.2069091796875\n",
      "epoch 4, val_loss 454.871826171875\n",
      "epoch 5, train_loss 1277.206787109375\n",
      "epoch 5, val_loss 454.87176513671875\n",
      "epoch 6, train_loss 1277.20654296875\n",
      "epoch 6, val_loss 454.8716125488281\n",
      "epoch 7, train_loss 1277.2066650390625\n",
      "epoch 7, val_loss 454.8716735839844\n",
      "epoch 8, train_loss 1277.20654296875\n",
      "epoch 8, val_loss 454.87158203125\n",
      "epoch 9, train_loss 1277.20654296875\n",
      "epoch 9, val_loss 454.8715515136719\n",
      "epoch 10, train_loss 1277.20654296875\n",
      "epoch 10, val_loss 454.8715515136719\n",
      "epoch 11, train_loss 1277.20654296875\n",
      "epoch 11, val_loss 454.8714904785156\n",
      "epoch 12, train_loss 1277.20654296875\n",
      "epoch 12, val_loss 454.8714294433594\n",
      "epoch 13, train_loss 1277.206298828125\n",
      "epoch 13, val_loss 454.8713684082031\n",
      "epoch 14, train_loss 1277.206298828125\n",
      "epoch 14, val_loss 454.871337890625\n",
      "epoch 15, train_loss 1277.2061767578125\n",
      "epoch 15, val_loss 454.87127685546875\n",
      "epoch 16, train_loss 1277.2061767578125\n",
      "epoch 16, val_loss 454.87115478515625\n",
      "epoch 17, train_loss 1277.2061767578125\n",
      "epoch 17, val_loss 454.8711242675781\n",
      "epoch 18, train_loss 1277.2060546875\n",
      "epoch 18, val_loss 454.8711242675781\n",
      "epoch 19, train_loss 1277.2059326171875\n",
      "epoch 19, val_loss 454.8710021972656\n",
      "epoch 20, train_loss 1277.2060546875\n",
      "epoch 20, val_loss 454.87103271484375\n",
      "epoch 21, train_loss 1277.2060546875\n",
      "epoch 21, val_loss 454.8710021972656\n",
      "epoch 22, train_loss 1277.2059326171875\n",
      "epoch 22, val_loss 454.87091064453125\n",
      "epoch 23, train_loss 1277.205810546875\n",
      "epoch 23, val_loss 454.87091064453125\n",
      "epoch 24, train_loss 1277.2056884765625\n",
      "epoch 24, val_loss 454.8708190917969\n",
      "epoch 25, train_loss 1277.2056884765625\n",
      "epoch 25, val_loss 454.8707580566406\n",
      "epoch 26, train_loss 1277.2056884765625\n",
      "epoch 26, val_loss 454.8706970214844\n",
      "epoch 27, train_loss 1277.2056884765625\n",
      "epoch 27, val_loss 454.87066650390625\n",
      "epoch 28, train_loss 1277.2054443359375\n",
      "epoch 28, val_loss 454.8705749511719\n",
      "epoch 29, train_loss 1277.2054443359375\n",
      "epoch 29, val_loss 454.87054443359375\n",
      "epoch 30, train_loss 1277.2054443359375\n",
      "epoch 30, val_loss 454.8705139160156\n",
      "epoch 31, train_loss 1277.2054443359375\n",
      "epoch 31, val_loss 454.8704528808594\n",
      "epoch 32, train_loss 1277.2054443359375\n",
      "epoch 32, val_loss 454.8704528808594\n",
      "epoch 33, train_loss 1277.2054443359375\n",
      "epoch 33, val_loss 454.870361328125\n",
      "epoch 34, train_loss 1277.2052001953125\n",
      "epoch 34, val_loss 454.8703308105469\n",
      "epoch 35, train_loss 1277.2052001953125\n",
      "epoch 35, val_loss 454.8702392578125\n",
      "epoch 36, train_loss 1277.2049560546875\n",
      "epoch 36, val_loss 454.8702392578125\n",
      "epoch 37, train_loss 1277.2049560546875\n",
      "epoch 37, val_loss 454.8702087402344\n",
      "epoch 38, train_loss 1277.2049560546875\n",
      "epoch 38, val_loss 454.8701171875\n",
      "epoch 39, train_loss 1277.2049560546875\n",
      "epoch 39, val_loss 454.8700866699219\n",
      "epoch 40, train_loss 1277.2049560546875\n",
      "epoch 40, val_loss 454.8699951171875\n",
      "epoch 41, train_loss 1277.2047119140625\n",
      "epoch 41, val_loss 454.869873046875\n",
      "epoch 42, train_loss 1277.204833984375\n",
      "epoch 42, val_loss 454.869873046875\n",
      "epoch 43, train_loss 1277.2047119140625\n",
      "epoch 43, val_loss 454.869873046875\n",
      "epoch 44, train_loss 1277.2047119140625\n",
      "epoch 44, val_loss 454.8698425292969\n",
      "epoch 45, train_loss 1277.20458984375\n",
      "epoch 45, val_loss 454.8697509765625\n",
      "epoch 46, train_loss 1277.2047119140625\n",
      "epoch 46, val_loss 454.8697509765625\n",
      "epoch 47, train_loss 1277.2047119140625\n",
      "epoch 47, val_loss 454.8696594238281\n",
      "epoch 48, train_loss 1277.2044677734375\n",
      "epoch 48, val_loss 454.8695983886719\n",
      "epoch 49, train_loss 1277.2044677734375\n",
      "epoch 49, val_loss 454.8695983886719\n",
      "epoch 50, train_loss 1277.2044677734375\n",
      "epoch 50, val_loss 454.8695068359375\n",
      "epoch 51, train_loss 1277.2044677734375\n",
      "epoch 51, val_loss 454.8694152832031\n",
      "epoch 52, train_loss 1277.204345703125\n",
      "epoch 52, val_loss 454.869384765625\n",
      "epoch 53, train_loss 1277.2044677734375\n",
      "epoch 53, val_loss 454.86932373046875\n",
      "epoch 54, train_loss 1277.204345703125\n",
      "epoch 54, val_loss 454.86932373046875\n",
      "epoch 55, train_loss 1277.2041015625\n",
      "epoch 55, val_loss 454.8692626953125\n",
      "epoch 56, train_loss 1277.2041015625\n",
      "epoch 56, val_loss 454.86920166015625\n",
      "epoch 57, train_loss 1277.2041015625\n",
      "epoch 57, val_loss 454.8691101074219\n",
      "epoch 58, train_loss 1277.203857421875\n",
      "epoch 58, val_loss 454.8691101074219\n",
      "epoch 59, train_loss 1277.2041015625\n",
      "epoch 59, val_loss 454.86907958984375\n",
      "epoch 60, train_loss 1277.2039794921875\n",
      "epoch 60, val_loss 454.8689880371094\n",
      "epoch 61, train_loss 1277.203857421875\n",
      "epoch 61, val_loss 454.8689880371094\n",
      "epoch 62, train_loss 1277.203857421875\n",
      "epoch 62, val_loss 454.8688659667969\n",
      "epoch 63, train_loss 1277.203857421875\n",
      "epoch 63, val_loss 454.86883544921875\n",
      "epoch 64, train_loss 1277.203857421875\n",
      "epoch 64, val_loss 454.8688049316406\n",
      "epoch 65, train_loss 1277.203857421875\n",
      "epoch 65, val_loss 454.8687438964844\n",
      "epoch 66, train_loss 1277.20361328125\n",
      "epoch 66, val_loss 454.86865234375\n",
      "epoch 67, train_loss 1277.2034912109375\n",
      "epoch 67, val_loss 454.86865234375\n",
      "epoch 68, train_loss 1277.203369140625\n",
      "epoch 68, val_loss 454.86865234375\n",
      "epoch 69, train_loss 1277.2034912109375\n",
      "epoch 69, val_loss 454.8685302734375\n",
      "epoch 70, train_loss 1277.203369140625\n",
      "epoch 70, val_loss 454.8685302734375\n",
      "epoch 71, train_loss 1277.203369140625\n",
      "epoch 71, val_loss 454.86846923828125\n",
      "epoch 72, train_loss 1277.203369140625\n",
      "epoch 72, val_loss 454.86846923828125\n",
      "epoch 73, train_loss 1277.203369140625\n",
      "epoch 73, val_loss 454.868408203125\n",
      "epoch 74, train_loss 1277.2032470703125\n",
      "epoch 74, val_loss 454.8682556152344\n",
      "epoch 75, train_loss 1277.203125\n",
      "epoch 75, val_loss 454.8681945800781\n",
      "epoch 76, train_loss 1277.2030029296875\n",
      "epoch 76, val_loss 454.8681945800781\n",
      "epoch 77, train_loss 1277.203125\n",
      "epoch 77, val_loss 454.8681640625\n",
      "epoch 78, train_loss 1277.203125\n",
      "epoch 78, val_loss 454.8680419921875\n",
      "epoch 79, train_loss 1277.2030029296875\n",
      "epoch 79, val_loss 454.8680419921875\n",
      "epoch 80, train_loss 1277.202880859375\n",
      "epoch 80, val_loss 454.8680419921875\n",
      "epoch 81, train_loss 1277.2027587890625\n",
      "epoch 81, val_loss 454.867919921875\n",
      "epoch 82, train_loss 1277.2027587890625\n",
      "epoch 82, val_loss 454.8679504394531\n",
      "epoch 83, train_loss 1277.2027587890625\n",
      "epoch 83, val_loss 454.8678894042969\n",
      "epoch 84, train_loss 1277.2027587890625\n",
      "epoch 84, val_loss 454.8678894042969\n",
      "epoch 85, train_loss 1277.20263671875\n",
      "epoch 85, val_loss 454.86773681640625\n",
      "epoch 86, train_loss 1277.2025146484375\n",
      "epoch 86, val_loss 454.8677062988281\n",
      "epoch 87, train_loss 1277.2025146484375\n",
      "epoch 87, val_loss 454.86761474609375\n",
      "epoch 88, train_loss 1277.202392578125\n",
      "epoch 88, val_loss 454.8675842285156\n",
      "epoch 89, train_loss 1277.202392578125\n",
      "epoch 89, val_loss 454.8675537109375\n",
      "epoch 90, train_loss 1277.202392578125\n",
      "epoch 90, val_loss 454.8675537109375\n",
      "epoch 91, train_loss 1277.2022705078125\n",
      "epoch 91, val_loss 454.8674621582031\n",
      "epoch 92, train_loss 1277.202392578125\n",
      "epoch 92, val_loss 454.867431640625\n",
      "epoch 93, train_loss 1277.2022705078125\n",
      "epoch 93, val_loss 454.86737060546875\n",
      "epoch 94, train_loss 1277.2022705078125\n",
      "epoch 94, val_loss 454.8672790527344\n",
      "epoch 95, train_loss 1277.2022705078125\n",
      "epoch 95, val_loss 454.8672790527344\n",
      "epoch 96, train_loss 1277.2021484375\n",
      "epoch 96, val_loss 454.86724853515625\n",
      "epoch 97, train_loss 1277.2020263671875\n",
      "epoch 97, val_loss 454.8671569824219\n",
      "epoch 98, train_loss 1277.2020263671875\n",
      "epoch 98, val_loss 454.86712646484375\n",
      "epoch 99, train_loss 1277.201904296875\n",
      "epoch 99, val_loss 454.8670959472656\n",
      "Parameter containing:\n",
      "tensor([2.8315e-23], requires_grad=True)\n",
      "iter 122, train_loss_regularization 0.7208390235900879\n",
      "iter 122, val_loss_regularization 0.7208390235900879\n",
      "epoch 0, train_loss 1277.2017822265625\n",
      "epoch 0, val_loss 454.86700439453125\n",
      "epoch 1, train_loss 1277.2017822265625\n",
      "epoch 1, val_loss 454.8669128417969\n",
      "epoch 2, train_loss 1277.2017822265625\n",
      "epoch 2, val_loss 454.8669128417969\n",
      "epoch 3, train_loss 1277.2017822265625\n",
      "epoch 3, val_loss 454.86688232421875\n",
      "epoch 4, train_loss 1277.2017822265625\n",
      "epoch 4, val_loss 454.86688232421875\n",
      "epoch 5, train_loss 1277.20166015625\n",
      "epoch 5, val_loss 454.8667907714844\n",
      "epoch 6, train_loss 1277.2015380859375\n",
      "epoch 6, val_loss 454.86676025390625\n",
      "epoch 7, train_loss 1277.2015380859375\n",
      "epoch 7, val_loss 454.86669921875\n",
      "epoch 8, train_loss 1277.201416015625\n",
      "epoch 8, val_loss 454.86663818359375\n",
      "epoch 9, train_loss 1277.2015380859375\n",
      "epoch 9, val_loss 454.8665466308594\n",
      "epoch 10, train_loss 1277.2012939453125\n",
      "epoch 10, val_loss 454.8664855957031\n",
      "epoch 11, train_loss 1277.2012939453125\n",
      "epoch 11, val_loss 454.866455078125\n",
      "epoch 12, train_loss 1277.2012939453125\n",
      "epoch 12, val_loss 454.8664245605469\n",
      "epoch 13, train_loss 1277.201171875\n",
      "epoch 13, val_loss 454.8663635253906\n",
      "epoch 14, train_loss 1277.2012939453125\n",
      "epoch 14, val_loss 454.8663024902344\n",
      "epoch 15, train_loss 1277.201171875\n",
      "epoch 15, val_loss 454.8663024902344\n",
      "epoch 16, train_loss 1277.2010498046875\n",
      "epoch 16, val_loss 454.8662109375\n",
      "epoch 17, train_loss 1277.2010498046875\n",
      "epoch 17, val_loss 454.8661804199219\n",
      "epoch 18, train_loss 1277.2010498046875\n",
      "epoch 18, val_loss 454.8661193847656\n",
      "epoch 19, train_loss 1277.200927734375\n",
      "epoch 19, val_loss 454.8660888671875\n",
      "epoch 20, train_loss 1277.200927734375\n",
      "epoch 20, val_loss 454.8659973144531\n",
      "epoch 21, train_loss 1277.2008056640625\n",
      "epoch 21, val_loss 454.865966796875\n",
      "epoch 22, train_loss 1277.2008056640625\n",
      "epoch 22, val_loss 454.86590576171875\n",
      "epoch 23, train_loss 1277.20068359375\n",
      "epoch 23, val_loss 454.8658752441406\n",
      "epoch 24, train_loss 1277.20068359375\n",
      "epoch 24, val_loss 454.86578369140625\n",
      "epoch 25, train_loss 1277.2005615234375\n",
      "epoch 25, val_loss 454.86572265625\n",
      "epoch 26, train_loss 1277.2005615234375\n",
      "epoch 26, val_loss 454.86572265625\n",
      "epoch 27, train_loss 1277.200439453125\n",
      "epoch 27, val_loss 454.86566162109375\n",
      "epoch 28, train_loss 1277.200439453125\n",
      "epoch 28, val_loss 454.8656311035156\n",
      "epoch 29, train_loss 1277.2001953125\n",
      "epoch 29, val_loss 454.8655700683594\n",
      "epoch 30, train_loss 1277.2003173828125\n",
      "epoch 30, val_loss 454.86553955078125\n",
      "epoch 31, train_loss 1277.2003173828125\n",
      "epoch 31, val_loss 454.86541748046875\n",
      "epoch 32, train_loss 1277.2003173828125\n",
      "epoch 32, val_loss 454.86541748046875\n",
      "epoch 33, train_loss 1277.2001953125\n",
      "epoch 33, val_loss 454.86541748046875\n",
      "epoch 34, train_loss 1277.199951171875\n",
      "epoch 34, val_loss 454.8653259277344\n",
      "epoch 35, train_loss 1277.2000732421875\n",
      "epoch 35, val_loss 454.8652648925781\n",
      "epoch 36, train_loss 1277.2000732421875\n",
      "epoch 36, val_loss 454.8652648925781\n",
      "epoch 37, train_loss 1277.2000732421875\n",
      "epoch 37, val_loss 454.86517333984375\n",
      "epoch 38, train_loss 1277.199951171875\n",
      "epoch 38, val_loss 454.8651123046875\n",
      "epoch 39, train_loss 1277.199951171875\n",
      "epoch 39, val_loss 454.8650817871094\n",
      "epoch 40, train_loss 1277.19970703125\n",
      "epoch 40, val_loss 454.8650817871094\n",
      "epoch 41, train_loss 1277.19970703125\n",
      "epoch 41, val_loss 454.864990234375\n",
      "epoch 42, train_loss 1277.1998291015625\n",
      "epoch 42, val_loss 454.86492919921875\n",
      "epoch 43, train_loss 1277.19970703125\n",
      "epoch 43, val_loss 454.8648376464844\n",
      "epoch 44, train_loss 1277.19970703125\n",
      "epoch 44, val_loss 454.8647766113281\n",
      "epoch 45, train_loss 1277.1998291015625\n",
      "epoch 45, val_loss 454.86474609375\n",
      "epoch 46, train_loss 1277.1995849609375\n",
      "epoch 46, val_loss 454.8647155761719\n",
      "epoch 47, train_loss 1277.19970703125\n",
      "epoch 47, val_loss 454.8646545410156\n",
      "epoch 48, train_loss 1277.199462890625\n",
      "epoch 48, val_loss 454.8646545410156\n",
      "epoch 49, train_loss 1277.199462890625\n",
      "epoch 49, val_loss 454.8646240234375\n",
      "epoch 50, train_loss 1277.199462890625\n",
      "epoch 50, val_loss 454.864501953125\n",
      "epoch 51, train_loss 1277.1993408203125\n",
      "epoch 51, val_loss 454.864501953125\n",
      "epoch 52, train_loss 1277.19921875\n",
      "epoch 52, val_loss 454.8644104003906\n",
      "epoch 53, train_loss 1277.19921875\n",
      "epoch 53, val_loss 454.8644104003906\n",
      "epoch 54, train_loss 1277.19921875\n",
      "epoch 54, val_loss 454.86431884765625\n",
      "epoch 55, train_loss 1277.1990966796875\n",
      "epoch 55, val_loss 454.8642883300781\n",
      "epoch 56, train_loss 1277.19921875\n",
      "epoch 56, val_loss 454.86419677734375\n",
      "epoch 57, train_loss 1277.1990966796875\n",
      "epoch 57, val_loss 454.8641662597656\n",
      "epoch 58, train_loss 1277.198974609375\n",
      "epoch 58, val_loss 454.8641662597656\n",
      "epoch 59, train_loss 1277.198974609375\n",
      "epoch 59, val_loss 454.8640441894531\n",
      "epoch 60, train_loss 1277.198974609375\n",
      "epoch 60, val_loss 454.8640441894531\n",
      "epoch 61, train_loss 1277.1988525390625\n",
      "epoch 61, val_loss 454.86395263671875\n",
      "epoch 62, train_loss 1277.1988525390625\n",
      "epoch 62, val_loss 454.8639221191406\n",
      "epoch 63, train_loss 1277.1986083984375\n",
      "epoch 63, val_loss 454.86395263671875\n",
      "epoch 64, train_loss 1277.1986083984375\n",
      "epoch 64, val_loss 454.8638610839844\n",
      "epoch 65, train_loss 1277.1986083984375\n",
      "epoch 65, val_loss 454.8638000488281\n",
      "epoch 66, train_loss 1277.198486328125\n",
      "epoch 66, val_loss 454.8637390136719\n",
      "epoch 67, train_loss 1277.1986083984375\n",
      "epoch 67, val_loss 454.8636169433594\n",
      "epoch 68, train_loss 1277.1986083984375\n",
      "epoch 68, val_loss 454.8636169433594\n",
      "epoch 69, train_loss 1277.198486328125\n",
      "epoch 69, val_loss 454.86358642578125\n",
      "epoch 70, train_loss 1277.1983642578125\n",
      "epoch 70, val_loss 454.8635559082031\n",
      "epoch 71, train_loss 1277.1983642578125\n",
      "epoch 71, val_loss 454.86346435546875\n",
      "epoch 72, train_loss 1277.1983642578125\n",
      "epoch 72, val_loss 454.86346435546875\n",
      "epoch 73, train_loss 1277.1982421875\n",
      "epoch 73, val_loss 454.8634033203125\n",
      "epoch 74, train_loss 1277.1982421875\n",
      "epoch 74, val_loss 454.8633728027344\n",
      "epoch 75, train_loss 1277.1981201171875\n",
      "epoch 75, val_loss 454.86328125\n",
      "epoch 76, train_loss 1277.197998046875\n",
      "epoch 76, val_loss 454.8632507324219\n",
      "epoch 77, train_loss 1277.197998046875\n",
      "epoch 77, val_loss 454.8631591796875\n",
      "epoch 78, train_loss 1277.197998046875\n",
      "epoch 78, val_loss 454.8631286621094\n",
      "epoch 79, train_loss 1277.1978759765625\n",
      "epoch 79, val_loss 454.86309814453125\n",
      "epoch 80, train_loss 1277.1978759765625\n",
      "epoch 80, val_loss 454.86309814453125\n",
      "epoch 81, train_loss 1277.1978759765625\n",
      "epoch 81, val_loss 454.8629455566406\n",
      "epoch 82, train_loss 1277.19775390625\n",
      "epoch 82, val_loss 454.8629455566406\n",
      "epoch 83, train_loss 1277.1976318359375\n",
      "epoch 83, val_loss 454.8629150390625\n",
      "epoch 84, train_loss 1277.1976318359375\n",
      "epoch 84, val_loss 454.8628234863281\n",
      "epoch 85, train_loss 1277.1976318359375\n",
      "epoch 85, val_loss 454.86279296875\n",
      "epoch 86, train_loss 1277.197509765625\n",
      "epoch 86, val_loss 454.8627624511719\n",
      "epoch 87, train_loss 1277.1976318359375\n",
      "epoch 87, val_loss 454.8627624511719\n",
      "epoch 88, train_loss 1277.1973876953125\n",
      "epoch 88, val_loss 454.8626708984375\n",
      "epoch 89, train_loss 1277.197509765625\n",
      "epoch 89, val_loss 454.8625793457031\n",
      "epoch 90, train_loss 1277.197509765625\n",
      "epoch 90, val_loss 454.862548828125\n",
      "epoch 91, train_loss 1277.197509765625\n",
      "epoch 91, val_loss 454.8624572753906\n",
      "epoch 92, train_loss 1277.197265625\n",
      "epoch 92, val_loss 454.8624572753906\n",
      "epoch 93, train_loss 1277.197265625\n",
      "epoch 93, val_loss 454.86236572265625\n",
      "epoch 94, train_loss 1277.1971435546875\n",
      "epoch 94, val_loss 454.8623352050781\n",
      "epoch 95, train_loss 1277.1971435546875\n",
      "epoch 95, val_loss 454.8623046875\n",
      "epoch 96, train_loss 1277.1971435546875\n",
      "epoch 96, val_loss 454.86224365234375\n",
      "epoch 97, train_loss 1277.1971435546875\n",
      "epoch 97, val_loss 454.86224365234375\n",
      "epoch 98, train_loss 1277.1968994140625\n",
      "epoch 98, val_loss 454.8622131347656\n",
      "epoch 99, train_loss 1277.19677734375\n",
      "epoch 99, val_loss 454.8621520996094\n",
      "Parameter containing:\n",
      "tensor([1.9581e-23], requires_grad=True)\n",
      "iter 123, train_loss_regularization 0.7196426391601562\n",
      "iter 123, val_loss_regularization 0.7196426391601562\n",
      "epoch 0, train_loss 1277.1968994140625\n",
      "epoch 0, val_loss 454.86212158203125\n",
      "epoch 1, train_loss 1277.1968994140625\n",
      "epoch 1, val_loss 454.8619079589844\n",
      "epoch 2, train_loss 1277.19677734375\n",
      "epoch 2, val_loss 454.8619689941406\n",
      "epoch 3, train_loss 1277.19677734375\n",
      "epoch 3, val_loss 454.8619079589844\n",
      "epoch 4, train_loss 1277.19677734375\n",
      "epoch 4, val_loss 454.8618469238281\n",
      "epoch 5, train_loss 1277.1966552734375\n",
      "epoch 5, val_loss 454.8617858886719\n",
      "epoch 6, train_loss 1277.196533203125\n",
      "epoch 6, val_loss 454.86175537109375\n",
      "epoch 7, train_loss 1277.196533203125\n",
      "epoch 7, val_loss 454.8616943359375\n",
      "epoch 8, train_loss 1277.196533203125\n",
      "epoch 8, val_loss 454.8616638183594\n",
      "epoch 9, train_loss 1277.196533203125\n",
      "epoch 9, val_loss 454.8616638183594\n",
      "epoch 10, train_loss 1277.1962890625\n",
      "epoch 10, val_loss 454.861572265625\n",
      "epoch 11, train_loss 1277.1962890625\n",
      "epoch 11, val_loss 454.86151123046875\n",
      "epoch 12, train_loss 1277.1962890625\n",
      "epoch 12, val_loss 454.86151123046875\n",
      "epoch 13, train_loss 1277.1961669921875\n",
      "epoch 13, val_loss 454.8614196777344\n",
      "epoch 14, train_loss 1277.196044921875\n",
      "epoch 14, val_loss 454.86138916015625\n",
      "epoch 15, train_loss 1277.196044921875\n",
      "epoch 15, val_loss 454.861328125\n",
      "epoch 16, train_loss 1277.196044921875\n",
      "epoch 16, val_loss 454.8612365722656\n",
      "epoch 17, train_loss 1277.196044921875\n",
      "epoch 17, val_loss 454.8612365722656\n",
      "epoch 18, train_loss 1277.196044921875\n",
      "epoch 18, val_loss 454.8612060546875\n",
      "epoch 19, train_loss 1277.196044921875\n",
      "epoch 19, val_loss 454.861083984375\n",
      "epoch 20, train_loss 1277.19580078125\n",
      "epoch 20, val_loss 454.8611145019531\n",
      "epoch 21, train_loss 1277.19580078125\n",
      "epoch 21, val_loss 454.8610534667969\n",
      "epoch 22, train_loss 1277.19580078125\n",
      "epoch 22, val_loss 454.8609924316406\n",
      "epoch 23, train_loss 1277.19580078125\n",
      "epoch 23, val_loss 454.8609313964844\n",
      "epoch 24, train_loss 1277.19580078125\n",
      "epoch 24, val_loss 454.8609313964844\n",
      "epoch 25, train_loss 1277.1956787109375\n",
      "epoch 25, val_loss 454.86083984375\n",
      "epoch 26, train_loss 1277.1954345703125\n",
      "epoch 26, val_loss 454.8607482910156\n",
      "epoch 27, train_loss 1277.195556640625\n",
      "epoch 27, val_loss 454.8607482910156\n",
      "epoch 28, train_loss 1277.195556640625\n",
      "epoch 28, val_loss 454.8606262207031\n",
      "epoch 29, train_loss 1277.1954345703125\n",
      "epoch 29, val_loss 454.8606262207031\n",
      "epoch 30, train_loss 1277.1954345703125\n",
      "epoch 30, val_loss 454.860595703125\n",
      "epoch 31, train_loss 1277.1954345703125\n",
      "epoch 31, val_loss 454.86053466796875\n",
      "epoch 32, train_loss 1277.1951904296875\n",
      "epoch 32, val_loss 454.8605041503906\n",
      "epoch 33, train_loss 1277.1951904296875\n",
      "epoch 33, val_loss 454.8604431152344\n",
      "epoch 34, train_loss 1277.1951904296875\n",
      "epoch 34, val_loss 454.86041259765625\n",
      "epoch 35, train_loss 1277.195068359375\n",
      "epoch 35, val_loss 454.8603210449219\n",
      "epoch 36, train_loss 1277.1951904296875\n",
      "epoch 36, val_loss 454.8602600097656\n",
      "epoch 37, train_loss 1277.1951904296875\n",
      "epoch 37, val_loss 454.8602600097656\n",
      "epoch 38, train_loss 1277.195068359375\n",
      "epoch 38, val_loss 454.86016845703125\n",
      "epoch 39, train_loss 1277.1949462890625\n",
      "epoch 39, val_loss 454.86016845703125\n",
      "epoch 40, train_loss 1277.1947021484375\n",
      "epoch 40, val_loss 454.8600769042969\n",
      "epoch 41, train_loss 1277.19482421875\n",
      "epoch 41, val_loss 454.86004638671875\n",
      "epoch 42, train_loss 1277.19482421875\n",
      "epoch 42, val_loss 454.8599853515625\n",
      "epoch 43, train_loss 1277.19482421875\n",
      "epoch 43, val_loss 454.8599548339844\n",
      "epoch 44, train_loss 1277.19482421875\n",
      "epoch 44, val_loss 454.85992431640625\n",
      "epoch 45, train_loss 1277.1947021484375\n",
      "epoch 45, val_loss 454.8598327636719\n",
      "epoch 46, train_loss 1277.1947021484375\n",
      "epoch 46, val_loss 454.8598327636719\n",
      "epoch 47, train_loss 1277.1944580078125\n",
      "epoch 47, val_loss 454.85980224609375\n",
      "epoch 48, train_loss 1277.1944580078125\n",
      "epoch 48, val_loss 454.85968017578125\n",
      "epoch 49, train_loss 1277.1944580078125\n",
      "epoch 49, val_loss 454.85968017578125\n",
      "epoch 50, train_loss 1277.1944580078125\n",
      "epoch 50, val_loss 454.859619140625\n",
      "epoch 51, train_loss 1277.1944580078125\n",
      "epoch 51, val_loss 454.8595275878906\n",
      "epoch 52, train_loss 1277.1942138671875\n",
      "epoch 52, val_loss 454.8594970703125\n",
      "epoch 53, train_loss 1277.1942138671875\n",
      "epoch 53, val_loss 454.8594665527344\n",
      "epoch 54, train_loss 1277.1942138671875\n",
      "epoch 54, val_loss 454.8594055175781\n",
      "epoch 55, train_loss 1277.1943359375\n",
      "epoch 55, val_loss 454.8593444824219\n",
      "epoch 56, train_loss 1277.1942138671875\n",
      "epoch 56, val_loss 454.859375\n",
      "epoch 57, train_loss 1277.1942138671875\n",
      "epoch 57, val_loss 454.8592529296875\n",
      "epoch 58, train_loss 1277.19384765625\n",
      "epoch 58, val_loss 454.8592529296875\n",
      "epoch 59, train_loss 1277.1939697265625\n",
      "epoch 59, val_loss 454.859130859375\n",
      "epoch 60, train_loss 1277.1939697265625\n",
      "epoch 60, val_loss 454.859130859375\n",
      "epoch 61, train_loss 1277.19384765625\n",
      "epoch 61, val_loss 454.85906982421875\n",
      "epoch 62, train_loss 1277.19384765625\n",
      "epoch 62, val_loss 454.8590087890625\n",
      "epoch 63, train_loss 1277.1937255859375\n",
      "epoch 63, val_loss 454.85894775390625\n",
      "epoch 64, train_loss 1277.1937255859375\n",
      "epoch 64, val_loss 454.85894775390625\n",
      "epoch 65, train_loss 1277.193603515625\n",
      "epoch 65, val_loss 454.85888671875\n",
      "epoch 66, train_loss 1277.193603515625\n",
      "epoch 66, val_loss 454.8587951660156\n",
      "epoch 67, train_loss 1277.193603515625\n",
      "epoch 67, val_loss 454.8587951660156\n",
      "epoch 68, train_loss 1277.193603515625\n",
      "epoch 68, val_loss 454.8587646484375\n",
      "epoch 69, train_loss 1277.193359375\n",
      "epoch 69, val_loss 454.85870361328125\n",
      "epoch 70, train_loss 1277.193359375\n",
      "epoch 70, val_loss 454.8586120605469\n",
      "epoch 71, train_loss 1277.193359375\n",
      "epoch 71, val_loss 454.85858154296875\n",
      "epoch 72, train_loss 1277.193359375\n",
      "epoch 72, val_loss 454.8584899902344\n",
      "epoch 73, train_loss 1277.193359375\n",
      "epoch 73, val_loss 454.85845947265625\n",
      "epoch 74, train_loss 1277.1932373046875\n",
      "epoch 74, val_loss 454.85845947265625\n",
      "epoch 75, train_loss 1277.193115234375\n",
      "epoch 75, val_loss 454.8583679199219\n",
      "epoch 76, train_loss 1277.193115234375\n",
      "epoch 76, val_loss 454.85833740234375\n",
      "epoch 77, train_loss 1277.193115234375\n",
      "epoch 77, val_loss 454.85833740234375\n",
      "epoch 78, train_loss 1277.193115234375\n",
      "epoch 78, val_loss 454.8582458496094\n",
      "epoch 79, train_loss 1277.19287109375\n",
      "epoch 79, val_loss 454.858154296875\n",
      "epoch 80, train_loss 1277.19287109375\n",
      "epoch 80, val_loss 454.858154296875\n",
      "epoch 81, train_loss 1277.19287109375\n",
      "epoch 81, val_loss 454.85809326171875\n",
      "epoch 82, train_loss 1277.19287109375\n",
      "epoch 82, val_loss 454.8580017089844\n",
      "epoch 83, train_loss 1277.192626953125\n",
      "epoch 83, val_loss 454.85797119140625\n",
      "epoch 84, train_loss 1277.192626953125\n",
      "epoch 84, val_loss 454.85797119140625\n",
      "epoch 85, train_loss 1277.192626953125\n",
      "epoch 85, val_loss 454.8578796386719\n",
      "epoch 86, train_loss 1277.192626953125\n",
      "epoch 86, val_loss 454.8578796386719\n",
      "epoch 87, train_loss 1277.192626953125\n",
      "epoch 87, val_loss 454.8577575683594\n",
      "epoch 88, train_loss 1277.192626953125\n",
      "epoch 88, val_loss 454.8576965332031\n",
      "epoch 89, train_loss 1277.192626953125\n",
      "epoch 89, val_loss 454.8576965332031\n",
      "epoch 90, train_loss 1277.192626953125\n",
      "epoch 90, val_loss 454.857666015625\n",
      "epoch 91, train_loss 1277.1923828125\n",
      "epoch 91, val_loss 454.857666015625\n",
      "epoch 92, train_loss 1277.1923828125\n",
      "epoch 92, val_loss 454.8575439453125\n",
      "epoch 93, train_loss 1277.1923828125\n",
      "epoch 93, val_loss 454.8575744628906\n",
      "epoch 94, train_loss 1277.192138671875\n",
      "epoch 94, val_loss 454.8575134277344\n",
      "epoch 95, train_loss 1277.1922607421875\n",
      "epoch 95, val_loss 454.857421875\n",
      "epoch 96, train_loss 1277.192138671875\n",
      "epoch 96, val_loss 454.8572998046875\n",
      "epoch 97, train_loss 1277.192138671875\n",
      "epoch 97, val_loss 454.8572998046875\n",
      "epoch 98, train_loss 1277.1920166015625\n",
      "epoch 98, val_loss 454.85723876953125\n",
      "epoch 99, train_loss 1277.1920166015625\n",
      "epoch 99, val_loss 454.857177734375\n",
      "Parameter containing:\n",
      "tensor([1.3546e-23], requires_grad=True)\n",
      "iter 124, train_loss_regularization 0.7184725999832153\n",
      "iter 124, val_loss_regularization 0.7184725999832153\n",
      "epoch 0, train_loss 1277.1920166015625\n",
      "epoch 0, val_loss 454.8572082519531\n",
      "epoch 1, train_loss 1277.1917724609375\n",
      "epoch 1, val_loss 454.85711669921875\n",
      "epoch 2, train_loss 1277.1917724609375\n",
      "epoch 2, val_loss 454.8570861816406\n",
      "epoch 3, train_loss 1277.1917724609375\n",
      "epoch 3, val_loss 454.8570556640625\n",
      "epoch 4, train_loss 1277.1917724609375\n",
      "epoch 4, val_loss 454.85699462890625\n",
      "epoch 5, train_loss 1277.1917724609375\n",
      "epoch 5, val_loss 454.8569030761719\n",
      "epoch 6, train_loss 1277.191650390625\n",
      "epoch 6, val_loss 454.8568420410156\n",
      "epoch 7, train_loss 1277.1915283203125\n",
      "epoch 7, val_loss 454.85675048828125\n",
      "epoch 8, train_loss 1277.1915283203125\n",
      "epoch 8, val_loss 454.8567810058594\n",
      "epoch 9, train_loss 1277.1915283203125\n",
      "epoch 9, val_loss 454.8567199707031\n",
      "epoch 10, train_loss 1277.1915283203125\n",
      "epoch 10, val_loss 454.8566589355469\n",
      "epoch 11, train_loss 1277.1915283203125\n",
      "epoch 11, val_loss 454.85662841796875\n",
      "epoch 12, train_loss 1277.191162109375\n",
      "epoch 12, val_loss 454.85662841796875\n",
      "epoch 13, train_loss 1277.1912841796875\n",
      "epoch 13, val_loss 454.8565368652344\n",
      "epoch 14, train_loss 1277.1912841796875\n",
      "epoch 14, val_loss 454.85650634765625\n",
      "epoch 15, train_loss 1277.1912841796875\n",
      "epoch 15, val_loss 454.8564453125\n",
      "epoch 16, train_loss 1277.191162109375\n",
      "epoch 16, val_loss 454.8564147949219\n",
      "epoch 17, train_loss 1277.1910400390625\n",
      "epoch 17, val_loss 454.8563232421875\n",
      "epoch 18, train_loss 1277.1910400390625\n",
      "epoch 18, val_loss 454.8562927246094\n",
      "epoch 19, train_loss 1277.1910400390625\n",
      "epoch 19, val_loss 454.856201171875\n",
      "epoch 20, train_loss 1277.19091796875\n",
      "epoch 20, val_loss 454.856201171875\n",
      "epoch 21, train_loss 1277.19091796875\n",
      "epoch 21, val_loss 454.8561706542969\n",
      "epoch 22, train_loss 1277.1907958984375\n",
      "epoch 22, val_loss 454.8560791015625\n",
      "epoch 23, train_loss 1277.1907958984375\n",
      "epoch 23, val_loss 454.8559875488281\n",
      "epoch 24, train_loss 1277.1907958984375\n",
      "epoch 24, val_loss 454.8559875488281\n",
      "epoch 25, train_loss 1277.190673828125\n",
      "epoch 25, val_loss 454.85595703125\n",
      "epoch 26, train_loss 1277.1905517578125\n",
      "epoch 26, val_loss 454.8559265136719\n",
      "epoch 27, train_loss 1277.1905517578125\n",
      "epoch 27, val_loss 454.8558654785156\n",
      "epoch 28, train_loss 1277.1905517578125\n",
      "epoch 28, val_loss 454.8558654785156\n",
      "epoch 29, train_loss 1277.1905517578125\n",
      "epoch 29, val_loss 454.8557434082031\n",
      "epoch 30, train_loss 1277.1905517578125\n",
      "epoch 30, val_loss 454.85565185546875\n",
      "epoch 31, train_loss 1277.1905517578125\n",
      "epoch 31, val_loss 454.85565185546875\n",
      "epoch 32, train_loss 1277.1903076171875\n",
      "epoch 32, val_loss 454.8555908203125\n",
      "epoch 33, train_loss 1277.1905517578125\n",
      "epoch 33, val_loss 454.8554992675781\n",
      "epoch 34, train_loss 1277.1903076171875\n",
      "epoch 34, val_loss 454.8554992675781\n",
      "epoch 35, train_loss 1277.1903076171875\n",
      "epoch 35, val_loss 454.8554992675781\n",
      "epoch 36, train_loss 1277.1903076171875\n",
      "epoch 36, val_loss 454.85540771484375\n",
      "epoch 37, train_loss 1277.1903076171875\n",
      "epoch 37, val_loss 454.8553771972656\n",
      "epoch 38, train_loss 1277.18994140625\n",
      "epoch 38, val_loss 454.8553771972656\n",
      "epoch 39, train_loss 1277.18994140625\n",
      "epoch 39, val_loss 454.8552551269531\n",
      "epoch 40, train_loss 1277.1900634765625\n",
      "epoch 40, val_loss 454.85516357421875\n",
      "epoch 41, train_loss 1277.18994140625\n",
      "epoch 41, val_loss 454.85516357421875\n",
      "epoch 42, train_loss 1277.18994140625\n",
      "epoch 42, val_loss 454.8551330566406\n",
      "epoch 43, train_loss 1277.18994140625\n",
      "epoch 43, val_loss 454.85504150390625\n",
      "epoch 44, train_loss 1277.1898193359375\n",
      "epoch 44, val_loss 454.85504150390625\n",
      "epoch 45, train_loss 1277.1898193359375\n",
      "epoch 45, val_loss 454.85504150390625\n",
      "epoch 46, train_loss 1277.189697265625\n",
      "epoch 46, val_loss 454.85491943359375\n",
      "epoch 47, train_loss 1277.1895751953125\n",
      "epoch 47, val_loss 454.8548278808594\n",
      "epoch 48, train_loss 1277.1895751953125\n",
      "epoch 48, val_loss 454.8548278808594\n",
      "epoch 49, train_loss 1277.1895751953125\n",
      "epoch 49, val_loss 454.8547058105469\n",
      "epoch 50, train_loss 1277.1895751953125\n",
      "epoch 50, val_loss 454.854736328125\n",
      "epoch 51, train_loss 1277.1895751953125\n",
      "epoch 51, val_loss 454.854736328125\n",
      "epoch 52, train_loss 1277.189453125\n",
      "epoch 52, val_loss 454.85467529296875\n",
      "epoch 53, train_loss 1277.189453125\n",
      "epoch 53, val_loss 454.8545837402344\n",
      "epoch 54, train_loss 1277.189208984375\n",
      "epoch 54, val_loss 454.85455322265625\n",
      "epoch 55, train_loss 1277.189453125\n",
      "epoch 55, val_loss 454.8544616699219\n",
      "epoch 56, train_loss 1277.189208984375\n",
      "epoch 56, val_loss 454.8544616699219\n",
      "epoch 57, train_loss 1277.1890869140625\n",
      "epoch 57, val_loss 454.8543701171875\n",
      "epoch 58, train_loss 1277.189208984375\n",
      "epoch 58, val_loss 454.8543395996094\n",
      "epoch 59, train_loss 1277.18896484375\n",
      "epoch 59, val_loss 454.8543395996094\n",
      "epoch 60, train_loss 1277.1890869140625\n",
      "epoch 60, val_loss 454.854248046875\n",
      "epoch 61, train_loss 1277.18896484375\n",
      "epoch 61, val_loss 454.8542175292969\n",
      "epoch 62, train_loss 1277.18896484375\n",
      "epoch 62, val_loss 454.8541564941406\n",
      "epoch 63, train_loss 1277.18896484375\n",
      "epoch 63, val_loss 454.8541259765625\n",
      "epoch 64, train_loss 1277.18896484375\n",
      "epoch 64, val_loss 454.8540954589844\n",
      "epoch 65, train_loss 1277.1888427734375\n",
      "epoch 65, val_loss 454.85400390625\n",
      "epoch 66, train_loss 1277.1885986328125\n",
      "epoch 66, val_loss 454.8539733886719\n",
      "epoch 67, train_loss 1277.1885986328125\n",
      "epoch 67, val_loss 454.8539123535156\n",
      "epoch 68, train_loss 1277.188720703125\n",
      "epoch 68, val_loss 454.85382080078125\n",
      "epoch 69, train_loss 1277.188720703125\n",
      "epoch 69, val_loss 454.85382080078125\n",
      "epoch 70, train_loss 1277.1884765625\n",
      "epoch 70, val_loss 454.8537902832031\n",
      "epoch 71, train_loss 1277.1884765625\n",
      "epoch 71, val_loss 454.85369873046875\n",
      "epoch 72, train_loss 1277.1884765625\n",
      "epoch 72, val_loss 454.85369873046875\n",
      "epoch 73, train_loss 1277.1884765625\n",
      "epoch 73, val_loss 454.8536682128906\n",
      "epoch 74, train_loss 1277.1884765625\n",
      "epoch 74, val_loss 454.85357666015625\n",
      "epoch 75, train_loss 1277.1883544921875\n",
      "epoch 75, val_loss 454.8535461425781\n",
      "epoch 76, train_loss 1277.188232421875\n",
      "epoch 76, val_loss 454.8535461425781\n",
      "epoch 77, train_loss 1277.1881103515625\n",
      "epoch 77, val_loss 454.8533630371094\n",
      "epoch 78, train_loss 1277.1881103515625\n",
      "epoch 78, val_loss 454.8534240722656\n",
      "epoch 79, train_loss 1277.1881103515625\n",
      "epoch 79, val_loss 454.85333251953125\n",
      "epoch 80, train_loss 1277.18798828125\n",
      "epoch 80, val_loss 454.8532409667969\n",
      "epoch 81, train_loss 1277.18798828125\n",
      "epoch 81, val_loss 454.85321044921875\n",
      "epoch 82, train_loss 1277.18798828125\n",
      "epoch 82, val_loss 454.85321044921875\n",
      "epoch 83, train_loss 1277.1878662109375\n",
      "epoch 83, val_loss 454.8531799316406\n",
      "epoch 84, train_loss 1277.1878662109375\n",
      "epoch 84, val_loss 454.85308837890625\n",
      "epoch 85, train_loss 1277.1878662109375\n",
      "epoch 85, val_loss 454.85302734375\n",
      "epoch 86, train_loss 1277.1878662109375\n",
      "epoch 86, val_loss 454.8529968261719\n",
      "epoch 87, train_loss 1277.1878662109375\n",
      "epoch 87, val_loss 454.8529052734375\n",
      "epoch 88, train_loss 1277.187744140625\n",
      "epoch 88, val_loss 454.8529052734375\n",
      "epoch 89, train_loss 1277.1876220703125\n",
      "epoch 89, val_loss 454.8528747558594\n",
      "epoch 90, train_loss 1277.1876220703125\n",
      "epoch 90, val_loss 454.852783203125\n",
      "epoch 91, train_loss 1277.1876220703125\n",
      "epoch 91, val_loss 454.8527526855469\n",
      "epoch 92, train_loss 1277.1876220703125\n",
      "epoch 92, val_loss 454.8526611328125\n",
      "epoch 93, train_loss 1277.1873779296875\n",
      "epoch 93, val_loss 454.8526611328125\n",
      "epoch 94, train_loss 1277.1875\n",
      "epoch 94, val_loss 454.8526306152344\n",
      "epoch 95, train_loss 1277.1873779296875\n",
      "epoch 95, val_loss 454.8525390625\n",
      "epoch 96, train_loss 1277.1873779296875\n",
      "epoch 96, val_loss 454.8525390625\n",
      "epoch 97, train_loss 1277.1873779296875\n",
      "epoch 97, val_loss 454.8524475097656\n",
      "epoch 98, train_loss 1277.187255859375\n",
      "epoch 98, val_loss 454.8524169921875\n",
      "epoch 99, train_loss 1277.1871337890625\n",
      "epoch 99, val_loss 454.8524169921875\n",
      "Parameter containing:\n",
      "tensor([9.3749e-24], requires_grad=True)\n",
      "iter 125, train_loss_regularization 0.7173284888267517\n",
      "iter 125, val_loss_regularization 0.7173284888267517\n",
      "epoch 0, train_loss 1277.18701171875\n",
      "epoch 0, val_loss 454.8523254394531\n",
      "epoch 1, train_loss 1277.18701171875\n",
      "epoch 1, val_loss 454.8522033691406\n",
      "epoch 2, train_loss 1277.1871337890625\n",
      "epoch 2, val_loss 454.8522033691406\n",
      "epoch 3, train_loss 1277.1868896484375\n",
      "epoch 3, val_loss 454.8522033691406\n",
      "epoch 4, train_loss 1277.1868896484375\n",
      "epoch 4, val_loss 454.85211181640625\n",
      "epoch 5, train_loss 1277.18701171875\n",
      "epoch 5, val_loss 454.8520812988281\n",
      "epoch 6, train_loss 1277.1868896484375\n",
      "epoch 6, val_loss 454.85205078125\n",
      "epoch 7, train_loss 1277.1865234375\n",
      "epoch 7, val_loss 454.8519592285156\n",
      "epoch 8, train_loss 1277.1866455078125\n",
      "epoch 8, val_loss 454.85186767578125\n",
      "epoch 9, train_loss 1277.1866455078125\n",
      "epoch 9, val_loss 454.85186767578125\n",
      "epoch 10, train_loss 1277.1865234375\n",
      "epoch 10, val_loss 454.8518371582031\n",
      "epoch 11, train_loss 1277.1866455078125\n",
      "epoch 11, val_loss 454.851806640625\n",
      "epoch 12, train_loss 1277.1865234375\n",
      "epoch 12, val_loss 454.85174560546875\n",
      "epoch 13, train_loss 1277.186279296875\n",
      "epoch 13, val_loss 454.8517150878906\n",
      "epoch 14, train_loss 1277.186279296875\n",
      "epoch 14, val_loss 454.85162353515625\n",
      "epoch 15, train_loss 1277.1864013671875\n",
      "epoch 15, val_loss 454.8516540527344\n",
      "epoch 16, train_loss 1277.186279296875\n",
      "epoch 16, val_loss 454.8515319824219\n",
      "epoch 17, train_loss 1277.186279296875\n",
      "epoch 17, val_loss 454.85150146484375\n",
      "epoch 18, train_loss 1277.1864013671875\n",
      "epoch 18, val_loss 454.8514709472656\n",
      "epoch 19, train_loss 1277.1861572265625\n",
      "epoch 19, val_loss 454.85137939453125\n",
      "epoch 20, train_loss 1277.18603515625\n",
      "epoch 20, val_loss 454.85137939453125\n",
      "epoch 21, train_loss 1277.18603515625\n",
      "epoch 21, val_loss 454.851318359375\n",
      "epoch 22, train_loss 1277.18603515625\n",
      "epoch 22, val_loss 454.85125732421875\n",
      "epoch 23, train_loss 1277.18603515625\n",
      "epoch 23, val_loss 454.85125732421875\n",
      "epoch 24, train_loss 1277.18603515625\n",
      "epoch 24, val_loss 454.8511657714844\n",
      "epoch 25, train_loss 1277.185791015625\n",
      "epoch 25, val_loss 454.85113525390625\n",
      "epoch 26, train_loss 1277.185791015625\n",
      "epoch 26, val_loss 454.8510437011719\n",
      "epoch 27, train_loss 1277.1856689453125\n",
      "epoch 27, val_loss 454.85101318359375\n",
      "epoch 28, train_loss 1277.185791015625\n",
      "epoch 28, val_loss 454.85101318359375\n",
      "epoch 29, train_loss 1277.1859130859375\n",
      "epoch 29, val_loss 454.8509521484375\n",
      "epoch 30, train_loss 1277.185791015625\n",
      "epoch 30, val_loss 454.8508605957031\n",
      "epoch 31, train_loss 1277.185546875\n",
      "epoch 31, val_loss 454.8507995605469\n",
      "epoch 32, train_loss 1277.185546875\n",
      "epoch 32, val_loss 454.8507995605469\n",
      "epoch 33, train_loss 1277.185546875\n",
      "epoch 33, val_loss 454.8507995605469\n",
      "epoch 34, train_loss 1277.185546875\n",
      "epoch 34, val_loss 454.8507080078125\n",
      "epoch 35, train_loss 1277.1854248046875\n",
      "epoch 35, val_loss 454.8506164550781\n",
      "epoch 36, train_loss 1277.1854248046875\n",
      "epoch 36, val_loss 454.8506164550781\n",
      "epoch 37, train_loss 1277.185302734375\n",
      "epoch 37, val_loss 454.8505554199219\n",
      "epoch 38, train_loss 1277.185302734375\n",
      "epoch 38, val_loss 454.8504638671875\n",
      "epoch 39, train_loss 1277.18505859375\n",
      "epoch 39, val_loss 454.85040283203125\n",
      "epoch 40, train_loss 1277.185302734375\n",
      "epoch 40, val_loss 454.85040283203125\n",
      "epoch 41, train_loss 1277.1851806640625\n",
      "epoch 41, val_loss 454.8503723144531\n",
      "epoch 42, train_loss 1277.18505859375\n",
      "epoch 42, val_loss 454.850341796875\n",
      "epoch 43, train_loss 1277.18505859375\n",
      "epoch 43, val_loss 454.8502502441406\n",
      "epoch 44, train_loss 1277.18505859375\n",
      "epoch 44, val_loss 454.8502197265625\n",
      "epoch 45, train_loss 1277.1849365234375\n",
      "epoch 45, val_loss 454.85015869140625\n",
      "epoch 46, train_loss 1277.18505859375\n",
      "epoch 46, val_loss 454.85009765625\n",
      "epoch 47, train_loss 1277.184814453125\n",
      "epoch 47, val_loss 454.85009765625\n",
      "epoch 48, train_loss 1277.184814453125\n",
      "epoch 48, val_loss 454.8500061035156\n",
      "epoch 49, train_loss 1277.1846923828125\n",
      "epoch 49, val_loss 454.8499450683594\n",
      "epoch 50, train_loss 1277.1846923828125\n",
      "epoch 50, val_loss 454.84991455078125\n",
      "epoch 51, train_loss 1277.1846923828125\n",
      "epoch 51, val_loss 454.8498840332031\n",
      "epoch 52, train_loss 1277.1846923828125\n",
      "epoch 52, val_loss 454.84979248046875\n",
      "epoch 53, train_loss 1277.1844482421875\n",
      "epoch 53, val_loss 454.8497619628906\n",
      "epoch 54, train_loss 1277.1844482421875\n",
      "epoch 54, val_loss 454.8497009277344\n",
      "epoch 55, train_loss 1277.1844482421875\n",
      "epoch 55, val_loss 454.84967041015625\n",
      "epoch 56, train_loss 1277.184326171875\n",
      "epoch 56, val_loss 454.8496398925781\n",
      "epoch 57, train_loss 1277.1842041015625\n",
      "epoch 57, val_loss 454.8495788574219\n",
      "epoch 58, train_loss 1277.1842041015625\n",
      "epoch 58, val_loss 454.8495788574219\n",
      "epoch 59, train_loss 1277.1842041015625\n",
      "epoch 59, val_loss 454.84954833984375\n",
      "epoch 60, train_loss 1277.184326171875\n",
      "epoch 60, val_loss 454.84942626953125\n",
      "epoch 61, train_loss 1277.1842041015625\n",
      "epoch 61, val_loss 454.849365234375\n",
      "epoch 62, train_loss 1277.18408203125\n",
      "epoch 62, val_loss 454.84930419921875\n",
      "epoch 63, train_loss 1277.1839599609375\n",
      "epoch 63, val_loss 454.8492431640625\n",
      "epoch 64, train_loss 1277.1839599609375\n",
      "epoch 64, val_loss 454.8492126464844\n",
      "epoch 65, train_loss 1277.1839599609375\n",
      "epoch 65, val_loss 454.8492126464844\n",
      "epoch 66, train_loss 1277.1839599609375\n",
      "epoch 66, val_loss 454.8491516113281\n",
      "epoch 67, train_loss 1277.1837158203125\n",
      "epoch 67, val_loss 454.8490905761719\n",
      "epoch 68, train_loss 1277.1837158203125\n",
      "epoch 68, val_loss 454.84912109375\n",
      "epoch 69, train_loss 1277.183837890625\n",
      "epoch 69, val_loss 454.8489990234375\n",
      "epoch 70, train_loss 1277.1837158203125\n",
      "epoch 70, val_loss 454.8489685058594\n",
      "epoch 71, train_loss 1277.1837158203125\n",
      "epoch 71, val_loss 454.8489074707031\n",
      "epoch 72, train_loss 1277.1837158203125\n",
      "epoch 72, val_loss 454.8488464355469\n",
      "epoch 73, train_loss 1277.1837158203125\n",
      "epoch 73, val_loss 454.8487854003906\n",
      "epoch 74, train_loss 1277.1837158203125\n",
      "epoch 74, val_loss 454.8487548828125\n",
      "epoch 75, train_loss 1277.18359375\n",
      "epoch 75, val_loss 454.8487548828125\n",
      "epoch 76, train_loss 1277.1834716796875\n",
      "epoch 76, val_loss 454.8486328125\n",
      "epoch 77, train_loss 1277.1834716796875\n",
      "epoch 77, val_loss 454.8486328125\n",
      "epoch 78, train_loss 1277.1834716796875\n",
      "epoch 78, val_loss 454.84857177734375\n",
      "epoch 79, train_loss 1277.183349609375\n",
      "epoch 79, val_loss 454.8485412597656\n",
      "epoch 80, train_loss 1277.183349609375\n",
      "epoch 80, val_loss 454.8485107421875\n",
      "epoch 81, train_loss 1277.1832275390625\n",
      "epoch 81, val_loss 454.84844970703125\n",
      "epoch 82, train_loss 1277.18310546875\n",
      "epoch 82, val_loss 454.84844970703125\n",
      "epoch 83, train_loss 1277.18310546875\n",
      "epoch 83, val_loss 454.84832763671875\n",
      "epoch 84, train_loss 1277.18310546875\n",
      "epoch 84, val_loss 454.84832763671875\n",
      "epoch 85, train_loss 1277.1832275390625\n",
      "epoch 85, val_loss 454.8482360839844\n",
      "epoch 86, train_loss 1277.1829833984375\n",
      "epoch 86, val_loss 454.8481750488281\n",
      "epoch 87, train_loss 1277.1829833984375\n",
      "epoch 87, val_loss 454.84808349609375\n",
      "epoch 88, train_loss 1277.182861328125\n",
      "epoch 88, val_loss 454.8481140136719\n",
      "epoch 89, train_loss 1277.182861328125\n",
      "epoch 89, val_loss 454.8480529785156\n",
      "epoch 90, train_loss 1277.182861328125\n",
      "epoch 90, val_loss 454.8479919433594\n",
      "epoch 91, train_loss 1277.1827392578125\n",
      "epoch 91, val_loss 454.8479919433594\n",
      "epoch 92, train_loss 1277.1827392578125\n",
      "epoch 92, val_loss 454.84796142578125\n",
      "epoch 93, train_loss 1277.1827392578125\n",
      "epoch 93, val_loss 454.8478698730469\n",
      "epoch 94, train_loss 1277.1827392578125\n",
      "epoch 94, val_loss 454.84783935546875\n",
      "epoch 95, train_loss 1277.1824951171875\n",
      "epoch 95, val_loss 454.84783935546875\n",
      "epoch 96, train_loss 1277.1824951171875\n",
      "epoch 96, val_loss 454.84771728515625\n",
      "epoch 97, train_loss 1277.1824951171875\n",
      "epoch 97, val_loss 454.8476257324219\n",
      "epoch 98, train_loss 1277.182373046875\n",
      "epoch 98, val_loss 454.8476257324219\n",
      "epoch 99, train_loss 1277.1822509765625\n",
      "epoch 99, val_loss 454.84759521484375\n",
      "Parameter containing:\n",
      "tensor([6.4903e-24], requires_grad=True)\n",
      "iter 126, train_loss_regularization 0.7162100076675415\n",
      "iter 126, val_loss_regularization 0.7162100076675415\n",
      "epoch 0, train_loss 1277.1822509765625\n",
      "epoch 0, val_loss 454.8475036621094\n",
      "epoch 1, train_loss 1277.182373046875\n",
      "epoch 1, val_loss 454.8475036621094\n",
      "epoch 2, train_loss 1277.1822509765625\n",
      "epoch 2, val_loss 454.84747314453125\n",
      "epoch 3, train_loss 1277.1822509765625\n",
      "epoch 3, val_loss 454.8473815917969\n",
      "epoch 4, train_loss 1277.18212890625\n",
      "epoch 4, val_loss 454.8473815917969\n",
      "epoch 5, train_loss 1277.18212890625\n",
      "epoch 5, val_loss 454.8472900390625\n",
      "epoch 6, train_loss 1277.18212890625\n",
      "epoch 6, val_loss 454.8472900390625\n",
      "epoch 7, train_loss 1277.18212890625\n",
      "epoch 7, val_loss 454.8471984863281\n",
      "epoch 8, train_loss 1277.181884765625\n",
      "epoch 8, val_loss 454.84716796875\n",
      "epoch 9, train_loss 1277.181884765625\n",
      "epoch 9, val_loss 454.8470764160156\n",
      "epoch 10, train_loss 1277.1817626953125\n",
      "epoch 10, val_loss 454.8470458984375\n",
      "epoch 11, train_loss 1277.1817626953125\n",
      "epoch 11, val_loss 454.84698486328125\n",
      "epoch 12, train_loss 1277.1817626953125\n",
      "epoch 12, val_loss 454.84698486328125\n",
      "epoch 13, train_loss 1277.181640625\n",
      "epoch 13, val_loss 454.846923828125\n",
      "epoch 14, train_loss 1277.181640625\n",
      "epoch 14, val_loss 454.8468322753906\n",
      "epoch 15, train_loss 1277.181640625\n",
      "epoch 15, val_loss 454.8468017578125\n",
      "epoch 16, train_loss 1277.1815185546875\n",
      "epoch 16, val_loss 454.84674072265625\n",
      "epoch 17, train_loss 1277.181640625\n",
      "epoch 17, val_loss 454.8467102050781\n",
      "epoch 18, train_loss 1277.181640625\n",
      "epoch 18, val_loss 454.8467102050781\n",
      "epoch 19, train_loss 1277.181640625\n",
      "epoch 19, val_loss 454.8466796875\n",
      "epoch 20, train_loss 1277.1815185546875\n",
      "epoch 20, val_loss 454.8465881347656\n",
      "epoch 21, train_loss 1277.1812744140625\n",
      "epoch 21, val_loss 454.8465881347656\n",
      "epoch 22, train_loss 1277.1812744140625\n",
      "epoch 22, val_loss 454.8464660644531\n",
      "epoch 23, train_loss 1277.18115234375\n",
      "epoch 23, val_loss 454.8464050292969\n",
      "epoch 24, train_loss 1277.18115234375\n",
      "epoch 24, val_loss 454.84637451171875\n",
      "epoch 25, train_loss 1277.18115234375\n",
      "epoch 25, val_loss 454.8463439941406\n",
      "epoch 26, train_loss 1277.18115234375\n",
      "epoch 26, val_loss 454.8462829589844\n",
      "epoch 27, train_loss 1277.18115234375\n",
      "epoch 27, val_loss 454.84625244140625\n",
      "epoch 28, train_loss 1277.180908203125\n",
      "epoch 28, val_loss 454.8462219238281\n",
      "epoch 29, train_loss 1277.180908203125\n",
      "epoch 29, val_loss 454.8461608886719\n",
      "epoch 30, train_loss 1277.1810302734375\n",
      "epoch 30, val_loss 454.84613037109375\n",
      "epoch 31, train_loss 1277.180908203125\n",
      "epoch 31, val_loss 454.8460388183594\n",
      "epoch 32, train_loss 1277.1807861328125\n",
      "epoch 32, val_loss 454.84600830078125\n",
      "epoch 33, train_loss 1277.1807861328125\n",
      "epoch 33, val_loss 454.845947265625\n",
      "epoch 34, train_loss 1277.1807861328125\n",
      "epoch 34, val_loss 454.8459167480469\n",
      "epoch 35, train_loss 1277.1807861328125\n",
      "epoch 35, val_loss 454.84588623046875\n",
      "epoch 36, train_loss 1277.1805419921875\n",
      "epoch 36, val_loss 454.8458251953125\n",
      "epoch 37, train_loss 1277.1805419921875\n",
      "epoch 37, val_loss 454.8458251953125\n",
      "epoch 38, train_loss 1277.1805419921875\n",
      "epoch 38, val_loss 454.8456726074219\n",
      "epoch 39, train_loss 1277.1805419921875\n",
      "epoch 39, val_loss 454.8456726074219\n",
      "epoch 40, train_loss 1277.180419921875\n",
      "epoch 40, val_loss 454.8456115722656\n",
      "epoch 41, train_loss 1277.180419921875\n",
      "epoch 41, val_loss 454.8455505371094\n",
      "epoch 42, train_loss 1277.1802978515625\n",
      "epoch 42, val_loss 454.8455505371094\n",
      "epoch 43, train_loss 1277.1802978515625\n",
      "epoch 43, val_loss 454.8455505371094\n",
      "epoch 44, train_loss 1277.1802978515625\n",
      "epoch 44, val_loss 454.8454895019531\n",
      "epoch 45, train_loss 1277.1802978515625\n",
      "epoch 45, val_loss 454.8453674316406\n",
      "epoch 46, train_loss 1277.18017578125\n",
      "epoch 46, val_loss 454.8453369140625\n",
      "epoch 47, train_loss 1277.1800537109375\n",
      "epoch 47, val_loss 454.8453369140625\n",
      "epoch 48, train_loss 1277.1800537109375\n",
      "epoch 48, val_loss 454.84515380859375\n",
      "epoch 49, train_loss 1277.1800537109375\n",
      "epoch 49, val_loss 454.84515380859375\n",
      "epoch 50, train_loss 1277.1800537109375\n",
      "epoch 50, val_loss 454.84515380859375\n",
      "epoch 51, train_loss 1277.179931640625\n",
      "epoch 51, val_loss 454.8451232910156\n",
      "epoch 52, train_loss 1277.1798095703125\n",
      "epoch 52, val_loss 454.84503173828125\n",
      "epoch 53, train_loss 1277.1798095703125\n",
      "epoch 53, val_loss 454.84503173828125\n",
      "epoch 54, train_loss 1277.1798095703125\n",
      "epoch 54, val_loss 454.8450012207031\n",
      "epoch 55, train_loss 1277.1796875\n",
      "epoch 55, val_loss 454.844970703125\n",
      "epoch 56, train_loss 1277.1795654296875\n",
      "epoch 56, val_loss 454.84490966796875\n",
      "epoch 57, train_loss 1277.1795654296875\n",
      "epoch 57, val_loss 454.8448181152344\n",
      "epoch 58, train_loss 1277.1796875\n",
      "epoch 58, val_loss 454.8447570800781\n",
      "epoch 59, train_loss 1277.1795654296875\n",
      "epoch 59, val_loss 454.8446960449219\n",
      "epoch 60, train_loss 1277.1795654296875\n",
      "epoch 60, val_loss 454.8446960449219\n",
      "epoch 61, train_loss 1277.179443359375\n",
      "epoch 61, val_loss 454.8446350097656\n",
      "epoch 62, train_loss 1277.179443359375\n",
      "epoch 62, val_loss 454.8445739746094\n",
      "epoch 63, train_loss 1277.179443359375\n",
      "epoch 63, val_loss 454.84454345703125\n",
      "epoch 64, train_loss 1277.1793212890625\n",
      "epoch 64, val_loss 454.8445129394531\n",
      "epoch 65, train_loss 1277.1793212890625\n",
      "epoch 65, val_loss 454.84442138671875\n",
      "epoch 66, train_loss 1277.1790771484375\n",
      "epoch 66, val_loss 454.84442138671875\n",
      "epoch 67, train_loss 1277.17919921875\n",
      "epoch 67, val_loss 454.8443298339844\n",
      "epoch 68, train_loss 1277.17919921875\n",
      "epoch 68, val_loss 454.8443298339844\n",
      "epoch 69, train_loss 1277.17919921875\n",
      "epoch 69, val_loss 454.84423828125\n",
      "epoch 70, train_loss 1277.1790771484375\n",
      "epoch 70, val_loss 454.84417724609375\n",
      "epoch 71, train_loss 1277.178955078125\n",
      "epoch 71, val_loss 454.84417724609375\n",
      "epoch 72, train_loss 1277.178955078125\n",
      "epoch 72, val_loss 454.8441162109375\n",
      "epoch 73, train_loss 1277.178955078125\n",
      "epoch 73, val_loss 454.8441162109375\n",
      "epoch 74, train_loss 1277.1788330078125\n",
      "epoch 74, val_loss 454.84405517578125\n",
      "epoch 75, train_loss 1277.1788330078125\n",
      "epoch 75, val_loss 454.8439636230469\n",
      "epoch 76, train_loss 1277.178955078125\n",
      "epoch 76, val_loss 454.8439025878906\n",
      "epoch 77, train_loss 1277.1787109375\n",
      "epoch 77, val_loss 454.8438415527344\n",
      "epoch 78, train_loss 1277.1785888671875\n",
      "epoch 78, val_loss 454.8438415527344\n",
      "epoch 79, train_loss 1277.178466796875\n",
      "epoch 79, val_loss 454.8438415527344\n",
      "epoch 80, train_loss 1277.1785888671875\n",
      "epoch 80, val_loss 454.8437805175781\n",
      "epoch 81, train_loss 1277.178466796875\n",
      "epoch 81, val_loss 454.8436584472656\n",
      "epoch 82, train_loss 1277.1785888671875\n",
      "epoch 82, val_loss 454.8436584472656\n",
      "epoch 83, train_loss 1277.178466796875\n",
      "epoch 83, val_loss 454.8436279296875\n",
      "epoch 84, train_loss 1277.1783447265625\n",
      "epoch 84, val_loss 454.8435363769531\n",
      "epoch 85, train_loss 1277.17822265625\n",
      "epoch 85, val_loss 454.843505859375\n",
      "epoch 86, train_loss 1277.1783447265625\n",
      "epoch 86, val_loss 454.8434143066406\n",
      "epoch 87, train_loss 1277.1783447265625\n",
      "epoch 87, val_loss 454.8433837890625\n",
      "epoch 88, train_loss 1277.17822265625\n",
      "epoch 88, val_loss 454.8433837890625\n",
      "epoch 89, train_loss 1277.17822265625\n",
      "epoch 89, val_loss 454.8433837890625\n",
      "epoch 90, train_loss 1277.177978515625\n",
      "epoch 90, val_loss 454.84326171875\n",
      "epoch 91, train_loss 1277.177978515625\n",
      "epoch 91, val_loss 454.84326171875\n",
      "epoch 92, train_loss 1277.177978515625\n",
      "epoch 92, val_loss 454.8431701660156\n",
      "epoch 93, train_loss 1277.1778564453125\n",
      "epoch 93, val_loss 454.84307861328125\n",
      "epoch 94, train_loss 1277.177978515625\n",
      "epoch 94, val_loss 454.8429870605469\n",
      "epoch 95, train_loss 1277.1778564453125\n",
      "epoch 95, val_loss 454.8430480957031\n",
      "epoch 96, train_loss 1277.1778564453125\n",
      "epoch 96, val_loss 454.8429870605469\n",
      "epoch 97, train_loss 1277.1778564453125\n",
      "epoch 97, val_loss 454.8429260253906\n",
      "epoch 98, train_loss 1277.1776123046875\n",
      "epoch 98, val_loss 454.8429260253906\n",
      "epoch 99, train_loss 1277.1776123046875\n",
      "epoch 99, val_loss 454.8428649902344\n",
      "Parameter containing:\n",
      "tensor([4.4949e-24], requires_grad=True)\n",
      "iter 127, train_loss_regularization 0.7151166796684265\n",
      "iter 127, val_loss_regularization 0.7151166796684265\n",
      "epoch 0, train_loss 1277.1776123046875\n",
      "epoch 0, val_loss 454.8428039550781\n",
      "epoch 1, train_loss 1277.1776123046875\n",
      "epoch 1, val_loss 454.84271240234375\n",
      "epoch 2, train_loss 1277.1776123046875\n",
      "epoch 2, val_loss 454.84271240234375\n",
      "epoch 3, train_loss 1277.177490234375\n",
      "epoch 3, val_loss 454.8426818847656\n",
      "epoch 4, train_loss 1277.1773681640625\n",
      "epoch 4, val_loss 454.84259033203125\n",
      "epoch 5, train_loss 1277.1773681640625\n",
      "epoch 5, val_loss 454.8424987792969\n",
      "epoch 6, train_loss 1277.1773681640625\n",
      "epoch 6, val_loss 454.8424987792969\n",
      "epoch 7, train_loss 1277.1773681640625\n",
      "epoch 7, val_loss 454.84246826171875\n",
      "epoch 8, train_loss 1277.1771240234375\n",
      "epoch 8, val_loss 454.8424072265625\n",
      "epoch 9, train_loss 1277.1773681640625\n",
      "epoch 9, val_loss 454.8423767089844\n",
      "epoch 10, train_loss 1277.1771240234375\n",
      "epoch 10, val_loss 454.84234619140625\n",
      "epoch 11, train_loss 1277.1771240234375\n",
      "epoch 11, val_loss 454.8422546386719\n",
      "epoch 12, train_loss 1277.1771240234375\n",
      "epoch 12, val_loss 454.8421936035156\n",
      "epoch 13, train_loss 1277.177001953125\n",
      "epoch 13, val_loss 454.8421936035156\n",
      "epoch 14, train_loss 1277.1768798828125\n",
      "epoch 14, val_loss 454.8421630859375\n",
      "epoch 15, train_loss 1277.1768798828125\n",
      "epoch 15, val_loss 454.8420715332031\n",
      "epoch 16, train_loss 1277.177001953125\n",
      "epoch 16, val_loss 454.8421325683594\n",
      "epoch 17, train_loss 1277.1768798828125\n",
      "epoch 17, val_loss 454.8420104980469\n",
      "epoch 18, train_loss 1277.1768798828125\n",
      "epoch 18, val_loss 454.8419494628906\n",
      "epoch 19, train_loss 1277.1768798828125\n",
      "epoch 19, val_loss 454.8418884277344\n",
      "epoch 20, train_loss 1277.1767578125\n",
      "epoch 20, val_loss 454.8418273925781\n",
      "epoch 21, train_loss 1277.1767578125\n",
      "epoch 21, val_loss 454.841796875\n",
      "epoch 22, train_loss 1277.1766357421875\n",
      "epoch 22, val_loss 454.841796875\n",
      "epoch 23, train_loss 1277.176513671875\n",
      "epoch 23, val_loss 454.8417053222656\n",
      "epoch 24, train_loss 1277.1766357421875\n",
      "epoch 24, val_loss 454.8417053222656\n",
      "epoch 25, train_loss 1277.176513671875\n",
      "epoch 25, val_loss 454.84161376953125\n",
      "epoch 26, train_loss 1277.1763916015625\n",
      "epoch 26, val_loss 454.8415832519531\n",
      "epoch 27, train_loss 1277.1763916015625\n",
      "epoch 27, val_loss 454.841552734375\n",
      "epoch 28, train_loss 1277.1763916015625\n",
      "epoch 28, val_loss 454.8414611816406\n",
      "epoch 29, train_loss 1277.1763916015625\n",
      "epoch 29, val_loss 454.8414611816406\n",
      "epoch 30, train_loss 1277.17626953125\n",
      "epoch 30, val_loss 454.84136962890625\n",
      "epoch 31, train_loss 1277.1763916015625\n",
      "epoch 31, val_loss 454.8412780761719\n",
      "epoch 32, train_loss 1277.176025390625\n",
      "epoch 32, val_loss 454.84124755859375\n",
      "epoch 33, train_loss 1277.176025390625\n",
      "epoch 33, val_loss 454.84124755859375\n",
      "epoch 34, train_loss 1277.176025390625\n",
      "epoch 34, val_loss 454.84124755859375\n",
      "epoch 35, train_loss 1277.176025390625\n",
      "epoch 35, val_loss 454.8411560058594\n",
      "epoch 36, train_loss 1277.1759033203125\n",
      "epoch 36, val_loss 454.84112548828125\n",
      "epoch 37, train_loss 1277.1759033203125\n",
      "epoch 37, val_loss 454.8410339355469\n",
      "epoch 38, train_loss 1277.17578125\n",
      "epoch 38, val_loss 454.84100341796875\n",
      "epoch 39, train_loss 1277.17578125\n",
      "epoch 39, val_loss 454.8409118652344\n",
      "epoch 40, train_loss 1277.17578125\n",
      "epoch 40, val_loss 454.8409729003906\n",
      "epoch 41, train_loss 1277.17578125\n",
      "epoch 41, val_loss 454.8408203125\n",
      "epoch 42, train_loss 1277.1756591796875\n",
      "epoch 42, val_loss 454.8408203125\n",
      "epoch 43, train_loss 1277.1756591796875\n",
      "epoch 43, val_loss 454.84075927734375\n",
      "epoch 44, train_loss 1277.175537109375\n",
      "epoch 44, val_loss 454.84075927734375\n",
      "epoch 45, train_loss 1277.175537109375\n",
      "epoch 45, val_loss 454.8406982421875\n",
      "epoch 46, train_loss 1277.1754150390625\n",
      "epoch 46, val_loss 454.840576171875\n",
      "epoch 47, train_loss 1277.1754150390625\n",
      "epoch 47, val_loss 454.840576171875\n",
      "epoch 48, train_loss 1277.175537109375\n",
      "epoch 48, val_loss 454.840576171875\n",
      "epoch 49, train_loss 1277.17529296875\n",
      "epoch 49, val_loss 454.8404541015625\n",
      "epoch 50, train_loss 1277.17529296875\n",
      "epoch 50, val_loss 454.8404541015625\n",
      "epoch 51, train_loss 1277.17529296875\n",
      "epoch 51, val_loss 454.8404541015625\n",
      "epoch 52, train_loss 1277.17529296875\n",
      "epoch 52, val_loss 454.8404235839844\n",
      "epoch 53, train_loss 1277.17529296875\n",
      "epoch 53, val_loss 454.8403015136719\n",
      "epoch 54, train_loss 1277.17529296875\n",
      "epoch 54, val_loss 454.8402404785156\n",
      "epoch 55, train_loss 1277.1751708984375\n",
      "epoch 55, val_loss 454.8402099609375\n",
      "epoch 56, train_loss 1277.175048828125\n",
      "epoch 56, val_loss 454.8401184082031\n",
      "epoch 57, train_loss 1277.1749267578125\n",
      "epoch 57, val_loss 454.8401184082031\n",
      "epoch 58, train_loss 1277.1749267578125\n",
      "epoch 58, val_loss 454.840087890625\n",
      "epoch 59, train_loss 1277.1749267578125\n",
      "epoch 59, val_loss 454.8399963378906\n",
      "epoch 60, train_loss 1277.1748046875\n",
      "epoch 60, val_loss 454.8399963378906\n",
      "epoch 61, train_loss 1277.1749267578125\n",
      "epoch 61, val_loss 454.8399658203125\n",
      "epoch 62, train_loss 1277.1748046875\n",
      "epoch 62, val_loss 454.8399658203125\n",
      "epoch 63, train_loss 1277.1748046875\n",
      "epoch 63, val_loss 454.83984375\n",
      "epoch 64, train_loss 1277.1746826171875\n",
      "epoch 64, val_loss 454.83978271484375\n",
      "epoch 65, train_loss 1277.1746826171875\n",
      "epoch 65, val_loss 454.8397521972656\n",
      "epoch 66, train_loss 1277.1748046875\n",
      "epoch 66, val_loss 454.83966064453125\n",
      "epoch 67, train_loss 1277.174560546875\n",
      "epoch 67, val_loss 454.8396301269531\n",
      "epoch 68, train_loss 1277.174560546875\n",
      "epoch 68, val_loss 454.83953857421875\n",
      "epoch 69, train_loss 1277.1744384765625\n",
      "epoch 69, val_loss 454.8395690917969\n",
      "epoch 70, train_loss 1277.17431640625\n",
      "epoch 70, val_loss 454.83953857421875\n",
      "epoch 71, train_loss 1277.17431640625\n",
      "epoch 71, val_loss 454.8395080566406\n",
      "epoch 72, train_loss 1277.17431640625\n",
      "epoch 72, val_loss 454.83941650390625\n",
      "epoch 73, train_loss 1277.1741943359375\n",
      "epoch 73, val_loss 454.83929443359375\n",
      "epoch 74, train_loss 1277.17431640625\n",
      "epoch 74, val_loss 454.8393249511719\n",
      "epoch 75, train_loss 1277.1741943359375\n",
      "epoch 75, val_loss 454.8392639160156\n",
      "epoch 76, train_loss 1277.1741943359375\n",
      "epoch 76, val_loss 454.8392639160156\n",
      "epoch 77, train_loss 1277.1741943359375\n",
      "epoch 77, val_loss 454.8392028808594\n",
      "epoch 78, train_loss 1277.1739501953125\n",
      "epoch 78, val_loss 454.8392028808594\n",
      "epoch 79, train_loss 1277.1739501953125\n",
      "epoch 79, val_loss 454.8390808105469\n",
      "epoch 80, train_loss 1277.1739501953125\n",
      "epoch 80, val_loss 454.8389892578125\n",
      "epoch 81, train_loss 1277.173828125\n",
      "epoch 81, val_loss 454.8389892578125\n",
      "epoch 82, train_loss 1277.1737060546875\n",
      "epoch 82, val_loss 454.8389587402344\n",
      "epoch 83, train_loss 1277.1737060546875\n",
      "epoch 83, val_loss 454.83892822265625\n",
      "epoch 84, train_loss 1277.1737060546875\n",
      "epoch 84, val_loss 454.8388366699219\n",
      "epoch 85, train_loss 1277.1737060546875\n",
      "epoch 85, val_loss 454.8388366699219\n",
      "epoch 86, train_loss 1277.1737060546875\n",
      "epoch 86, val_loss 454.83880615234375\n",
      "epoch 87, train_loss 1277.1737060546875\n",
      "epoch 87, val_loss 454.8387145996094\n",
      "epoch 88, train_loss 1277.1734619140625\n",
      "epoch 88, val_loss 454.8386535644531\n",
      "epoch 89, train_loss 1277.1734619140625\n",
      "epoch 89, val_loss 454.8385925292969\n",
      "epoch 90, train_loss 1277.1734619140625\n",
      "epoch 90, val_loss 454.8385925292969\n",
      "epoch 91, train_loss 1277.1734619140625\n",
      "epoch 91, val_loss 454.8384704589844\n",
      "epoch 92, train_loss 1277.1734619140625\n",
      "epoch 92, val_loss 454.8385009765625\n",
      "epoch 93, train_loss 1277.17333984375\n",
      "epoch 93, val_loss 454.8384094238281\n",
      "epoch 94, train_loss 1277.1732177734375\n",
      "epoch 94, val_loss 454.83837890625\n",
      "epoch 95, train_loss 1277.1732177734375\n",
      "epoch 95, val_loss 454.8382873535156\n",
      "epoch 96, train_loss 1277.1732177734375\n",
      "epoch 96, val_loss 454.8383483886719\n",
      "epoch 97, train_loss 1277.1732177734375\n",
      "epoch 97, val_loss 454.8382873535156\n",
      "epoch 98, train_loss 1277.173095703125\n",
      "epoch 98, val_loss 454.838134765625\n",
      "epoch 99, train_loss 1277.1732177734375\n",
      "epoch 99, val_loss 454.838134765625\n",
      "Parameter containing:\n",
      "tensor([3.1140e-24], requires_grad=True)\n",
      "iter 128, train_loss_regularization 0.7140482068061829\n",
      "iter 128, val_loss_regularization 0.7140482068061829\n",
      "epoch 0, train_loss 1277.1729736328125\n",
      "epoch 0, val_loss 454.838134765625\n",
      "epoch 1, train_loss 1277.1728515625\n",
      "epoch 1, val_loss 454.83807373046875\n",
      "epoch 2, train_loss 1277.1728515625\n",
      "epoch 2, val_loss 454.8380126953125\n",
      "epoch 3, train_loss 1277.1728515625\n",
      "epoch 3, val_loss 454.8380126953125\n",
      "epoch 4, train_loss 1277.1728515625\n",
      "epoch 4, val_loss 454.8379211425781\n",
      "epoch 5, train_loss 1277.1729736328125\n",
      "epoch 5, val_loss 454.83782958984375\n",
      "epoch 6, train_loss 1277.1729736328125\n",
      "epoch 6, val_loss 454.83782958984375\n",
      "epoch 7, train_loss 1277.172607421875\n",
      "epoch 7, val_loss 454.8377990722656\n",
      "epoch 8, train_loss 1277.172607421875\n",
      "epoch 8, val_loss 454.8377380371094\n",
      "epoch 9, train_loss 1277.172607421875\n",
      "epoch 9, val_loss 454.8376770019531\n",
      "epoch 10, train_loss 1277.172607421875\n",
      "epoch 10, val_loss 454.8376770019531\n",
      "epoch 11, train_loss 1277.1724853515625\n",
      "epoch 11, val_loss 454.8376159667969\n",
      "epoch 12, train_loss 1277.172607421875\n",
      "epoch 12, val_loss 454.8375549316406\n",
      "epoch 13, train_loss 1277.17236328125\n",
      "epoch 13, val_loss 454.8375549316406\n",
      "epoch 14, train_loss 1277.1722412109375\n",
      "epoch 14, val_loss 454.8374938964844\n",
      "epoch 15, train_loss 1277.1722412109375\n",
      "epoch 15, val_loss 454.8373718261719\n",
      "epoch 16, train_loss 1277.17236328125\n",
      "epoch 16, val_loss 454.83734130859375\n",
      "epoch 17, train_loss 1277.1722412109375\n",
      "epoch 17, val_loss 454.83734130859375\n",
      "epoch 18, train_loss 1277.17236328125\n",
      "epoch 18, val_loss 454.83721923828125\n",
      "epoch 19, train_loss 1277.1722412109375\n",
      "epoch 19, val_loss 454.83721923828125\n",
      "epoch 20, train_loss 1277.172119140625\n",
      "epoch 20, val_loss 454.837158203125\n",
      "epoch 21, train_loss 1277.172119140625\n",
      "epoch 21, val_loss 454.8371276855469\n",
      "epoch 22, train_loss 1277.172119140625\n",
      "epoch 22, val_loss 454.83709716796875\n",
      "epoch 23, train_loss 1277.171875\n",
      "epoch 23, val_loss 454.83709716796875\n",
      "epoch 24, train_loss 1277.171875\n",
      "epoch 24, val_loss 454.8370361328125\n",
      "epoch 25, train_loss 1277.171875\n",
      "epoch 25, val_loss 454.8369445800781\n",
      "epoch 26, train_loss 1277.171875\n",
      "epoch 26, val_loss 454.8369140625\n",
      "epoch 27, train_loss 1277.1717529296875\n",
      "epoch 27, val_loss 454.8368225097656\n",
      "epoch 28, train_loss 1277.171630859375\n",
      "epoch 28, val_loss 454.8367614746094\n",
      "epoch 29, train_loss 1277.171630859375\n",
      "epoch 29, val_loss 454.8367614746094\n",
      "epoch 30, train_loss 1277.171630859375\n",
      "epoch 30, val_loss 454.8367004394531\n",
      "epoch 31, train_loss 1277.171630859375\n",
      "epoch 31, val_loss 454.836669921875\n",
      "epoch 32, train_loss 1277.1715087890625\n",
      "epoch 32, val_loss 454.8366394042969\n",
      "epoch 33, train_loss 1277.1715087890625\n",
      "epoch 33, val_loss 454.8366394042969\n",
      "epoch 34, train_loss 1277.171630859375\n",
      "epoch 34, val_loss 454.83648681640625\n",
      "epoch 35, train_loss 1277.17138671875\n",
      "epoch 35, val_loss 454.8364562988281\n",
      "epoch 36, train_loss 1277.17138671875\n",
      "epoch 36, val_loss 454.8364562988281\n",
      "epoch 37, train_loss 1277.17138671875\n",
      "epoch 37, val_loss 454.83642578125\n",
      "epoch 38, train_loss 1277.1712646484375\n",
      "epoch 38, val_loss 454.8363342285156\n",
      "epoch 39, train_loss 1277.171142578125\n",
      "epoch 39, val_loss 454.8363342285156\n",
      "epoch 40, train_loss 1277.171142578125\n",
      "epoch 40, val_loss 454.83624267578125\n",
      "epoch 41, train_loss 1277.171142578125\n",
      "epoch 41, val_loss 454.8362121582031\n",
      "epoch 42, train_loss 1277.171142578125\n",
      "epoch 42, val_loss 454.836181640625\n",
      "epoch 43, train_loss 1277.1710205078125\n",
      "epoch 43, val_loss 454.8360290527344\n",
      "epoch 44, train_loss 1277.171142578125\n",
      "epoch 44, val_loss 454.83599853515625\n",
      "epoch 45, train_loss 1277.1708984375\n",
      "epoch 45, val_loss 454.83599853515625\n",
      "epoch 46, train_loss 1277.1707763671875\n",
      "epoch 46, val_loss 454.83599853515625\n",
      "epoch 47, train_loss 1277.1707763671875\n",
      "epoch 47, val_loss 454.8359069824219\n",
      "epoch 48, train_loss 1277.1708984375\n",
      "epoch 48, val_loss 454.83587646484375\n",
      "epoch 49, train_loss 1277.1708984375\n",
      "epoch 49, val_loss 454.83587646484375\n",
      "epoch 50, train_loss 1277.170654296875\n",
      "epoch 50, val_loss 454.8357849121094\n",
      "epoch 51, train_loss 1277.1707763671875\n",
      "epoch 51, val_loss 454.83575439453125\n",
      "epoch 52, train_loss 1277.170654296875\n",
      "epoch 52, val_loss 454.83575439453125\n",
      "epoch 53, train_loss 1277.170654296875\n",
      "epoch 53, val_loss 454.83563232421875\n",
      "epoch 54, train_loss 1277.1705322265625\n",
      "epoch 54, val_loss 454.8355712890625\n",
      "epoch 55, train_loss 1277.1705322265625\n",
      "epoch 55, val_loss 454.83551025390625\n",
      "epoch 56, train_loss 1277.1705322265625\n",
      "epoch 56, val_loss 454.83551025390625\n",
      "epoch 57, train_loss 1277.17041015625\n",
      "epoch 57, val_loss 454.83544921875\n",
      "epoch 58, train_loss 1277.1702880859375\n",
      "epoch 58, val_loss 454.8354187011719\n",
      "epoch 59, train_loss 1277.1702880859375\n",
      "epoch 59, val_loss 454.83538818359375\n",
      "epoch 60, train_loss 1277.1702880859375\n",
      "epoch 60, val_loss 454.83538818359375\n",
      "epoch 61, train_loss 1277.170166015625\n",
      "epoch 61, val_loss 454.8352355957031\n",
      "epoch 62, train_loss 1277.1702880859375\n",
      "epoch 62, val_loss 454.835205078125\n",
      "epoch 63, train_loss 1277.1702880859375\n",
      "epoch 63, val_loss 454.835205078125\n",
      "epoch 64, train_loss 1277.1700439453125\n",
      "epoch 64, val_loss 454.8351745605469\n",
      "epoch 65, train_loss 1277.1700439453125\n",
      "epoch 65, val_loss 454.8350830078125\n",
      "epoch 66, train_loss 1277.1700439453125\n",
      "epoch 66, val_loss 454.8350524902344\n",
      "epoch 67, train_loss 1277.1700439453125\n",
      "epoch 67, val_loss 454.8349914550781\n",
      "epoch 68, train_loss 1277.1700439453125\n",
      "epoch 68, val_loss 454.8349304199219\n",
      "epoch 69, train_loss 1277.1697998046875\n",
      "epoch 69, val_loss 454.8349304199219\n",
      "epoch 70, train_loss 1277.169921875\n",
      "epoch 70, val_loss 454.8348693847656\n",
      "epoch 71, train_loss 1277.1697998046875\n",
      "epoch 71, val_loss 454.83477783203125\n",
      "epoch 72, train_loss 1277.169677734375\n",
      "epoch 72, val_loss 454.83477783203125\n",
      "epoch 73, train_loss 1277.169677734375\n",
      "epoch 73, val_loss 454.8347473144531\n",
      "epoch 74, train_loss 1277.1695556640625\n",
      "epoch 74, val_loss 454.83465576171875\n",
      "epoch 75, train_loss 1277.1695556640625\n",
      "epoch 75, val_loss 454.8346252441406\n",
      "epoch 76, train_loss 1277.1695556640625\n",
      "epoch 76, val_loss 454.8345947265625\n",
      "epoch 77, train_loss 1277.169677734375\n",
      "epoch 77, val_loss 454.8345947265625\n",
      "epoch 78, train_loss 1277.1695556640625\n",
      "epoch 78, val_loss 454.8345031738281\n",
      "epoch 79, train_loss 1277.16943359375\n",
      "epoch 79, val_loss 454.83441162109375\n",
      "epoch 80, train_loss 1277.1693115234375\n",
      "epoch 80, val_loss 454.8343811035156\n",
      "epoch 81, train_loss 1277.1693115234375\n",
      "epoch 81, val_loss 454.8343200683594\n",
      "epoch 82, train_loss 1277.169189453125\n",
      "epoch 82, val_loss 454.8343200683594\n",
      "epoch 83, train_loss 1277.169189453125\n",
      "epoch 83, val_loss 454.83428955078125\n",
      "epoch 84, train_loss 1277.1689453125\n",
      "epoch 84, val_loss 454.8341979980469\n",
      "epoch 85, train_loss 1277.169189453125\n",
      "epoch 85, val_loss 454.8341979980469\n",
      "epoch 86, train_loss 1277.1690673828125\n",
      "epoch 86, val_loss 454.8341369628906\n",
      "epoch 87, train_loss 1277.1689453125\n",
      "epoch 87, val_loss 454.8341369628906\n",
      "epoch 88, train_loss 1277.1689453125\n",
      "epoch 88, val_loss 454.8340759277344\n",
      "epoch 89, train_loss 1277.1689453125\n",
      "epoch 89, val_loss 454.8340148925781\n",
      "epoch 90, train_loss 1277.1689453125\n",
      "epoch 90, val_loss 454.83392333984375\n",
      "epoch 91, train_loss 1277.1689453125\n",
      "epoch 91, val_loss 454.8338623046875\n",
      "epoch 92, train_loss 1277.1689453125\n",
      "epoch 92, val_loss 454.8338317871094\n",
      "epoch 93, train_loss 1277.168701171875\n",
      "epoch 93, val_loss 454.83380126953125\n",
      "epoch 94, train_loss 1277.168701171875\n",
      "epoch 94, val_loss 454.833740234375\n",
      "epoch 95, train_loss 1277.168701171875\n",
      "epoch 95, val_loss 454.8337097167969\n",
      "epoch 96, train_loss 1277.168701171875\n",
      "epoch 96, val_loss 454.83367919921875\n",
      "epoch 97, train_loss 1277.168701171875\n",
      "epoch 97, val_loss 454.8335876464844\n",
      "epoch 98, train_loss 1277.16845703125\n",
      "epoch 98, val_loss 454.8335876464844\n",
      "epoch 99, train_loss 1277.16845703125\n",
      "epoch 99, val_loss 454.8335266113281\n",
      "Parameter containing:\n",
      "tensor([2.1581e-24], requires_grad=True)\n",
      "iter 129, train_loss_regularization 0.7130042910575867\n",
      "iter 129, val_loss_regularization 0.7130042910575867\n",
      "epoch 0, train_loss 1277.16845703125\n",
      "epoch 0, val_loss 454.8334655761719\n",
      "epoch 1, train_loss 1277.16845703125\n",
      "epoch 1, val_loss 454.8334655761719\n",
      "epoch 2, train_loss 1277.16845703125\n",
      "epoch 2, val_loss 454.8333740234375\n",
      "epoch 3, train_loss 1277.16845703125\n",
      "epoch 3, val_loss 454.8333435058594\n",
      "epoch 4, train_loss 1277.16845703125\n",
      "epoch 4, val_loss 454.833251953125\n",
      "epoch 5, train_loss 1277.168212890625\n",
      "epoch 5, val_loss 454.833251953125\n",
      "epoch 6, train_loss 1277.168212890625\n",
      "epoch 6, val_loss 454.833251953125\n",
      "epoch 7, train_loss 1277.168212890625\n",
      "epoch 7, val_loss 454.83306884765625\n",
      "epoch 8, train_loss 1277.1680908203125\n",
      "epoch 8, val_loss 454.83306884765625\n",
      "epoch 9, train_loss 1277.168212890625\n",
      "epoch 9, val_loss 454.8330383300781\n",
      "epoch 10, train_loss 1277.168212890625\n",
      "epoch 10, val_loss 454.8330383300781\n",
      "epoch 11, train_loss 1277.16796875\n",
      "epoch 11, val_loss 454.83294677734375\n",
      "epoch 12, train_loss 1277.1678466796875\n",
      "epoch 12, val_loss 454.83294677734375\n",
      "epoch 13, train_loss 1277.1678466796875\n",
      "epoch 13, val_loss 454.83294677734375\n",
      "epoch 14, train_loss 1277.167724609375\n",
      "epoch 14, val_loss 454.83282470703125\n",
      "epoch 15, train_loss 1277.167724609375\n",
      "epoch 15, val_loss 454.8327941894531\n",
      "epoch 16, train_loss 1277.167724609375\n",
      "epoch 16, val_loss 454.83270263671875\n",
      "epoch 17, train_loss 1277.167724609375\n",
      "epoch 17, val_loss 454.8326110839844\n",
      "epoch 18, train_loss 1277.167724609375\n",
      "epoch 18, val_loss 454.8326110839844\n",
      "epoch 19, train_loss 1277.1676025390625\n",
      "epoch 19, val_loss 454.83258056640625\n",
      "epoch 20, train_loss 1277.16748046875\n",
      "epoch 20, val_loss 454.83258056640625\n",
      "epoch 21, train_loss 1277.1676025390625\n",
      "epoch 21, val_loss 454.8324890136719\n",
      "epoch 22, train_loss 1277.16748046875\n",
      "epoch 22, val_loss 454.83245849609375\n",
      "epoch 23, train_loss 1277.16748046875\n",
      "epoch 23, val_loss 454.8324279785156\n",
      "epoch 24, train_loss 1277.16748046875\n",
      "epoch 24, val_loss 454.8323669433594\n",
      "epoch 25, train_loss 1277.1673583984375\n",
      "epoch 25, val_loss 454.8323059082031\n",
      "epoch 26, train_loss 1277.1673583984375\n",
      "epoch 26, val_loss 454.8323059082031\n",
      "epoch 27, train_loss 1277.167236328125\n",
      "epoch 27, val_loss 454.8322448730469\n",
      "epoch 28, train_loss 1277.167236328125\n",
      "epoch 28, val_loss 454.8321533203125\n",
      "epoch 29, train_loss 1277.1671142578125\n",
      "epoch 29, val_loss 454.8321228027344\n",
      "epoch 30, train_loss 1277.1671142578125\n",
      "epoch 30, val_loss 454.83209228515625\n",
      "epoch 31, train_loss 1277.1668701171875\n",
      "epoch 31, val_loss 454.83209228515625\n",
      "epoch 32, train_loss 1277.1668701171875\n",
      "epoch 32, val_loss 454.8320007324219\n",
      "epoch 33, train_loss 1277.1669921875\n",
      "epoch 33, val_loss 454.8320007324219\n",
      "epoch 34, train_loss 1277.1668701171875\n",
      "epoch 34, val_loss 454.8318786621094\n",
      "epoch 35, train_loss 1277.1671142578125\n",
      "epoch 35, val_loss 454.83184814453125\n",
      "epoch 36, train_loss 1277.1668701171875\n",
      "epoch 36, val_loss 454.83184814453125\n",
      "epoch 37, train_loss 1277.1669921875\n",
      "epoch 37, val_loss 454.831787109375\n",
      "epoch 38, train_loss 1277.166748046875\n",
      "epoch 38, val_loss 454.8317565917969\n",
      "epoch 39, train_loss 1277.166748046875\n",
      "epoch 39, val_loss 454.8316650390625\n",
      "epoch 40, train_loss 1277.166748046875\n",
      "epoch 40, val_loss 454.8316650390625\n",
      "epoch 41, train_loss 1277.1666259765625\n",
      "epoch 41, val_loss 454.8315734863281\n",
      "epoch 42, train_loss 1277.1666259765625\n",
      "epoch 42, val_loss 454.83154296875\n",
      "epoch 43, train_loss 1277.1666259765625\n",
      "epoch 43, val_loss 454.8315124511719\n",
      "epoch 44, train_loss 1277.16650390625\n",
      "epoch 44, val_loss 454.8314208984375\n",
      "epoch 45, train_loss 1277.16650390625\n",
      "epoch 45, val_loss 454.8314208984375\n",
      "epoch 46, train_loss 1277.1663818359375\n",
      "epoch 46, val_loss 454.8313293457031\n",
      "epoch 47, train_loss 1277.1663818359375\n",
      "epoch 47, val_loss 454.8313293457031\n",
      "epoch 48, train_loss 1277.1663818359375\n",
      "epoch 48, val_loss 454.83123779296875\n",
      "epoch 49, train_loss 1277.1663818359375\n",
      "epoch 49, val_loss 454.83123779296875\n",
      "epoch 50, train_loss 1277.1663818359375\n",
      "epoch 50, val_loss 454.8312072753906\n",
      "epoch 51, train_loss 1277.166259765625\n",
      "epoch 51, val_loss 454.8311767578125\n",
      "epoch 52, train_loss 1277.1661376953125\n",
      "epoch 52, val_loss 454.8310852050781\n",
      "epoch 53, train_loss 1277.1661376953125\n",
      "epoch 53, val_loss 454.83099365234375\n",
      "epoch 54, train_loss 1277.1661376953125\n",
      "epoch 54, val_loss 454.83099365234375\n",
      "epoch 55, train_loss 1277.166015625\n",
      "epoch 55, val_loss 454.8309020996094\n",
      "epoch 56, train_loss 1277.166015625\n",
      "epoch 56, val_loss 454.8309020996094\n",
      "epoch 57, train_loss 1277.1658935546875\n",
      "epoch 57, val_loss 454.8308410644531\n",
      "epoch 58, train_loss 1277.1658935546875\n",
      "epoch 58, val_loss 454.8308410644531\n",
      "epoch 59, train_loss 1277.1658935546875\n",
      "epoch 59, val_loss 454.8307800292969\n",
      "epoch 60, train_loss 1277.1658935546875\n",
      "epoch 60, val_loss 454.8306579589844\n",
      "epoch 61, train_loss 1277.1658935546875\n",
      "epoch 61, val_loss 454.8306579589844\n",
      "epoch 62, train_loss 1277.1656494140625\n",
      "epoch 62, val_loss 454.8306579589844\n",
      "epoch 63, train_loss 1277.16552734375\n",
      "epoch 63, val_loss 454.83062744140625\n",
      "epoch 64, train_loss 1277.16552734375\n",
      "epoch 64, val_loss 454.83050537109375\n",
      "epoch 65, train_loss 1277.16552734375\n",
      "epoch 65, val_loss 454.83050537109375\n",
      "epoch 66, train_loss 1277.16552734375\n",
      "epoch 66, val_loss 454.8304138183594\n",
      "epoch 67, train_loss 1277.16552734375\n",
      "epoch 67, val_loss 454.830322265625\n",
      "epoch 68, train_loss 1277.16552734375\n",
      "epoch 68, val_loss 454.83038330078125\n",
      "epoch 69, train_loss 1277.16552734375\n",
      "epoch 69, val_loss 454.8302917480469\n",
      "epoch 70, train_loss 1277.165283203125\n",
      "epoch 70, val_loss 454.83026123046875\n",
      "epoch 71, train_loss 1277.165283203125\n",
      "epoch 71, val_loss 454.8301696777344\n",
      "epoch 72, train_loss 1277.165283203125\n",
      "epoch 72, val_loss 454.8301696777344\n",
      "epoch 73, train_loss 1277.1651611328125\n",
      "epoch 73, val_loss 454.83013916015625\n",
      "epoch 74, train_loss 1277.1651611328125\n",
      "epoch 74, val_loss 454.830078125\n",
      "epoch 75, train_loss 1277.1650390625\n",
      "epoch 75, val_loss 454.8300476074219\n",
      "epoch 76, train_loss 1277.1650390625\n",
      "epoch 76, val_loss 454.8300476074219\n",
      "epoch 77, train_loss 1277.164794921875\n",
      "epoch 77, val_loss 454.8299865722656\n",
      "epoch 78, train_loss 1277.1650390625\n",
      "epoch 78, val_loss 454.8298645019531\n",
      "epoch 79, train_loss 1277.1650390625\n",
      "epoch 79, val_loss 454.8298034667969\n",
      "epoch 80, train_loss 1277.1649169921875\n",
      "epoch 80, val_loss 454.8298034667969\n",
      "epoch 81, train_loss 1277.1650390625\n",
      "epoch 81, val_loss 454.8297119140625\n",
      "epoch 82, train_loss 1277.1649169921875\n",
      "epoch 82, val_loss 454.8297119140625\n",
      "epoch 83, train_loss 1277.164794921875\n",
      "epoch 83, val_loss 454.8296203613281\n",
      "epoch 84, train_loss 1277.1646728515625\n",
      "epoch 84, val_loss 454.8296203613281\n",
      "epoch 85, train_loss 1277.164794921875\n",
      "epoch 85, val_loss 454.82958984375\n",
      "epoch 86, train_loss 1277.16455078125\n",
      "epoch 86, val_loss 454.82952880859375\n",
      "epoch 87, train_loss 1277.1646728515625\n",
      "epoch 87, val_loss 454.8294982910156\n",
      "epoch 88, train_loss 1277.16455078125\n",
      "epoch 88, val_loss 454.82940673828125\n",
      "epoch 89, train_loss 1277.1644287109375\n",
      "epoch 89, val_loss 454.8293762207031\n",
      "epoch 90, train_loss 1277.164306640625\n",
      "epoch 90, val_loss 454.829345703125\n",
      "epoch 91, train_loss 1277.164306640625\n",
      "epoch 91, val_loss 454.82928466796875\n",
      "epoch 92, train_loss 1277.1644287109375\n",
      "epoch 92, val_loss 454.8291931152344\n",
      "epoch 93, train_loss 1277.1644287109375\n",
      "epoch 93, val_loss 454.8291931152344\n",
      "epoch 94, train_loss 1277.164306640625\n",
      "epoch 94, val_loss 454.82916259765625\n",
      "epoch 95, train_loss 1277.164306640625\n",
      "epoch 95, val_loss 454.8291320800781\n",
      "epoch 96, train_loss 1277.164306640625\n",
      "epoch 96, val_loss 454.8290710449219\n",
      "epoch 97, train_loss 1277.1641845703125\n",
      "epoch 97, val_loss 454.8290100097656\n",
      "epoch 98, train_loss 1277.1640625\n",
      "epoch 98, val_loss 454.8289489746094\n",
      "epoch 99, train_loss 1277.1639404296875\n",
      "epoch 99, val_loss 454.82891845703125\n",
      "Parameter containing:\n",
      "tensor([1.4961e-24], requires_grad=True)\n",
      "iter 130, train_loss_regularization 0.7119845151901245\n",
      "iter 130, val_loss_regularization 0.7119845151901245\n",
      "epoch 0, train_loss 1277.1639404296875\n",
      "epoch 0, val_loss 454.8288879394531\n",
      "epoch 1, train_loss 1277.1639404296875\n",
      "epoch 1, val_loss 454.8288269042969\n",
      "epoch 2, train_loss 1277.1639404296875\n",
      "epoch 2, val_loss 454.8288269042969\n",
      "epoch 3, train_loss 1277.1639404296875\n",
      "epoch 3, val_loss 454.8287353515625\n",
      "epoch 4, train_loss 1277.163818359375\n",
      "epoch 4, val_loss 454.8287048339844\n",
      "epoch 5, train_loss 1277.1636962890625\n",
      "epoch 5, val_loss 454.82867431640625\n",
      "epoch 6, train_loss 1277.1636962890625\n",
      "epoch 6, val_loss 454.8285827636719\n",
      "epoch 7, train_loss 1277.16357421875\n",
      "epoch 7, val_loss 454.82855224609375\n",
      "epoch 8, train_loss 1277.16357421875\n",
      "epoch 8, val_loss 454.82855224609375\n",
      "epoch 9, train_loss 1277.1634521484375\n",
      "epoch 9, val_loss 454.8284912109375\n",
      "epoch 10, train_loss 1277.1634521484375\n",
      "epoch 10, val_loss 454.82843017578125\n",
      "epoch 11, train_loss 1277.16357421875\n",
      "epoch 11, val_loss 454.82843017578125\n",
      "epoch 12, train_loss 1277.1634521484375\n",
      "epoch 12, val_loss 454.828369140625\n",
      "epoch 13, train_loss 1277.163330078125\n",
      "epoch 13, val_loss 454.8283386230469\n",
      "epoch 14, train_loss 1277.1632080078125\n",
      "epoch 14, val_loss 454.8282775878906\n",
      "epoch 15, train_loss 1277.163330078125\n",
      "epoch 15, val_loss 454.8282165527344\n",
      "epoch 16, train_loss 1277.163330078125\n",
      "epoch 16, val_loss 454.8281555175781\n",
      "epoch 17, train_loss 1277.1632080078125\n",
      "epoch 17, val_loss 454.8280944824219\n",
      "epoch 18, train_loss 1277.1632080078125\n",
      "epoch 18, val_loss 454.8280334472656\n",
      "epoch 19, train_loss 1277.1630859375\n",
      "epoch 19, val_loss 454.8280334472656\n",
      "epoch 20, train_loss 1277.1630859375\n",
      "epoch 20, val_loss 454.8280029296875\n",
      "epoch 21, train_loss 1277.1630859375\n",
      "epoch 21, val_loss 454.8279113769531\n",
      "epoch 22, train_loss 1277.1630859375\n",
      "epoch 22, val_loss 454.827880859375\n",
      "epoch 23, train_loss 1277.1630859375\n",
      "epoch 23, val_loss 454.8279113769531\n",
      "epoch 24, train_loss 1277.1629638671875\n",
      "epoch 24, val_loss 454.82769775390625\n",
      "epoch 25, train_loss 1277.1629638671875\n",
      "epoch 25, val_loss 454.8277587890625\n",
      "epoch 26, train_loss 1277.1627197265625\n",
      "epoch 26, val_loss 454.8276672363281\n",
      "epoch 27, train_loss 1277.1627197265625\n",
      "epoch 27, val_loss 454.82769775390625\n",
      "epoch 28, train_loss 1277.162841796875\n",
      "epoch 28, val_loss 454.82757568359375\n",
      "epoch 29, train_loss 1277.1627197265625\n",
      "epoch 29, val_loss 454.8275451660156\n",
      "epoch 30, train_loss 1277.1627197265625\n",
      "epoch 30, val_loss 454.8275451660156\n",
      "epoch 31, train_loss 1277.16259765625\n",
      "epoch 31, val_loss 454.8275146484375\n",
      "epoch 32, train_loss 1277.16259765625\n",
      "epoch 32, val_loss 454.82745361328125\n",
      "epoch 33, train_loss 1277.162353515625\n",
      "epoch 33, val_loss 454.8274230957031\n",
      "epoch 34, train_loss 1277.1624755859375\n",
      "epoch 34, val_loss 454.82733154296875\n",
      "epoch 35, train_loss 1277.1624755859375\n",
      "epoch 35, val_loss 454.8273010253906\n",
      "epoch 36, train_loss 1277.1624755859375\n",
      "epoch 36, val_loss 454.82720947265625\n",
      "epoch 37, train_loss 1277.162353515625\n",
      "epoch 37, val_loss 454.82720947265625\n",
      "epoch 38, train_loss 1277.162353515625\n",
      "epoch 38, val_loss 454.82720947265625\n",
      "epoch 39, train_loss 1277.162353515625\n",
      "epoch 39, val_loss 454.82708740234375\n",
      "epoch 40, train_loss 1277.1622314453125\n",
      "epoch 40, val_loss 454.82708740234375\n",
      "epoch 41, train_loss 1277.1622314453125\n",
      "epoch 41, val_loss 454.8270568847656\n",
      "epoch 42, train_loss 1277.162109375\n",
      "epoch 42, val_loss 454.826904296875\n",
      "epoch 43, train_loss 1277.162109375\n",
      "epoch 43, val_loss 454.826904296875\n",
      "epoch 44, train_loss 1277.162109375\n",
      "epoch 44, val_loss 454.826904296875\n",
      "epoch 45, train_loss 1277.1619873046875\n",
      "epoch 45, val_loss 454.82684326171875\n",
      "epoch 46, train_loss 1277.161865234375\n",
      "epoch 46, val_loss 454.8267822265625\n",
      "epoch 47, train_loss 1277.161865234375\n",
      "epoch 47, val_loss 454.8267517089844\n",
      "epoch 48, train_loss 1277.1617431640625\n",
      "epoch 48, val_loss 454.82672119140625\n",
      "epoch 49, train_loss 1277.1617431640625\n",
      "epoch 49, val_loss 454.82672119140625\n",
      "epoch 50, train_loss 1277.161865234375\n",
      "epoch 50, val_loss 454.82666015625\n",
      "epoch 51, train_loss 1277.16162109375\n",
      "epoch 51, val_loss 454.8265686035156\n",
      "epoch 52, train_loss 1277.16162109375\n",
      "epoch 52, val_loss 454.8265686035156\n",
      "epoch 53, train_loss 1277.1617431640625\n",
      "epoch 53, val_loss 454.8264465332031\n",
      "epoch 54, train_loss 1277.16162109375\n",
      "epoch 54, val_loss 454.826416015625\n",
      "epoch 55, train_loss 1277.16162109375\n",
      "epoch 55, val_loss 454.8263854980469\n",
      "epoch 56, train_loss 1277.1614990234375\n",
      "epoch 56, val_loss 454.8263244628906\n",
      "epoch 57, train_loss 1277.161376953125\n",
      "epoch 57, val_loss 454.8263244628906\n",
      "epoch 58, train_loss 1277.161376953125\n",
      "epoch 58, val_loss 454.8262939453125\n",
      "epoch 59, train_loss 1277.161376953125\n",
      "epoch 59, val_loss 454.8262023925781\n",
      "epoch 60, train_loss 1277.161376953125\n",
      "epoch 60, val_loss 454.826171875\n",
      "epoch 61, train_loss 1277.161376953125\n",
      "epoch 61, val_loss 454.8260803222656\n",
      "epoch 62, train_loss 1277.1611328125\n",
      "epoch 62, val_loss 454.8260498046875\n",
      "epoch 63, train_loss 1277.1611328125\n",
      "epoch 63, val_loss 454.82598876953125\n",
      "epoch 64, train_loss 1277.1611328125\n",
      "epoch 64, val_loss 454.82598876953125\n",
      "epoch 65, train_loss 1277.1611328125\n",
      "epoch 65, val_loss 454.825927734375\n",
      "epoch 66, train_loss 1277.1611328125\n",
      "epoch 66, val_loss 454.825927734375\n",
      "epoch 67, train_loss 1277.1610107421875\n",
      "epoch 67, val_loss 454.8258361816406\n",
      "epoch 68, train_loss 1277.1610107421875\n",
      "epoch 68, val_loss 454.82574462890625\n",
      "epoch 69, train_loss 1277.160888671875\n",
      "epoch 69, val_loss 454.8257141113281\n",
      "epoch 70, train_loss 1277.160888671875\n",
      "epoch 70, val_loss 454.8257141113281\n",
      "epoch 71, train_loss 1277.160888671875\n",
      "epoch 71, val_loss 454.82562255859375\n",
      "epoch 72, train_loss 1277.160888671875\n",
      "epoch 72, val_loss 454.82562255859375\n",
      "epoch 73, train_loss 1277.160888671875\n",
      "epoch 73, val_loss 454.8255920410156\n",
      "epoch 74, train_loss 1277.160888671875\n",
      "epoch 74, val_loss 454.8255310058594\n",
      "epoch 75, train_loss 1277.160888671875\n",
      "epoch 75, val_loss 454.8254699707031\n",
      "epoch 76, train_loss 1277.160888671875\n",
      "epoch 76, val_loss 454.8254699707031\n",
      "epoch 77, train_loss 1277.1605224609375\n",
      "epoch 77, val_loss 454.8254699707031\n",
      "epoch 78, train_loss 1277.1605224609375\n",
      "epoch 78, val_loss 454.8253479003906\n",
      "epoch 79, train_loss 1277.160400390625\n",
      "epoch 79, val_loss 454.82525634765625\n",
      "epoch 80, train_loss 1277.160400390625\n",
      "epoch 80, val_loss 454.82525634765625\n",
      "epoch 81, train_loss 1277.1605224609375\n",
      "epoch 81, val_loss 454.8251647949219\n",
      "epoch 82, train_loss 1277.1605224609375\n",
      "epoch 82, val_loss 454.82513427734375\n",
      "epoch 83, train_loss 1277.1602783203125\n",
      "epoch 83, val_loss 454.82513427734375\n",
      "epoch 84, train_loss 1277.1602783203125\n",
      "epoch 84, val_loss 454.8250732421875\n",
      "epoch 85, train_loss 1277.1602783203125\n",
      "epoch 85, val_loss 454.82501220703125\n",
      "epoch 86, train_loss 1277.1602783203125\n",
      "epoch 86, val_loss 454.82501220703125\n",
      "epoch 87, train_loss 1277.16015625\n",
      "epoch 87, val_loss 454.82501220703125\n",
      "epoch 88, train_loss 1277.16015625\n",
      "epoch 88, val_loss 454.8249206542969\n",
      "epoch 89, train_loss 1277.16015625\n",
      "epoch 89, val_loss 454.8248291015625\n",
      "epoch 90, train_loss 1277.1600341796875\n",
      "epoch 90, val_loss 454.8247375488281\n",
      "epoch 91, train_loss 1277.1600341796875\n",
      "epoch 91, val_loss 454.8247375488281\n",
      "epoch 92, train_loss 1277.1597900390625\n",
      "epoch 92, val_loss 454.82470703125\n",
      "epoch 93, train_loss 1277.1597900390625\n",
      "epoch 93, val_loss 454.8246765136719\n",
      "epoch 94, train_loss 1277.1597900390625\n",
      "epoch 94, val_loss 454.8246154785156\n",
      "epoch 95, train_loss 1277.1597900390625\n",
      "epoch 95, val_loss 454.8246154785156\n",
      "epoch 96, train_loss 1277.1597900390625\n",
      "epoch 96, val_loss 454.8245544433594\n",
      "epoch 97, train_loss 1277.15966796875\n",
      "epoch 97, val_loss 454.824462890625\n",
      "epoch 98, train_loss 1277.15966796875\n",
      "epoch 98, val_loss 454.82440185546875\n",
      "epoch 99, train_loss 1277.15966796875\n",
      "epoch 99, val_loss 454.8243713378906\n",
      "Parameter containing:\n",
      "tensor([1.0375e-24], requires_grad=True)\n",
      "iter 131, train_loss_regularization 0.7109887003898621\n",
      "iter 131, val_loss_regularization 0.7109887003898621\n",
      "epoch 0, train_loss 1277.1595458984375\n",
      "epoch 0, val_loss 454.8243713378906\n",
      "epoch 1, train_loss 1277.15966796875\n",
      "epoch 1, val_loss 454.82427978515625\n",
      "epoch 2, train_loss 1277.1595458984375\n",
      "epoch 2, val_loss 454.82427978515625\n",
      "epoch 3, train_loss 1277.1595458984375\n",
      "epoch 3, val_loss 454.82415771484375\n",
      "epoch 4, train_loss 1277.1593017578125\n",
      "epoch 4, val_loss 454.82421875\n",
      "epoch 5, train_loss 1277.1593017578125\n",
      "epoch 5, val_loss 454.8241271972656\n",
      "epoch 6, train_loss 1277.1593017578125\n",
      "epoch 6, val_loss 454.8240966796875\n",
      "epoch 7, train_loss 1277.1591796875\n",
      "epoch 7, val_loss 454.82403564453125\n",
      "epoch 8, train_loss 1277.1591796875\n",
      "epoch 8, val_loss 454.8239440917969\n",
      "epoch 9, train_loss 1277.1591796875\n",
      "epoch 9, val_loss 454.82391357421875\n",
      "epoch 10, train_loss 1277.1591796875\n",
      "epoch 10, val_loss 454.8238830566406\n",
      "epoch 11, train_loss 1277.1591796875\n",
      "epoch 11, val_loss 454.8238830566406\n",
      "epoch 12, train_loss 1277.1590576171875\n",
      "epoch 12, val_loss 454.8238220214844\n",
      "epoch 13, train_loss 1277.1590576171875\n",
      "epoch 13, val_loss 454.82379150390625\n",
      "epoch 14, train_loss 1277.1590576171875\n",
      "epoch 14, val_loss 454.8236999511719\n",
      "epoch 15, train_loss 1277.1588134765625\n",
      "epoch 15, val_loss 454.8236999511719\n",
      "epoch 16, train_loss 1277.1588134765625\n",
      "epoch 16, val_loss 454.82366943359375\n",
      "epoch 17, train_loss 1277.15869140625\n",
      "epoch 17, val_loss 454.82354736328125\n",
      "epoch 18, train_loss 1277.1588134765625\n",
      "epoch 18, val_loss 454.823486328125\n",
      "epoch 19, train_loss 1277.15869140625\n",
      "epoch 19, val_loss 454.8234558105469\n",
      "epoch 20, train_loss 1277.15869140625\n",
      "epoch 20, val_loss 454.8234558105469\n",
      "epoch 21, train_loss 1277.15869140625\n",
      "epoch 21, val_loss 454.8233642578125\n",
      "epoch 22, train_loss 1277.15869140625\n",
      "epoch 22, val_loss 454.8233642578125\n",
      "epoch 23, train_loss 1277.15869140625\n",
      "epoch 23, val_loss 454.8233337402344\n",
      "epoch 24, train_loss 1277.1585693359375\n",
      "epoch 24, val_loss 454.82330322265625\n",
      "epoch 25, train_loss 1277.1585693359375\n",
      "epoch 25, val_loss 454.8232116699219\n",
      "epoch 26, train_loss 1277.158447265625\n",
      "epoch 26, val_loss 454.82318115234375\n",
      "epoch 27, train_loss 1277.158447265625\n",
      "epoch 27, val_loss 454.8231201171875\n",
      "epoch 28, train_loss 1277.158447265625\n",
      "epoch 28, val_loss 454.8230895996094\n",
      "epoch 29, train_loss 1277.1583251953125\n",
      "epoch 29, val_loss 454.822998046875\n",
      "epoch 30, train_loss 1277.1583251953125\n",
      "epoch 30, val_loss 454.822998046875\n",
      "epoch 31, train_loss 1277.158203125\n",
      "epoch 31, val_loss 454.822998046875\n",
      "epoch 32, train_loss 1277.158203125\n",
      "epoch 32, val_loss 454.8228759765625\n",
      "epoch 33, train_loss 1277.158203125\n",
      "epoch 33, val_loss 454.8228454589844\n",
      "epoch 34, train_loss 1277.1580810546875\n",
      "epoch 34, val_loss 454.8228454589844\n",
      "epoch 35, train_loss 1277.1580810546875\n",
      "epoch 35, val_loss 454.82275390625\n",
      "epoch 36, train_loss 1277.157958984375\n",
      "epoch 36, val_loss 454.8227233886719\n",
      "epoch 37, train_loss 1277.157958984375\n",
      "epoch 37, val_loss 454.8227233886719\n",
      "epoch 38, train_loss 1277.157958984375\n",
      "epoch 38, val_loss 454.8226623535156\n",
      "epoch 39, train_loss 1277.157958984375\n",
      "epoch 39, val_loss 454.8226318359375\n",
      "epoch 40, train_loss 1277.15771484375\n",
      "epoch 40, val_loss 454.8225402832031\n",
      "epoch 41, train_loss 1277.15771484375\n",
      "epoch 41, val_loss 454.822509765625\n",
      "epoch 42, train_loss 1277.15771484375\n",
      "epoch 42, val_loss 454.82244873046875\n",
      "epoch 43, train_loss 1277.15771484375\n",
      "epoch 43, val_loss 454.8223876953125\n",
      "epoch 44, train_loss 1277.15771484375\n",
      "epoch 44, val_loss 454.82232666015625\n",
      "epoch 45, train_loss 1277.1575927734375\n",
      "epoch 45, val_loss 454.82232666015625\n",
      "epoch 46, train_loss 1277.15771484375\n",
      "epoch 46, val_loss 454.8222351074219\n",
      "epoch 47, train_loss 1277.157470703125\n",
      "epoch 47, val_loss 454.8222351074219\n",
      "epoch 48, train_loss 1277.157470703125\n",
      "epoch 48, val_loss 454.82220458984375\n",
      "epoch 49, train_loss 1277.157470703125\n",
      "epoch 49, val_loss 454.82220458984375\n",
      "epoch 50, train_loss 1277.1573486328125\n",
      "epoch 50, val_loss 454.82208251953125\n",
      "epoch 51, train_loss 1277.1572265625\n",
      "epoch 51, val_loss 454.82208251953125\n",
      "epoch 52, train_loss 1277.1572265625\n",
      "epoch 52, val_loss 454.8220520019531\n",
      "epoch 53, train_loss 1277.1572265625\n",
      "epoch 53, val_loss 454.8219299316406\n",
      "epoch 54, train_loss 1277.1572265625\n",
      "epoch 54, val_loss 454.8218688964844\n",
      "epoch 55, train_loss 1277.156982421875\n",
      "epoch 55, val_loss 454.8218688964844\n",
      "epoch 56, train_loss 1277.1572265625\n",
      "epoch 56, val_loss 454.82183837890625\n",
      "epoch 57, train_loss 1277.1572265625\n",
      "epoch 57, val_loss 454.82183837890625\n",
      "epoch 58, train_loss 1277.1571044921875\n",
      "epoch 58, val_loss 454.82171630859375\n",
      "epoch 59, train_loss 1277.1572265625\n",
      "epoch 59, val_loss 454.82171630859375\n",
      "epoch 60, train_loss 1277.1568603515625\n",
      "epoch 60, val_loss 454.8216552734375\n",
      "epoch 61, train_loss 1277.156982421875\n",
      "epoch 61, val_loss 454.8216247558594\n",
      "epoch 62, train_loss 1277.1568603515625\n",
      "epoch 62, val_loss 454.82159423828125\n",
      "epoch 63, train_loss 1277.15673828125\n",
      "epoch 63, val_loss 454.8215026855469\n",
      "epoch 64, train_loss 1277.15673828125\n",
      "epoch 64, val_loss 454.82147216796875\n",
      "epoch 65, train_loss 1277.15673828125\n",
      "epoch 65, val_loss 454.82147216796875\n",
      "epoch 66, train_loss 1277.1566162109375\n",
      "epoch 66, val_loss 454.8213806152344\n",
      "epoch 67, train_loss 1277.1566162109375\n",
      "epoch 67, val_loss 454.8213195800781\n",
      "epoch 68, train_loss 1277.1566162109375\n",
      "epoch 68, val_loss 454.8212585449219\n",
      "epoch 69, train_loss 1277.1566162109375\n",
      "epoch 69, val_loss 454.8212585449219\n",
      "epoch 70, train_loss 1277.1566162109375\n",
      "epoch 70, val_loss 454.8211975097656\n",
      "epoch 71, train_loss 1277.1566162109375\n",
      "epoch 71, val_loss 454.8211364746094\n",
      "epoch 72, train_loss 1277.1566162109375\n",
      "epoch 72, val_loss 454.8211364746094\n",
      "epoch 73, train_loss 1277.156494140625\n",
      "epoch 73, val_loss 454.821044921875\n",
      "epoch 74, train_loss 1277.156494140625\n",
      "epoch 74, val_loss 454.821044921875\n",
      "epoch 75, train_loss 1277.1563720703125\n",
      "epoch 75, val_loss 454.8209533691406\n",
      "epoch 76, train_loss 1277.1563720703125\n",
      "epoch 76, val_loss 454.8209533691406\n",
      "epoch 77, train_loss 1277.1563720703125\n",
      "epoch 77, val_loss 454.8209228515625\n",
      "epoch 78, train_loss 1277.1561279296875\n",
      "epoch 78, val_loss 454.82086181640625\n",
      "epoch 79, train_loss 1277.15625\n",
      "epoch 79, val_loss 454.8208312988281\n",
      "epoch 80, train_loss 1277.1561279296875\n",
      "epoch 80, val_loss 454.82073974609375\n",
      "epoch 81, train_loss 1277.1561279296875\n",
      "epoch 81, val_loss 454.8207092285156\n",
      "epoch 82, train_loss 1277.1561279296875\n",
      "epoch 82, val_loss 454.8206787109375\n",
      "epoch 83, train_loss 1277.1561279296875\n",
      "epoch 83, val_loss 454.8205871582031\n",
      "epoch 84, train_loss 1277.1558837890625\n",
      "epoch 84, val_loss 454.8205871582031\n",
      "epoch 85, train_loss 1277.1558837890625\n",
      "epoch 85, val_loss 454.820556640625\n",
      "epoch 86, train_loss 1277.15576171875\n",
      "epoch 86, val_loss 454.82049560546875\n",
      "epoch 87, train_loss 1277.15576171875\n",
      "epoch 87, val_loss 454.8204650878906\n",
      "epoch 88, train_loss 1277.15576171875\n",
      "epoch 88, val_loss 454.8204040527344\n",
      "epoch 89, train_loss 1277.1558837890625\n",
      "epoch 89, val_loss 454.8204040527344\n",
      "epoch 90, train_loss 1277.15576171875\n",
      "epoch 90, val_loss 454.8202819824219\n",
      "epoch 91, train_loss 1277.1556396484375\n",
      "epoch 91, val_loss 454.82025146484375\n",
      "epoch 92, train_loss 1277.155517578125\n",
      "epoch 92, val_loss 454.82025146484375\n",
      "epoch 93, train_loss 1277.1556396484375\n",
      "epoch 93, val_loss 454.82012939453125\n",
      "epoch 94, train_loss 1277.155517578125\n",
      "epoch 94, val_loss 454.820068359375\n",
      "epoch 95, train_loss 1277.155517578125\n",
      "epoch 95, val_loss 454.820068359375\n",
      "epoch 96, train_loss 1277.1553955078125\n",
      "epoch 96, val_loss 454.8200378417969\n",
      "epoch 97, train_loss 1277.1552734375\n",
      "epoch 97, val_loss 454.82000732421875\n",
      "epoch 98, train_loss 1277.1552734375\n",
      "epoch 98, val_loss 454.82000732421875\n",
      "epoch 99, train_loss 1277.1551513671875\n",
      "epoch 99, val_loss 454.8199157714844\n",
      "Parameter containing:\n",
      "tensor([7.1971e-25], requires_grad=True)\n",
      "iter 132, train_loss_regularization 0.7100163698196411\n",
      "iter 132, val_loss_regularization 0.7100163698196411\n",
      "epoch 0, train_loss 1277.155029296875\n",
      "epoch 0, val_loss 454.8199157714844\n",
      "epoch 1, train_loss 1277.1551513671875\n",
      "epoch 1, val_loss 454.8197937011719\n",
      "epoch 2, train_loss 1277.1551513671875\n",
      "epoch 2, val_loss 454.8197937011719\n",
      "epoch 3, train_loss 1277.155029296875\n",
      "epoch 3, val_loss 454.81976318359375\n",
      "epoch 4, train_loss 1277.1551513671875\n",
      "epoch 4, val_loss 454.8196716308594\n",
      "epoch 5, train_loss 1277.1549072265625\n",
      "epoch 5, val_loss 454.8196716308594\n",
      "epoch 6, train_loss 1277.1549072265625\n",
      "epoch 6, val_loss 454.819580078125\n",
      "epoch 7, train_loss 1277.15478515625\n",
      "epoch 7, val_loss 454.819580078125\n",
      "epoch 8, train_loss 1277.1549072265625\n",
      "epoch 8, val_loss 454.8194885253906\n",
      "epoch 9, train_loss 1277.1549072265625\n",
      "epoch 9, val_loss 454.8194580078125\n",
      "epoch 10, train_loss 1277.1549072265625\n",
      "epoch 10, val_loss 454.8194274902344\n",
      "epoch 11, train_loss 1277.15478515625\n",
      "epoch 11, val_loss 454.8193359375\n",
      "epoch 12, train_loss 1277.1546630859375\n",
      "epoch 12, val_loss 454.8193359375\n",
      "epoch 13, train_loss 1277.154541015625\n",
      "epoch 13, val_loss 454.8193054199219\n",
      "epoch 14, train_loss 1277.154541015625\n",
      "epoch 14, val_loss 454.8192443847656\n",
      "epoch 15, train_loss 1277.1546630859375\n",
      "epoch 15, val_loss 454.8192138671875\n",
      "epoch 16, train_loss 1277.1546630859375\n",
      "epoch 16, val_loss 454.8192138671875\n",
      "epoch 17, train_loss 1277.1546630859375\n",
      "epoch 17, val_loss 454.81915283203125\n",
      "epoch 18, train_loss 1277.154541015625\n",
      "epoch 18, val_loss 454.8190002441406\n",
      "epoch 19, train_loss 1277.154541015625\n",
      "epoch 19, val_loss 454.8190002441406\n",
      "epoch 20, train_loss 1277.154296875\n",
      "epoch 20, val_loss 454.8189697265625\n",
      "epoch 21, train_loss 1277.154296875\n",
      "epoch 21, val_loss 454.8188781738281\n",
      "epoch 22, train_loss 1277.1544189453125\n",
      "epoch 22, val_loss 454.81890869140625\n",
      "epoch 23, train_loss 1277.154296875\n",
      "epoch 23, val_loss 454.8188781738281\n",
      "epoch 24, train_loss 1277.154296875\n",
      "epoch 24, val_loss 454.81878662109375\n",
      "epoch 25, train_loss 1277.154052734375\n",
      "epoch 25, val_loss 454.8187561035156\n",
      "epoch 26, train_loss 1277.1541748046875\n",
      "epoch 26, val_loss 454.8186950683594\n",
      "epoch 27, train_loss 1277.154052734375\n",
      "epoch 27, val_loss 454.81866455078125\n",
      "epoch 28, train_loss 1277.154052734375\n",
      "epoch 28, val_loss 454.8185729980469\n",
      "epoch 29, train_loss 1277.154052734375\n",
      "epoch 29, val_loss 454.8186340332031\n",
      "epoch 30, train_loss 1277.154052734375\n",
      "epoch 30, val_loss 454.8185119628906\n",
      "epoch 31, train_loss 1277.15380859375\n",
      "epoch 31, val_loss 454.8185119628906\n",
      "epoch 32, train_loss 1277.154052734375\n",
      "epoch 32, val_loss 454.81842041015625\n",
      "epoch 33, train_loss 1277.1539306640625\n",
      "epoch 33, val_loss 454.81842041015625\n",
      "epoch 34, train_loss 1277.15380859375\n",
      "epoch 34, val_loss 454.8183898925781\n",
      "epoch 35, train_loss 1277.1536865234375\n",
      "epoch 35, val_loss 454.81829833984375\n",
      "epoch 36, train_loss 1277.1536865234375\n",
      "epoch 36, val_loss 454.8182373046875\n",
      "epoch 37, train_loss 1277.153564453125\n",
      "epoch 37, val_loss 454.81817626953125\n",
      "epoch 38, train_loss 1277.1536865234375\n",
      "epoch 38, val_loss 454.81817626953125\n",
      "epoch 39, train_loss 1277.1534423828125\n",
      "epoch 39, val_loss 454.818115234375\n",
      "epoch 40, train_loss 1277.1534423828125\n",
      "epoch 40, val_loss 454.818115234375\n",
      "epoch 41, train_loss 1277.1534423828125\n",
      "epoch 41, val_loss 454.81805419921875\n",
      "epoch 42, train_loss 1277.1534423828125\n",
      "epoch 42, val_loss 454.8179931640625\n",
      "epoch 43, train_loss 1277.1534423828125\n",
      "epoch 43, val_loss 454.8179016113281\n",
      "epoch 44, train_loss 1277.1531982421875\n",
      "epoch 44, val_loss 454.81787109375\n",
      "epoch 45, train_loss 1277.1531982421875\n",
      "epoch 45, val_loss 454.81787109375\n",
      "epoch 46, train_loss 1277.1531982421875\n",
      "epoch 46, val_loss 454.8177490234375\n",
      "epoch 47, train_loss 1277.1531982421875\n",
      "epoch 47, val_loss 454.8177795410156\n",
      "epoch 48, train_loss 1277.1531982421875\n",
      "epoch 48, val_loss 454.8177185058594\n",
      "epoch 49, train_loss 1277.1531982421875\n",
      "epoch 49, val_loss 454.8177185058594\n",
      "epoch 50, train_loss 1277.1529541015625\n",
      "epoch 50, val_loss 454.8176574707031\n",
      "epoch 51, train_loss 1277.153076171875\n",
      "epoch 51, val_loss 454.817626953125\n",
      "epoch 52, train_loss 1277.1529541015625\n",
      "epoch 52, val_loss 454.8175354003906\n",
      "epoch 53, train_loss 1277.1529541015625\n",
      "epoch 53, val_loss 454.8175354003906\n",
      "epoch 54, train_loss 1277.1529541015625\n",
      "epoch 54, val_loss 454.8175048828125\n",
      "epoch 55, train_loss 1277.1529541015625\n",
      "epoch 55, val_loss 454.8173828125\n",
      "epoch 56, train_loss 1277.1529541015625\n",
      "epoch 56, val_loss 454.81732177734375\n",
      "epoch 57, train_loss 1277.15283203125\n",
      "epoch 57, val_loss 454.81732177734375\n",
      "epoch 58, train_loss 1277.15283203125\n",
      "epoch 58, val_loss 454.8172607421875\n",
      "epoch 59, train_loss 1277.152587890625\n",
      "epoch 59, val_loss 454.81719970703125\n",
      "epoch 60, train_loss 1277.152587890625\n",
      "epoch 60, val_loss 454.81719970703125\n",
      "epoch 61, train_loss 1277.1527099609375\n",
      "epoch 61, val_loss 454.81719970703125\n",
      "epoch 62, train_loss 1277.1527099609375\n",
      "epoch 62, val_loss 454.817138671875\n",
      "epoch 63, train_loss 1277.1527099609375\n",
      "epoch 63, val_loss 454.81707763671875\n",
      "epoch 64, train_loss 1277.1524658203125\n",
      "epoch 64, val_loss 454.8169860839844\n",
      "epoch 65, train_loss 1277.1524658203125\n",
      "epoch 65, val_loss 454.81695556640625\n",
      "epoch 66, train_loss 1277.1524658203125\n",
      "epoch 66, val_loss 454.8168640136719\n",
      "epoch 67, train_loss 1277.1524658203125\n",
      "epoch 67, val_loss 454.8168640136719\n",
      "epoch 68, train_loss 1277.1524658203125\n",
      "epoch 68, val_loss 454.81683349609375\n",
      "epoch 69, train_loss 1277.15234375\n",
      "epoch 69, val_loss 454.81683349609375\n",
      "epoch 70, train_loss 1277.1522216796875\n",
      "epoch 70, val_loss 454.8167419433594\n",
      "epoch 71, train_loss 1277.1522216796875\n",
      "epoch 71, val_loss 454.81671142578125\n",
      "epoch 72, train_loss 1277.1519775390625\n",
      "epoch 72, val_loss 454.81671142578125\n",
      "epoch 73, train_loss 1277.152099609375\n",
      "epoch 73, val_loss 454.8166198730469\n",
      "epoch 74, train_loss 1277.1519775390625\n",
      "epoch 74, val_loss 454.8165283203125\n",
      "epoch 75, train_loss 1277.152099609375\n",
      "epoch 75, val_loss 454.8164978027344\n",
      "epoch 76, train_loss 1277.1522216796875\n",
      "epoch 76, val_loss 454.8164978027344\n",
      "epoch 77, train_loss 1277.152099609375\n",
      "epoch 77, val_loss 454.81640625\n",
      "epoch 78, train_loss 1277.1519775390625\n",
      "epoch 78, val_loss 454.81640625\n",
      "epoch 79, train_loss 1277.1519775390625\n",
      "epoch 79, val_loss 454.81634521484375\n",
      "epoch 80, train_loss 1277.1517333984375\n",
      "epoch 80, val_loss 454.81634521484375\n",
      "epoch 81, train_loss 1277.1517333984375\n",
      "epoch 81, val_loss 454.8162841796875\n",
      "epoch 82, train_loss 1277.151611328125\n",
      "epoch 82, val_loss 454.816162109375\n",
      "epoch 83, train_loss 1277.1517333984375\n",
      "epoch 83, val_loss 454.816162109375\n",
      "epoch 84, train_loss 1277.151611328125\n",
      "epoch 84, val_loss 454.8160705566406\n",
      "epoch 85, train_loss 1277.151611328125\n",
      "epoch 85, val_loss 454.8160400390625\n",
      "epoch 86, train_loss 1277.1514892578125\n",
      "epoch 86, val_loss 454.8160400390625\n",
      "epoch 87, train_loss 1277.1514892578125\n",
      "epoch 87, val_loss 454.8160095214844\n",
      "epoch 88, train_loss 1277.1513671875\n",
      "epoch 88, val_loss 454.8159484863281\n",
      "epoch 89, train_loss 1277.1513671875\n",
      "epoch 89, val_loss 454.81591796875\n",
      "epoch 90, train_loss 1277.1513671875\n",
      "epoch 90, val_loss 454.81591796875\n",
      "epoch 91, train_loss 1277.1513671875\n",
      "epoch 91, val_loss 454.8158264160156\n",
      "epoch 92, train_loss 1277.1513671875\n",
      "epoch 92, val_loss 454.81573486328125\n",
      "epoch 93, train_loss 1277.1512451171875\n",
      "epoch 93, val_loss 454.81573486328125\n",
      "epoch 94, train_loss 1277.151123046875\n",
      "epoch 94, val_loss 454.815673828125\n",
      "epoch 95, train_loss 1277.151123046875\n",
      "epoch 95, val_loss 454.81561279296875\n",
      "epoch 96, train_loss 1277.1510009765625\n",
      "epoch 96, val_loss 454.8155822753906\n",
      "epoch 97, train_loss 1277.151123046875\n",
      "epoch 97, val_loss 454.8155822753906\n",
      "epoch 98, train_loss 1277.151123046875\n",
      "epoch 98, val_loss 454.81549072265625\n",
      "epoch 99, train_loss 1277.1510009765625\n",
      "epoch 99, val_loss 454.81549072265625\n",
      "Parameter containing:\n",
      "tensor([4.9941e-25], requires_grad=True)\n",
      "iter 133, train_loss_regularization 0.709067165851593\n",
      "iter 133, val_loss_regularization 0.709067165851593\n",
      "epoch 0, train_loss 1277.15087890625\n",
      "epoch 0, val_loss 454.8154602050781\n",
      "epoch 1, train_loss 1277.15087890625\n",
      "epoch 1, val_loss 454.81536865234375\n",
      "epoch 2, train_loss 1277.15087890625\n",
      "epoch 2, val_loss 454.8153381347656\n",
      "epoch 3, train_loss 1277.15087890625\n",
      "epoch 3, val_loss 454.81524658203125\n",
      "epoch 4, train_loss 1277.15087890625\n",
      "epoch 4, val_loss 454.81524658203125\n",
      "epoch 5, train_loss 1277.15087890625\n",
      "epoch 5, val_loss 454.81524658203125\n",
      "epoch 6, train_loss 1277.150634765625\n",
      "epoch 6, val_loss 454.8152160644531\n",
      "epoch 7, train_loss 1277.150634765625\n",
      "epoch 7, val_loss 454.81512451171875\n",
      "epoch 8, train_loss 1277.150634765625\n",
      "epoch 8, val_loss 454.8150939941406\n",
      "epoch 9, train_loss 1277.1505126953125\n",
      "epoch 9, val_loss 454.8150329589844\n",
      "epoch 10, train_loss 1277.1505126953125\n",
      "epoch 10, val_loss 454.8149108886719\n",
      "epoch 11, train_loss 1277.1505126953125\n",
      "epoch 11, val_loss 454.8149108886719\n",
      "epoch 12, train_loss 1277.150634765625\n",
      "epoch 12, val_loss 454.81488037109375\n",
      "epoch 13, train_loss 1277.1505126953125\n",
      "epoch 13, val_loss 454.81488037109375\n",
      "epoch 14, train_loss 1277.150390625\n",
      "epoch 14, val_loss 454.81475830078125\n",
      "epoch 15, train_loss 1277.150390625\n",
      "epoch 15, val_loss 454.81475830078125\n",
      "epoch 16, train_loss 1277.1502685546875\n",
      "epoch 16, val_loss 454.81475830078125\n",
      "epoch 17, train_loss 1277.1502685546875\n",
      "epoch 17, val_loss 454.8146667480469\n",
      "epoch 18, train_loss 1277.1502685546875\n",
      "epoch 18, val_loss 454.8146667480469\n",
      "epoch 19, train_loss 1277.150390625\n",
      "epoch 19, val_loss 454.81463623046875\n",
      "epoch 20, train_loss 1277.150146484375\n",
      "epoch 20, val_loss 454.8145446777344\n",
      "epoch 21, train_loss 1277.1500244140625\n",
      "epoch 21, val_loss 454.8144226074219\n",
      "epoch 22, train_loss 1277.14990234375\n",
      "epoch 22, val_loss 454.814453125\n",
      "epoch 23, train_loss 1277.1500244140625\n",
      "epoch 23, val_loss 454.8144226074219\n",
      "epoch 24, train_loss 1277.14990234375\n",
      "epoch 24, val_loss 454.8143615722656\n",
      "epoch 25, train_loss 1277.14990234375\n",
      "epoch 25, val_loss 454.8143005371094\n",
      "epoch 26, train_loss 1277.1497802734375\n",
      "epoch 26, val_loss 454.8143005371094\n",
      "epoch 27, train_loss 1277.14990234375\n",
      "epoch 27, val_loss 454.8142395019531\n",
      "epoch 28, train_loss 1277.1497802734375\n",
      "epoch 28, val_loss 454.814208984375\n",
      "epoch 29, train_loss 1277.149658203125\n",
      "epoch 29, val_loss 454.8141174316406\n",
      "epoch 30, train_loss 1277.1495361328125\n",
      "epoch 30, val_loss 454.8141174316406\n",
      "epoch 31, train_loss 1277.1495361328125\n",
      "epoch 31, val_loss 454.8140869140625\n",
      "epoch 32, train_loss 1277.1495361328125\n",
      "epoch 32, val_loss 454.8139953613281\n",
      "epoch 33, train_loss 1277.1495361328125\n",
      "epoch 33, val_loss 454.8139953613281\n",
      "epoch 34, train_loss 1277.1495361328125\n",
      "epoch 34, val_loss 454.81390380859375\n",
      "epoch 35, train_loss 1277.1494140625\n",
      "epoch 35, val_loss 454.81390380859375\n",
      "epoch 36, train_loss 1277.1492919921875\n",
      "epoch 36, val_loss 454.8138427734375\n",
      "epoch 37, train_loss 1277.1494140625\n",
      "epoch 37, val_loss 454.8138427734375\n",
      "epoch 38, train_loss 1277.1494140625\n",
      "epoch 38, val_loss 454.8137512207031\n",
      "epoch 39, train_loss 1277.1492919921875\n",
      "epoch 39, val_loss 454.81365966796875\n",
      "epoch 40, train_loss 1277.1492919921875\n",
      "epoch 40, val_loss 454.8136291503906\n",
      "epoch 41, train_loss 1277.1492919921875\n",
      "epoch 41, val_loss 454.8136291503906\n",
      "epoch 42, train_loss 1277.149169921875\n",
      "epoch 42, val_loss 454.8135681152344\n",
      "epoch 43, train_loss 1277.1490478515625\n",
      "epoch 43, val_loss 454.81353759765625\n",
      "epoch 44, train_loss 1277.1490478515625\n",
      "epoch 44, val_loss 454.8135070800781\n",
      "epoch 45, train_loss 1277.1490478515625\n",
      "epoch 45, val_loss 454.8134460449219\n",
      "epoch 46, train_loss 1277.1490478515625\n",
      "epoch 46, val_loss 454.81341552734375\n",
      "epoch 47, train_loss 1277.1490478515625\n",
      "epoch 47, val_loss 454.81341552734375\n",
      "epoch 48, train_loss 1277.14892578125\n",
      "epoch 48, val_loss 454.81329345703125\n",
      "epoch 49, train_loss 1277.1488037109375\n",
      "epoch 49, val_loss 454.8132629394531\n",
      "epoch 50, train_loss 1277.14892578125\n",
      "epoch 50, val_loss 454.8132019042969\n",
      "epoch 51, train_loss 1277.14892578125\n",
      "epoch 51, val_loss 454.8131103515625\n",
      "epoch 52, train_loss 1277.1488037109375\n",
      "epoch 52, val_loss 454.8131103515625\n",
      "epoch 53, train_loss 1277.14892578125\n",
      "epoch 53, val_loss 454.8131103515625\n",
      "epoch 54, train_loss 1277.1488037109375\n",
      "epoch 54, val_loss 454.81304931640625\n",
      "epoch 55, train_loss 1277.1485595703125\n",
      "epoch 55, val_loss 454.81298828125\n",
      "epoch 56, train_loss 1277.1485595703125\n",
      "epoch 56, val_loss 454.81298828125\n",
      "epoch 57, train_loss 1277.1485595703125\n",
      "epoch 57, val_loss 454.8129577636719\n",
      "epoch 58, train_loss 1277.1485595703125\n",
      "epoch 58, val_loss 454.8128662109375\n",
      "epoch 59, train_loss 1277.1484375\n",
      "epoch 59, val_loss 454.812744140625\n",
      "epoch 60, train_loss 1277.1484375\n",
      "epoch 60, val_loss 454.812744140625\n",
      "epoch 61, train_loss 1277.1483154296875\n",
      "epoch 61, val_loss 454.812744140625\n",
      "epoch 62, train_loss 1277.1483154296875\n",
      "epoch 62, val_loss 454.8126525878906\n",
      "epoch 63, train_loss 1277.1483154296875\n",
      "epoch 63, val_loss 454.8126220703125\n",
      "epoch 64, train_loss 1277.1483154296875\n",
      "epoch 64, val_loss 454.8126220703125\n",
      "epoch 65, train_loss 1277.148193359375\n",
      "epoch 65, val_loss 454.8125305175781\n",
      "epoch 66, train_loss 1277.148193359375\n",
      "epoch 66, val_loss 454.8125305175781\n",
      "epoch 67, train_loss 1277.148193359375\n",
      "epoch 67, val_loss 454.8125\n",
      "epoch 68, train_loss 1277.14794921875\n",
      "epoch 68, val_loss 454.8123779296875\n",
      "epoch 69, train_loss 1277.1480712890625\n",
      "epoch 69, val_loss 454.8123779296875\n",
      "epoch 70, train_loss 1277.14794921875\n",
      "epoch 70, val_loss 454.8123474121094\n",
      "epoch 71, train_loss 1277.14794921875\n",
      "epoch 71, val_loss 454.8122863769531\n",
      "epoch 72, train_loss 1277.14794921875\n",
      "epoch 72, val_loss 454.81219482421875\n",
      "epoch 73, train_loss 1277.14794921875\n",
      "epoch 73, val_loss 454.812255859375\n",
      "epoch 74, train_loss 1277.1478271484375\n",
      "epoch 74, val_loss 454.8121643066406\n",
      "epoch 75, train_loss 1277.147705078125\n",
      "epoch 75, val_loss 454.8121337890625\n",
      "epoch 76, train_loss 1277.147705078125\n",
      "epoch 76, val_loss 454.8120422363281\n",
      "epoch 77, train_loss 1277.147705078125\n",
      "epoch 77, val_loss 454.8120422363281\n",
      "epoch 78, train_loss 1277.147705078125\n",
      "epoch 78, val_loss 454.81201171875\n",
      "epoch 79, train_loss 1277.147705078125\n",
      "epoch 79, val_loss 454.81195068359375\n",
      "epoch 80, train_loss 1277.1474609375\n",
      "epoch 80, val_loss 454.8118896484375\n",
      "epoch 81, train_loss 1277.1474609375\n",
      "epoch 81, val_loss 454.8119201660156\n",
      "epoch 82, train_loss 1277.1474609375\n",
      "epoch 82, val_loss 454.81182861328125\n",
      "epoch 83, train_loss 1277.1474609375\n",
      "epoch 83, val_loss 454.8117980957031\n",
      "epoch 84, train_loss 1277.1474609375\n",
      "epoch 84, val_loss 454.8117370605469\n",
      "epoch 85, train_loss 1277.1473388671875\n",
      "epoch 85, val_loss 454.81170654296875\n",
      "epoch 86, train_loss 1277.1474609375\n",
      "epoch 86, val_loss 454.8116149902344\n",
      "epoch 87, train_loss 1277.1473388671875\n",
      "epoch 87, val_loss 454.8115539550781\n",
      "epoch 88, train_loss 1277.1473388671875\n",
      "epoch 88, val_loss 454.8114929199219\n",
      "epoch 89, train_loss 1277.147216796875\n",
      "epoch 89, val_loss 454.8115539550781\n",
      "epoch 90, train_loss 1277.147216796875\n",
      "epoch 90, val_loss 454.81146240234375\n",
      "epoch 91, train_loss 1277.147216796875\n",
      "epoch 91, val_loss 454.8114318847656\n",
      "epoch 92, train_loss 1277.147216796875\n",
      "epoch 92, val_loss 454.8114318847656\n",
      "epoch 93, train_loss 1277.1470947265625\n",
      "epoch 93, val_loss 454.81134033203125\n",
      "epoch 94, train_loss 1277.1470947265625\n",
      "epoch 94, val_loss 454.81134033203125\n",
      "epoch 95, train_loss 1277.14697265625\n",
      "epoch 95, val_loss 454.8112487792969\n",
      "epoch 96, train_loss 1277.14697265625\n",
      "epoch 96, val_loss 454.81121826171875\n",
      "epoch 97, train_loss 1277.14697265625\n",
      "epoch 97, val_loss 454.8111572265625\n",
      "epoch 98, train_loss 1277.14697265625\n",
      "epoch 98, val_loss 454.8111572265625\n",
      "epoch 99, train_loss 1277.14697265625\n",
      "epoch 99, val_loss 454.81103515625\n",
      "Parameter containing:\n",
      "tensor([3.4665e-25], requires_grad=True)\n",
      "iter 134, train_loss_regularization 0.7081408500671387\n",
      "iter 134, val_loss_regularization 0.7081408500671387\n",
      "epoch 0, train_loss 1277.14697265625\n",
      "epoch 0, val_loss 454.81103515625\n",
      "epoch 1, train_loss 1277.146728515625\n",
      "epoch 1, val_loss 454.8109436035156\n",
      "epoch 2, train_loss 1277.1466064453125\n",
      "epoch 2, val_loss 454.8109436035156\n",
      "epoch 3, train_loss 1277.1466064453125\n",
      "epoch 3, val_loss 454.8109130859375\n",
      "epoch 4, train_loss 1277.1466064453125\n",
      "epoch 4, val_loss 454.8108825683594\n",
      "epoch 5, train_loss 1277.146484375\n",
      "epoch 5, val_loss 454.8107604980469\n",
      "epoch 6, train_loss 1277.146484375\n",
      "epoch 6, val_loss 454.8107604980469\n",
      "epoch 7, train_loss 1277.1466064453125\n",
      "epoch 7, val_loss 454.8106994628906\n",
      "epoch 8, train_loss 1277.1463623046875\n",
      "epoch 8, val_loss 454.8106994628906\n",
      "epoch 9, train_loss 1277.146484375\n",
      "epoch 9, val_loss 454.8106384277344\n",
      "epoch 10, train_loss 1277.146240234375\n",
      "epoch 10, val_loss 454.8106384277344\n",
      "epoch 11, train_loss 1277.146240234375\n",
      "epoch 11, val_loss 454.8105773925781\n",
      "epoch 12, train_loss 1277.146240234375\n",
      "epoch 12, val_loss 454.81048583984375\n",
      "epoch 13, train_loss 1277.1463623046875\n",
      "epoch 13, val_loss 454.81048583984375\n",
      "epoch 14, train_loss 1277.1463623046875\n",
      "epoch 14, val_loss 454.81036376953125\n",
      "epoch 15, train_loss 1277.1461181640625\n",
      "epoch 15, val_loss 454.8103332519531\n",
      "epoch 16, train_loss 1277.14599609375\n",
      "epoch 16, val_loss 454.8103332519531\n",
      "epoch 17, train_loss 1277.1461181640625\n",
      "epoch 17, val_loss 454.81024169921875\n",
      "epoch 18, train_loss 1277.14599609375\n",
      "epoch 18, val_loss 454.81024169921875\n",
      "epoch 19, train_loss 1277.14599609375\n",
      "epoch 19, val_loss 454.8102111816406\n",
      "epoch 20, train_loss 1277.1458740234375\n",
      "epoch 20, val_loss 454.81011962890625\n",
      "epoch 21, train_loss 1277.1458740234375\n",
      "epoch 21, val_loss 454.81011962890625\n",
      "epoch 22, train_loss 1277.1458740234375\n",
      "epoch 22, val_loss 454.8100891113281\n",
      "epoch 23, train_loss 1277.145751953125\n",
      "epoch 23, val_loss 454.8100891113281\n",
      "epoch 24, train_loss 1277.1456298828125\n",
      "epoch 24, val_loss 454.8099670410156\n",
      "epoch 25, train_loss 1277.145751953125\n",
      "epoch 25, val_loss 454.8099060058594\n",
      "epoch 26, train_loss 1277.1456298828125\n",
      "epoch 26, val_loss 454.8098449707031\n",
      "epoch 27, train_loss 1277.1456298828125\n",
      "epoch 27, val_loss 454.8098449707031\n",
      "epoch 28, train_loss 1277.1456298828125\n",
      "epoch 28, val_loss 454.80975341796875\n",
      "epoch 29, train_loss 1277.1456298828125\n",
      "epoch 29, val_loss 454.80975341796875\n",
      "epoch 30, train_loss 1277.1455078125\n",
      "epoch 30, val_loss 454.80975341796875\n",
      "epoch 31, train_loss 1277.1453857421875\n",
      "epoch 31, val_loss 454.8096618652344\n",
      "epoch 32, train_loss 1277.1453857421875\n",
      "epoch 32, val_loss 454.8096618652344\n",
      "epoch 33, train_loss 1277.1453857421875\n",
      "epoch 33, val_loss 454.80963134765625\n",
      "epoch 34, train_loss 1277.1453857421875\n",
      "epoch 34, val_loss 454.8095397949219\n",
      "epoch 35, train_loss 1277.1453857421875\n",
      "epoch 35, val_loss 454.8094482421875\n",
      "epoch 36, train_loss 1277.145263671875\n",
      "epoch 36, val_loss 454.8094482421875\n",
      "epoch 37, train_loss 1277.1453857421875\n",
      "epoch 37, val_loss 454.8094177246094\n",
      "epoch 38, train_loss 1277.145263671875\n",
      "epoch 38, val_loss 454.8094177246094\n",
      "epoch 39, train_loss 1277.1453857421875\n",
      "epoch 39, val_loss 454.809326171875\n",
      "epoch 40, train_loss 1277.14501953125\n",
      "epoch 40, val_loss 454.80926513671875\n",
      "epoch 41, train_loss 1277.1451416015625\n",
      "epoch 41, val_loss 454.80926513671875\n",
      "epoch 42, train_loss 1277.1451416015625\n",
      "epoch 42, val_loss 454.80926513671875\n",
      "epoch 43, train_loss 1277.14501953125\n",
      "epoch 43, val_loss 454.8091125488281\n",
      "epoch 44, train_loss 1277.14501953125\n",
      "epoch 44, val_loss 454.8091125488281\n",
      "epoch 45, train_loss 1277.14501953125\n",
      "epoch 45, val_loss 454.80908203125\n",
      "epoch 46, train_loss 1277.1448974609375\n",
      "epoch 46, val_loss 454.8089904785156\n",
      "epoch 47, train_loss 1277.144775390625\n",
      "epoch 47, val_loss 454.8089904785156\n",
      "epoch 48, train_loss 1277.144775390625\n",
      "epoch 48, val_loss 454.8089904785156\n",
      "epoch 49, train_loss 1277.144775390625\n",
      "epoch 49, val_loss 454.8089294433594\n",
      "epoch 50, train_loss 1277.144775390625\n",
      "epoch 50, val_loss 454.8088684082031\n",
      "epoch 51, train_loss 1277.1446533203125\n",
      "epoch 51, val_loss 454.808837890625\n",
      "epoch 52, train_loss 1277.14453125\n",
      "epoch 52, val_loss 454.8087463378906\n",
      "epoch 53, train_loss 1277.14453125\n",
      "epoch 53, val_loss 454.8087158203125\n",
      "epoch 54, train_loss 1277.1444091796875\n",
      "epoch 54, val_loss 454.80859375\n",
      "epoch 55, train_loss 1277.14453125\n",
      "epoch 55, val_loss 454.80865478515625\n",
      "epoch 56, train_loss 1277.144287109375\n",
      "epoch 56, val_loss 454.80859375\n",
      "epoch 57, train_loss 1277.144287109375\n",
      "epoch 57, val_loss 454.80853271484375\n",
      "epoch 58, train_loss 1277.144287109375\n",
      "epoch 58, val_loss 454.8085021972656\n",
      "epoch 59, train_loss 1277.144287109375\n",
      "epoch 59, val_loss 454.8085021972656\n",
      "epoch 60, train_loss 1277.144287109375\n",
      "epoch 60, val_loss 454.8084716796875\n",
      "epoch 61, train_loss 1277.14404296875\n",
      "epoch 61, val_loss 454.8083801269531\n",
      "epoch 62, train_loss 1277.1441650390625\n",
      "epoch 62, val_loss 454.8083190917969\n",
      "epoch 63, train_loss 1277.1441650390625\n",
      "epoch 63, val_loss 454.80828857421875\n",
      "epoch 64, train_loss 1277.1441650390625\n",
      "epoch 64, val_loss 454.8082580566406\n",
      "epoch 65, train_loss 1277.14404296875\n",
      "epoch 65, val_loss 454.80816650390625\n",
      "epoch 66, train_loss 1277.14404296875\n",
      "epoch 66, val_loss 454.8081359863281\n",
      "epoch 67, train_loss 1277.1439208984375\n",
      "epoch 67, val_loss 454.8081359863281\n",
      "epoch 68, train_loss 1277.1439208984375\n",
      "epoch 68, val_loss 454.8081359863281\n",
      "epoch 69, train_loss 1277.1439208984375\n",
      "epoch 69, val_loss 454.8080139160156\n",
      "epoch 70, train_loss 1277.143798828125\n",
      "epoch 70, val_loss 454.8080139160156\n",
      "epoch 71, train_loss 1277.143798828125\n",
      "epoch 71, val_loss 454.8079528808594\n",
      "epoch 72, train_loss 1277.143798828125\n",
      "epoch 72, val_loss 454.80792236328125\n",
      "epoch 73, train_loss 1277.143798828125\n",
      "epoch 73, val_loss 454.8078308105469\n",
      "epoch 74, train_loss 1277.143798828125\n",
      "epoch 74, val_loss 454.8078308105469\n",
      "epoch 75, train_loss 1277.143798828125\n",
      "epoch 75, val_loss 454.8078308105469\n",
      "epoch 76, train_loss 1277.1436767578125\n",
      "epoch 76, val_loss 454.8077392578125\n",
      "epoch 77, train_loss 1277.1435546875\n",
      "epoch 77, val_loss 454.80767822265625\n",
      "epoch 78, train_loss 1277.1435546875\n",
      "epoch 78, val_loss 454.80767822265625\n",
      "epoch 79, train_loss 1277.143310546875\n",
      "epoch 79, val_loss 454.8076171875\n",
      "epoch 80, train_loss 1277.1435546875\n",
      "epoch 80, val_loss 454.80755615234375\n",
      "epoch 81, train_loss 1277.1434326171875\n",
      "epoch 81, val_loss 454.8074951171875\n",
      "epoch 82, train_loss 1277.143310546875\n",
      "epoch 82, val_loss 454.8074645996094\n",
      "epoch 83, train_loss 1277.1434326171875\n",
      "epoch 83, val_loss 454.8074035644531\n",
      "epoch 84, train_loss 1277.143310546875\n",
      "epoch 84, val_loss 454.807373046875\n",
      "epoch 85, train_loss 1277.143310546875\n",
      "epoch 85, val_loss 454.8072814941406\n",
      "epoch 86, train_loss 1277.1431884765625\n",
      "epoch 86, val_loss 454.8072814941406\n",
      "epoch 87, train_loss 1277.143310546875\n",
      "epoch 87, val_loss 454.8072509765625\n",
      "epoch 88, train_loss 1277.1431884765625\n",
      "epoch 88, val_loss 454.8071594238281\n",
      "epoch 89, train_loss 1277.143310546875\n",
      "epoch 89, val_loss 454.8072204589844\n",
      "epoch 90, train_loss 1277.1431884765625\n",
      "epoch 90, val_loss 454.80712890625\n",
      "epoch 91, train_loss 1277.1429443359375\n",
      "epoch 91, val_loss 454.8070373535156\n",
      "epoch 92, train_loss 1277.1429443359375\n",
      "epoch 92, val_loss 454.80694580078125\n",
      "epoch 93, train_loss 1277.1429443359375\n",
      "epoch 93, val_loss 454.8070068359375\n",
      "epoch 94, train_loss 1277.142822265625\n",
      "epoch 94, val_loss 454.8069152832031\n",
      "epoch 95, train_loss 1277.142822265625\n",
      "epoch 95, val_loss 454.806884765625\n",
      "epoch 96, train_loss 1277.1427001953125\n",
      "epoch 96, val_loss 454.80682373046875\n",
      "epoch 97, train_loss 1277.1427001953125\n",
      "epoch 97, val_loss 454.80682373046875\n",
      "epoch 98, train_loss 1277.1427001953125\n",
      "epoch 98, val_loss 454.80682373046875\n",
      "epoch 99, train_loss 1277.1427001953125\n",
      "epoch 99, val_loss 454.8067626953125\n",
      "Parameter containing:\n",
      "tensor([2.4069e-25], requires_grad=True)\n",
      "iter 135, train_loss_regularization 0.7072310447692871\n",
      "iter 135, val_loss_regularization 0.7072310447692871\n",
      "epoch 0, train_loss 1277.142578125\n",
      "epoch 0, val_loss 454.80670166015625\n",
      "epoch 1, train_loss 1277.1427001953125\n",
      "epoch 1, val_loss 454.8066711425781\n",
      "epoch 2, train_loss 1277.1424560546875\n",
      "epoch 2, val_loss 454.8066101074219\n",
      "epoch 3, train_loss 1277.1427001953125\n",
      "epoch 3, val_loss 454.8065490722656\n",
      "epoch 4, train_loss 1277.1424560546875\n",
      "epoch 4, val_loss 454.8065490722656\n",
      "epoch 5, train_loss 1277.142578125\n",
      "epoch 5, val_loss 454.8064880371094\n",
      "epoch 6, train_loss 1277.142333984375\n",
      "epoch 6, val_loss 454.8064270019531\n",
      "epoch 7, train_loss 1277.1422119140625\n",
      "epoch 7, val_loss 454.8063659667969\n",
      "epoch 8, train_loss 1277.1422119140625\n",
      "epoch 8, val_loss 454.8063659667969\n",
      "epoch 9, train_loss 1277.1422119140625\n",
      "epoch 9, val_loss 454.8063049316406\n",
      "epoch 10, train_loss 1277.1422119140625\n",
      "epoch 10, val_loss 454.8062438964844\n",
      "epoch 11, train_loss 1277.1422119140625\n",
      "epoch 11, val_loss 454.8062438964844\n",
      "epoch 12, train_loss 1277.1422119140625\n",
      "epoch 12, val_loss 454.80615234375\n",
      "epoch 13, train_loss 1277.1419677734375\n",
      "epoch 13, val_loss 454.80609130859375\n",
      "epoch 14, train_loss 1277.1419677734375\n",
      "epoch 14, val_loss 454.80609130859375\n",
      "epoch 15, train_loss 1277.1419677734375\n",
      "epoch 15, val_loss 454.80609130859375\n",
      "epoch 16, train_loss 1277.1419677734375\n",
      "epoch 16, val_loss 454.8059997558594\n",
      "epoch 17, train_loss 1277.1419677734375\n",
      "epoch 17, val_loss 454.80596923828125\n",
      "epoch 18, train_loss 1277.1419677734375\n",
      "epoch 18, val_loss 454.80596923828125\n",
      "epoch 19, train_loss 1277.1419677734375\n",
      "epoch 19, val_loss 454.8058776855469\n",
      "epoch 20, train_loss 1277.141845703125\n",
      "epoch 20, val_loss 454.8057861328125\n",
      "epoch 21, train_loss 1277.1417236328125\n",
      "epoch 21, val_loss 454.8057861328125\n",
      "epoch 22, train_loss 1277.1416015625\n",
      "epoch 22, val_loss 454.8056945800781\n",
      "epoch 23, train_loss 1277.1417236328125\n",
      "epoch 23, val_loss 454.8056945800781\n",
      "epoch 24, train_loss 1277.1417236328125\n",
      "epoch 24, val_loss 454.8056335449219\n",
      "epoch 25, train_loss 1277.1417236328125\n",
      "epoch 25, val_loss 454.8056335449219\n",
      "epoch 26, train_loss 1277.1417236328125\n",
      "epoch 26, val_loss 454.8055725097656\n",
      "epoch 27, train_loss 1277.141357421875\n",
      "epoch 27, val_loss 454.8055419921875\n",
      "epoch 28, train_loss 1277.141357421875\n",
      "epoch 28, val_loss 454.8055114746094\n",
      "epoch 29, train_loss 1277.141357421875\n",
      "epoch 29, val_loss 454.8054504394531\n",
      "epoch 30, train_loss 1277.141357421875\n",
      "epoch 30, val_loss 454.805419921875\n",
      "epoch 31, train_loss 1277.141357421875\n",
      "epoch 31, val_loss 454.8053894042969\n",
      "epoch 32, train_loss 1277.141357421875\n",
      "epoch 32, val_loss 454.8052978515625\n",
      "epoch 33, train_loss 1277.141357421875\n",
      "epoch 33, val_loss 454.8052062988281\n",
      "epoch 34, train_loss 1277.141357421875\n",
      "epoch 34, val_loss 454.80523681640625\n",
      "epoch 35, train_loss 1277.141357421875\n",
      "epoch 35, val_loss 454.8052062988281\n",
      "epoch 36, train_loss 1277.1412353515625\n",
      "epoch 36, val_loss 454.80511474609375\n",
      "epoch 37, train_loss 1277.1412353515625\n",
      "epoch 37, val_loss 454.8050537109375\n",
      "epoch 38, train_loss 1277.14111328125\n",
      "epoch 38, val_loss 454.8050537109375\n",
      "epoch 39, train_loss 1277.14111328125\n",
      "epoch 39, val_loss 454.80499267578125\n",
      "epoch 40, train_loss 1277.14111328125\n",
      "epoch 40, val_loss 454.804931640625\n",
      "epoch 41, train_loss 1277.140869140625\n",
      "epoch 41, val_loss 454.804931640625\n",
      "epoch 42, train_loss 1277.140869140625\n",
      "epoch 42, val_loss 454.80487060546875\n",
      "epoch 43, train_loss 1277.140869140625\n",
      "epoch 43, val_loss 454.80487060546875\n",
      "epoch 44, train_loss 1277.1407470703125\n",
      "epoch 44, val_loss 454.8047790527344\n",
      "epoch 45, train_loss 1277.140869140625\n",
      "epoch 45, val_loss 454.80474853515625\n",
      "epoch 46, train_loss 1277.140869140625\n",
      "epoch 46, val_loss 454.80474853515625\n",
      "epoch 47, train_loss 1277.140869140625\n",
      "epoch 47, val_loss 454.8045959472656\n",
      "epoch 48, train_loss 1277.140625\n",
      "epoch 48, val_loss 454.8045349121094\n",
      "epoch 49, train_loss 1277.140625\n",
      "epoch 49, val_loss 454.8045959472656\n",
      "epoch 50, train_loss 1277.140625\n",
      "epoch 50, val_loss 454.80450439453125\n",
      "epoch 51, train_loss 1277.1405029296875\n",
      "epoch 51, val_loss 454.804443359375\n",
      "epoch 52, train_loss 1277.140625\n",
      "epoch 52, val_loss 454.804443359375\n",
      "epoch 53, train_loss 1277.140380859375\n",
      "epoch 53, val_loss 454.804443359375\n",
      "epoch 54, train_loss 1277.140380859375\n",
      "epoch 54, val_loss 454.80438232421875\n",
      "epoch 55, train_loss 1277.140380859375\n",
      "epoch 55, val_loss 454.8043212890625\n",
      "epoch 56, train_loss 1277.1402587890625\n",
      "epoch 56, val_loss 454.80426025390625\n",
      "epoch 57, train_loss 1277.140380859375\n",
      "epoch 57, val_loss 454.80426025390625\n",
      "epoch 58, train_loss 1277.1402587890625\n",
      "epoch 58, val_loss 454.8041687011719\n",
      "epoch 59, train_loss 1277.14013671875\n",
      "epoch 59, val_loss 454.8041687011719\n",
      "epoch 60, train_loss 1277.14013671875\n",
      "epoch 60, val_loss 454.80413818359375\n",
      "epoch 61, train_loss 1277.1402587890625\n",
      "epoch 61, val_loss 454.8040771484375\n",
      "epoch 62, train_loss 1277.14013671875\n",
      "epoch 62, val_loss 454.8040466308594\n",
      "epoch 63, train_loss 1277.14013671875\n",
      "epoch 63, val_loss 454.8039855957031\n",
      "epoch 64, train_loss 1277.14013671875\n",
      "epoch 64, val_loss 454.8039245605469\n",
      "epoch 65, train_loss 1277.139892578125\n",
      "epoch 65, val_loss 454.8039245605469\n",
      "epoch 66, train_loss 1277.139892578125\n",
      "epoch 66, val_loss 454.8038024902344\n",
      "epoch 67, train_loss 1277.139892578125\n",
      "epoch 67, val_loss 454.8038024902344\n",
      "epoch 68, train_loss 1277.1397705078125\n",
      "epoch 68, val_loss 454.8037414550781\n",
      "epoch 69, train_loss 1277.1396484375\n",
      "epoch 69, val_loss 454.8037109375\n",
      "epoch 70, train_loss 1277.1397705078125\n",
      "epoch 70, val_loss 454.8036804199219\n",
      "epoch 71, train_loss 1277.139892578125\n",
      "epoch 71, val_loss 454.8035888671875\n",
      "epoch 72, train_loss 1277.139892578125\n",
      "epoch 72, val_loss 454.8035888671875\n",
      "epoch 73, train_loss 1277.1396484375\n",
      "epoch 73, val_loss 454.80352783203125\n",
      "epoch 74, train_loss 1277.1395263671875\n",
      "epoch 74, val_loss 454.8034973144531\n",
      "epoch 75, train_loss 1277.1395263671875\n",
      "epoch 75, val_loss 454.80340576171875\n",
      "epoch 76, train_loss 1277.1395263671875\n",
      "epoch 76, val_loss 454.8033752441406\n",
      "epoch 77, train_loss 1277.1395263671875\n",
      "epoch 77, val_loss 454.8033752441406\n",
      "epoch 78, train_loss 1277.1395263671875\n",
      "epoch 78, val_loss 454.80328369140625\n",
      "epoch 79, train_loss 1277.1395263671875\n",
      "epoch 79, val_loss 454.80328369140625\n",
      "epoch 80, train_loss 1277.139404296875\n",
      "epoch 80, val_loss 454.80322265625\n",
      "epoch 81, train_loss 1277.1392822265625\n",
      "epoch 81, val_loss 454.80322265625\n",
      "epoch 82, train_loss 1277.1392822265625\n",
      "epoch 82, val_loss 454.80322265625\n",
      "epoch 83, train_loss 1277.13916015625\n",
      "epoch 83, val_loss 454.8031311035156\n",
      "epoch 84, train_loss 1277.13916015625\n",
      "epoch 84, val_loss 454.8031311035156\n",
      "epoch 85, train_loss 1277.13916015625\n",
      "epoch 85, val_loss 454.8030700683594\n",
      "epoch 86, train_loss 1277.13916015625\n",
      "epoch 86, val_loss 454.8029479980469\n",
      "epoch 87, train_loss 1277.1390380859375\n",
      "epoch 87, val_loss 454.8029479980469\n",
      "epoch 88, train_loss 1277.1390380859375\n",
      "epoch 88, val_loss 454.80291748046875\n",
      "epoch 89, train_loss 1277.1390380859375\n",
      "epoch 89, val_loss 454.8028869628906\n",
      "epoch 90, train_loss 1277.1387939453125\n",
      "epoch 90, val_loss 454.80279541015625\n",
      "epoch 91, train_loss 1277.1387939453125\n",
      "epoch 91, val_loss 454.80279541015625\n",
      "epoch 92, train_loss 1277.1387939453125\n",
      "epoch 92, val_loss 454.8027038574219\n",
      "epoch 93, train_loss 1277.1387939453125\n",
      "epoch 93, val_loss 454.8027038574219\n",
      "epoch 94, train_loss 1277.1387939453125\n",
      "epoch 94, val_loss 454.8027038574219\n",
      "epoch 95, train_loss 1277.1387939453125\n",
      "epoch 95, val_loss 454.8026123046875\n",
      "epoch 96, train_loss 1277.1387939453125\n",
      "epoch 96, val_loss 454.80255126953125\n",
      "epoch 97, train_loss 1277.1387939453125\n",
      "epoch 97, val_loss 454.80255126953125\n",
      "epoch 98, train_loss 1277.138671875\n",
      "epoch 98, val_loss 454.8024597167969\n",
      "epoch 99, train_loss 1277.1385498046875\n",
      "epoch 99, val_loss 454.8023681640625\n",
      "Parameter containing:\n",
      "tensor([1.6716e-25], requires_grad=True)\n",
      "iter 136, train_loss_regularization 0.7063407897949219\n",
      "iter 136, val_loss_regularization 0.7063407897949219\n",
      "epoch 0, train_loss 1277.1385498046875\n",
      "epoch 0, val_loss 454.8023681640625\n",
      "epoch 1, train_loss 1277.1385498046875\n",
      "epoch 1, val_loss 454.8023681640625\n",
      "epoch 2, train_loss 1277.1385498046875\n",
      "epoch 2, val_loss 454.8022766113281\n",
      "epoch 3, train_loss 1277.1383056640625\n",
      "epoch 3, val_loss 454.80224609375\n",
      "epoch 4, train_loss 1277.13818359375\n",
      "epoch 4, val_loss 454.8021545410156\n",
      "epoch 5, train_loss 1277.13818359375\n",
      "epoch 5, val_loss 454.8021545410156\n",
      "epoch 6, train_loss 1277.1383056640625\n",
      "epoch 6, val_loss 454.8021545410156\n",
      "epoch 7, train_loss 1277.1383056640625\n",
      "epoch 7, val_loss 454.8020324707031\n",
      "epoch 8, train_loss 1277.1383056640625\n",
      "epoch 8, val_loss 454.8020324707031\n",
      "epoch 9, train_loss 1277.13818359375\n",
      "epoch 9, val_loss 454.802001953125\n",
      "epoch 10, train_loss 1277.13818359375\n",
      "epoch 10, val_loss 454.802001953125\n",
      "epoch 11, train_loss 1277.13818359375\n",
      "epoch 11, val_loss 454.8019104003906\n",
      "epoch 12, train_loss 1277.1380615234375\n",
      "epoch 12, val_loss 454.8019104003906\n",
      "epoch 13, train_loss 1277.13818359375\n",
      "epoch 13, val_loss 454.8018798828125\n",
      "epoch 14, train_loss 1277.137939453125\n",
      "epoch 14, val_loss 454.8017578125\n",
      "epoch 15, train_loss 1277.137939453125\n",
      "epoch 15, val_loss 454.80169677734375\n",
      "epoch 16, train_loss 1277.137939453125\n",
      "epoch 16, val_loss 454.8016662597656\n",
      "epoch 17, train_loss 1277.137939453125\n",
      "epoch 17, val_loss 454.8016662597656\n",
      "epoch 18, train_loss 1277.1378173828125\n",
      "epoch 18, val_loss 454.80157470703125\n",
      "epoch 19, train_loss 1277.1376953125\n",
      "epoch 19, val_loss 454.80157470703125\n",
      "epoch 20, train_loss 1277.1376953125\n",
      "epoch 20, val_loss 454.8015441894531\n",
      "epoch 21, train_loss 1277.1376953125\n",
      "epoch 21, val_loss 454.801513671875\n",
      "epoch 22, train_loss 1277.1376953125\n",
      "epoch 22, val_loss 454.80145263671875\n",
      "epoch 23, train_loss 1277.1376953125\n",
      "epoch 23, val_loss 454.80145263671875\n",
      "epoch 24, train_loss 1277.1376953125\n",
      "epoch 24, val_loss 454.8013610839844\n",
      "epoch 25, train_loss 1277.1376953125\n",
      "epoch 25, val_loss 454.80133056640625\n",
      "epoch 26, train_loss 1277.1376953125\n",
      "epoch 26, val_loss 454.80120849609375\n",
      "epoch 27, train_loss 1277.137451171875\n",
      "epoch 27, val_loss 454.80120849609375\n",
      "epoch 28, train_loss 1277.1373291015625\n",
      "epoch 28, val_loss 454.80120849609375\n",
      "epoch 29, train_loss 1277.137451171875\n",
      "epoch 29, val_loss 454.8011779785156\n",
      "epoch 30, train_loss 1277.1373291015625\n",
      "epoch 30, val_loss 454.80108642578125\n",
      "epoch 31, train_loss 1277.137451171875\n",
      "epoch 31, val_loss 454.80108642578125\n",
      "epoch 32, train_loss 1277.1373291015625\n",
      "epoch 32, val_loss 454.8010559082031\n",
      "epoch 33, train_loss 1277.13720703125\n",
      "epoch 33, val_loss 454.80096435546875\n",
      "epoch 34, train_loss 1277.13720703125\n",
      "epoch 34, val_loss 454.8009033203125\n",
      "epoch 35, train_loss 1277.136962890625\n",
      "epoch 35, val_loss 454.8008728027344\n",
      "epoch 36, train_loss 1277.136962890625\n",
      "epoch 36, val_loss 454.80084228515625\n",
      "epoch 37, train_loss 1277.136962890625\n",
      "epoch 37, val_loss 454.80078125\n",
      "epoch 38, train_loss 1277.13720703125\n",
      "epoch 38, val_loss 454.8007507324219\n",
      "epoch 39, train_loss 1277.13720703125\n",
      "epoch 39, val_loss 454.80072021484375\n",
      "epoch 40, train_loss 1277.136962890625\n",
      "epoch 40, val_loss 454.80072021484375\n",
      "epoch 41, train_loss 1277.1368408203125\n",
      "epoch 41, val_loss 454.8006286621094\n",
      "epoch 42, train_loss 1277.13671875\n",
      "epoch 42, val_loss 454.8006286621094\n",
      "epoch 43, train_loss 1277.13671875\n",
      "epoch 43, val_loss 454.8005065917969\n",
      "epoch 44, train_loss 1277.1368408203125\n",
      "epoch 44, val_loss 454.8004455566406\n",
      "epoch 45, train_loss 1277.13671875\n",
      "epoch 45, val_loss 454.8004455566406\n",
      "epoch 46, train_loss 1277.13671875\n",
      "epoch 46, val_loss 454.8004455566406\n",
      "epoch 47, train_loss 1277.13671875\n",
      "epoch 47, val_loss 454.8003845214844\n",
      "epoch 48, train_loss 1277.1365966796875\n",
      "epoch 48, val_loss 454.8003845214844\n",
      "epoch 49, train_loss 1277.136474609375\n",
      "epoch 49, val_loss 454.8003234863281\n",
      "epoch 50, train_loss 1277.136474609375\n",
      "epoch 50, val_loss 454.8002624511719\n",
      "epoch 51, train_loss 1277.136474609375\n",
      "epoch 51, val_loss 454.8002624511719\n",
      "epoch 52, train_loss 1277.136474609375\n",
      "epoch 52, val_loss 454.8000793457031\n",
      "epoch 53, train_loss 1277.136474609375\n",
      "epoch 53, val_loss 454.80010986328125\n",
      "epoch 54, train_loss 1277.136474609375\n",
      "epoch 54, val_loss 454.800048828125\n",
      "epoch 55, train_loss 1277.13623046875\n",
      "epoch 55, val_loss 454.800048828125\n",
      "epoch 56, train_loss 1277.13623046875\n",
      "epoch 56, val_loss 454.7999572753906\n",
      "epoch 57, train_loss 1277.13623046875\n",
      "epoch 57, val_loss 454.7999572753906\n",
      "epoch 58, train_loss 1277.13623046875\n",
      "epoch 58, val_loss 454.7999267578125\n",
      "epoch 59, train_loss 1277.1363525390625\n",
      "epoch 59, val_loss 454.79986572265625\n",
      "epoch 60, train_loss 1277.13623046875\n",
      "epoch 60, val_loss 454.79986572265625\n",
      "epoch 61, train_loss 1277.1361083984375\n",
      "epoch 61, val_loss 454.7998046875\n",
      "epoch 62, train_loss 1277.1361083984375\n",
      "epoch 62, val_loss 454.79974365234375\n",
      "epoch 63, train_loss 1277.1361083984375\n",
      "epoch 63, val_loss 454.7996520996094\n",
      "epoch 64, train_loss 1277.135986328125\n",
      "epoch 64, val_loss 454.79962158203125\n",
      "epoch 65, train_loss 1277.135986328125\n",
      "epoch 65, val_loss 454.7996520996094\n",
      "epoch 66, train_loss 1277.1358642578125\n",
      "epoch 66, val_loss 454.7995910644531\n",
      "epoch 67, train_loss 1277.135986328125\n",
      "epoch 67, val_loss 454.79949951171875\n",
      "epoch 68, train_loss 1277.1358642578125\n",
      "epoch 68, val_loss 454.7994689941406\n",
      "epoch 69, train_loss 1277.1357421875\n",
      "epoch 69, val_loss 454.7994689941406\n",
      "epoch 70, train_loss 1277.1357421875\n",
      "epoch 70, val_loss 454.7994079589844\n",
      "epoch 71, train_loss 1277.1357421875\n",
      "epoch 71, val_loss 454.7993469238281\n",
      "epoch 72, train_loss 1277.1356201171875\n",
      "epoch 72, val_loss 454.7993469238281\n",
      "epoch 73, train_loss 1277.1356201171875\n",
      "epoch 73, val_loss 454.79925537109375\n",
      "epoch 74, train_loss 1277.1356201171875\n",
      "epoch 74, val_loss 454.79925537109375\n",
      "epoch 75, train_loss 1277.1356201171875\n",
      "epoch 75, val_loss 454.7991638183594\n",
      "epoch 76, train_loss 1277.135498046875\n",
      "epoch 76, val_loss 454.7991943359375\n",
      "epoch 77, train_loss 1277.135498046875\n",
      "epoch 77, val_loss 454.79913330078125\n",
      "epoch 78, train_loss 1277.1353759765625\n",
      "epoch 78, val_loss 454.799072265625\n",
      "epoch 79, train_loss 1277.1353759765625\n",
      "epoch 79, val_loss 454.79901123046875\n",
      "epoch 80, train_loss 1277.1353759765625\n",
      "epoch 80, val_loss 454.79901123046875\n",
      "epoch 81, train_loss 1277.13525390625\n",
      "epoch 81, val_loss 454.7989501953125\n",
      "epoch 82, train_loss 1277.1351318359375\n",
      "epoch 82, val_loss 454.79888916015625\n",
      "epoch 83, train_loss 1277.1351318359375\n",
      "epoch 83, val_loss 454.7987976074219\n",
      "epoch 84, train_loss 1277.13525390625\n",
      "epoch 84, val_loss 454.7987976074219\n",
      "epoch 85, train_loss 1277.13525390625\n",
      "epoch 85, val_loss 454.798828125\n",
      "epoch 86, train_loss 1277.1351318359375\n",
      "epoch 86, val_loss 454.7986755371094\n",
      "epoch 87, train_loss 1277.1351318359375\n",
      "epoch 87, val_loss 454.7986755371094\n",
      "epoch 88, train_loss 1277.1351318359375\n",
      "epoch 88, val_loss 454.7987060546875\n",
      "epoch 89, train_loss 1277.135009765625\n",
      "epoch 89, val_loss 454.7986145019531\n",
      "epoch 90, train_loss 1277.1348876953125\n",
      "epoch 90, val_loss 454.7985534667969\n",
      "epoch 91, train_loss 1277.1348876953125\n",
      "epoch 91, val_loss 454.7984924316406\n",
      "epoch 92, train_loss 1277.1348876953125\n",
      "epoch 92, val_loss 454.7984924316406\n",
      "epoch 93, train_loss 1277.1348876953125\n",
      "epoch 93, val_loss 454.7984313964844\n",
      "epoch 94, train_loss 1277.134765625\n",
      "epoch 94, val_loss 454.79833984375\n",
      "epoch 95, train_loss 1277.1346435546875\n",
      "epoch 95, val_loss 454.79827880859375\n",
      "epoch 96, train_loss 1277.1346435546875\n",
      "epoch 96, val_loss 454.79833984375\n",
      "epoch 97, train_loss 1277.134521484375\n",
      "epoch 97, val_loss 454.7982482910156\n",
      "epoch 98, train_loss 1277.1346435546875\n",
      "epoch 98, val_loss 454.7982177734375\n",
      "epoch 99, train_loss 1277.1346435546875\n",
      "epoch 99, val_loss 454.79815673828125\n",
      "Parameter containing:\n",
      "tensor([1.1613e-25], requires_grad=True)\n",
      "iter 137, train_loss_regularization 0.705472469329834\n",
      "iter 137, val_loss_regularization 0.705472469329834\n",
      "epoch 0, train_loss 1277.1346435546875\n",
      "epoch 0, val_loss 454.79815673828125\n",
      "epoch 1, train_loss 1277.1346435546875\n",
      "epoch 1, val_loss 454.79803466796875\n",
      "epoch 2, train_loss 1277.1343994140625\n",
      "epoch 2, val_loss 454.79803466796875\n",
      "epoch 3, train_loss 1277.134521484375\n",
      "epoch 3, val_loss 454.79803466796875\n",
      "epoch 4, train_loss 1277.1343994140625\n",
      "epoch 4, val_loss 454.7979431152344\n",
      "epoch 5, train_loss 1277.13427734375\n",
      "epoch 5, val_loss 454.7978210449219\n",
      "epoch 6, train_loss 1277.13427734375\n",
      "epoch 6, val_loss 454.7978210449219\n",
      "epoch 7, train_loss 1277.13427734375\n",
      "epoch 7, val_loss 454.7978820800781\n",
      "epoch 8, train_loss 1277.13427734375\n",
      "epoch 8, val_loss 454.7978210449219\n",
      "epoch 9, train_loss 1277.1341552734375\n",
      "epoch 9, val_loss 454.7977600097656\n",
      "epoch 10, train_loss 1277.1341552734375\n",
      "epoch 10, val_loss 454.7977600097656\n",
      "epoch 11, train_loss 1277.134033203125\n",
      "epoch 11, val_loss 454.7976379394531\n",
      "epoch 12, train_loss 1277.134033203125\n",
      "epoch 12, val_loss 454.7976379394531\n",
      "epoch 13, train_loss 1277.134033203125\n",
      "epoch 13, val_loss 454.79754638671875\n",
      "epoch 14, train_loss 1277.1341552734375\n",
      "epoch 14, val_loss 454.79754638671875\n",
      "epoch 15, train_loss 1277.134033203125\n",
      "epoch 15, val_loss 454.7974853515625\n",
      "epoch 16, train_loss 1277.134033203125\n",
      "epoch 16, val_loss 454.7974853515625\n",
      "epoch 17, train_loss 1277.1337890625\n",
      "epoch 17, val_loss 454.7974548339844\n",
      "epoch 18, train_loss 1277.1337890625\n",
      "epoch 18, val_loss 454.79736328125\n",
      "epoch 19, train_loss 1277.1337890625\n",
      "epoch 19, val_loss 454.7973327636719\n",
      "epoch 20, train_loss 1277.1337890625\n",
      "epoch 20, val_loss 454.7972412109375\n",
      "epoch 21, train_loss 1277.1337890625\n",
      "epoch 21, val_loss 454.7972106933594\n",
      "epoch 22, train_loss 1277.1336669921875\n",
      "epoch 22, val_loss 454.79718017578125\n",
      "epoch 23, train_loss 1277.1336669921875\n",
      "epoch 23, val_loss 454.797119140625\n",
      "epoch 24, train_loss 1277.133544921875\n",
      "epoch 24, val_loss 454.797119140625\n",
      "epoch 25, train_loss 1277.133544921875\n",
      "epoch 25, val_loss 454.7970275878906\n",
      "epoch 26, train_loss 1277.133544921875\n",
      "epoch 26, val_loss 454.7970275878906\n",
      "epoch 27, train_loss 1277.133544921875\n",
      "epoch 27, val_loss 454.7969970703125\n",
      "epoch 28, train_loss 1277.133544921875\n",
      "epoch 28, val_loss 454.7969970703125\n",
      "epoch 29, train_loss 1277.13330078125\n",
      "epoch 29, val_loss 454.7969055175781\n",
      "epoch 30, train_loss 1277.1334228515625\n",
      "epoch 30, val_loss 454.796875\n",
      "epoch 31, train_loss 1277.1334228515625\n",
      "epoch 31, val_loss 454.796875\n",
      "epoch 32, train_loss 1277.13330078125\n",
      "epoch 32, val_loss 454.7967834472656\n",
      "epoch 33, train_loss 1277.1331787109375\n",
      "epoch 33, val_loss 454.7967224121094\n",
      "epoch 34, train_loss 1277.1331787109375\n",
      "epoch 34, val_loss 454.7967224121094\n",
      "epoch 35, train_loss 1277.133056640625\n",
      "epoch 35, val_loss 454.796630859375\n",
      "epoch 36, train_loss 1277.1331787109375\n",
      "epoch 36, val_loss 454.79656982421875\n",
      "epoch 37, train_loss 1277.133056640625\n",
      "epoch 37, val_loss 454.79656982421875\n",
      "epoch 38, train_loss 1277.133056640625\n",
      "epoch 38, val_loss 454.7965393066406\n",
      "epoch 39, train_loss 1277.133056640625\n",
      "epoch 39, val_loss 454.7965087890625\n",
      "epoch 40, train_loss 1277.1329345703125\n",
      "epoch 40, val_loss 454.7964172363281\n",
      "epoch 41, train_loss 1277.1328125\n",
      "epoch 41, val_loss 454.79638671875\n",
      "epoch 42, train_loss 1277.1326904296875\n",
      "epoch 42, val_loss 454.79638671875\n",
      "epoch 43, train_loss 1277.1328125\n",
      "epoch 43, val_loss 454.7962951660156\n",
      "epoch 44, train_loss 1277.1326904296875\n",
      "epoch 44, val_loss 454.7962951660156\n",
      "epoch 45, train_loss 1277.1326904296875\n",
      "epoch 45, val_loss 454.7962646484375\n",
      "epoch 46, train_loss 1277.1328125\n",
      "epoch 46, val_loss 454.7962646484375\n",
      "epoch 47, train_loss 1277.1328125\n",
      "epoch 47, val_loss 454.79608154296875\n",
      "epoch 48, train_loss 1277.1326904296875\n",
      "epoch 48, val_loss 454.79608154296875\n",
      "epoch 49, train_loss 1277.132568359375\n",
      "epoch 49, val_loss 454.7960510253906\n",
      "epoch 50, train_loss 1277.132568359375\n",
      "epoch 50, val_loss 454.7959899902344\n",
      "epoch 51, train_loss 1277.132568359375\n",
      "epoch 51, val_loss 454.79595947265625\n",
      "epoch 52, train_loss 1277.132568359375\n",
      "epoch 52, val_loss 454.7959289550781\n",
      "epoch 53, train_loss 1277.132568359375\n",
      "epoch 53, val_loss 454.7959289550781\n",
      "epoch 54, train_loss 1277.1324462890625\n",
      "epoch 54, val_loss 454.79583740234375\n",
      "epoch 55, train_loss 1277.13232421875\n",
      "epoch 55, val_loss 454.7958068847656\n",
      "epoch 56, train_loss 1277.1322021484375\n",
      "epoch 56, val_loss 454.7957458496094\n",
      "epoch 57, train_loss 1277.1322021484375\n",
      "epoch 57, val_loss 454.7958068847656\n",
      "epoch 58, train_loss 1277.13232421875\n",
      "epoch 58, val_loss 454.795654296875\n",
      "epoch 59, train_loss 1277.13232421875\n",
      "epoch 59, val_loss 454.7956237792969\n",
      "epoch 60, train_loss 1277.1322021484375\n",
      "epoch 60, val_loss 454.79559326171875\n",
      "epoch 61, train_loss 1277.1322021484375\n",
      "epoch 61, val_loss 454.7955322265625\n",
      "epoch 62, train_loss 1277.132080078125\n",
      "epoch 62, val_loss 454.79547119140625\n",
      "epoch 63, train_loss 1277.1319580078125\n",
      "epoch 63, val_loss 454.79547119140625\n",
      "epoch 64, train_loss 1277.132080078125\n",
      "epoch 64, val_loss 454.79547119140625\n",
      "epoch 65, train_loss 1277.1319580078125\n",
      "epoch 65, val_loss 454.79541015625\n",
      "epoch 66, train_loss 1277.1319580078125\n",
      "epoch 66, val_loss 454.7953186035156\n",
      "epoch 67, train_loss 1277.1319580078125\n",
      "epoch 67, val_loss 454.7952880859375\n",
      "epoch 68, train_loss 1277.1318359375\n",
      "epoch 68, val_loss 454.7953186035156\n",
      "epoch 69, train_loss 1277.1317138671875\n",
      "epoch 69, val_loss 454.7951965332031\n",
      "epoch 70, train_loss 1277.1317138671875\n",
      "epoch 70, val_loss 454.795166015625\n",
      "epoch 71, train_loss 1277.1317138671875\n",
      "epoch 71, val_loss 454.795166015625\n",
      "epoch 72, train_loss 1277.1317138671875\n",
      "epoch 72, val_loss 454.7951354980469\n",
      "epoch 73, train_loss 1277.1317138671875\n",
      "epoch 73, val_loss 454.7950134277344\n",
      "epoch 74, train_loss 1277.1317138671875\n",
      "epoch 74, val_loss 454.7950134277344\n",
      "epoch 75, train_loss 1277.1317138671875\n",
      "epoch 75, val_loss 454.7949523925781\n",
      "epoch 76, train_loss 1277.1314697265625\n",
      "epoch 76, val_loss 454.794921875\n",
      "epoch 77, train_loss 1277.1314697265625\n",
      "epoch 77, val_loss 454.794921875\n",
      "epoch 78, train_loss 1277.1314697265625\n",
      "epoch 78, val_loss 454.7947998046875\n",
      "epoch 79, train_loss 1277.1314697265625\n",
      "epoch 79, val_loss 454.7947998046875\n",
      "epoch 80, train_loss 1277.1314697265625\n",
      "epoch 80, val_loss 454.79473876953125\n",
      "epoch 81, train_loss 1277.1312255859375\n",
      "epoch 81, val_loss 454.7947082519531\n",
      "epoch 82, train_loss 1277.13134765625\n",
      "epoch 82, val_loss 454.79461669921875\n",
      "epoch 83, train_loss 1277.1312255859375\n",
      "epoch 83, val_loss 454.794677734375\n",
      "epoch 84, train_loss 1277.1312255859375\n",
      "epoch 84, val_loss 454.79461669921875\n",
      "epoch 85, train_loss 1277.1312255859375\n",
      "epoch 85, val_loss 454.7945556640625\n",
      "epoch 86, train_loss 1277.131103515625\n",
      "epoch 86, val_loss 454.79449462890625\n",
      "epoch 87, train_loss 1277.131103515625\n",
      "epoch 87, val_loss 454.7944030761719\n",
      "epoch 88, train_loss 1277.131103515625\n",
      "epoch 88, val_loss 454.7944030761719\n",
      "epoch 89, train_loss 1277.1309814453125\n",
      "epoch 89, val_loss 454.79437255859375\n",
      "epoch 90, train_loss 1277.1309814453125\n",
      "epoch 90, val_loss 454.7942810058594\n",
      "epoch 91, train_loss 1277.1309814453125\n",
      "epoch 91, val_loss 454.7942810058594\n",
      "epoch 92, train_loss 1277.131103515625\n",
      "epoch 92, val_loss 454.7942199707031\n",
      "epoch 93, train_loss 1277.130859375\n",
      "epoch 93, val_loss 454.7942199707031\n",
      "epoch 94, train_loss 1277.130859375\n",
      "epoch 94, val_loss 454.7941589355469\n",
      "epoch 95, train_loss 1277.130859375\n",
      "epoch 95, val_loss 454.7941589355469\n",
      "epoch 96, train_loss 1277.130859375\n",
      "epoch 96, val_loss 454.79412841796875\n",
      "epoch 97, train_loss 1277.130615234375\n",
      "epoch 97, val_loss 454.7940979003906\n",
      "epoch 98, train_loss 1277.130615234375\n",
      "epoch 98, val_loss 454.79400634765625\n",
      "epoch 99, train_loss 1277.130615234375\n",
      "epoch 99, val_loss 454.79400634765625\n",
      "Parameter containing:\n",
      "tensor([8.0700e-26], requires_grad=True)\n",
      "iter 138, train_loss_regularization 0.7046258449554443\n",
      "iter 138, val_loss_regularization 0.7046258449554443\n",
      "epoch 0, train_loss 1277.130615234375\n",
      "epoch 0, val_loss 454.7939147949219\n",
      "epoch 1, train_loss 1277.130615234375\n",
      "epoch 1, val_loss 454.7938232421875\n",
      "epoch 2, train_loss 1277.130615234375\n",
      "epoch 2, val_loss 454.7937927246094\n",
      "epoch 3, train_loss 1277.130615234375\n",
      "epoch 3, val_loss 454.7937927246094\n",
      "epoch 4, train_loss 1277.1304931640625\n",
      "epoch 4, val_loss 454.79376220703125\n",
      "epoch 5, train_loss 1277.1304931640625\n",
      "epoch 5, val_loss 454.793701171875\n",
      "epoch 6, train_loss 1277.13037109375\n",
      "epoch 6, val_loss 454.793701171875\n",
      "epoch 7, train_loss 1277.13037109375\n",
      "epoch 7, val_loss 454.7936706542969\n",
      "epoch 8, train_loss 1277.13037109375\n",
      "epoch 8, val_loss 454.79364013671875\n",
      "epoch 9, train_loss 1277.1302490234375\n",
      "epoch 9, val_loss 454.7935485839844\n",
      "epoch 10, train_loss 1277.130126953125\n",
      "epoch 10, val_loss 454.7934875488281\n",
      "epoch 11, train_loss 1277.130126953125\n",
      "epoch 11, val_loss 454.7934875488281\n",
      "epoch 12, train_loss 1277.130126953125\n",
      "epoch 12, val_loss 454.7934265136719\n",
      "epoch 13, train_loss 1277.130126953125\n",
      "epoch 13, val_loss 454.7933654785156\n",
      "epoch 14, train_loss 1277.130126953125\n",
      "epoch 14, val_loss 454.7933654785156\n",
      "epoch 15, train_loss 1277.130126953125\n",
      "epoch 15, val_loss 454.7933349609375\n",
      "epoch 16, train_loss 1277.130126953125\n",
      "epoch 16, val_loss 454.7932434082031\n",
      "epoch 17, train_loss 1277.130126953125\n",
      "epoch 17, val_loss 454.7932434082031\n",
      "epoch 18, train_loss 1277.1298828125\n",
      "epoch 18, val_loss 454.7931213378906\n",
      "epoch 19, train_loss 1277.1297607421875\n",
      "epoch 19, val_loss 454.7930908203125\n",
      "epoch 20, train_loss 1277.1297607421875\n",
      "epoch 20, val_loss 454.79302978515625\n",
      "epoch 21, train_loss 1277.1297607421875\n",
      "epoch 21, val_loss 454.79302978515625\n",
      "epoch 22, train_loss 1277.1298828125\n",
      "epoch 22, val_loss 454.7929992675781\n",
      "epoch 23, train_loss 1277.1297607421875\n",
      "epoch 23, val_loss 454.79296875\n",
      "epoch 24, train_loss 1277.129638671875\n",
      "epoch 24, val_loss 454.79290771484375\n",
      "epoch 25, train_loss 1277.129638671875\n",
      "epoch 25, val_loss 454.79290771484375\n",
      "epoch 26, train_loss 1277.129638671875\n",
      "epoch 26, val_loss 454.7928771972656\n",
      "epoch 27, train_loss 1277.129638671875\n",
      "epoch 27, val_loss 454.79278564453125\n",
      "epoch 28, train_loss 1277.1295166015625\n",
      "epoch 28, val_loss 454.79266357421875\n",
      "epoch 29, train_loss 1277.1295166015625\n",
      "epoch 29, val_loss 454.79266357421875\n",
      "epoch 30, train_loss 1277.1295166015625\n",
      "epoch 30, val_loss 454.79266357421875\n",
      "epoch 31, train_loss 1277.1292724609375\n",
      "epoch 31, val_loss 454.7925720214844\n",
      "epoch 32, train_loss 1277.1292724609375\n",
      "epoch 32, val_loss 454.7925720214844\n",
      "epoch 33, train_loss 1277.1292724609375\n",
      "epoch 33, val_loss 454.79254150390625\n",
      "epoch 34, train_loss 1277.129150390625\n",
      "epoch 34, val_loss 454.79254150390625\n",
      "epoch 35, train_loss 1277.1292724609375\n",
      "epoch 35, val_loss 454.7924499511719\n",
      "epoch 36, train_loss 1277.1292724609375\n",
      "epoch 36, val_loss 454.7924499511719\n",
      "epoch 37, train_loss 1277.1292724609375\n",
      "epoch 37, val_loss 454.7923889160156\n",
      "epoch 38, train_loss 1277.1292724609375\n",
      "epoch 38, val_loss 454.79229736328125\n",
      "epoch 39, train_loss 1277.1290283203125\n",
      "epoch 39, val_loss 454.792236328125\n",
      "epoch 40, train_loss 1277.1290283203125\n",
      "epoch 40, val_loss 454.79229736328125\n",
      "epoch 41, train_loss 1277.1290283203125\n",
      "epoch 41, val_loss 454.792236328125\n",
      "epoch 42, train_loss 1277.1290283203125\n",
      "epoch 42, val_loss 454.79217529296875\n",
      "epoch 43, train_loss 1277.1290283203125\n",
      "epoch 43, val_loss 454.79217529296875\n",
      "epoch 44, train_loss 1277.1290283203125\n",
      "epoch 44, val_loss 454.7920837402344\n",
      "epoch 45, train_loss 1277.1287841796875\n",
      "epoch 45, val_loss 454.7920837402344\n",
      "epoch 46, train_loss 1277.1287841796875\n",
      "epoch 46, val_loss 454.7919616699219\n",
      "epoch 47, train_loss 1277.128662109375\n",
      "epoch 47, val_loss 454.79193115234375\n",
      "epoch 48, train_loss 1277.128662109375\n",
      "epoch 48, val_loss 454.7919616699219\n",
      "epoch 49, train_loss 1277.128662109375\n",
      "epoch 49, val_loss 454.7918701171875\n",
      "epoch 50, train_loss 1277.1285400390625\n",
      "epoch 50, val_loss 454.7918395996094\n",
      "epoch 51, train_loss 1277.128662109375\n",
      "epoch 51, val_loss 454.7917785644531\n",
      "epoch 52, train_loss 1277.1287841796875\n",
      "epoch 52, val_loss 454.7917785644531\n",
      "epoch 53, train_loss 1277.128662109375\n",
      "epoch 53, val_loss 454.791748046875\n",
      "epoch 54, train_loss 1277.12841796875\n",
      "epoch 54, val_loss 454.7917175292969\n",
      "epoch 55, train_loss 1277.1285400390625\n",
      "epoch 55, val_loss 454.7915954589844\n",
      "epoch 56, train_loss 1277.1285400390625\n",
      "epoch 56, val_loss 454.7915954589844\n",
      "epoch 57, train_loss 1277.1282958984375\n",
      "epoch 57, val_loss 454.7915344238281\n",
      "epoch 58, train_loss 1277.1285400390625\n",
      "epoch 58, val_loss 454.79150390625\n",
      "epoch 59, train_loss 1277.1282958984375\n",
      "epoch 59, val_loss 454.7914123535156\n",
      "epoch 60, train_loss 1277.1282958984375\n",
      "epoch 60, val_loss 454.7914123535156\n",
      "epoch 61, train_loss 1277.128173828125\n",
      "epoch 61, val_loss 454.7913818359375\n",
      "epoch 62, train_loss 1277.1280517578125\n",
      "epoch 62, val_loss 454.79132080078125\n",
      "epoch 63, train_loss 1277.1280517578125\n",
      "epoch 63, val_loss 454.7912902832031\n",
      "epoch 64, train_loss 1277.128173828125\n",
      "epoch 64, val_loss 454.7912902832031\n",
      "epoch 65, train_loss 1277.1280517578125\n",
      "epoch 65, val_loss 454.79119873046875\n",
      "epoch 66, train_loss 1277.1280517578125\n",
      "epoch 66, val_loss 454.79119873046875\n",
      "epoch 67, train_loss 1277.1280517578125\n",
      "epoch 67, val_loss 454.7911376953125\n",
      "epoch 68, train_loss 1277.1280517578125\n",
      "epoch 68, val_loss 454.79107666015625\n",
      "epoch 69, train_loss 1277.1280517578125\n",
      "epoch 69, val_loss 454.79107666015625\n",
      "epoch 70, train_loss 1277.1278076171875\n",
      "epoch 70, val_loss 454.7909851074219\n",
      "epoch 71, train_loss 1277.1278076171875\n",
      "epoch 71, val_loss 454.7909240722656\n",
      "epoch 72, train_loss 1277.1278076171875\n",
      "epoch 72, val_loss 454.7909240722656\n",
      "epoch 73, train_loss 1277.127685546875\n",
      "epoch 73, val_loss 454.79083251953125\n",
      "epoch 74, train_loss 1277.1278076171875\n",
      "epoch 74, val_loss 454.7908630371094\n",
      "epoch 75, train_loss 1277.1278076171875\n",
      "epoch 75, val_loss 454.79083251953125\n",
      "epoch 76, train_loss 1277.1275634765625\n",
      "epoch 76, val_loss 454.79083251953125\n",
      "epoch 77, train_loss 1277.1278076171875\n",
      "epoch 77, val_loss 454.7906799316406\n",
      "epoch 78, train_loss 1277.1275634765625\n",
      "epoch 78, val_loss 454.7906799316406\n",
      "epoch 79, train_loss 1277.1275634765625\n",
      "epoch 79, val_loss 454.7906188964844\n",
      "epoch 80, train_loss 1277.12744140625\n",
      "epoch 80, val_loss 454.7906188964844\n",
      "epoch 81, train_loss 1277.12744140625\n",
      "epoch 81, val_loss 454.79052734375\n",
      "epoch 82, train_loss 1277.12744140625\n",
      "epoch 82, val_loss 454.7904968261719\n",
      "epoch 83, train_loss 1277.12744140625\n",
      "epoch 83, val_loss 454.7904968261719\n",
      "epoch 84, train_loss 1277.12744140625\n",
      "epoch 84, val_loss 454.7904968261719\n",
      "epoch 85, train_loss 1277.12744140625\n",
      "epoch 85, val_loss 454.7903747558594\n",
      "epoch 86, train_loss 1277.1273193359375\n",
      "epoch 86, val_loss 454.790283203125\n",
      "epoch 87, train_loss 1277.127197265625\n",
      "epoch 87, val_loss 454.790283203125\n",
      "epoch 88, train_loss 1277.127197265625\n",
      "epoch 88, val_loss 454.7902526855469\n",
      "epoch 89, train_loss 1277.127197265625\n",
      "epoch 89, val_loss 454.79022216796875\n",
      "epoch 90, train_loss 1277.127197265625\n",
      "epoch 90, val_loss 454.7901611328125\n",
      "epoch 91, train_loss 1277.127197265625\n",
      "epoch 91, val_loss 454.7901306152344\n",
      "epoch 92, train_loss 1277.126953125\n",
      "epoch 92, val_loss 454.7901611328125\n",
      "epoch 93, train_loss 1277.126953125\n",
      "epoch 93, val_loss 454.7900390625\n",
      "epoch 94, train_loss 1277.1268310546875\n",
      "epoch 94, val_loss 454.7900390625\n",
      "epoch 95, train_loss 1277.126953125\n",
      "epoch 95, val_loss 454.7900085449219\n",
      "epoch 96, train_loss 1277.126953125\n",
      "epoch 96, val_loss 454.7898864746094\n",
      "epoch 97, train_loss 1277.126708984375\n",
      "epoch 97, val_loss 454.7898254394531\n",
      "epoch 98, train_loss 1277.1268310546875\n",
      "epoch 98, val_loss 454.789794921875\n",
      "epoch 99, train_loss 1277.1268310546875\n",
      "epoch 99, val_loss 454.7897644042969\n",
      "Parameter containing:\n",
      "tensor([5.6095e-26], requires_grad=True)\n",
      "iter 139, train_loss_regularization 0.7038004398345947\n",
      "iter 139, val_loss_regularization 0.7038004398345947\n",
      "epoch 0, train_loss 1277.126708984375\n",
      "epoch 0, val_loss 454.7897033691406\n",
      "epoch 1, train_loss 1277.1265869140625\n",
      "epoch 1, val_loss 454.7897033691406\n",
      "epoch 2, train_loss 1277.1265869140625\n",
      "epoch 2, val_loss 454.7897033691406\n",
      "epoch 3, train_loss 1277.1265869140625\n",
      "epoch 3, val_loss 454.7896728515625\n",
      "epoch 4, train_loss 1277.1265869140625\n",
      "epoch 4, val_loss 454.78961181640625\n",
      "epoch 5, train_loss 1277.12646484375\n",
      "epoch 5, val_loss 454.7895812988281\n",
      "epoch 6, train_loss 1277.12646484375\n",
      "epoch 6, val_loss 454.78948974609375\n",
      "epoch 7, train_loss 1277.12646484375\n",
      "epoch 7, val_loss 454.7894592285156\n",
      "epoch 8, train_loss 1277.12646484375\n",
      "epoch 8, val_loss 454.7894287109375\n",
      "epoch 9, train_loss 1277.1263427734375\n",
      "epoch 9, val_loss 454.7894287109375\n",
      "epoch 10, train_loss 1277.126220703125\n",
      "epoch 10, val_loss 454.7893371582031\n",
      "epoch 11, train_loss 1277.126220703125\n",
      "epoch 11, val_loss 454.7893371582031\n",
      "epoch 12, train_loss 1277.126220703125\n",
      "epoch 12, val_loss 454.7892150878906\n",
      "epoch 13, train_loss 1277.1260986328125\n",
      "epoch 13, val_loss 454.78924560546875\n",
      "epoch 14, train_loss 1277.126220703125\n",
      "epoch 14, val_loss 454.7892150878906\n",
      "epoch 15, train_loss 1277.126220703125\n",
      "epoch 15, val_loss 454.7891540527344\n",
      "epoch 16, train_loss 1277.1260986328125\n",
      "epoch 16, val_loss 454.7890930175781\n",
      "epoch 17, train_loss 1277.1259765625\n",
      "epoch 17, val_loss 454.7890319824219\n",
      "epoch 18, train_loss 1277.1259765625\n",
      "epoch 18, val_loss 454.78900146484375\n",
      "epoch 19, train_loss 1277.1259765625\n",
      "epoch 19, val_loss 454.7889709472656\n",
      "epoch 20, train_loss 1277.1259765625\n",
      "epoch 20, val_loss 454.7889099121094\n",
      "epoch 21, train_loss 1277.1259765625\n",
      "epoch 21, val_loss 454.78887939453125\n",
      "epoch 22, train_loss 1277.1258544921875\n",
      "epoch 22, val_loss 454.78887939453125\n",
      "epoch 23, train_loss 1277.1259765625\n",
      "epoch 23, val_loss 454.78887939453125\n",
      "epoch 24, train_loss 1277.125732421875\n",
      "epoch 24, val_loss 454.7887878417969\n",
      "epoch 25, train_loss 1277.125732421875\n",
      "epoch 25, val_loss 454.78875732421875\n",
      "epoch 26, train_loss 1277.1258544921875\n",
      "epoch 26, val_loss 454.7886657714844\n",
      "epoch 27, train_loss 1277.1256103515625\n",
      "epoch 27, val_loss 454.78863525390625\n",
      "epoch 28, train_loss 1277.1256103515625\n",
      "epoch 28, val_loss 454.7885437011719\n",
      "epoch 29, train_loss 1277.1256103515625\n",
      "epoch 29, val_loss 454.78857421875\n",
      "epoch 30, train_loss 1277.1256103515625\n",
      "epoch 30, val_loss 454.7885437011719\n",
      "epoch 31, train_loss 1277.1256103515625\n",
      "epoch 31, val_loss 454.7884521484375\n",
      "epoch 32, train_loss 1277.1256103515625\n",
      "epoch 32, val_loss 454.7884521484375\n",
      "epoch 33, train_loss 1277.1256103515625\n",
      "epoch 33, val_loss 454.7884216308594\n",
      "epoch 34, train_loss 1277.1256103515625\n",
      "epoch 34, val_loss 454.7884216308594\n",
      "epoch 35, train_loss 1277.1253662109375\n",
      "epoch 35, val_loss 454.7882995605469\n",
      "epoch 36, train_loss 1277.125244140625\n",
      "epoch 36, val_loss 454.7882385253906\n",
      "epoch 37, train_loss 1277.1253662109375\n",
      "epoch 37, val_loss 454.7882385253906\n",
      "epoch 38, train_loss 1277.125244140625\n",
      "epoch 38, val_loss 454.7882080078125\n",
      "epoch 39, train_loss 1277.125244140625\n",
      "epoch 39, val_loss 454.7880859375\n",
      "epoch 40, train_loss 1277.1251220703125\n",
      "epoch 40, val_loss 454.7880859375\n",
      "epoch 41, train_loss 1277.1251220703125\n",
      "epoch 41, val_loss 454.7880859375\n",
      "epoch 42, train_loss 1277.125\n",
      "epoch 42, val_loss 454.7880554199219\n",
      "epoch 43, train_loss 1277.1251220703125\n",
      "epoch 43, val_loss 454.7879638671875\n",
      "epoch 44, train_loss 1277.1251220703125\n",
      "epoch 44, val_loss 454.78790283203125\n",
      "epoch 45, train_loss 1277.125\n",
      "epoch 45, val_loss 454.7879638671875\n",
      "epoch 46, train_loss 1277.1251220703125\n",
      "epoch 46, val_loss 454.787841796875\n",
      "epoch 47, train_loss 1277.125\n",
      "epoch 47, val_loss 454.78778076171875\n",
      "epoch 48, train_loss 1277.125\n",
      "epoch 48, val_loss 454.7877502441406\n",
      "epoch 49, train_loss 1277.1248779296875\n",
      "epoch 49, val_loss 454.7877502441406\n",
      "epoch 50, train_loss 1277.1246337890625\n",
      "epoch 50, val_loss 454.7877502441406\n",
      "epoch 51, train_loss 1277.124755859375\n",
      "epoch 51, val_loss 454.7876281738281\n",
      "epoch 52, train_loss 1277.1246337890625\n",
      "epoch 52, val_loss 454.78759765625\n",
      "epoch 53, train_loss 1277.1246337890625\n",
      "epoch 53, val_loss 454.78759765625\n",
      "epoch 54, train_loss 1277.1246337890625\n",
      "epoch 54, val_loss 454.7875061035156\n",
      "epoch 55, train_loss 1277.1246337890625\n",
      "epoch 55, val_loss 454.7875061035156\n",
      "epoch 56, train_loss 1277.1243896484375\n",
      "epoch 56, val_loss 454.7874450683594\n",
      "epoch 57, train_loss 1277.12451171875\n",
      "epoch 57, val_loss 454.78741455078125\n",
      "epoch 58, train_loss 1277.1243896484375\n",
      "epoch 58, val_loss 454.7873229980469\n",
      "epoch 59, train_loss 1277.1246337890625\n",
      "epoch 59, val_loss 454.7873229980469\n",
      "epoch 60, train_loss 1277.1246337890625\n",
      "epoch 60, val_loss 454.7872619628906\n",
      "epoch 61, train_loss 1277.12451171875\n",
      "epoch 61, val_loss 454.78729248046875\n",
      "epoch 62, train_loss 1277.1243896484375\n",
      "epoch 62, val_loss 454.7872009277344\n",
      "epoch 63, train_loss 1277.1241455078125\n",
      "epoch 63, val_loss 454.78717041015625\n",
      "epoch 64, train_loss 1277.124267578125\n",
      "epoch 64, val_loss 454.7871398925781\n",
      "epoch 65, train_loss 1277.1243896484375\n",
      "epoch 65, val_loss 454.7870788574219\n",
      "epoch 66, train_loss 1277.1241455078125\n",
      "epoch 66, val_loss 454.7869873046875\n",
      "epoch 67, train_loss 1277.124267578125\n",
      "epoch 67, val_loss 454.7869873046875\n",
      "epoch 68, train_loss 1277.1241455078125\n",
      "epoch 68, val_loss 454.7869567871094\n",
      "epoch 69, train_loss 1277.1241455078125\n",
      "epoch 69, val_loss 454.78692626953125\n",
      "epoch 70, train_loss 1277.1241455078125\n",
      "epoch 70, val_loss 454.786865234375\n",
      "epoch 71, train_loss 1277.1240234375\n",
      "epoch 71, val_loss 454.7868347167969\n",
      "epoch 72, train_loss 1277.1239013671875\n",
      "epoch 72, val_loss 454.78680419921875\n",
      "epoch 73, train_loss 1277.1239013671875\n",
      "epoch 73, val_loss 454.7867431640625\n",
      "epoch 74, train_loss 1277.1239013671875\n",
      "epoch 74, val_loss 454.7867126464844\n",
      "epoch 75, train_loss 1277.1239013671875\n",
      "epoch 75, val_loss 454.7867126464844\n",
      "epoch 76, train_loss 1277.1239013671875\n",
      "epoch 76, val_loss 454.78662109375\n",
      "epoch 77, train_loss 1277.1239013671875\n",
      "epoch 77, val_loss 454.7865295410156\n",
      "epoch 78, train_loss 1277.1236572265625\n",
      "epoch 78, val_loss 454.7864990234375\n",
      "epoch 79, train_loss 1277.1236572265625\n",
      "epoch 79, val_loss 454.7865295410156\n",
      "epoch 80, train_loss 1277.1236572265625\n",
      "epoch 80, val_loss 454.7864685058594\n",
      "epoch 81, train_loss 1277.1236572265625\n",
      "epoch 81, val_loss 454.7864074707031\n",
      "epoch 82, train_loss 1277.12353515625\n",
      "epoch 82, val_loss 454.786376953125\n",
      "epoch 83, train_loss 1277.12353515625\n",
      "epoch 83, val_loss 454.7863464355469\n",
      "epoch 84, train_loss 1277.12353515625\n",
      "epoch 84, val_loss 454.7862854003906\n",
      "epoch 85, train_loss 1277.12353515625\n",
      "epoch 85, val_loss 454.7862548828125\n",
      "epoch 86, train_loss 1277.1234130859375\n",
      "epoch 86, val_loss 454.78619384765625\n",
      "epoch 87, train_loss 1277.1234130859375\n",
      "epoch 87, val_loss 454.7861633300781\n",
      "epoch 88, train_loss 1277.123291015625\n",
      "epoch 88, val_loss 454.7861633300781\n",
      "epoch 89, train_loss 1277.123291015625\n",
      "epoch 89, val_loss 454.7860412597656\n",
      "epoch 90, train_loss 1277.123291015625\n",
      "epoch 90, val_loss 454.7860412597656\n",
      "epoch 91, train_loss 1277.123291015625\n",
      "epoch 91, val_loss 454.7860412597656\n",
      "epoch 92, train_loss 1277.1231689453125\n",
      "epoch 92, val_loss 454.7860412597656\n",
      "epoch 93, train_loss 1277.1231689453125\n",
      "epoch 93, val_loss 454.7859191894531\n",
      "epoch 94, train_loss 1277.123046875\n",
      "epoch 94, val_loss 454.7859191894531\n",
      "epoch 95, train_loss 1277.123046875\n",
      "epoch 95, val_loss 454.785888671875\n",
      "epoch 96, train_loss 1277.123046875\n",
      "epoch 96, val_loss 454.7857971191406\n",
      "epoch 97, train_loss 1277.123046875\n",
      "epoch 97, val_loss 454.78570556640625\n",
      "epoch 98, train_loss 1277.1229248046875\n",
      "epoch 98, val_loss 454.78570556640625\n",
      "epoch 99, train_loss 1277.1229248046875\n",
      "epoch 99, val_loss 454.7856750488281\n",
      "Parameter containing:\n",
      "tensor([3.9002e-26], requires_grad=True)\n",
      "iter 140, train_loss_regularization 0.7029961347579956\n",
      "iter 140, val_loss_regularization 0.7029961347579956\n",
      "epoch 0, train_loss 1277.1229248046875\n",
      "epoch 0, val_loss 454.7856750488281\n",
      "epoch 1, train_loss 1277.1226806640625\n",
      "epoch 1, val_loss 454.78558349609375\n",
      "epoch 2, train_loss 1277.1226806640625\n",
      "epoch 2, val_loss 454.78558349609375\n",
      "epoch 3, train_loss 1277.1226806640625\n",
      "epoch 3, val_loss 454.7855529785156\n",
      "epoch 4, train_loss 1277.122802734375\n",
      "epoch 4, val_loss 454.7854919433594\n",
      "epoch 5, train_loss 1277.1226806640625\n",
      "epoch 5, val_loss 454.78546142578125\n",
      "epoch 6, train_loss 1277.12255859375\n",
      "epoch 6, val_loss 454.7854309082031\n",
      "epoch 7, train_loss 1277.1226806640625\n",
      "epoch 7, val_loss 454.78533935546875\n",
      "epoch 8, train_loss 1277.12255859375\n",
      "epoch 8, val_loss 454.78533935546875\n",
      "epoch 9, train_loss 1277.12255859375\n",
      "epoch 9, val_loss 454.78521728515625\n",
      "epoch 10, train_loss 1277.1224365234375\n",
      "epoch 10, val_loss 454.78521728515625\n",
      "epoch 11, train_loss 1277.12255859375\n",
      "epoch 11, val_loss 454.78521728515625\n",
      "epoch 12, train_loss 1277.12255859375\n",
      "epoch 12, val_loss 454.7851257324219\n",
      "epoch 13, train_loss 1277.12255859375\n",
      "epoch 13, val_loss 454.78509521484375\n",
      "epoch 14, train_loss 1277.12255859375\n",
      "epoch 14, val_loss 454.78509521484375\n",
      "epoch 15, train_loss 1277.122314453125\n",
      "epoch 15, val_loss 454.7850036621094\n",
      "epoch 16, train_loss 1277.122314453125\n",
      "epoch 16, val_loss 454.7850036621094\n",
      "epoch 17, train_loss 1277.1221923828125\n",
      "epoch 17, val_loss 454.784912109375\n",
      "epoch 18, train_loss 1277.1221923828125\n",
      "epoch 18, val_loss 454.784912109375\n",
      "epoch 19, train_loss 1277.1221923828125\n",
      "epoch 19, val_loss 454.7848815917969\n",
      "epoch 20, train_loss 1277.1220703125\n",
      "epoch 20, val_loss 454.7847900390625\n",
      "epoch 21, train_loss 1277.1221923828125\n",
      "epoch 21, val_loss 454.7848205566406\n",
      "epoch 22, train_loss 1277.1219482421875\n",
      "epoch 22, val_loss 454.7847595214844\n",
      "epoch 23, train_loss 1277.1220703125\n",
      "epoch 23, val_loss 454.7846984863281\n",
      "epoch 24, train_loss 1277.1221923828125\n",
      "epoch 24, val_loss 454.7846374511719\n",
      "epoch 25, train_loss 1277.1220703125\n",
      "epoch 25, val_loss 454.7845458984375\n",
      "epoch 26, train_loss 1277.1219482421875\n",
      "epoch 26, val_loss 454.7845458984375\n",
      "epoch 27, train_loss 1277.121826171875\n",
      "epoch 27, val_loss 454.78448486328125\n",
      "epoch 28, train_loss 1277.1219482421875\n",
      "epoch 28, val_loss 454.7844543457031\n",
      "epoch 29, train_loss 1277.1217041015625\n",
      "epoch 29, val_loss 454.784423828125\n",
      "epoch 30, train_loss 1277.121826171875\n",
      "epoch 30, val_loss 454.784423828125\n",
      "epoch 31, train_loss 1277.1217041015625\n",
      "epoch 31, val_loss 454.784423828125\n",
      "epoch 32, train_loss 1277.1217041015625\n",
      "epoch 32, val_loss 454.7843322753906\n",
      "epoch 33, train_loss 1277.1214599609375\n",
      "epoch 33, val_loss 454.7843017578125\n",
      "epoch 34, train_loss 1277.12158203125\n",
      "epoch 34, val_loss 454.7843017578125\n",
      "epoch 35, train_loss 1277.1217041015625\n",
      "epoch 35, val_loss 454.7842102050781\n",
      "epoch 36, train_loss 1277.1214599609375\n",
      "epoch 36, val_loss 454.7841796875\n",
      "epoch 37, train_loss 1277.1214599609375\n",
      "epoch 37, val_loss 454.7840881347656\n",
      "epoch 38, train_loss 1277.1214599609375\n",
      "epoch 38, val_loss 454.7840881347656\n",
      "epoch 39, train_loss 1277.1214599609375\n",
      "epoch 39, val_loss 454.7840270996094\n",
      "epoch 40, train_loss 1277.121337890625\n",
      "epoch 40, val_loss 454.7839660644531\n",
      "epoch 41, train_loss 1277.1214599609375\n",
      "epoch 41, val_loss 454.78399658203125\n",
      "epoch 42, train_loss 1277.1212158203125\n",
      "epoch 42, val_loss 454.7839050292969\n",
      "epoch 43, train_loss 1277.1212158203125\n",
      "epoch 43, val_loss 454.7839050292969\n",
      "epoch 44, train_loss 1277.1212158203125\n",
      "epoch 44, val_loss 454.7838439941406\n",
      "epoch 45, train_loss 1277.1212158203125\n",
      "epoch 45, val_loss 454.7838439941406\n",
      "epoch 46, train_loss 1277.12109375\n",
      "epoch 46, val_loss 454.7837829589844\n",
      "epoch 47, train_loss 1277.1209716796875\n",
      "epoch 47, val_loss 454.78375244140625\n",
      "epoch 48, train_loss 1277.1209716796875\n",
      "epoch 48, val_loss 454.7836608886719\n",
      "epoch 49, train_loss 1277.1209716796875\n",
      "epoch 49, val_loss 454.78363037109375\n",
      "epoch 50, train_loss 1277.1209716796875\n",
      "epoch 50, val_loss 454.7835693359375\n",
      "epoch 51, train_loss 1277.1209716796875\n",
      "epoch 51, val_loss 454.7835388183594\n",
      "epoch 52, train_loss 1277.120849609375\n",
      "epoch 52, val_loss 454.78350830078125\n",
      "epoch 53, train_loss 1277.1209716796875\n",
      "epoch 53, val_loss 454.783447265625\n",
      "epoch 54, train_loss 1277.1209716796875\n",
      "epoch 54, val_loss 454.7834167480469\n",
      "epoch 55, train_loss 1277.1209716796875\n",
      "epoch 55, val_loss 454.78338623046875\n",
      "epoch 56, train_loss 1277.1209716796875\n",
      "epoch 56, val_loss 454.7833251953125\n",
      "epoch 57, train_loss 1277.120849609375\n",
      "epoch 57, val_loss 454.7832946777344\n",
      "epoch 58, train_loss 1277.1207275390625\n",
      "epoch 58, val_loss 454.7832946777344\n",
      "epoch 59, train_loss 1277.1207275390625\n",
      "epoch 59, val_loss 454.7832946777344\n",
      "epoch 60, train_loss 1277.1207275390625\n",
      "epoch 60, val_loss 454.783203125\n",
      "epoch 61, train_loss 1277.1207275390625\n",
      "epoch 61, val_loss 454.7831726074219\n",
      "epoch 62, train_loss 1277.12060546875\n",
      "epoch 62, val_loss 454.7831115722656\n",
      "epoch 63, train_loss 1277.1207275390625\n",
      "epoch 63, val_loss 454.7830810546875\n",
      "epoch 64, train_loss 1277.1204833984375\n",
      "epoch 64, val_loss 454.7830810546875\n",
      "epoch 65, train_loss 1277.1204833984375\n",
      "epoch 65, val_loss 454.7829895019531\n",
      "epoch 66, train_loss 1277.1204833984375\n",
      "epoch 66, val_loss 454.7829284667969\n",
      "epoch 67, train_loss 1277.1202392578125\n",
      "epoch 67, val_loss 454.7828674316406\n",
      "epoch 68, train_loss 1277.1201171875\n",
      "epoch 68, val_loss 454.7828369140625\n",
      "epoch 69, train_loss 1277.120361328125\n",
      "epoch 69, val_loss 454.7828063964844\n",
      "epoch 70, train_loss 1277.1202392578125\n",
      "epoch 70, val_loss 454.7828063964844\n",
      "epoch 71, train_loss 1277.1202392578125\n",
      "epoch 71, val_loss 454.78265380859375\n",
      "epoch 72, train_loss 1277.1201171875\n",
      "epoch 72, val_loss 454.78271484375\n",
      "epoch 73, train_loss 1277.1201171875\n",
      "epoch 73, val_loss 454.78265380859375\n",
      "epoch 74, train_loss 1277.1199951171875\n",
      "epoch 74, val_loss 454.78265380859375\n",
      "epoch 75, train_loss 1277.1199951171875\n",
      "epoch 75, val_loss 454.7825927734375\n",
      "epoch 76, train_loss 1277.1199951171875\n",
      "epoch 76, val_loss 454.78253173828125\n",
      "epoch 77, train_loss 1277.1199951171875\n",
      "epoch 77, val_loss 454.78253173828125\n",
      "epoch 78, train_loss 1277.1199951171875\n",
      "epoch 78, val_loss 454.782470703125\n",
      "epoch 79, train_loss 1277.119873046875\n",
      "epoch 79, val_loss 454.7823791503906\n",
      "epoch 80, train_loss 1277.119873046875\n",
      "epoch 80, val_loss 454.7823181152344\n",
      "epoch 81, train_loss 1277.1197509765625\n",
      "epoch 81, val_loss 454.7823181152344\n",
      "epoch 82, train_loss 1277.11962890625\n",
      "epoch 82, val_loss 454.7823181152344\n",
      "epoch 83, train_loss 1277.11962890625\n",
      "epoch 83, val_loss 454.7821960449219\n",
      "epoch 84, train_loss 1277.1197509765625\n",
      "epoch 84, val_loss 454.7821960449219\n",
      "epoch 85, train_loss 1277.1197509765625\n",
      "epoch 85, val_loss 454.78216552734375\n",
      "epoch 86, train_loss 1277.1197509765625\n",
      "epoch 86, val_loss 454.7820739746094\n",
      "epoch 87, train_loss 1277.1197509765625\n",
      "epoch 87, val_loss 454.78204345703125\n",
      "epoch 88, train_loss 1277.11962890625\n",
      "epoch 88, val_loss 454.7820739746094\n",
      "epoch 89, train_loss 1277.1195068359375\n",
      "epoch 89, val_loss 454.78204345703125\n",
      "epoch 90, train_loss 1277.1195068359375\n",
      "epoch 90, val_loss 454.7820129394531\n",
      "epoch 91, train_loss 1277.119384765625\n",
      "epoch 91, val_loss 454.78192138671875\n",
      "epoch 92, train_loss 1277.119384765625\n",
      "epoch 92, val_loss 454.78192138671875\n",
      "epoch 93, train_loss 1277.119384765625\n",
      "epoch 93, val_loss 454.7818603515625\n",
      "epoch 94, train_loss 1277.1195068359375\n",
      "epoch 94, val_loss 454.78179931640625\n",
      "epoch 95, train_loss 1277.119384765625\n",
      "epoch 95, val_loss 454.78173828125\n",
      "epoch 96, train_loss 1277.119140625\n",
      "epoch 96, val_loss 454.78167724609375\n",
      "epoch 97, train_loss 1277.119140625\n",
      "epoch 97, val_loss 454.7817077636719\n",
      "epoch 98, train_loss 1277.119140625\n",
      "epoch 98, val_loss 454.78167724609375\n",
      "epoch 99, train_loss 1277.119140625\n",
      "epoch 99, val_loss 454.7815856933594\n",
      "Parameter containing:\n",
      "tensor([2.7124e-26], requires_grad=True)\n",
      "iter 141, train_loss_regularization 0.7022125124931335\n",
      "iter 141, val_loss_regularization 0.7022125124931335\n",
      "epoch 0, train_loss 1277.119140625\n",
      "epoch 0, val_loss 454.7815856933594\n",
      "epoch 1, train_loss 1277.1192626953125\n",
      "epoch 1, val_loss 454.781494140625\n",
      "epoch 2, train_loss 1277.119140625\n",
      "epoch 2, val_loss 454.781494140625\n",
      "epoch 3, train_loss 1277.1190185546875\n",
      "epoch 3, val_loss 454.7814636230469\n",
      "epoch 4, train_loss 1277.119140625\n",
      "epoch 4, val_loss 454.7814636230469\n",
      "epoch 5, train_loss 1277.1190185546875\n",
      "epoch 5, val_loss 454.7813415527344\n",
      "epoch 6, train_loss 1277.119140625\n",
      "epoch 6, val_loss 454.7812805175781\n",
      "epoch 7, train_loss 1277.118896484375\n",
      "epoch 7, val_loss 454.78125\n",
      "epoch 8, train_loss 1277.1190185546875\n",
      "epoch 8, val_loss 454.7812194824219\n",
      "epoch 9, train_loss 1277.118896484375\n",
      "epoch 9, val_loss 454.7811584472656\n",
      "epoch 10, train_loss 1277.11865234375\n",
      "epoch 10, val_loss 454.7810974121094\n",
      "epoch 11, train_loss 1277.11865234375\n",
      "epoch 11, val_loss 454.7811279296875\n",
      "epoch 12, train_loss 1277.11865234375\n",
      "epoch 12, val_loss 454.7810974121094\n",
      "epoch 13, train_loss 1277.1185302734375\n",
      "epoch 13, val_loss 454.7810974121094\n",
      "epoch 14, train_loss 1277.1185302734375\n",
      "epoch 14, val_loss 454.781005859375\n",
      "epoch 15, train_loss 1277.1185302734375\n",
      "epoch 15, val_loss 454.78094482421875\n",
      "epoch 16, train_loss 1277.1185302734375\n",
      "epoch 16, val_loss 454.7809143066406\n",
      "epoch 17, train_loss 1277.1185302734375\n",
      "epoch 17, val_loss 454.7808837890625\n",
      "epoch 18, train_loss 1277.1185302734375\n",
      "epoch 18, val_loss 454.78082275390625\n",
      "epoch 19, train_loss 1277.118408203125\n",
      "epoch 19, val_loss 454.7807922363281\n",
      "epoch 20, train_loss 1277.118408203125\n",
      "epoch 20, val_loss 454.7807922363281\n",
      "epoch 21, train_loss 1277.1182861328125\n",
      "epoch 21, val_loss 454.78076171875\n",
      "epoch 22, train_loss 1277.1182861328125\n",
      "epoch 22, val_loss 454.7806701660156\n",
      "epoch 23, train_loss 1277.1182861328125\n",
      "epoch 23, val_loss 454.7806396484375\n",
      "epoch 24, train_loss 1277.1182861328125\n",
      "epoch 24, val_loss 454.78057861328125\n",
      "epoch 25, train_loss 1277.1181640625\n",
      "epoch 25, val_loss 454.78057861328125\n",
      "epoch 26, train_loss 1277.1181640625\n",
      "epoch 26, val_loss 454.7804870605469\n",
      "epoch 27, train_loss 1277.1180419921875\n",
      "epoch 27, val_loss 454.7804260253906\n",
      "epoch 28, train_loss 1277.1180419921875\n",
      "epoch 28, val_loss 454.7804260253906\n",
      "epoch 29, train_loss 1277.117919921875\n",
      "epoch 29, val_loss 454.78045654296875\n",
      "epoch 30, train_loss 1277.1180419921875\n",
      "epoch 30, val_loss 454.78033447265625\n",
      "epoch 31, train_loss 1277.1180419921875\n",
      "epoch 31, val_loss 454.7803039550781\n",
      "epoch 32, train_loss 1277.1180419921875\n",
      "epoch 32, val_loss 454.7803039550781\n",
      "epoch 33, train_loss 1277.117919921875\n",
      "epoch 33, val_loss 454.7803039550781\n",
      "epoch 34, train_loss 1277.1177978515625\n",
      "epoch 34, val_loss 454.78021240234375\n",
      "epoch 35, train_loss 1277.1177978515625\n",
      "epoch 35, val_loss 454.7801818847656\n",
      "epoch 36, train_loss 1277.1177978515625\n",
      "epoch 36, val_loss 454.78009033203125\n",
      "epoch 37, train_loss 1277.1177978515625\n",
      "epoch 37, val_loss 454.780029296875\n",
      "epoch 38, train_loss 1277.11767578125\n",
      "epoch 38, val_loss 454.7799987792969\n",
      "epoch 39, train_loss 1277.1175537109375\n",
      "epoch 39, val_loss 454.77996826171875\n",
      "epoch 40, train_loss 1277.11767578125\n",
      "epoch 40, val_loss 454.77996826171875\n",
      "epoch 41, train_loss 1277.1175537109375\n",
      "epoch 41, val_loss 454.77996826171875\n",
      "epoch 42, train_loss 1277.1175537109375\n",
      "epoch 42, val_loss 454.77984619140625\n",
      "epoch 43, train_loss 1277.117431640625\n",
      "epoch 43, val_loss 454.77984619140625\n",
      "epoch 44, train_loss 1277.11767578125\n",
      "epoch 44, val_loss 454.77984619140625\n",
      "epoch 45, train_loss 1277.117431640625\n",
      "epoch 45, val_loss 454.7797546386719\n",
      "epoch 46, train_loss 1277.1175537109375\n",
      "epoch 46, val_loss 454.7796630859375\n",
      "epoch 47, train_loss 1277.1173095703125\n",
      "epoch 47, val_loss 454.7796630859375\n",
      "epoch 48, train_loss 1277.1173095703125\n",
      "epoch 48, val_loss 454.7796325683594\n",
      "epoch 49, train_loss 1277.1175537109375\n",
      "epoch 49, val_loss 454.7796325683594\n",
      "epoch 50, train_loss 1277.117431640625\n",
      "epoch 50, val_loss 454.7795104980469\n",
      "epoch 51, train_loss 1277.1173095703125\n",
      "epoch 51, val_loss 454.7795104980469\n",
      "epoch 52, train_loss 1277.1171875\n",
      "epoch 52, val_loss 454.7794189453125\n",
      "epoch 53, train_loss 1277.1171875\n",
      "epoch 53, val_loss 454.7794494628906\n",
      "epoch 54, train_loss 1277.1173095703125\n",
      "epoch 54, val_loss 454.7793884277344\n",
      "epoch 55, train_loss 1277.1171875\n",
      "epoch 55, val_loss 454.7793273925781\n",
      "epoch 56, train_loss 1277.116943359375\n",
      "epoch 56, val_loss 454.779296875\n",
      "epoch 57, train_loss 1277.116943359375\n",
      "epoch 57, val_loss 454.77923583984375\n",
      "epoch 58, train_loss 1277.116943359375\n",
      "epoch 58, val_loss 454.7792053222656\n",
      "epoch 59, train_loss 1277.116943359375\n",
      "epoch 59, val_loss 454.7792053222656\n",
      "epoch 60, train_loss 1277.116943359375\n",
      "epoch 60, val_loss 454.7791748046875\n",
      "epoch 61, train_loss 1277.11669921875\n",
      "epoch 61, val_loss 454.7791748046875\n",
      "epoch 62, train_loss 1277.1168212890625\n",
      "epoch 62, val_loss 454.779052734375\n",
      "epoch 63, train_loss 1277.11669921875\n",
      "epoch 63, val_loss 454.77899169921875\n",
      "epoch 64, train_loss 1277.1168212890625\n",
      "epoch 64, val_loss 454.77899169921875\n",
      "epoch 65, train_loss 1277.1168212890625\n",
      "epoch 65, val_loss 454.7789611816406\n",
      "epoch 66, train_loss 1277.11669921875\n",
      "epoch 66, val_loss 454.77886962890625\n",
      "epoch 67, train_loss 1277.1165771484375\n",
      "epoch 67, val_loss 454.7788391113281\n",
      "epoch 68, train_loss 1277.1165771484375\n",
      "epoch 68, val_loss 454.7787780761719\n",
      "epoch 69, train_loss 1277.1165771484375\n",
      "epoch 69, val_loss 454.77874755859375\n",
      "epoch 70, train_loss 1277.116455078125\n",
      "epoch 70, val_loss 454.77874755859375\n",
      "epoch 71, train_loss 1277.116455078125\n",
      "epoch 71, val_loss 454.7786560058594\n",
      "epoch 72, train_loss 1277.116455078125\n",
      "epoch 72, val_loss 454.7786560058594\n",
      "epoch 73, train_loss 1277.116455078125\n",
      "epoch 73, val_loss 454.7785949707031\n",
      "epoch 74, train_loss 1277.116455078125\n",
      "epoch 74, val_loss 454.7785949707031\n",
      "epoch 75, train_loss 1277.116455078125\n",
      "epoch 75, val_loss 454.7785339355469\n",
      "epoch 76, train_loss 1277.1163330078125\n",
      "epoch 76, val_loss 454.7785339355469\n",
      "epoch 77, train_loss 1277.1162109375\n",
      "epoch 77, val_loss 454.7784118652344\n",
      "epoch 78, train_loss 1277.1162109375\n",
      "epoch 78, val_loss 454.7784118652344\n",
      "epoch 79, train_loss 1277.1162109375\n",
      "epoch 79, val_loss 454.7784118652344\n",
      "epoch 80, train_loss 1277.1162109375\n",
      "epoch 80, val_loss 454.7782897949219\n",
      "epoch 81, train_loss 1277.1162109375\n",
      "epoch 81, val_loss 454.77825927734375\n",
      "epoch 82, train_loss 1277.1160888671875\n",
      "epoch 82, val_loss 454.7782897949219\n",
      "epoch 83, train_loss 1277.115966796875\n",
      "epoch 83, val_loss 454.77825927734375\n",
      "epoch 84, train_loss 1277.1160888671875\n",
      "epoch 84, val_loss 454.7781982421875\n",
      "epoch 85, train_loss 1277.115966796875\n",
      "epoch 85, val_loss 454.77813720703125\n",
      "epoch 86, train_loss 1277.115966796875\n",
      "epoch 86, val_loss 454.778076171875\n",
      "epoch 87, train_loss 1277.115966796875\n",
      "epoch 87, val_loss 454.7780456542969\n",
      "epoch 88, train_loss 1277.1158447265625\n",
      "epoch 88, val_loss 454.7780456542969\n",
      "epoch 89, train_loss 1277.1158447265625\n",
      "epoch 89, val_loss 454.7779235839844\n",
      "epoch 90, train_loss 1277.1158447265625\n",
      "epoch 90, val_loss 454.7779541015625\n",
      "epoch 91, train_loss 1277.1158447265625\n",
      "epoch 91, val_loss 454.77783203125\n",
      "epoch 92, train_loss 1277.11572265625\n",
      "epoch 92, val_loss 454.77783203125\n",
      "epoch 93, train_loss 1277.11572265625\n",
      "epoch 93, val_loss 454.77783203125\n",
      "epoch 94, train_loss 1277.1156005859375\n",
      "epoch 94, val_loss 454.7778015136719\n",
      "epoch 95, train_loss 1277.11572265625\n",
      "epoch 95, val_loss 454.7777404785156\n",
      "epoch 96, train_loss 1277.11572265625\n",
      "epoch 96, val_loss 454.7777099609375\n",
      "epoch 97, train_loss 1277.11572265625\n",
      "epoch 97, val_loss 454.7776184082031\n",
      "epoch 98, train_loss 1277.11572265625\n",
      "epoch 98, val_loss 454.777587890625\n",
      "epoch 99, train_loss 1277.115478515625\n",
      "epoch 99, val_loss 454.77752685546875\n",
      "Parameter containing:\n",
      "tensor([1.8869e-26], requires_grad=True)\n",
      "iter 142, train_loss_regularization 0.7014492154121399\n",
      "iter 142, val_loss_regularization 0.7014492154121399\n",
      "epoch 0, train_loss 1277.115478515625\n",
      "epoch 0, val_loss 454.77752685546875\n",
      "epoch 1, train_loss 1277.1153564453125\n",
      "epoch 1, val_loss 454.7774658203125\n",
      "epoch 2, train_loss 1277.1153564453125\n",
      "epoch 2, val_loss 454.77740478515625\n",
      "epoch 3, train_loss 1277.1153564453125\n",
      "epoch 3, val_loss 454.77740478515625\n",
      "epoch 4, train_loss 1277.1153564453125\n",
      "epoch 4, val_loss 454.77740478515625\n",
      "epoch 5, train_loss 1277.115234375\n",
      "epoch 5, val_loss 454.7773742675781\n",
      "epoch 6, train_loss 1277.1151123046875\n",
      "epoch 6, val_loss 454.7773742675781\n",
      "epoch 7, train_loss 1277.1151123046875\n",
      "epoch 7, val_loss 454.7772521972656\n",
      "epoch 8, train_loss 1277.1151123046875\n",
      "epoch 8, val_loss 454.7772216796875\n",
      "epoch 9, train_loss 1277.114990234375\n",
      "epoch 9, val_loss 454.7771301269531\n",
      "epoch 10, train_loss 1277.114990234375\n",
      "epoch 10, val_loss 454.7770690917969\n",
      "epoch 11, train_loss 1277.1148681640625\n",
      "epoch 11, val_loss 454.7771301269531\n",
      "epoch 12, train_loss 1277.1151123046875\n",
      "epoch 12, val_loss 454.7770690917969\n",
      "epoch 13, train_loss 1277.1151123046875\n",
      "epoch 13, val_loss 454.7769470214844\n",
      "epoch 14, train_loss 1277.114990234375\n",
      "epoch 14, val_loss 454.7769470214844\n",
      "epoch 15, train_loss 1277.114990234375\n",
      "epoch 15, val_loss 454.7769470214844\n",
      "epoch 16, train_loss 1277.11474609375\n",
      "epoch 16, val_loss 454.77691650390625\n",
      "epoch 17, train_loss 1277.11474609375\n",
      "epoch 17, val_loss 454.77679443359375\n",
      "epoch 18, train_loss 1277.11474609375\n",
      "epoch 18, val_loss 454.77679443359375\n",
      "epoch 19, train_loss 1277.11474609375\n",
      "epoch 19, val_loss 454.77679443359375\n",
      "epoch 20, train_loss 1277.11474609375\n",
      "epoch 20, val_loss 454.77679443359375\n",
      "epoch 21, train_loss 1277.11474609375\n",
      "epoch 21, val_loss 454.77667236328125\n",
      "epoch 22, train_loss 1277.1146240234375\n",
      "epoch 22, val_loss 454.776611328125\n",
      "epoch 23, train_loss 1277.1143798828125\n",
      "epoch 23, val_loss 454.7765808105469\n",
      "epoch 24, train_loss 1277.1143798828125\n",
      "epoch 24, val_loss 454.776611328125\n",
      "epoch 25, train_loss 1277.114501953125\n",
      "epoch 25, val_loss 454.7764892578125\n",
      "epoch 26, train_loss 1277.114501953125\n",
      "epoch 26, val_loss 454.7764587402344\n",
      "epoch 27, train_loss 1277.114501953125\n",
      "epoch 27, val_loss 454.7764587402344\n",
      "epoch 28, train_loss 1277.114501953125\n",
      "epoch 28, val_loss 454.7763671875\n",
      "epoch 29, train_loss 1277.114501953125\n",
      "epoch 29, val_loss 454.7763366699219\n",
      "epoch 30, train_loss 1277.1143798828125\n",
      "epoch 30, val_loss 454.77630615234375\n",
      "epoch 31, train_loss 1277.1142578125\n",
      "epoch 31, val_loss 454.7763366699219\n",
      "epoch 32, train_loss 1277.1141357421875\n",
      "epoch 32, val_loss 454.77630615234375\n",
      "epoch 33, train_loss 1277.1141357421875\n",
      "epoch 33, val_loss 454.7762145996094\n",
      "epoch 34, train_loss 1277.1141357421875\n",
      "epoch 34, val_loss 454.7762145996094\n",
      "epoch 35, train_loss 1277.1142578125\n",
      "epoch 35, val_loss 454.776123046875\n",
      "epoch 36, train_loss 1277.1142578125\n",
      "epoch 36, val_loss 454.776123046875\n",
      "epoch 37, train_loss 1277.1141357421875\n",
      "epoch 37, val_loss 454.7760009765625\n",
      "epoch 38, train_loss 1277.1138916015625\n",
      "epoch 38, val_loss 454.7759704589844\n",
      "epoch 39, train_loss 1277.1138916015625\n",
      "epoch 39, val_loss 454.7759704589844\n",
      "epoch 40, train_loss 1277.1138916015625\n",
      "epoch 40, val_loss 454.7759094238281\n",
      "epoch 41, train_loss 1277.1141357421875\n",
      "epoch 41, val_loss 454.77587890625\n",
      "epoch 42, train_loss 1277.114013671875\n",
      "epoch 42, val_loss 454.77587890625\n",
      "epoch 43, train_loss 1277.1138916015625\n",
      "epoch 43, val_loss 454.7757873535156\n",
      "epoch 44, train_loss 1277.1138916015625\n",
      "epoch 44, val_loss 454.7757873535156\n",
      "epoch 45, train_loss 1277.11376953125\n",
      "epoch 45, val_loss 454.7757568359375\n",
      "epoch 46, train_loss 1277.1136474609375\n",
      "epoch 46, val_loss 454.7756652832031\n",
      "epoch 47, train_loss 1277.1136474609375\n",
      "epoch 47, val_loss 454.775634765625\n",
      "epoch 48, train_loss 1277.1136474609375\n",
      "epoch 48, val_loss 454.775634765625\n",
      "epoch 49, train_loss 1277.1136474609375\n",
      "epoch 49, val_loss 454.77557373046875\n",
      "epoch 50, train_loss 1277.113525390625\n",
      "epoch 50, val_loss 454.77557373046875\n",
      "epoch 51, train_loss 1277.1136474609375\n",
      "epoch 51, val_loss 454.77545166015625\n",
      "epoch 52, train_loss 1277.113525390625\n",
      "epoch 52, val_loss 454.77545166015625\n",
      "epoch 53, train_loss 1277.113525390625\n",
      "epoch 53, val_loss 454.7754211425781\n",
      "epoch 54, train_loss 1277.11328125\n",
      "epoch 54, val_loss 454.7753601074219\n",
      "epoch 55, train_loss 1277.11328125\n",
      "epoch 55, val_loss 454.7752990722656\n",
      "epoch 56, train_loss 1277.11328125\n",
      "epoch 56, val_loss 454.7753601074219\n",
      "epoch 57, train_loss 1277.11328125\n",
      "epoch 57, val_loss 454.77520751953125\n",
      "epoch 58, train_loss 1277.1134033203125\n",
      "epoch 58, val_loss 454.77520751953125\n",
      "epoch 59, train_loss 1277.11328125\n",
      "epoch 59, val_loss 454.7751770019531\n",
      "epoch 60, train_loss 1277.1131591796875\n",
      "epoch 60, val_loss 454.7751770019531\n",
      "epoch 61, train_loss 1277.1131591796875\n",
      "epoch 61, val_loss 454.7751159667969\n",
      "epoch 62, train_loss 1277.1131591796875\n",
      "epoch 62, val_loss 454.7750549316406\n",
      "epoch 63, train_loss 1277.1131591796875\n",
      "epoch 63, val_loss 454.7750549316406\n",
      "epoch 64, train_loss 1277.113037109375\n",
      "epoch 64, val_loss 454.77490234375\n",
      "epoch 65, train_loss 1277.113037109375\n",
      "epoch 65, val_loss 454.77496337890625\n",
      "epoch 66, train_loss 1277.113037109375\n",
      "epoch 66, val_loss 454.7748718261719\n",
      "epoch 67, train_loss 1277.113037109375\n",
      "epoch 67, val_loss 454.77484130859375\n",
      "epoch 68, train_loss 1277.11279296875\n",
      "epoch 68, val_loss 454.7747802734375\n",
      "epoch 69, train_loss 1277.11279296875\n",
      "epoch 69, val_loss 454.7747497558594\n",
      "epoch 70, train_loss 1277.1126708984375\n",
      "epoch 70, val_loss 454.77471923828125\n",
      "epoch 71, train_loss 1277.11279296875\n",
      "epoch 71, val_loss 454.7747497558594\n",
      "epoch 72, train_loss 1277.11279296875\n",
      "epoch 72, val_loss 454.774658203125\n",
      "epoch 73, train_loss 1277.1126708984375\n",
      "epoch 73, val_loss 454.7746276855469\n",
      "epoch 74, train_loss 1277.1126708984375\n",
      "epoch 74, val_loss 454.77459716796875\n",
      "epoch 75, train_loss 1277.1126708984375\n",
      "epoch 75, val_loss 454.7745361328125\n",
      "epoch 76, train_loss 1277.1126708984375\n",
      "epoch 76, val_loss 454.7745361328125\n",
      "epoch 77, train_loss 1277.1126708984375\n",
      "epoch 77, val_loss 454.7745056152344\n",
      "epoch 78, train_loss 1277.1126708984375\n",
      "epoch 78, val_loss 454.7743835449219\n",
      "epoch 79, train_loss 1277.112548828125\n",
      "epoch 79, val_loss 454.7743835449219\n",
      "epoch 80, train_loss 1277.112548828125\n",
      "epoch 80, val_loss 454.7742919921875\n",
      "epoch 81, train_loss 1277.1124267578125\n",
      "epoch 81, val_loss 454.7742614746094\n",
      "epoch 82, train_loss 1277.1123046875\n",
      "epoch 82, val_loss 454.7742614746094\n",
      "epoch 83, train_loss 1277.1124267578125\n",
      "epoch 83, val_loss 454.774169921875\n",
      "epoch 84, train_loss 1277.1123046875\n",
      "epoch 84, val_loss 454.7742004394531\n",
      "epoch 85, train_loss 1277.1123046875\n",
      "epoch 85, val_loss 454.7741394042969\n",
      "epoch 86, train_loss 1277.1123046875\n",
      "epoch 86, val_loss 454.7741394042969\n",
      "epoch 87, train_loss 1277.1123046875\n",
      "epoch 87, val_loss 454.7740478515625\n",
      "epoch 88, train_loss 1277.1121826171875\n",
      "epoch 88, val_loss 454.7740478515625\n",
      "epoch 89, train_loss 1277.1121826171875\n",
      "epoch 89, val_loss 454.77398681640625\n",
      "epoch 90, train_loss 1277.1121826171875\n",
      "epoch 90, val_loss 454.7739562988281\n",
      "epoch 91, train_loss 1277.1121826171875\n",
      "epoch 91, val_loss 454.77392578125\n",
      "epoch 92, train_loss 1277.1121826171875\n",
      "epoch 92, val_loss 454.77392578125\n",
      "epoch 93, train_loss 1277.112060546875\n",
      "epoch 93, val_loss 454.77374267578125\n",
      "epoch 94, train_loss 1277.112060546875\n",
      "epoch 94, val_loss 454.77374267578125\n",
      "epoch 95, train_loss 1277.112060546875\n",
      "epoch 95, val_loss 454.77374267578125\n",
      "epoch 96, train_loss 1277.1119384765625\n",
      "epoch 96, val_loss 454.7737121582031\n",
      "epoch 97, train_loss 1277.11181640625\n",
      "epoch 97, val_loss 454.773681640625\n",
      "epoch 98, train_loss 1277.11181640625\n",
      "epoch 98, val_loss 454.7735900878906\n",
      "epoch 99, train_loss 1277.1116943359375\n",
      "epoch 99, val_loss 454.7735290527344\n",
      "Parameter containing:\n",
      "tensor([1.3129e-26], requires_grad=True)\n",
      "iter 143, train_loss_regularization 0.7007061839103699\n",
      "iter 143, val_loss_regularization 0.7007061839103699\n",
      "epoch 0, train_loss 1277.1116943359375\n",
      "epoch 0, val_loss 454.77349853515625\n",
      "epoch 1, train_loss 1277.1116943359375\n",
      "epoch 1, val_loss 454.7734680175781\n",
      "epoch 2, train_loss 1277.111572265625\n",
      "epoch 2, val_loss 454.7734680175781\n",
      "epoch 3, train_loss 1277.1116943359375\n",
      "epoch 3, val_loss 454.7734069824219\n",
      "epoch 4, train_loss 1277.1114501953125\n",
      "epoch 4, val_loss 454.7733459472656\n",
      "epoch 5, train_loss 1277.1114501953125\n",
      "epoch 5, val_loss 454.7733459472656\n",
      "epoch 6, train_loss 1277.111572265625\n",
      "epoch 6, val_loss 454.7733459472656\n",
      "epoch 7, train_loss 1277.111572265625\n",
      "epoch 7, val_loss 454.7733459472656\n",
      "epoch 8, train_loss 1277.1116943359375\n",
      "epoch 8, val_loss 454.7731628417969\n",
      "epoch 9, train_loss 1277.1114501953125\n",
      "epoch 9, val_loss 454.77313232421875\n",
      "epoch 10, train_loss 1277.111328125\n",
      "epoch 10, val_loss 454.7731628417969\n",
      "epoch 11, train_loss 1277.1112060546875\n",
      "epoch 11, val_loss 454.7730712890625\n",
      "epoch 12, train_loss 1277.111328125\n",
      "epoch 12, val_loss 454.77301025390625\n",
      "epoch 13, train_loss 1277.1112060546875\n",
      "epoch 13, val_loss 454.7730407714844\n",
      "epoch 14, train_loss 1277.1112060546875\n",
      "epoch 14, val_loss 454.77301025390625\n",
      "epoch 15, train_loss 1277.1112060546875\n",
      "epoch 15, val_loss 454.77294921875\n",
      "epoch 16, train_loss 1277.1112060546875\n",
      "epoch 16, val_loss 454.7729187011719\n",
      "epoch 17, train_loss 1277.1112060546875\n",
      "epoch 17, val_loss 454.77288818359375\n",
      "epoch 18, train_loss 1277.111083984375\n",
      "epoch 18, val_loss 454.7728271484375\n",
      "epoch 19, train_loss 1277.111083984375\n",
      "epoch 19, val_loss 454.7727966308594\n",
      "epoch 20, train_loss 1277.1112060546875\n",
      "epoch 20, val_loss 454.772705078125\n",
      "epoch 21, train_loss 1277.1109619140625\n",
      "epoch 21, val_loss 454.7726745605469\n",
      "epoch 22, train_loss 1277.111083984375\n",
      "epoch 22, val_loss 454.7726745605469\n",
      "epoch 23, train_loss 1277.111083984375\n",
      "epoch 23, val_loss 454.7726135253906\n",
      "epoch 24, train_loss 1277.1109619140625\n",
      "epoch 24, val_loss 454.7725830078125\n",
      "epoch 25, train_loss 1277.1109619140625\n",
      "epoch 25, val_loss 454.7725524902344\n",
      "epoch 26, train_loss 1277.11083984375\n",
      "epoch 26, val_loss 454.7725524902344\n",
      "epoch 27, train_loss 1277.11083984375\n",
      "epoch 27, val_loss 454.7724914550781\n",
      "epoch 28, train_loss 1277.1107177734375\n",
      "epoch 28, val_loss 454.7724304199219\n",
      "epoch 29, train_loss 1277.1107177734375\n",
      "epoch 29, val_loss 454.7723693847656\n",
      "epoch 30, train_loss 1277.1107177734375\n",
      "epoch 30, val_loss 454.7723388671875\n",
      "epoch 31, train_loss 1277.1107177734375\n",
      "epoch 31, val_loss 454.7723388671875\n",
      "epoch 32, train_loss 1277.110595703125\n",
      "epoch 32, val_loss 454.77227783203125\n",
      "epoch 33, train_loss 1277.110595703125\n",
      "epoch 33, val_loss 454.7722473144531\n",
      "epoch 34, train_loss 1277.1104736328125\n",
      "epoch 34, val_loss 454.772216796875\n",
      "epoch 35, train_loss 1277.1104736328125\n",
      "epoch 35, val_loss 454.77215576171875\n",
      "epoch 36, train_loss 1277.110595703125\n",
      "epoch 36, val_loss 454.7720947265625\n",
      "epoch 37, train_loss 1277.1104736328125\n",
      "epoch 37, val_loss 454.7720947265625\n",
      "epoch 38, train_loss 1277.1104736328125\n",
      "epoch 38, val_loss 454.7720031738281\n",
      "epoch 39, train_loss 1277.1103515625\n",
      "epoch 39, val_loss 454.77203369140625\n",
      "epoch 40, train_loss 1277.1103515625\n",
      "epoch 40, val_loss 454.77197265625\n",
      "epoch 41, train_loss 1277.1103515625\n",
      "epoch 41, val_loss 454.7718811035156\n",
      "epoch 42, train_loss 1277.1103515625\n",
      "epoch 42, val_loss 454.7718811035156\n",
      "epoch 43, train_loss 1277.110107421875\n",
      "epoch 43, val_loss 454.7718811035156\n",
      "epoch 44, train_loss 1277.110107421875\n",
      "epoch 44, val_loss 454.77178955078125\n",
      "epoch 45, train_loss 1277.110107421875\n",
      "epoch 45, val_loss 454.7717590332031\n",
      "epoch 46, train_loss 1277.110107421875\n",
      "epoch 46, val_loss 454.7717590332031\n",
      "epoch 47, train_loss 1277.110107421875\n",
      "epoch 47, val_loss 454.7716979980469\n",
      "epoch 48, train_loss 1277.110107421875\n",
      "epoch 48, val_loss 454.7716979980469\n",
      "epoch 49, train_loss 1277.110107421875\n",
      "epoch 49, val_loss 454.7716369628906\n",
      "epoch 50, train_loss 1277.10986328125\n",
      "epoch 50, val_loss 454.7716369628906\n",
      "epoch 51, train_loss 1277.10986328125\n",
      "epoch 51, val_loss 454.7715148925781\n",
      "epoch 52, train_loss 1277.10986328125\n",
      "epoch 52, val_loss 454.7715148925781\n",
      "epoch 53, train_loss 1277.10986328125\n",
      "epoch 53, val_loss 454.7714538574219\n",
      "epoch 54, train_loss 1277.1097412109375\n",
      "epoch 54, val_loss 454.7713623046875\n",
      "epoch 55, train_loss 1277.1097412109375\n",
      "epoch 55, val_loss 454.7713623046875\n",
      "epoch 56, train_loss 1277.1097412109375\n",
      "epoch 56, val_loss 454.7713623046875\n",
      "epoch 57, train_loss 1277.10986328125\n",
      "epoch 57, val_loss 454.77130126953125\n",
      "epoch 58, train_loss 1277.10986328125\n",
      "epoch 58, val_loss 454.771240234375\n",
      "epoch 59, train_loss 1277.109619140625\n",
      "epoch 59, val_loss 454.77117919921875\n",
      "epoch 60, train_loss 1277.109619140625\n",
      "epoch 60, val_loss 454.7712097167969\n",
      "epoch 61, train_loss 1277.1094970703125\n",
      "epoch 61, val_loss 454.7710876464844\n",
      "epoch 62, train_loss 1277.109619140625\n",
      "epoch 62, val_loss 454.7710876464844\n",
      "epoch 63, train_loss 1277.109619140625\n",
      "epoch 63, val_loss 454.7710266113281\n",
      "epoch 64, train_loss 1277.109619140625\n",
      "epoch 64, val_loss 454.7710876464844\n",
      "epoch 65, train_loss 1277.109619140625\n",
      "epoch 65, val_loss 454.7709045410156\n",
      "epoch 66, train_loss 1277.109375\n",
      "epoch 66, val_loss 454.7709045410156\n",
      "epoch 67, train_loss 1277.109375\n",
      "epoch 67, val_loss 454.7709045410156\n",
      "epoch 68, train_loss 1277.109375\n",
      "epoch 68, val_loss 454.7708740234375\n",
      "epoch 69, train_loss 1277.109375\n",
      "epoch 69, val_loss 454.7707824707031\n",
      "epoch 70, train_loss 1277.109375\n",
      "epoch 70, val_loss 454.770751953125\n",
      "epoch 71, train_loss 1277.1092529296875\n",
      "epoch 71, val_loss 454.7707214355469\n",
      "epoch 72, train_loss 1277.1092529296875\n",
      "epoch 72, val_loss 454.7706604003906\n",
      "epoch 73, train_loss 1277.109375\n",
      "epoch 73, val_loss 454.7706298828125\n",
      "epoch 74, train_loss 1277.109130859375\n",
      "epoch 74, val_loss 454.77056884765625\n",
      "epoch 75, train_loss 1277.109130859375\n",
      "epoch 75, val_loss 454.77056884765625\n",
      "epoch 76, train_loss 1277.1090087890625\n",
      "epoch 76, val_loss 454.77056884765625\n",
      "epoch 77, train_loss 1277.109130859375\n",
      "epoch 77, val_loss 454.7705078125\n",
      "epoch 78, train_loss 1277.1090087890625\n",
      "epoch 78, val_loss 454.77044677734375\n",
      "epoch 79, train_loss 1277.1090087890625\n",
      "epoch 79, val_loss 454.7704162597656\n",
      "epoch 80, train_loss 1277.1090087890625\n",
      "epoch 80, val_loss 454.77032470703125\n",
      "epoch 81, train_loss 1277.1090087890625\n",
      "epoch 81, val_loss 454.7702941894531\n",
      "epoch 82, train_loss 1277.108642578125\n",
      "epoch 82, val_loss 454.7702941894531\n",
      "epoch 83, train_loss 1277.108642578125\n",
      "epoch 83, val_loss 454.770263671875\n",
      "epoch 84, train_loss 1277.108642578125\n",
      "epoch 84, val_loss 454.77020263671875\n",
      "epoch 85, train_loss 1277.1087646484375\n",
      "epoch 85, val_loss 454.7701721191406\n",
      "epoch 86, train_loss 1277.1087646484375\n",
      "epoch 86, val_loss 454.7701110839844\n",
      "epoch 87, train_loss 1277.108642578125\n",
      "epoch 87, val_loss 454.7701110839844\n",
      "epoch 88, train_loss 1277.1087646484375\n",
      "epoch 88, val_loss 454.77008056640625\n",
      "epoch 89, train_loss 1277.108642578125\n",
      "epoch 89, val_loss 454.7699890136719\n",
      "epoch 90, train_loss 1277.108642578125\n",
      "epoch 90, val_loss 454.7699890136719\n",
      "epoch 91, train_loss 1277.1085205078125\n",
      "epoch 91, val_loss 454.76995849609375\n",
      "epoch 92, train_loss 1277.1085205078125\n",
      "epoch 92, val_loss 454.7699279785156\n",
      "epoch 93, train_loss 1277.1082763671875\n",
      "epoch 93, val_loss 454.7698669433594\n",
      "epoch 94, train_loss 1277.1085205078125\n",
      "epoch 94, val_loss 454.76983642578125\n",
      "epoch 95, train_loss 1277.1083984375\n",
      "epoch 95, val_loss 454.76983642578125\n",
      "epoch 96, train_loss 1277.1082763671875\n",
      "epoch 96, val_loss 454.76971435546875\n",
      "epoch 97, train_loss 1277.108154296875\n",
      "epoch 97, val_loss 454.76971435546875\n",
      "epoch 98, train_loss 1277.1080322265625\n",
      "epoch 98, val_loss 454.7696533203125\n",
      "epoch 99, train_loss 1277.108154296875\n",
      "epoch 99, val_loss 454.7696228027344\n",
      "Parameter containing:\n",
      "tensor([9.1374e-27], requires_grad=True)\n",
      "iter 144, train_loss_regularization 0.6999830603599548\n",
      "iter 144, val_loss_regularization 0.6999830603599548\n",
      "epoch 0, train_loss 1277.108154296875\n",
      "epoch 0, val_loss 454.7696228027344\n",
      "epoch 1, train_loss 1277.108154296875\n",
      "epoch 1, val_loss 454.7695007324219\n",
      "epoch 2, train_loss 1277.108154296875\n",
      "epoch 2, val_loss 454.7695007324219\n",
      "epoch 3, train_loss 1277.1080322265625\n",
      "epoch 3, val_loss 454.76947021484375\n",
      "epoch 4, train_loss 1277.10791015625\n",
      "epoch 4, val_loss 454.76947021484375\n",
      "epoch 5, train_loss 1277.1080322265625\n",
      "epoch 5, val_loss 454.7693786621094\n",
      "epoch 6, train_loss 1277.1080322265625\n",
      "epoch 6, val_loss 454.7693786621094\n",
      "epoch 7, train_loss 1277.1080322265625\n",
      "epoch 7, val_loss 454.76934814453125\n",
      "epoch 8, train_loss 1277.10791015625\n",
      "epoch 8, val_loss 454.769287109375\n",
      "epoch 9, train_loss 1277.1080322265625\n",
      "epoch 9, val_loss 454.7692565917969\n",
      "epoch 10, train_loss 1277.1077880859375\n",
      "epoch 10, val_loss 454.7691650390625\n",
      "epoch 11, train_loss 1277.107666015625\n",
      "epoch 11, val_loss 454.7691345214844\n",
      "epoch 12, train_loss 1277.107666015625\n",
      "epoch 12, val_loss 454.7691345214844\n",
      "epoch 13, train_loss 1277.107666015625\n",
      "epoch 13, val_loss 454.76904296875\n",
      "epoch 14, train_loss 1277.1077880859375\n",
      "epoch 14, val_loss 454.7690124511719\n",
      "epoch 15, train_loss 1277.1075439453125\n",
      "epoch 15, val_loss 454.7690124511719\n",
      "epoch 16, train_loss 1277.1075439453125\n",
      "epoch 16, val_loss 454.7689514160156\n",
      "epoch 17, train_loss 1277.1075439453125\n",
      "epoch 17, val_loss 454.76885986328125\n",
      "epoch 18, train_loss 1277.1075439453125\n",
      "epoch 18, val_loss 454.76885986328125\n",
      "epoch 19, train_loss 1277.1075439453125\n",
      "epoch 19, val_loss 454.76885986328125\n",
      "epoch 20, train_loss 1277.1075439453125\n",
      "epoch 20, val_loss 454.76885986328125\n",
      "epoch 21, train_loss 1277.107421875\n",
      "epoch 21, val_loss 454.768798828125\n",
      "epoch 22, train_loss 1277.107421875\n",
      "epoch 22, val_loss 454.76873779296875\n",
      "epoch 23, train_loss 1277.1072998046875\n",
      "epoch 23, val_loss 454.7686767578125\n",
      "epoch 24, train_loss 1277.1072998046875\n",
      "epoch 24, val_loss 454.76861572265625\n",
      "epoch 25, train_loss 1277.1072998046875\n",
      "epoch 25, val_loss 454.7685546875\n",
      "epoch 26, train_loss 1277.1072998046875\n",
      "epoch 26, val_loss 454.76849365234375\n",
      "epoch 27, train_loss 1277.1070556640625\n",
      "epoch 27, val_loss 454.7685546875\n",
      "epoch 28, train_loss 1277.107177734375\n",
      "epoch 28, val_loss 454.7685546875\n",
      "epoch 29, train_loss 1277.107177734375\n",
      "epoch 29, val_loss 454.7684631347656\n",
      "epoch 30, train_loss 1277.107177734375\n",
      "epoch 30, val_loss 454.7684020996094\n",
      "epoch 31, train_loss 1277.107177734375\n",
      "epoch 31, val_loss 454.7683410644531\n",
      "epoch 32, train_loss 1277.107177734375\n",
      "epoch 32, val_loss 454.76837158203125\n",
      "epoch 33, train_loss 1277.1070556640625\n",
      "epoch 33, val_loss 454.76824951171875\n",
      "epoch 34, train_loss 1277.1070556640625\n",
      "epoch 34, val_loss 454.76824951171875\n",
      "epoch 35, train_loss 1277.1070556640625\n",
      "epoch 35, val_loss 454.7682189941406\n",
      "epoch 36, train_loss 1277.10693359375\n",
      "epoch 36, val_loss 454.7682189941406\n",
      "epoch 37, train_loss 1277.1068115234375\n",
      "epoch 37, val_loss 454.76812744140625\n",
      "epoch 38, train_loss 1277.10693359375\n",
      "epoch 38, val_loss 454.7680969238281\n",
      "epoch 39, train_loss 1277.1068115234375\n",
      "epoch 39, val_loss 454.7680969238281\n",
      "epoch 40, train_loss 1277.106689453125\n",
      "epoch 40, val_loss 454.7680358886719\n",
      "epoch 41, train_loss 1277.106689453125\n",
      "epoch 41, val_loss 454.7679138183594\n",
      "epoch 42, train_loss 1277.106689453125\n",
      "epoch 42, val_loss 454.76788330078125\n",
      "epoch 43, train_loss 1277.106689453125\n",
      "epoch 43, val_loss 454.76788330078125\n",
      "epoch 44, train_loss 1277.1065673828125\n",
      "epoch 44, val_loss 454.767822265625\n",
      "epoch 45, train_loss 1277.1065673828125\n",
      "epoch 45, val_loss 454.7677917480469\n",
      "epoch 46, train_loss 1277.1065673828125\n",
      "epoch 46, val_loss 454.7677917480469\n",
      "epoch 47, train_loss 1277.1064453125\n",
      "epoch 47, val_loss 454.76776123046875\n",
      "epoch 48, train_loss 1277.1065673828125\n",
      "epoch 48, val_loss 454.76776123046875\n",
      "epoch 49, train_loss 1277.1064453125\n",
      "epoch 49, val_loss 454.7676696777344\n",
      "epoch 50, train_loss 1277.1063232421875\n",
      "epoch 50, val_loss 454.7676696777344\n",
      "epoch 51, train_loss 1277.1063232421875\n",
      "epoch 51, val_loss 454.7676696777344\n",
      "epoch 52, train_loss 1277.1064453125\n",
      "epoch 52, val_loss 454.767578125\n",
      "epoch 53, train_loss 1277.1064453125\n",
      "epoch 53, val_loss 454.7674865722656\n",
      "epoch 54, train_loss 1277.1064453125\n",
      "epoch 54, val_loss 454.7674255371094\n",
      "epoch 55, train_loss 1277.106201171875\n",
      "epoch 55, val_loss 454.7674255371094\n",
      "epoch 56, train_loss 1277.106201171875\n",
      "epoch 56, val_loss 454.7673645019531\n",
      "epoch 57, train_loss 1277.106201171875\n",
      "epoch 57, val_loss 454.7674255371094\n",
      "epoch 58, train_loss 1277.1060791015625\n",
      "epoch 58, val_loss 454.767333984375\n",
      "epoch 59, train_loss 1277.1060791015625\n",
      "epoch 59, val_loss 454.7673034667969\n",
      "epoch 60, train_loss 1277.106201171875\n",
      "epoch 60, val_loss 454.7673034667969\n",
      "epoch 61, train_loss 1277.1060791015625\n",
      "epoch 61, val_loss 454.7672424316406\n",
      "epoch 62, train_loss 1277.10595703125\n",
      "epoch 62, val_loss 454.7671203613281\n",
      "epoch 63, train_loss 1277.10595703125\n",
      "epoch 63, val_loss 454.7671203613281\n",
      "epoch 64, train_loss 1277.1058349609375\n",
      "epoch 64, val_loss 454.76708984375\n",
      "epoch 65, train_loss 1277.1058349609375\n",
      "epoch 65, val_loss 454.76702880859375\n",
      "epoch 66, train_loss 1277.105712890625\n",
      "epoch 66, val_loss 454.7669982910156\n",
      "epoch 67, train_loss 1277.1058349609375\n",
      "epoch 67, val_loss 454.7669982910156\n",
      "epoch 68, train_loss 1277.1058349609375\n",
      "epoch 68, val_loss 454.76690673828125\n",
      "epoch 69, train_loss 1277.105712890625\n",
      "epoch 69, val_loss 454.76690673828125\n",
      "epoch 70, train_loss 1277.1058349609375\n",
      "epoch 70, val_loss 454.766845703125\n",
      "epoch 71, train_loss 1277.105712890625\n",
      "epoch 71, val_loss 454.76678466796875\n",
      "epoch 72, train_loss 1277.1055908203125\n",
      "epoch 72, val_loss 454.76678466796875\n",
      "epoch 73, train_loss 1277.1055908203125\n",
      "epoch 73, val_loss 454.7666931152344\n",
      "epoch 74, train_loss 1277.1055908203125\n",
      "epoch 74, val_loss 454.76666259765625\n",
      "epoch 75, train_loss 1277.1055908203125\n",
      "epoch 75, val_loss 454.7665710449219\n",
      "epoch 76, train_loss 1277.1055908203125\n",
      "epoch 76, val_loss 454.7666320800781\n",
      "epoch 77, train_loss 1277.10546875\n",
      "epoch 77, val_loss 454.76654052734375\n",
      "epoch 78, train_loss 1277.1053466796875\n",
      "epoch 78, val_loss 454.76654052734375\n",
      "epoch 79, train_loss 1277.105224609375\n",
      "epoch 79, val_loss 454.76654052734375\n",
      "epoch 80, train_loss 1277.10546875\n",
      "epoch 80, val_loss 454.7665100097656\n",
      "epoch 81, train_loss 1277.1053466796875\n",
      "epoch 81, val_loss 454.7664489746094\n",
      "epoch 82, train_loss 1277.1053466796875\n",
      "epoch 82, val_loss 454.76641845703125\n",
      "epoch 83, train_loss 1277.105224609375\n",
      "epoch 83, val_loss 454.76641845703125\n",
      "epoch 84, train_loss 1277.105224609375\n",
      "epoch 84, val_loss 454.76629638671875\n",
      "epoch 85, train_loss 1277.105224609375\n",
      "epoch 85, val_loss 454.7662353515625\n",
      "epoch 86, train_loss 1277.105224609375\n",
      "epoch 86, val_loss 454.7662048339844\n",
      "epoch 87, train_loss 1277.105224609375\n",
      "epoch 87, val_loss 454.7662048339844\n",
      "epoch 88, train_loss 1277.10498046875\n",
      "epoch 88, val_loss 454.76617431640625\n",
      "epoch 89, train_loss 1277.10498046875\n",
      "epoch 89, val_loss 454.76611328125\n",
      "epoch 90, train_loss 1277.1048583984375\n",
      "epoch 90, val_loss 454.7660827636719\n",
      "epoch 91, train_loss 1277.1051025390625\n",
      "epoch 91, val_loss 454.76605224609375\n",
      "epoch 92, train_loss 1277.1048583984375\n",
      "epoch 92, val_loss 454.7659912109375\n",
      "epoch 93, train_loss 1277.1048583984375\n",
      "epoch 93, val_loss 454.7659606933594\n",
      "epoch 94, train_loss 1277.1048583984375\n",
      "epoch 94, val_loss 454.76593017578125\n",
      "epoch 95, train_loss 1277.1048583984375\n",
      "epoch 95, val_loss 454.765869140625\n",
      "epoch 96, train_loss 1277.1048583984375\n",
      "epoch 96, val_loss 454.7658386230469\n",
      "epoch 97, train_loss 1277.1048583984375\n",
      "epoch 97, val_loss 454.7658386230469\n",
      "epoch 98, train_loss 1277.1046142578125\n",
      "epoch 98, val_loss 454.7657775878906\n",
      "epoch 99, train_loss 1277.104736328125\n",
      "epoch 99, val_loss 454.7657165527344\n",
      "Parameter containing:\n",
      "tensor([6.3609e-27], requires_grad=True)\n",
      "iter 145, train_loss_regularization 0.6992795467376709\n",
      "iter 145, val_loss_regularization 0.6992795467376709\n",
      "epoch 0, train_loss 1277.1046142578125\n",
      "epoch 0, val_loss 454.7657165527344\n",
      "epoch 1, train_loss 1277.1046142578125\n",
      "epoch 1, val_loss 454.7656555175781\n",
      "epoch 2, train_loss 1277.1046142578125\n",
      "epoch 2, val_loss 454.765625\n",
      "epoch 3, train_loss 1277.1046142578125\n",
      "epoch 3, val_loss 454.7655334472656\n",
      "epoch 4, train_loss 1277.1046142578125\n",
      "epoch 4, val_loss 454.7655944824219\n",
      "epoch 5, train_loss 1277.1044921875\n",
      "epoch 5, val_loss 454.7655029296875\n",
      "epoch 6, train_loss 1277.1043701171875\n",
      "epoch 6, val_loss 454.7654724121094\n",
      "epoch 7, train_loss 1277.1043701171875\n",
      "epoch 7, val_loss 454.7654113769531\n",
      "epoch 8, train_loss 1277.1043701171875\n",
      "epoch 8, val_loss 454.765380859375\n",
      "epoch 9, train_loss 1277.1043701171875\n",
      "epoch 9, val_loss 454.76531982421875\n",
      "epoch 10, train_loss 1277.1043701171875\n",
      "epoch 10, val_loss 454.7652893066406\n",
      "epoch 11, train_loss 1277.1044921875\n",
      "epoch 11, val_loss 454.7652893066406\n",
      "epoch 12, train_loss 1277.1043701171875\n",
      "epoch 12, val_loss 454.7652587890625\n",
      "epoch 13, train_loss 1277.104248046875\n",
      "epoch 13, val_loss 454.76519775390625\n",
      "epoch 14, train_loss 1277.1041259765625\n",
      "epoch 14, val_loss 454.76513671875\n",
      "epoch 15, train_loss 1277.1041259765625\n",
      "epoch 15, val_loss 454.7650451660156\n",
      "epoch 16, train_loss 1277.1041259765625\n",
      "epoch 16, val_loss 454.7650451660156\n",
      "epoch 17, train_loss 1277.1041259765625\n",
      "epoch 17, val_loss 454.7650451660156\n",
      "epoch 18, train_loss 1277.1041259765625\n",
      "epoch 18, val_loss 454.76495361328125\n",
      "epoch 19, train_loss 1277.10400390625\n",
      "epoch 19, val_loss 454.7649230957031\n",
      "epoch 20, train_loss 1277.1038818359375\n",
      "epoch 20, val_loss 454.7649230957031\n",
      "epoch 21, train_loss 1277.1038818359375\n",
      "epoch 21, val_loss 454.7648620605469\n",
      "epoch 22, train_loss 1277.1038818359375\n",
      "epoch 22, val_loss 454.7648010253906\n",
      "epoch 23, train_loss 1277.103759765625\n",
      "epoch 23, val_loss 454.7648010253906\n",
      "epoch 24, train_loss 1277.103759765625\n",
      "epoch 24, val_loss 454.7648010253906\n",
      "epoch 25, train_loss 1277.103759765625\n",
      "epoch 25, val_loss 454.7647399902344\n",
      "epoch 26, train_loss 1277.1038818359375\n",
      "epoch 26, val_loss 454.76470947265625\n",
      "epoch 27, train_loss 1277.1038818359375\n",
      "epoch 27, val_loss 454.76458740234375\n",
      "epoch 28, train_loss 1277.103515625\n",
      "epoch 28, val_loss 454.7646179199219\n",
      "epoch 29, train_loss 1277.1036376953125\n",
      "epoch 29, val_loss 454.7645568847656\n",
      "epoch 30, train_loss 1277.103515625\n",
      "epoch 30, val_loss 454.7644958496094\n",
      "epoch 31, train_loss 1277.103515625\n",
      "epoch 31, val_loss 454.76446533203125\n",
      "epoch 32, train_loss 1277.1036376953125\n",
      "epoch 32, val_loss 454.76446533203125\n",
      "epoch 33, train_loss 1277.1036376953125\n",
      "epoch 33, val_loss 454.764404296875\n",
      "epoch 34, train_loss 1277.1033935546875\n",
      "epoch 34, val_loss 454.76434326171875\n",
      "epoch 35, train_loss 1277.1033935546875\n",
      "epoch 35, val_loss 454.7642822265625\n",
      "epoch 36, train_loss 1277.1033935546875\n",
      "epoch 36, val_loss 454.7642822265625\n",
      "epoch 37, train_loss 1277.103271484375\n",
      "epoch 37, val_loss 454.7642517089844\n",
      "epoch 38, train_loss 1277.1033935546875\n",
      "epoch 38, val_loss 454.76422119140625\n",
      "epoch 39, train_loss 1277.1033935546875\n",
      "epoch 39, val_loss 454.7641296386719\n",
      "epoch 40, train_loss 1277.103271484375\n",
      "epoch 40, val_loss 454.76416015625\n",
      "epoch 41, train_loss 1277.103271484375\n",
      "epoch 41, val_loss 454.7641296386719\n",
      "epoch 42, train_loss 1277.1031494140625\n",
      "epoch 42, val_loss 454.7640686035156\n",
      "epoch 43, train_loss 1277.10302734375\n",
      "epoch 43, val_loss 454.7640075683594\n",
      "epoch 44, train_loss 1277.10302734375\n",
      "epoch 44, val_loss 454.7640075683594\n",
      "epoch 45, train_loss 1277.1031494140625\n",
      "epoch 45, val_loss 454.763916015625\n",
      "epoch 46, train_loss 1277.1031494140625\n",
      "epoch 46, val_loss 454.763916015625\n",
      "epoch 47, train_loss 1277.10302734375\n",
      "epoch 47, val_loss 454.7638244628906\n",
      "epoch 48, train_loss 1277.10302734375\n",
      "epoch 48, val_loss 454.7637634277344\n",
      "epoch 49, train_loss 1277.10302734375\n",
      "epoch 49, val_loss 454.7637939453125\n",
      "epoch 50, train_loss 1277.102783203125\n",
      "epoch 50, val_loss 454.7637023925781\n",
      "epoch 51, train_loss 1277.102783203125\n",
      "epoch 51, val_loss 454.7637023925781\n",
      "epoch 52, train_loss 1277.102783203125\n",
      "epoch 52, val_loss 454.763671875\n",
      "epoch 53, train_loss 1277.102783203125\n",
      "epoch 53, val_loss 454.76361083984375\n",
      "epoch 54, train_loss 1277.102783203125\n",
      "epoch 54, val_loss 454.763671875\n",
      "epoch 55, train_loss 1277.102783203125\n",
      "epoch 55, val_loss 454.7635803222656\n",
      "epoch 56, train_loss 1277.1026611328125\n",
      "epoch 56, val_loss 454.7635498046875\n",
      "epoch 57, train_loss 1277.1025390625\n",
      "epoch 57, val_loss 454.763427734375\n",
      "epoch 58, train_loss 1277.1025390625\n",
      "epoch 58, val_loss 454.763427734375\n",
      "epoch 59, train_loss 1277.1025390625\n",
      "epoch 59, val_loss 454.7633361816406\n",
      "epoch 60, train_loss 1277.1025390625\n",
      "epoch 60, val_loss 454.7633361816406\n",
      "epoch 61, train_loss 1277.1025390625\n",
      "epoch 61, val_loss 454.7633361816406\n",
      "epoch 62, train_loss 1277.1025390625\n",
      "epoch 62, val_loss 454.7633056640625\n",
      "epoch 63, train_loss 1277.1025390625\n",
      "epoch 63, val_loss 454.76324462890625\n",
      "epoch 64, train_loss 1277.1025390625\n",
      "epoch 64, val_loss 454.7632141113281\n",
      "epoch 65, train_loss 1277.1025390625\n",
      "epoch 65, val_loss 454.7631530761719\n",
      "epoch 66, train_loss 1277.102294921875\n",
      "epoch 66, val_loss 454.7631530761719\n",
      "epoch 67, train_loss 1277.102294921875\n",
      "epoch 67, val_loss 454.7630920410156\n",
      "epoch 68, train_loss 1277.102294921875\n",
      "epoch 68, val_loss 454.7630310058594\n",
      "epoch 69, train_loss 1277.102294921875\n",
      "epoch 69, val_loss 454.7629699707031\n",
      "epoch 70, train_loss 1277.1021728515625\n",
      "epoch 70, val_loss 454.76300048828125\n",
      "epoch 71, train_loss 1277.1021728515625\n",
      "epoch 71, val_loss 454.76300048828125\n",
      "epoch 72, train_loss 1277.1021728515625\n",
      "epoch 72, val_loss 454.76287841796875\n",
      "epoch 73, train_loss 1277.1021728515625\n",
      "epoch 73, val_loss 454.76287841796875\n",
      "epoch 74, train_loss 1277.10205078125\n",
      "epoch 74, val_loss 454.7628479003906\n",
      "epoch 75, train_loss 1277.10205078125\n",
      "epoch 75, val_loss 454.7627868652344\n",
      "epoch 76, train_loss 1277.10205078125\n",
      "epoch 76, val_loss 454.7626953125\n",
      "epoch 77, train_loss 1277.10205078125\n",
      "epoch 77, val_loss 454.7626647949219\n",
      "epoch 78, train_loss 1277.10205078125\n",
      "epoch 78, val_loss 454.76263427734375\n",
      "epoch 79, train_loss 1277.10205078125\n",
      "epoch 79, val_loss 454.7626647949219\n",
      "epoch 80, train_loss 1277.101806640625\n",
      "epoch 80, val_loss 454.7625732421875\n",
      "epoch 81, train_loss 1277.101806640625\n",
      "epoch 81, val_loss 454.7625427246094\n",
      "epoch 82, train_loss 1277.101806640625\n",
      "epoch 82, val_loss 454.7625732421875\n",
      "epoch 83, train_loss 1277.1019287109375\n",
      "epoch 83, val_loss 454.76251220703125\n",
      "epoch 84, train_loss 1277.1019287109375\n",
      "epoch 84, val_loss 454.7624206542969\n",
      "epoch 85, train_loss 1277.101806640625\n",
      "epoch 85, val_loss 454.7624206542969\n",
      "epoch 86, train_loss 1277.101806640625\n",
      "epoch 86, val_loss 454.7624206542969\n",
      "epoch 87, train_loss 1277.1016845703125\n",
      "epoch 87, val_loss 454.7623291015625\n",
      "epoch 88, train_loss 1277.1016845703125\n",
      "epoch 88, val_loss 454.76220703125\n",
      "epoch 89, train_loss 1277.1014404296875\n",
      "epoch 89, val_loss 454.76220703125\n",
      "epoch 90, train_loss 1277.1014404296875\n",
      "epoch 90, val_loss 454.7621765136719\n",
      "epoch 91, train_loss 1277.101318359375\n",
      "epoch 91, val_loss 454.76220703125\n",
      "epoch 92, train_loss 1277.1015625\n",
      "epoch 92, val_loss 454.7621154785156\n",
      "epoch 93, train_loss 1277.1015625\n",
      "epoch 93, val_loss 454.7620849609375\n",
      "epoch 94, train_loss 1277.1014404296875\n",
      "epoch 94, val_loss 454.7620849609375\n",
      "epoch 95, train_loss 1277.1014404296875\n",
      "epoch 95, val_loss 454.7620849609375\n",
      "epoch 96, train_loss 1277.101318359375\n",
      "epoch 96, val_loss 454.761962890625\n",
      "epoch 97, train_loss 1277.1011962890625\n",
      "epoch 97, val_loss 454.76190185546875\n",
      "epoch 98, train_loss 1277.1011962890625\n",
      "epoch 98, val_loss 454.7618713378906\n",
      "epoch 99, train_loss 1277.1011962890625\n",
      "epoch 99, val_loss 454.7618713378906\n",
      "Parameter containing:\n",
      "tensor([4.4291e-27], requires_grad=True)\n",
      "iter 146, train_loss_regularization 0.6985953450202942\n",
      "iter 146, val_loss_regularization 0.6985953450202942\n",
      "epoch 0, train_loss 1277.1011962890625\n",
      "epoch 0, val_loss 454.7618408203125\n",
      "epoch 1, train_loss 1277.1011962890625\n",
      "epoch 1, val_loss 454.7617492675781\n",
      "epoch 2, train_loss 1277.10107421875\n",
      "epoch 2, val_loss 454.76171875\n",
      "epoch 3, train_loss 1277.10107421875\n",
      "epoch 3, val_loss 454.76171875\n",
      "epoch 4, train_loss 1277.1009521484375\n",
      "epoch 4, val_loss 454.7616271972656\n",
      "epoch 5, train_loss 1277.1009521484375\n",
      "epoch 5, val_loss 454.7616271972656\n",
      "epoch 6, train_loss 1277.1009521484375\n",
      "epoch 6, val_loss 454.7616271972656\n",
      "epoch 7, train_loss 1277.1009521484375\n",
      "epoch 7, val_loss 454.7615966796875\n",
      "epoch 8, train_loss 1277.1009521484375\n",
      "epoch 8, val_loss 454.76153564453125\n",
      "epoch 9, train_loss 1277.1009521484375\n",
      "epoch 9, val_loss 454.7614440917969\n",
      "epoch 10, train_loss 1277.1009521484375\n",
      "epoch 10, val_loss 454.76141357421875\n",
      "epoch 11, train_loss 1277.100830078125\n",
      "epoch 11, val_loss 454.7613830566406\n",
      "epoch 12, train_loss 1277.100830078125\n",
      "epoch 12, val_loss 454.7613830566406\n",
      "epoch 13, train_loss 1277.1007080078125\n",
      "epoch 13, val_loss 454.7613220214844\n",
      "epoch 14, train_loss 1277.1007080078125\n",
      "epoch 14, val_loss 454.76129150390625\n",
      "epoch 15, train_loss 1277.1007080078125\n",
      "epoch 15, val_loss 454.7612609863281\n",
      "epoch 16, train_loss 1277.1007080078125\n",
      "epoch 16, val_loss 454.76116943359375\n",
      "epoch 17, train_loss 1277.1005859375\n",
      "epoch 17, val_loss 454.7610778808594\n",
      "epoch 18, train_loss 1277.1004638671875\n",
      "epoch 18, val_loss 454.7610778808594\n",
      "epoch 19, train_loss 1277.1004638671875\n",
      "epoch 19, val_loss 454.7610778808594\n",
      "epoch 20, train_loss 1277.100341796875\n",
      "epoch 20, val_loss 454.7609558105469\n",
      "epoch 21, train_loss 1277.1004638671875\n",
      "epoch 21, val_loss 454.760986328125\n",
      "epoch 22, train_loss 1277.1004638671875\n",
      "epoch 22, val_loss 454.760986328125\n",
      "epoch 23, train_loss 1277.1004638671875\n",
      "epoch 23, val_loss 454.76092529296875\n",
      "epoch 24, train_loss 1277.1004638671875\n",
      "epoch 24, val_loss 454.76092529296875\n",
      "epoch 25, train_loss 1277.1004638671875\n",
      "epoch 25, val_loss 454.7608642578125\n",
      "epoch 26, train_loss 1277.1004638671875\n",
      "epoch 26, val_loss 454.76080322265625\n",
      "epoch 27, train_loss 1277.1004638671875\n",
      "epoch 27, val_loss 454.76080322265625\n",
      "epoch 28, train_loss 1277.1002197265625\n",
      "epoch 28, val_loss 454.76080322265625\n",
      "epoch 29, train_loss 1277.1002197265625\n",
      "epoch 29, val_loss 454.7607421875\n",
      "epoch 30, train_loss 1277.1002197265625\n",
      "epoch 30, val_loss 454.76068115234375\n",
      "epoch 31, train_loss 1277.1002197265625\n",
      "epoch 31, val_loss 454.7605895996094\n",
      "epoch 32, train_loss 1277.10009765625\n",
      "epoch 32, val_loss 454.7605895996094\n",
      "epoch 33, train_loss 1277.0999755859375\n",
      "epoch 33, val_loss 454.760498046875\n",
      "epoch 34, train_loss 1277.0999755859375\n",
      "epoch 34, val_loss 454.760498046875\n",
      "epoch 35, train_loss 1277.10009765625\n",
      "epoch 35, val_loss 454.7604675292969\n",
      "epoch 36, train_loss 1277.099853515625\n",
      "epoch 36, val_loss 454.7604675292969\n",
      "epoch 37, train_loss 1277.099853515625\n",
      "epoch 37, val_loss 454.7603759765625\n",
      "epoch 38, train_loss 1277.099853515625\n",
      "epoch 38, val_loss 454.7603759765625\n",
      "epoch 39, train_loss 1277.099853515625\n",
      "epoch 39, val_loss 454.7603454589844\n",
      "epoch 40, train_loss 1277.099853515625\n",
      "epoch 40, val_loss 454.7602844238281\n",
      "epoch 41, train_loss 1277.099853515625\n",
      "epoch 41, val_loss 454.7602233886719\n",
      "epoch 42, train_loss 1277.0999755859375\n",
      "epoch 42, val_loss 454.7601623535156\n",
      "epoch 43, train_loss 1277.0997314453125\n",
      "epoch 43, val_loss 454.7602233886719\n",
      "epoch 44, train_loss 1277.099609375\n",
      "epoch 44, val_loss 454.7601318359375\n",
      "epoch 45, train_loss 1277.099609375\n",
      "epoch 45, val_loss 454.76007080078125\n",
      "epoch 46, train_loss 1277.099609375\n",
      "epoch 46, val_loss 454.7600402832031\n",
      "epoch 47, train_loss 1277.0994873046875\n",
      "epoch 47, val_loss 454.760009765625\n",
      "epoch 48, train_loss 1277.099365234375\n",
      "epoch 48, val_loss 454.760009765625\n",
      "epoch 49, train_loss 1277.099365234375\n",
      "epoch 49, val_loss 454.7599182128906\n",
      "epoch 50, train_loss 1277.099365234375\n",
      "epoch 50, val_loss 454.7598876953125\n",
      "epoch 51, train_loss 1277.099365234375\n",
      "epoch 51, val_loss 454.7598876953125\n",
      "epoch 52, train_loss 1277.099365234375\n",
      "epoch 52, val_loss 454.7597961425781\n",
      "epoch 53, train_loss 1277.099365234375\n",
      "epoch 53, val_loss 454.7597351074219\n",
      "epoch 54, train_loss 1277.0992431640625\n",
      "epoch 54, val_loss 454.75970458984375\n",
      "epoch 55, train_loss 1277.09912109375\n",
      "epoch 55, val_loss 454.75970458984375\n",
      "epoch 56, train_loss 1277.0992431640625\n",
      "epoch 56, val_loss 454.7596740722656\n",
      "epoch 57, train_loss 1277.099365234375\n",
      "epoch 57, val_loss 454.7596740722656\n",
      "epoch 58, train_loss 1277.099365234375\n",
      "epoch 58, val_loss 454.75958251953125\n",
      "epoch 59, train_loss 1277.099365234375\n",
      "epoch 59, val_loss 454.75958251953125\n",
      "epoch 60, train_loss 1277.09912109375\n",
      "epoch 60, val_loss 454.7594909667969\n",
      "epoch 61, train_loss 1277.0989990234375\n",
      "epoch 61, val_loss 454.7594299316406\n",
      "epoch 62, train_loss 1277.098876953125\n",
      "epoch 62, val_loss 454.7593688964844\n",
      "epoch 63, train_loss 1277.098876953125\n",
      "epoch 63, val_loss 454.7594299316406\n",
      "epoch 64, train_loss 1277.098876953125\n",
      "epoch 64, val_loss 454.7593688964844\n",
      "epoch 65, train_loss 1277.098876953125\n",
      "epoch 65, val_loss 454.75933837890625\n",
      "epoch 66, train_loss 1277.098876953125\n",
      "epoch 66, val_loss 454.7592468261719\n",
      "epoch 67, train_loss 1277.098876953125\n",
      "epoch 67, val_loss 454.75921630859375\n",
      "epoch 68, train_loss 1277.098876953125\n",
      "epoch 68, val_loss 454.75921630859375\n",
      "epoch 69, train_loss 1277.0986328125\n",
      "epoch 69, val_loss 454.75921630859375\n",
      "epoch 70, train_loss 1277.098876953125\n",
      "epoch 70, val_loss 454.7591247558594\n",
      "epoch 71, train_loss 1277.0986328125\n",
      "epoch 71, val_loss 454.7591247558594\n",
      "epoch 72, train_loss 1277.0986328125\n",
      "epoch 72, val_loss 454.759033203125\n",
      "epoch 73, train_loss 1277.098876953125\n",
      "epoch 73, val_loss 454.75909423828125\n",
      "epoch 74, train_loss 1277.098876953125\n",
      "epoch 74, val_loss 454.75897216796875\n",
      "epoch 75, train_loss 1277.0986328125\n",
      "epoch 75, val_loss 454.7589111328125\n",
      "epoch 76, train_loss 1277.0986328125\n",
      "epoch 76, val_loss 454.7589111328125\n",
      "epoch 77, train_loss 1277.0985107421875\n",
      "epoch 77, val_loss 454.7588806152344\n",
      "epoch 78, train_loss 1277.0985107421875\n",
      "epoch 78, val_loss 454.7587890625\n",
      "epoch 79, train_loss 1277.0985107421875\n",
      "epoch 79, val_loss 454.7587890625\n",
      "epoch 80, train_loss 1277.098388671875\n",
      "epoch 80, val_loss 454.7587890625\n",
      "epoch 81, train_loss 1277.098388671875\n",
      "epoch 81, val_loss 454.7587585449219\n",
      "epoch 82, train_loss 1277.0985107421875\n",
      "epoch 82, val_loss 454.7586364746094\n",
      "epoch 83, train_loss 1277.098388671875\n",
      "epoch 83, val_loss 454.7586364746094\n",
      "epoch 84, train_loss 1277.098388671875\n",
      "epoch 84, val_loss 454.7586364746094\n",
      "epoch 85, train_loss 1277.0982666015625\n",
      "epoch 85, val_loss 454.758544921875\n",
      "epoch 86, train_loss 1277.09814453125\n",
      "epoch 86, val_loss 454.758544921875\n",
      "epoch 87, train_loss 1277.0982666015625\n",
      "epoch 87, val_loss 454.7584533691406\n",
      "epoch 88, train_loss 1277.0982666015625\n",
      "epoch 88, val_loss 454.7584533691406\n",
      "epoch 89, train_loss 1277.09814453125\n",
      "epoch 89, val_loss 454.7584533691406\n",
      "epoch 90, train_loss 1277.09814453125\n",
      "epoch 90, val_loss 454.7584228515625\n",
      "epoch 91, train_loss 1277.0980224609375\n",
      "epoch 91, val_loss 454.7583312988281\n",
      "epoch 92, train_loss 1277.0980224609375\n",
      "epoch 92, val_loss 454.7583312988281\n",
      "epoch 93, train_loss 1277.0980224609375\n",
      "epoch 93, val_loss 454.75823974609375\n",
      "epoch 94, train_loss 1277.0977783203125\n",
      "epoch 94, val_loss 454.7581787109375\n",
      "epoch 95, train_loss 1277.0977783203125\n",
      "epoch 95, val_loss 454.7581787109375\n",
      "epoch 96, train_loss 1277.097900390625\n",
      "epoch 96, val_loss 454.75811767578125\n",
      "epoch 97, train_loss 1277.0980224609375\n",
      "epoch 97, val_loss 454.7580871582031\n",
      "epoch 98, train_loss 1277.0980224609375\n",
      "epoch 98, val_loss 454.758056640625\n",
      "epoch 99, train_loss 1277.0977783203125\n",
      "epoch 99, val_loss 454.758056640625\n",
      "Parameter containing:\n",
      "tensor([3.0847e-27], requires_grad=True)\n",
      "iter 147, train_loss_regularization 0.6979302167892456\n",
      "iter 147, val_loss_regularization 0.6979302167892456\n",
      "epoch 0, train_loss 1277.09765625\n",
      "epoch 0, val_loss 454.75799560546875\n",
      "epoch 1, train_loss 1277.09765625\n",
      "epoch 1, val_loss 454.75799560546875\n",
      "epoch 2, train_loss 1277.0975341796875\n",
      "epoch 2, val_loss 454.7579650878906\n",
      "epoch 3, train_loss 1277.0975341796875\n",
      "epoch 3, val_loss 454.7578430175781\n",
      "epoch 4, train_loss 1277.0975341796875\n",
      "epoch 4, val_loss 454.7577819824219\n",
      "epoch 5, train_loss 1277.0975341796875\n",
      "epoch 5, val_loss 454.7577819824219\n",
      "epoch 6, train_loss 1277.0975341796875\n",
      "epoch 6, val_loss 454.75775146484375\n",
      "epoch 7, train_loss 1277.097412109375\n",
      "epoch 7, val_loss 454.75775146484375\n",
      "epoch 8, train_loss 1277.0975341796875\n",
      "epoch 8, val_loss 454.7576599121094\n",
      "epoch 9, train_loss 1277.0972900390625\n",
      "epoch 9, val_loss 454.75762939453125\n",
      "epoch 10, train_loss 1277.0972900390625\n",
      "epoch 10, val_loss 454.757568359375\n",
      "epoch 11, train_loss 1277.097412109375\n",
      "epoch 11, val_loss 454.7575378417969\n",
      "epoch 12, train_loss 1277.097412109375\n",
      "epoch 12, val_loss 454.7575378417969\n",
      "epoch 13, train_loss 1277.0972900390625\n",
      "epoch 13, val_loss 454.75750732421875\n",
      "epoch 14, train_loss 1277.0972900390625\n",
      "epoch 14, val_loss 454.7574462890625\n",
      "epoch 15, train_loss 1277.0972900390625\n",
      "epoch 15, val_loss 454.7574157714844\n",
      "epoch 16, train_loss 1277.09716796875\n",
      "epoch 16, val_loss 454.75732421875\n",
      "epoch 17, train_loss 1277.0972900390625\n",
      "epoch 17, val_loss 454.75732421875\n",
      "epoch 18, train_loss 1277.0970458984375\n",
      "epoch 18, val_loss 454.75732421875\n",
      "epoch 19, train_loss 1277.0970458984375\n",
      "epoch 19, val_loss 454.75726318359375\n",
      "epoch 20, train_loss 1277.0970458984375\n",
      "epoch 20, val_loss 454.75726318359375\n",
      "epoch 21, train_loss 1277.0970458984375\n",
      "epoch 21, val_loss 454.7571716308594\n",
      "epoch 22, train_loss 1277.0970458984375\n",
      "epoch 22, val_loss 454.7571716308594\n",
      "epoch 23, train_loss 1277.096923828125\n",
      "epoch 23, val_loss 454.7571105957031\n",
      "epoch 24, train_loss 1277.096923828125\n",
      "epoch 24, val_loss 454.7570495605469\n",
      "epoch 25, train_loss 1277.096923828125\n",
      "epoch 25, val_loss 454.7569580078125\n",
      "epoch 26, train_loss 1277.0970458984375\n",
      "epoch 26, val_loss 454.7569885253906\n",
      "epoch 27, train_loss 1277.0970458984375\n",
      "epoch 27, val_loss 454.7569885253906\n",
      "epoch 28, train_loss 1277.096923828125\n",
      "epoch 28, val_loss 454.7568664550781\n",
      "epoch 29, train_loss 1277.0968017578125\n",
      "epoch 29, val_loss 454.7569274902344\n",
      "epoch 30, train_loss 1277.0966796875\n",
      "epoch 30, val_loss 454.7568664550781\n",
      "epoch 31, train_loss 1277.0966796875\n",
      "epoch 31, val_loss 454.7568359375\n",
      "epoch 32, train_loss 1277.0966796875\n",
      "epoch 32, val_loss 454.7568054199219\n",
      "epoch 33, train_loss 1277.0966796875\n",
      "epoch 33, val_loss 454.7567443847656\n",
      "epoch 34, train_loss 1277.0965576171875\n",
      "epoch 34, val_loss 454.7567443847656\n",
      "epoch 35, train_loss 1277.0965576171875\n",
      "epoch 35, val_loss 454.75665283203125\n",
      "epoch 36, train_loss 1277.0965576171875\n",
      "epoch 36, val_loss 454.756591796875\n",
      "epoch 37, train_loss 1277.096435546875\n",
      "epoch 37, val_loss 454.756591796875\n",
      "epoch 38, train_loss 1277.0963134765625\n",
      "epoch 38, val_loss 454.7565002441406\n",
      "epoch 39, train_loss 1277.0965576171875\n",
      "epoch 39, val_loss 454.75653076171875\n",
      "epoch 40, train_loss 1277.0963134765625\n",
      "epoch 40, val_loss 454.7564697265625\n",
      "epoch 41, train_loss 1277.0963134765625\n",
      "epoch 41, val_loss 454.7564697265625\n",
      "epoch 42, train_loss 1277.0963134765625\n",
      "epoch 42, val_loss 454.75640869140625\n",
      "epoch 43, train_loss 1277.09619140625\n",
      "epoch 43, val_loss 454.7563781738281\n",
      "epoch 44, train_loss 1277.09619140625\n",
      "epoch 44, val_loss 454.75634765625\n",
      "epoch 45, train_loss 1277.09619140625\n",
      "epoch 45, val_loss 454.7562561035156\n",
      "epoch 46, train_loss 1277.095947265625\n",
      "epoch 46, val_loss 454.7561950683594\n",
      "epoch 47, train_loss 1277.095947265625\n",
      "epoch 47, val_loss 454.7562561035156\n",
      "epoch 48, train_loss 1277.0960693359375\n",
      "epoch 48, val_loss 454.7561340332031\n",
      "epoch 49, train_loss 1277.095947265625\n",
      "epoch 49, val_loss 454.7561340332031\n",
      "epoch 50, train_loss 1277.0960693359375\n",
      "epoch 50, val_loss 454.7561340332031\n",
      "epoch 51, train_loss 1277.09619140625\n",
      "epoch 51, val_loss 454.7560729980469\n",
      "epoch 52, train_loss 1277.095947265625\n",
      "epoch 52, val_loss 454.75604248046875\n",
      "epoch 53, train_loss 1277.095947265625\n",
      "epoch 53, val_loss 454.7559509277344\n",
      "epoch 54, train_loss 1277.095947265625\n",
      "epoch 54, val_loss 454.75592041015625\n",
      "epoch 55, train_loss 1277.095947265625\n",
      "epoch 55, val_loss 454.75592041015625\n",
      "epoch 56, train_loss 1277.095947265625\n",
      "epoch 56, val_loss 454.7558898925781\n",
      "epoch 57, train_loss 1277.0958251953125\n",
      "epoch 57, val_loss 454.75579833984375\n",
      "epoch 58, train_loss 1277.0958251953125\n",
      "epoch 58, val_loss 454.75579833984375\n",
      "epoch 59, train_loss 1277.095947265625\n",
      "epoch 59, val_loss 454.7557373046875\n",
      "epoch 60, train_loss 1277.095703125\n",
      "epoch 60, val_loss 454.7557067871094\n",
      "epoch 61, train_loss 1277.095703125\n",
      "epoch 61, val_loss 454.75567626953125\n",
      "epoch 62, train_loss 1277.095703125\n",
      "epoch 62, val_loss 454.755615234375\n",
      "epoch 63, train_loss 1277.095458984375\n",
      "epoch 63, val_loss 454.755615234375\n",
      "epoch 64, train_loss 1277.0955810546875\n",
      "epoch 64, val_loss 454.7555847167969\n",
      "epoch 65, train_loss 1277.095458984375\n",
      "epoch 65, val_loss 454.7554931640625\n",
      "epoch 66, train_loss 1277.095458984375\n",
      "epoch 66, val_loss 454.7554016113281\n",
      "epoch 67, train_loss 1277.095458984375\n",
      "epoch 67, val_loss 454.7554016113281\n",
      "epoch 68, train_loss 1277.095458984375\n",
      "epoch 68, val_loss 454.75537109375\n",
      "epoch 69, train_loss 1277.0953369140625\n",
      "epoch 69, val_loss 454.75537109375\n",
      "epoch 70, train_loss 1277.09521484375\n",
      "epoch 70, val_loss 454.7552795410156\n",
      "epoch 71, train_loss 1277.095458984375\n",
      "epoch 71, val_loss 454.7552795410156\n",
      "epoch 72, train_loss 1277.095458984375\n",
      "epoch 72, val_loss 454.7552490234375\n",
      "epoch 73, train_loss 1277.095458984375\n",
      "epoch 73, val_loss 454.7552185058594\n",
      "epoch 74, train_loss 1277.09521484375\n",
      "epoch 74, val_loss 454.7552185058594\n",
      "epoch 75, train_loss 1277.09521484375\n",
      "epoch 75, val_loss 454.7551574707031\n",
      "epoch 76, train_loss 1277.09521484375\n",
      "epoch 76, val_loss 454.755126953125\n",
      "epoch 77, train_loss 1277.09521484375\n",
      "epoch 77, val_loss 454.7550354003906\n",
      "epoch 78, train_loss 1277.094970703125\n",
      "epoch 78, val_loss 454.7550354003906\n",
      "epoch 79, train_loss 1277.0950927734375\n",
      "epoch 79, val_loss 454.7550048828125\n",
      "epoch 80, train_loss 1277.0950927734375\n",
      "epoch 80, val_loss 454.75494384765625\n",
      "epoch 81, train_loss 1277.094970703125\n",
      "epoch 81, val_loss 454.75494384765625\n",
      "epoch 82, train_loss 1277.094970703125\n",
      "epoch 82, val_loss 454.7549133300781\n",
      "epoch 83, train_loss 1277.094970703125\n",
      "epoch 83, val_loss 454.75482177734375\n",
      "epoch 84, train_loss 1277.094970703125\n",
      "epoch 84, val_loss 454.7547912597656\n",
      "epoch 85, train_loss 1277.094970703125\n",
      "epoch 85, val_loss 454.7547912597656\n",
      "epoch 86, train_loss 1277.0947265625\n",
      "epoch 86, val_loss 454.75469970703125\n",
      "epoch 87, train_loss 1277.0948486328125\n",
      "epoch 87, val_loss 454.75469970703125\n",
      "epoch 88, train_loss 1277.0947265625\n",
      "epoch 88, val_loss 454.754638671875\n",
      "epoch 89, train_loss 1277.0948486328125\n",
      "epoch 89, val_loss 454.754638671875\n",
      "epoch 90, train_loss 1277.0947265625\n",
      "epoch 90, val_loss 454.7545471191406\n",
      "epoch 91, train_loss 1277.0947265625\n",
      "epoch 91, val_loss 454.75457763671875\n",
      "epoch 92, train_loss 1277.0947265625\n",
      "epoch 92, val_loss 454.75445556640625\n",
      "epoch 93, train_loss 1277.0947265625\n",
      "epoch 93, val_loss 454.75445556640625\n",
      "epoch 94, train_loss 1277.094482421875\n",
      "epoch 94, val_loss 454.7544250488281\n",
      "epoch 95, train_loss 1277.0943603515625\n",
      "epoch 95, val_loss 454.7543640136719\n",
      "epoch 96, train_loss 1277.0943603515625\n",
      "epoch 96, val_loss 454.7543640136719\n",
      "epoch 97, train_loss 1277.094482421875\n",
      "epoch 97, val_loss 454.75433349609375\n",
      "epoch 98, train_loss 1277.094482421875\n",
      "epoch 98, val_loss 454.7542419433594\n",
      "epoch 99, train_loss 1277.0943603515625\n",
      "epoch 99, val_loss 454.7542419433594\n",
      "Parameter containing:\n",
      "tensor([2.1488e-27], requires_grad=True)\n",
      "iter 148, train_loss_regularization 0.6972838640213013\n",
      "iter 148, val_loss_regularization 0.6972838640213013\n",
      "epoch 0, train_loss 1277.094482421875\n",
      "epoch 0, val_loss 454.75421142578125\n",
      "epoch 1, train_loss 1277.094482421875\n",
      "epoch 1, val_loss 454.7541809082031\n",
      "epoch 2, train_loss 1277.0943603515625\n",
      "epoch 2, val_loss 454.7541198730469\n",
      "epoch 3, train_loss 1277.0943603515625\n",
      "epoch 3, val_loss 454.75408935546875\n",
      "epoch 4, train_loss 1277.09423828125\n",
      "epoch 4, val_loss 454.7540283203125\n",
      "epoch 5, train_loss 1277.0941162109375\n",
      "epoch 5, val_loss 454.7540283203125\n",
      "epoch 6, train_loss 1277.0941162109375\n",
      "epoch 6, val_loss 454.7539978027344\n",
      "epoch 7, train_loss 1277.0941162109375\n",
      "epoch 7, val_loss 454.75396728515625\n",
      "epoch 8, train_loss 1277.09423828125\n",
      "epoch 8, val_loss 454.75396728515625\n",
      "epoch 9, train_loss 1277.09423828125\n",
      "epoch 9, val_loss 454.75390625\n",
      "epoch 10, train_loss 1277.0941162109375\n",
      "epoch 10, val_loss 454.7538757324219\n",
      "epoch 11, train_loss 1277.0938720703125\n",
      "epoch 11, val_loss 454.7537841796875\n",
      "epoch 12, train_loss 1277.0938720703125\n",
      "epoch 12, val_loss 454.75372314453125\n",
      "epoch 13, train_loss 1277.093994140625\n",
      "epoch 13, val_loss 454.75372314453125\n",
      "epoch 14, train_loss 1277.093994140625\n",
      "epoch 14, val_loss 454.753662109375\n",
      "epoch 15, train_loss 1277.0938720703125\n",
      "epoch 15, val_loss 454.7535705566406\n",
      "epoch 16, train_loss 1277.0938720703125\n",
      "epoch 16, val_loss 454.7535705566406\n",
      "epoch 17, train_loss 1277.0938720703125\n",
      "epoch 17, val_loss 454.7535705566406\n",
      "epoch 18, train_loss 1277.0938720703125\n",
      "epoch 18, val_loss 454.7535400390625\n",
      "epoch 19, train_loss 1277.0936279296875\n",
      "epoch 19, val_loss 454.7534484863281\n",
      "epoch 20, train_loss 1277.0936279296875\n",
      "epoch 20, val_loss 454.75341796875\n",
      "epoch 21, train_loss 1277.09375\n",
      "epoch 21, val_loss 454.7533874511719\n",
      "epoch 22, train_loss 1277.0936279296875\n",
      "epoch 22, val_loss 454.7533874511719\n",
      "epoch 23, train_loss 1277.0936279296875\n",
      "epoch 23, val_loss 454.7533874511719\n",
      "epoch 24, train_loss 1277.0936279296875\n",
      "epoch 24, val_loss 454.7532958984375\n",
      "epoch 25, train_loss 1277.0936279296875\n",
      "epoch 25, val_loss 454.75323486328125\n",
      "epoch 26, train_loss 1277.0936279296875\n",
      "epoch 26, val_loss 454.75323486328125\n",
      "epoch 27, train_loss 1277.0933837890625\n",
      "epoch 27, val_loss 454.75323486328125\n",
      "epoch 28, train_loss 1277.0933837890625\n",
      "epoch 28, val_loss 454.75311279296875\n",
      "epoch 29, train_loss 1277.0933837890625\n",
      "epoch 29, val_loss 454.7530822753906\n",
      "epoch 30, train_loss 1277.0933837890625\n",
      "epoch 30, val_loss 454.7530822753906\n",
      "epoch 31, train_loss 1277.0933837890625\n",
      "epoch 31, val_loss 454.7530517578125\n",
      "epoch 32, train_loss 1277.0933837890625\n",
      "epoch 32, val_loss 454.7529602050781\n",
      "epoch 33, train_loss 1277.0933837890625\n",
      "epoch 33, val_loss 454.7529296875\n",
      "epoch 34, train_loss 1277.0931396484375\n",
      "epoch 34, val_loss 454.7529296875\n",
      "epoch 35, train_loss 1277.0931396484375\n",
      "epoch 35, val_loss 454.7529296875\n",
      "epoch 36, train_loss 1277.0931396484375\n",
      "epoch 36, val_loss 454.7528381347656\n",
      "epoch 37, train_loss 1277.09326171875\n",
      "epoch 37, val_loss 454.7527770996094\n",
      "epoch 38, train_loss 1277.09326171875\n",
      "epoch 38, val_loss 454.7527770996094\n",
      "epoch 39, train_loss 1277.09326171875\n",
      "epoch 39, val_loss 454.75274658203125\n",
      "epoch 40, train_loss 1277.0931396484375\n",
      "epoch 40, val_loss 454.7527160644531\n",
      "epoch 41, train_loss 1277.0931396484375\n",
      "epoch 41, val_loss 454.75262451171875\n",
      "epoch 42, train_loss 1277.093017578125\n",
      "epoch 42, val_loss 454.75262451171875\n",
      "epoch 43, train_loss 1277.093017578125\n",
      "epoch 43, val_loss 454.7525329589844\n",
      "epoch 44, train_loss 1277.093017578125\n",
      "epoch 44, val_loss 454.7524719238281\n",
      "epoch 45, train_loss 1277.0927734375\n",
      "epoch 45, val_loss 454.7524719238281\n",
      "epoch 46, train_loss 1277.0928955078125\n",
      "epoch 46, val_loss 454.7524719238281\n",
      "epoch 47, train_loss 1277.0927734375\n",
      "epoch 47, val_loss 454.7524719238281\n",
      "epoch 48, train_loss 1277.0927734375\n",
      "epoch 48, val_loss 454.7524108886719\n",
      "epoch 49, train_loss 1277.0927734375\n",
      "epoch 49, val_loss 454.75238037109375\n",
      "epoch 50, train_loss 1277.0927734375\n",
      "epoch 50, val_loss 454.7523193359375\n",
      "epoch 51, train_loss 1277.092529296875\n",
      "epoch 51, val_loss 454.7522888183594\n",
      "epoch 52, train_loss 1277.0926513671875\n",
      "epoch 52, val_loss 454.75225830078125\n",
      "epoch 53, train_loss 1277.092529296875\n",
      "epoch 53, val_loss 454.7521667480469\n",
      "epoch 54, train_loss 1277.092529296875\n",
      "epoch 54, val_loss 454.7521667480469\n",
      "epoch 55, train_loss 1277.0926513671875\n",
      "epoch 55, val_loss 454.75213623046875\n",
      "epoch 56, train_loss 1277.0926513671875\n",
      "epoch 56, val_loss 454.75213623046875\n",
      "epoch 57, train_loss 1277.092529296875\n",
      "epoch 57, val_loss 454.7520446777344\n",
      "epoch 58, train_loss 1277.0924072265625\n",
      "epoch 58, val_loss 454.75201416015625\n",
      "epoch 59, train_loss 1277.0924072265625\n",
      "epoch 59, val_loss 454.75201416015625\n",
      "epoch 60, train_loss 1277.0924072265625\n",
      "epoch 60, val_loss 454.751953125\n",
      "epoch 61, train_loss 1277.0924072265625\n",
      "epoch 61, val_loss 454.7518615722656\n",
      "epoch 62, train_loss 1277.09228515625\n",
      "epoch 62, val_loss 454.7518615722656\n",
      "epoch 63, train_loss 1277.0921630859375\n",
      "epoch 63, val_loss 454.7518310546875\n",
      "epoch 64, train_loss 1277.0921630859375\n",
      "epoch 64, val_loss 454.7518005371094\n",
      "epoch 65, train_loss 1277.0921630859375\n",
      "epoch 65, val_loss 454.7517395019531\n",
      "epoch 66, train_loss 1277.0921630859375\n",
      "epoch 66, val_loss 454.7517395019531\n",
      "epoch 67, train_loss 1277.092041015625\n",
      "epoch 67, val_loss 454.7516784667969\n",
      "epoch 68, train_loss 1277.092041015625\n",
      "epoch 68, val_loss 454.7516784667969\n",
      "epoch 69, train_loss 1277.092041015625\n",
      "epoch 69, val_loss 454.7516784667969\n",
      "epoch 70, train_loss 1277.092041015625\n",
      "epoch 70, val_loss 454.7515869140625\n",
      "epoch 71, train_loss 1277.092041015625\n",
      "epoch 71, val_loss 454.7515563964844\n",
      "epoch 72, train_loss 1277.092041015625\n",
      "epoch 72, val_loss 454.7514953613281\n",
      "epoch 73, train_loss 1277.092041015625\n",
      "epoch 73, val_loss 454.75140380859375\n",
      "epoch 74, train_loss 1277.0919189453125\n",
      "epoch 74, val_loss 454.7513732910156\n",
      "epoch 75, train_loss 1277.0919189453125\n",
      "epoch 75, val_loss 454.7513427734375\n",
      "epoch 76, train_loss 1277.0919189453125\n",
      "epoch 76, val_loss 454.7513427734375\n",
      "epoch 77, train_loss 1277.091796875\n",
      "epoch 77, val_loss 454.7513427734375\n",
      "epoch 78, train_loss 1277.0916748046875\n",
      "epoch 78, val_loss 454.7512512207031\n",
      "epoch 79, train_loss 1277.091796875\n",
      "epoch 79, val_loss 454.7512512207031\n",
      "epoch 80, train_loss 1277.0916748046875\n",
      "epoch 80, val_loss 454.751220703125\n",
      "epoch 81, train_loss 1277.091552734375\n",
      "epoch 81, val_loss 454.751220703125\n",
      "epoch 82, train_loss 1277.091552734375\n",
      "epoch 82, val_loss 454.7511291503906\n",
      "epoch 83, train_loss 1277.091552734375\n",
      "epoch 83, val_loss 454.7510681152344\n",
      "epoch 84, train_loss 1277.091796875\n",
      "epoch 84, val_loss 454.75103759765625\n",
      "epoch 85, train_loss 1277.091796875\n",
      "epoch 85, val_loss 454.7510070800781\n",
      "epoch 86, train_loss 1277.091552734375\n",
      "epoch 86, val_loss 454.7509460449219\n",
      "epoch 87, train_loss 1277.091552734375\n",
      "epoch 87, val_loss 454.75091552734375\n",
      "epoch 88, train_loss 1277.091552734375\n",
      "epoch 88, val_loss 454.75091552734375\n",
      "epoch 89, train_loss 1277.091552734375\n",
      "epoch 89, val_loss 454.7508239746094\n",
      "epoch 90, train_loss 1277.0914306640625\n",
      "epoch 90, val_loss 454.7508239746094\n",
      "epoch 91, train_loss 1277.09130859375\n",
      "epoch 91, val_loss 454.7507629394531\n",
      "epoch 92, train_loss 1277.09130859375\n",
      "epoch 92, val_loss 454.7507629394531\n",
      "epoch 93, train_loss 1277.09130859375\n",
      "epoch 93, val_loss 454.7507019042969\n",
      "epoch 94, train_loss 1277.09130859375\n",
      "epoch 94, val_loss 454.75067138671875\n",
      "epoch 95, train_loss 1277.09130859375\n",
      "epoch 95, val_loss 454.7506103515625\n",
      "epoch 96, train_loss 1277.0911865234375\n",
      "epoch 96, val_loss 454.7506103515625\n",
      "epoch 97, train_loss 1277.0911865234375\n",
      "epoch 97, val_loss 454.75054931640625\n",
      "epoch 98, train_loss 1277.0911865234375\n",
      "epoch 98, val_loss 454.75054931640625\n",
      "epoch 99, train_loss 1277.091064453125\n",
      "epoch 99, val_loss 454.75048828125\n",
      "Parameter containing:\n",
      "tensor([1.4972e-27], requires_grad=True)\n",
      "iter 149, train_loss_regularization 0.6966561079025269\n",
      "iter 149, val_loss_regularization 0.6966561079025269\n",
      "epoch 0, train_loss 1277.091064453125\n",
      "epoch 0, val_loss 454.7504577636719\n",
      "epoch 1, train_loss 1277.091064453125\n",
      "epoch 1, val_loss 454.7504577636719\n",
      "epoch 2, train_loss 1277.0911865234375\n",
      "epoch 2, val_loss 454.7503662109375\n",
      "epoch 3, train_loss 1277.0909423828125\n",
      "epoch 3, val_loss 454.7503356933594\n",
      "epoch 4, train_loss 1277.0909423828125\n",
      "epoch 4, val_loss 454.7503356933594\n",
      "epoch 5, train_loss 1277.0908203125\n",
      "epoch 5, val_loss 454.750244140625\n",
      "epoch 6, train_loss 1277.0908203125\n",
      "epoch 6, val_loss 454.7502136230469\n",
      "epoch 7, train_loss 1277.0906982421875\n",
      "epoch 7, val_loss 454.7502136230469\n",
      "epoch 8, train_loss 1277.0908203125\n",
      "epoch 8, val_loss 454.7501220703125\n",
      "epoch 9, train_loss 1277.0908203125\n",
      "epoch 9, val_loss 454.7501220703125\n",
      "epoch 10, train_loss 1277.0906982421875\n",
      "epoch 10, val_loss 454.7501220703125\n",
      "epoch 11, train_loss 1277.0906982421875\n",
      "epoch 11, val_loss 454.7500305175781\n",
      "epoch 12, train_loss 1277.0906982421875\n",
      "epoch 12, val_loss 454.7500305175781\n",
      "epoch 13, train_loss 1277.090576171875\n",
      "epoch 13, val_loss 454.7499694824219\n",
      "epoch 14, train_loss 1277.0904541015625\n",
      "epoch 14, val_loss 454.7499694824219\n",
      "epoch 15, train_loss 1277.090576171875\n",
      "epoch 15, val_loss 454.7498779296875\n",
      "epoch 16, train_loss 1277.0904541015625\n",
      "epoch 16, val_loss 454.7498474121094\n",
      "epoch 17, train_loss 1277.090576171875\n",
      "epoch 17, val_loss 454.7498474121094\n",
      "epoch 18, train_loss 1277.0904541015625\n",
      "epoch 18, val_loss 454.7498474121094\n",
      "epoch 19, train_loss 1277.0904541015625\n",
      "epoch 19, val_loss 454.749755859375\n",
      "epoch 20, train_loss 1277.0904541015625\n",
      "epoch 20, val_loss 454.7496643066406\n",
      "epoch 21, train_loss 1277.0902099609375\n",
      "epoch 21, val_loss 454.7496643066406\n",
      "epoch 22, train_loss 1277.0902099609375\n",
      "epoch 22, val_loss 454.7496337890625\n",
      "epoch 23, train_loss 1277.0902099609375\n",
      "epoch 23, val_loss 454.74957275390625\n",
      "epoch 24, train_loss 1277.09033203125\n",
      "epoch 24, val_loss 454.7495422363281\n",
      "epoch 25, train_loss 1277.0902099609375\n",
      "epoch 25, val_loss 454.74951171875\n",
      "epoch 26, train_loss 1277.0902099609375\n",
      "epoch 26, val_loss 454.74951171875\n",
      "epoch 27, train_loss 1277.0902099609375\n",
      "epoch 27, val_loss 454.7494201660156\n",
      "epoch 28, train_loss 1277.0902099609375\n",
      "epoch 28, val_loss 454.74932861328125\n",
      "epoch 29, train_loss 1277.090087890625\n",
      "epoch 29, val_loss 454.74932861328125\n",
      "epoch 30, train_loss 1277.0899658203125\n",
      "epoch 30, val_loss 454.74932861328125\n",
      "epoch 31, train_loss 1277.0899658203125\n",
      "epoch 31, val_loss 454.74932861328125\n",
      "epoch 32, train_loss 1277.0899658203125\n",
      "epoch 32, val_loss 454.7492370605469\n",
      "epoch 33, train_loss 1277.090087890625\n",
      "epoch 33, val_loss 454.74920654296875\n",
      "epoch 34, train_loss 1277.0899658203125\n",
      "epoch 34, val_loss 454.7492370605469\n",
      "epoch 35, train_loss 1277.0899658203125\n",
      "epoch 35, val_loss 454.7491760253906\n",
      "epoch 36, train_loss 1277.0899658203125\n",
      "epoch 36, val_loss 454.74908447265625\n",
      "epoch 37, train_loss 1277.0899658203125\n",
      "epoch 37, val_loss 454.7490539550781\n",
      "epoch 38, train_loss 1277.0897216796875\n",
      "epoch 38, val_loss 454.7490539550781\n",
      "epoch 39, train_loss 1277.08984375\n",
      "epoch 39, val_loss 454.7489929199219\n",
      "epoch 40, train_loss 1277.0897216796875\n",
      "epoch 40, val_loss 454.74896240234375\n",
      "epoch 41, train_loss 1277.0897216796875\n",
      "epoch 41, val_loss 454.7489318847656\n",
      "epoch 42, train_loss 1277.0897216796875\n",
      "epoch 42, val_loss 454.7488708496094\n",
      "epoch 43, train_loss 1277.0897216796875\n",
      "epoch 43, val_loss 454.7488708496094\n",
      "epoch 44, train_loss 1277.0897216796875\n",
      "epoch 44, val_loss 454.7488708496094\n",
      "epoch 45, train_loss 1277.089599609375\n",
      "epoch 45, val_loss 454.7487487792969\n",
      "epoch 46, train_loss 1277.0894775390625\n",
      "epoch 46, val_loss 454.7487487792969\n",
      "epoch 47, train_loss 1277.0894775390625\n",
      "epoch 47, val_loss 454.7487487792969\n",
      "epoch 48, train_loss 1277.0894775390625\n",
      "epoch 48, val_loss 454.74871826171875\n",
      "epoch 49, train_loss 1277.0894775390625\n",
      "epoch 49, val_loss 454.7486267089844\n",
      "epoch 50, train_loss 1277.0894775390625\n",
      "epoch 50, val_loss 454.74859619140625\n",
      "epoch 51, train_loss 1277.0894775390625\n",
      "epoch 51, val_loss 454.74859619140625\n",
      "epoch 52, train_loss 1277.08935546875\n",
      "epoch 52, val_loss 454.7484436035156\n",
      "epoch 53, train_loss 1277.0894775390625\n",
      "epoch 53, val_loss 454.7484436035156\n",
      "epoch 54, train_loss 1277.08935546875\n",
      "epoch 54, val_loss 454.7484130859375\n",
      "epoch 55, train_loss 1277.0892333984375\n",
      "epoch 55, val_loss 454.7484130859375\n",
      "epoch 56, train_loss 1277.089111328125\n",
      "epoch 56, val_loss 454.7483825683594\n",
      "epoch 57, train_loss 1277.089111328125\n",
      "epoch 57, val_loss 454.7483825683594\n",
      "epoch 58, train_loss 1277.089111328125\n",
      "epoch 58, val_loss 454.7482604980469\n",
      "epoch 59, train_loss 1277.089111328125\n",
      "epoch 59, val_loss 454.7481994628906\n",
      "epoch 60, train_loss 1277.089111328125\n",
      "epoch 60, val_loss 454.7482604980469\n",
      "epoch 61, train_loss 1277.089111328125\n",
      "epoch 61, val_loss 454.7481994628906\n",
      "epoch 62, train_loss 1277.089111328125\n",
      "epoch 62, val_loss 454.7481384277344\n",
      "epoch 63, train_loss 1277.0888671875\n",
      "epoch 63, val_loss 454.7481384277344\n",
      "epoch 64, train_loss 1277.0888671875\n",
      "epoch 64, val_loss 454.7480773925781\n",
      "epoch 65, train_loss 1277.0887451171875\n",
      "epoch 65, val_loss 454.7480773925781\n",
      "epoch 66, train_loss 1277.0887451171875\n",
      "epoch 66, val_loss 454.74798583984375\n",
      "epoch 67, train_loss 1277.0888671875\n",
      "epoch 67, val_loss 454.74798583984375\n",
      "epoch 68, train_loss 1277.0889892578125\n",
      "epoch 68, val_loss 454.7479553222656\n",
      "epoch 69, train_loss 1277.0888671875\n",
      "epoch 69, val_loss 454.7479248046875\n",
      "epoch 70, train_loss 1277.0888671875\n",
      "epoch 70, val_loss 454.74786376953125\n",
      "epoch 71, train_loss 1277.0887451171875\n",
      "epoch 71, val_loss 454.747802734375\n",
      "epoch 72, train_loss 1277.088623046875\n",
      "epoch 72, val_loss 454.7477111816406\n",
      "epoch 73, train_loss 1277.0887451171875\n",
      "epoch 73, val_loss 454.74774169921875\n",
      "epoch 74, train_loss 1277.0887451171875\n",
      "epoch 74, val_loss 454.7477111816406\n",
      "epoch 75, train_loss 1277.088623046875\n",
      "epoch 75, val_loss 454.7476806640625\n",
      "epoch 76, train_loss 1277.088623046875\n",
      "epoch 76, val_loss 454.7475891113281\n",
      "epoch 77, train_loss 1277.088623046875\n",
      "epoch 77, val_loss 454.7475891113281\n",
      "epoch 78, train_loss 1277.088623046875\n",
      "epoch 78, val_loss 454.7475891113281\n",
      "epoch 79, train_loss 1277.08837890625\n",
      "epoch 79, val_loss 454.7475280761719\n",
      "epoch 80, train_loss 1277.08837890625\n",
      "epoch 80, val_loss 454.7474670410156\n",
      "epoch 81, train_loss 1277.0882568359375\n",
      "epoch 81, val_loss 454.7474670410156\n",
      "epoch 82, train_loss 1277.0882568359375\n",
      "epoch 82, val_loss 454.7474060058594\n",
      "epoch 83, train_loss 1277.08837890625\n",
      "epoch 83, val_loss 454.74737548828125\n",
      "epoch 84, train_loss 1277.08837890625\n",
      "epoch 84, val_loss 454.74725341796875\n",
      "epoch 85, train_loss 1277.08837890625\n",
      "epoch 85, val_loss 454.7472839355469\n",
      "epoch 86, train_loss 1277.08837890625\n",
      "epoch 86, val_loss 454.74725341796875\n",
      "epoch 87, train_loss 1277.0882568359375\n",
      "epoch 87, val_loss 454.74725341796875\n",
      "epoch 88, train_loss 1277.0882568359375\n",
      "epoch 88, val_loss 454.7472229003906\n",
      "epoch 89, train_loss 1277.088134765625\n",
      "epoch 89, val_loss 454.74713134765625\n",
      "epoch 90, train_loss 1277.088134765625\n",
      "epoch 90, val_loss 454.7471618652344\n",
      "epoch 91, train_loss 1277.088134765625\n",
      "epoch 91, val_loss 454.7470703125\n",
      "epoch 92, train_loss 1277.088134765625\n",
      "epoch 92, val_loss 454.74700927734375\n",
      "epoch 93, train_loss 1277.0880126953125\n",
      "epoch 93, val_loss 454.7469482421875\n",
      "epoch 94, train_loss 1277.0880126953125\n",
      "epoch 94, val_loss 454.7469482421875\n",
      "epoch 95, train_loss 1277.087890625\n",
      "epoch 95, val_loss 454.7469482421875\n",
      "epoch 96, train_loss 1277.0877685546875\n",
      "epoch 96, val_loss 454.74688720703125\n",
      "epoch 97, train_loss 1277.0877685546875\n",
      "epoch 97, val_loss 454.746826171875\n",
      "epoch 98, train_loss 1277.0877685546875\n",
      "epoch 98, val_loss 454.7467956542969\n",
      "epoch 99, train_loss 1277.087646484375\n",
      "epoch 99, val_loss 454.7467956542969\n",
      "Parameter containing:\n",
      "tensor([1.0434e-27], requires_grad=True)\n",
      "iter 150, train_loss_regularization 0.6960465908050537\n",
      "iter 150, val_loss_regularization 0.6960465908050537\n",
      "epoch 0, train_loss 1277.087646484375\n",
      "epoch 0, val_loss 454.7467956542969\n",
      "epoch 1, train_loss 1277.087890625\n",
      "epoch 1, val_loss 454.7467041015625\n",
      "epoch 2, train_loss 1277.087890625\n",
      "epoch 2, val_loss 454.7466125488281\n",
      "epoch 3, train_loss 1277.0877685546875\n",
      "epoch 3, val_loss 454.7466125488281\n",
      "epoch 4, train_loss 1277.0877685546875\n",
      "epoch 4, val_loss 454.74658203125\n",
      "epoch 5, train_loss 1277.0875244140625\n",
      "epoch 5, val_loss 454.7464904785156\n",
      "epoch 6, train_loss 1277.0875244140625\n",
      "epoch 6, val_loss 454.7464904785156\n",
      "epoch 7, train_loss 1277.0875244140625\n",
      "epoch 7, val_loss 454.7464904785156\n",
      "epoch 8, train_loss 1277.0875244140625\n",
      "epoch 8, val_loss 454.7464904785156\n",
      "epoch 9, train_loss 1277.0875244140625\n",
      "epoch 9, val_loss 454.7464294433594\n",
      "epoch 10, train_loss 1277.0875244140625\n",
      "epoch 10, val_loss 454.7464294433594\n",
      "epoch 11, train_loss 1277.0872802734375\n",
      "epoch 11, val_loss 454.7464294433594\n",
      "epoch 12, train_loss 1277.08740234375\n",
      "epoch 12, val_loss 454.746337890625\n",
      "epoch 13, train_loss 1277.0872802734375\n",
      "epoch 13, val_loss 454.7462463378906\n",
      "epoch 14, train_loss 1277.0872802734375\n",
      "epoch 14, val_loss 454.74615478515625\n",
      "epoch 15, train_loss 1277.0872802734375\n",
      "epoch 15, val_loss 454.74615478515625\n",
      "epoch 16, train_loss 1277.0872802734375\n",
      "epoch 16, val_loss 454.74615478515625\n",
      "epoch 17, train_loss 1277.0872802734375\n",
      "epoch 17, val_loss 454.74609375\n",
      "epoch 18, train_loss 1277.0872802734375\n",
      "epoch 18, val_loss 454.74603271484375\n",
      "epoch 19, train_loss 1277.0872802734375\n",
      "epoch 19, val_loss 454.74603271484375\n",
      "epoch 20, train_loss 1277.0872802734375\n",
      "epoch 20, val_loss 454.7460021972656\n",
      "epoch 21, train_loss 1277.0872802734375\n",
      "epoch 21, val_loss 454.7460021972656\n",
      "epoch 22, train_loss 1277.087158203125\n",
      "epoch 22, val_loss 454.7459716796875\n",
      "epoch 23, train_loss 1277.0870361328125\n",
      "epoch 23, val_loss 454.74591064453125\n",
      "epoch 24, train_loss 1277.0870361328125\n",
      "epoch 24, val_loss 454.7458190917969\n",
      "epoch 25, train_loss 1277.0870361328125\n",
      "epoch 25, val_loss 454.7458801269531\n",
      "epoch 26, train_loss 1277.0869140625\n",
      "epoch 26, val_loss 454.74578857421875\n",
      "epoch 27, train_loss 1277.0867919921875\n",
      "epoch 27, val_loss 454.7457580566406\n",
      "epoch 28, train_loss 1277.0867919921875\n",
      "epoch 28, val_loss 454.74566650390625\n",
      "epoch 29, train_loss 1277.0867919921875\n",
      "epoch 29, val_loss 454.7456970214844\n",
      "epoch 30, train_loss 1277.0867919921875\n",
      "epoch 30, val_loss 454.7455749511719\n",
      "epoch 31, train_loss 1277.0867919921875\n",
      "epoch 31, val_loss 454.7456359863281\n",
      "epoch 32, train_loss 1277.0867919921875\n",
      "epoch 32, val_loss 454.7455749511719\n",
      "epoch 33, train_loss 1277.0867919921875\n",
      "epoch 33, val_loss 454.74554443359375\n",
      "epoch 34, train_loss 1277.0867919921875\n",
      "epoch 34, val_loss 454.7455139160156\n",
      "epoch 35, train_loss 1277.086669921875\n",
      "epoch 35, val_loss 454.74542236328125\n",
      "epoch 36, train_loss 1277.086669921875\n",
      "epoch 36, val_loss 454.74542236328125\n",
      "epoch 37, train_loss 1277.086669921875\n",
      "epoch 37, val_loss 454.7453308105469\n",
      "epoch 38, train_loss 1277.086669921875\n",
      "epoch 38, val_loss 454.745361328125\n",
      "epoch 39, train_loss 1277.0865478515625\n",
      "epoch 39, val_loss 454.74530029296875\n",
      "epoch 40, train_loss 1277.0865478515625\n",
      "epoch 40, val_loss 454.74530029296875\n",
      "epoch 41, train_loss 1277.08642578125\n",
      "epoch 41, val_loss 454.74530029296875\n",
      "epoch 42, train_loss 1277.08642578125\n",
      "epoch 42, val_loss 454.7452392578125\n",
      "epoch 43, train_loss 1277.0863037109375\n",
      "epoch 43, val_loss 454.74517822265625\n",
      "epoch 44, train_loss 1277.0863037109375\n",
      "epoch 44, val_loss 454.74517822265625\n",
      "epoch 45, train_loss 1277.0863037109375\n",
      "epoch 45, val_loss 454.7450866699219\n",
      "epoch 46, train_loss 1277.0863037109375\n",
      "epoch 46, val_loss 454.7450866699219\n",
      "epoch 47, train_loss 1277.0863037109375\n",
      "epoch 47, val_loss 454.7449645996094\n",
      "epoch 48, train_loss 1277.0860595703125\n",
      "epoch 48, val_loss 454.7449035644531\n",
      "epoch 49, train_loss 1277.0860595703125\n",
      "epoch 49, val_loss 454.7449035644531\n",
      "epoch 50, train_loss 1277.0863037109375\n",
      "epoch 50, val_loss 454.744873046875\n",
      "epoch 51, train_loss 1277.086181640625\n",
      "epoch 51, val_loss 454.7449035644531\n",
      "epoch 52, train_loss 1277.086181640625\n",
      "epoch 52, val_loss 454.7448425292969\n",
      "epoch 53, train_loss 1277.0860595703125\n",
      "epoch 53, val_loss 454.7448425292969\n",
      "epoch 54, train_loss 1277.0860595703125\n",
      "epoch 54, val_loss 454.7447814941406\n",
      "epoch 55, train_loss 1277.086181640625\n",
      "epoch 55, val_loss 454.7447509765625\n",
      "epoch 56, train_loss 1277.0859375\n",
      "epoch 56, val_loss 454.7447204589844\n",
      "epoch 57, train_loss 1277.0859375\n",
      "epoch 57, val_loss 454.74462890625\n",
      "epoch 58, train_loss 1277.0859375\n",
      "epoch 58, val_loss 454.7445983886719\n",
      "epoch 59, train_loss 1277.0859375\n",
      "epoch 59, val_loss 454.7445983886719\n",
      "epoch 60, train_loss 1277.085693359375\n",
      "epoch 60, val_loss 454.7445068359375\n",
      "epoch 61, train_loss 1277.0858154296875\n",
      "epoch 61, val_loss 454.7445068359375\n",
      "epoch 62, train_loss 1277.0858154296875\n",
      "epoch 62, val_loss 454.74444580078125\n",
      "epoch 63, train_loss 1277.0858154296875\n",
      "epoch 63, val_loss 454.7444152832031\n",
      "epoch 64, train_loss 1277.0858154296875\n",
      "epoch 64, val_loss 454.744384765625\n",
      "epoch 65, train_loss 1277.0855712890625\n",
      "epoch 65, val_loss 454.74432373046875\n",
      "epoch 66, train_loss 1277.085693359375\n",
      "epoch 66, val_loss 454.74432373046875\n",
      "epoch 67, train_loss 1277.085693359375\n",
      "epoch 67, val_loss 454.7442932128906\n",
      "epoch 68, train_loss 1277.0855712890625\n",
      "epoch 68, val_loss 454.7442626953125\n",
      "epoch 69, train_loss 1277.08544921875\n",
      "epoch 69, val_loss 454.74420166015625\n",
      "epoch 70, train_loss 1277.08544921875\n",
      "epoch 70, val_loss 454.7441711425781\n",
      "epoch 71, train_loss 1277.08544921875\n",
      "epoch 71, val_loss 454.7441101074219\n",
      "epoch 72, train_loss 1277.08544921875\n",
      "epoch 72, val_loss 454.7441101074219\n",
      "epoch 73, train_loss 1277.08544921875\n",
      "epoch 73, val_loss 454.7440490722656\n",
      "epoch 74, train_loss 1277.08544921875\n",
      "epoch 74, val_loss 454.7439880371094\n",
      "epoch 75, train_loss 1277.0853271484375\n",
      "epoch 75, val_loss 454.7439880371094\n",
      "epoch 76, train_loss 1277.0853271484375\n",
      "epoch 76, val_loss 454.74395751953125\n",
      "epoch 77, train_loss 1277.085205078125\n",
      "epoch 77, val_loss 454.7439270019531\n",
      "epoch 78, train_loss 1277.085205078125\n",
      "epoch 78, val_loss 454.7438659667969\n",
      "epoch 79, train_loss 1277.085205078125\n",
      "epoch 79, val_loss 454.7438049316406\n",
      "epoch 80, train_loss 1277.085205078125\n",
      "epoch 80, val_loss 454.7438049316406\n",
      "epoch 81, train_loss 1277.085205078125\n",
      "epoch 81, val_loss 454.7438049316406\n",
      "epoch 82, train_loss 1277.085205078125\n",
      "epoch 82, val_loss 454.74371337890625\n",
      "epoch 83, train_loss 1277.0849609375\n",
      "epoch 83, val_loss 454.74371337890625\n",
      "epoch 84, train_loss 1277.0850830078125\n",
      "epoch 84, val_loss 454.74365234375\n",
      "epoch 85, train_loss 1277.0849609375\n",
      "epoch 85, val_loss 454.74359130859375\n",
      "epoch 86, train_loss 1277.0849609375\n",
      "epoch 86, val_loss 454.74359130859375\n",
      "epoch 87, train_loss 1277.0849609375\n",
      "epoch 87, val_loss 454.74359130859375\n",
      "epoch 88, train_loss 1277.0848388671875\n",
      "epoch 88, val_loss 454.7435302734375\n",
      "epoch 89, train_loss 1277.0849609375\n",
      "epoch 89, val_loss 454.7434997558594\n",
      "epoch 90, train_loss 1277.0849609375\n",
      "epoch 90, val_loss 454.743408203125\n",
      "epoch 91, train_loss 1277.0848388671875\n",
      "epoch 91, val_loss 454.74334716796875\n",
      "epoch 92, train_loss 1277.0848388671875\n",
      "epoch 92, val_loss 454.743408203125\n",
      "epoch 93, train_loss 1277.0848388671875\n",
      "epoch 93, val_loss 454.7432861328125\n",
      "epoch 94, train_loss 1277.084716796875\n",
      "epoch 94, val_loss 454.74334716796875\n",
      "epoch 95, train_loss 1277.084716796875\n",
      "epoch 95, val_loss 454.7432556152344\n",
      "epoch 96, train_loss 1277.084716796875\n",
      "epoch 96, val_loss 454.7431945800781\n",
      "epoch 97, train_loss 1277.0845947265625\n",
      "epoch 97, val_loss 454.7431640625\n",
      "epoch 98, train_loss 1277.084716796875\n",
      "epoch 98, val_loss 454.7431640625\n",
      "epoch 99, train_loss 1277.0849609375\n",
      "epoch 99, val_loss 454.7431335449219\n",
      "Parameter containing:\n",
      "tensor([7.2728e-28], requires_grad=True)\n",
      "iter 151, train_loss_regularization 0.6954550743103027\n",
      "iter 151, val_loss_regularization 0.6954550743103027\n",
      "epoch 0, train_loss 1277.0845947265625\n",
      "epoch 0, val_loss 454.7430725097656\n",
      "epoch 1, train_loss 1277.0845947265625\n",
      "epoch 1, val_loss 454.7430419921875\n",
      "epoch 2, train_loss 1277.0845947265625\n",
      "epoch 2, val_loss 454.7430114746094\n",
      "epoch 3, train_loss 1277.08447265625\n",
      "epoch 3, val_loss 454.7429504394531\n",
      "epoch 4, train_loss 1277.08447265625\n",
      "epoch 4, val_loss 454.7430114746094\n",
      "epoch 5, train_loss 1277.08447265625\n",
      "epoch 5, val_loss 454.7428894042969\n",
      "epoch 6, train_loss 1277.0843505859375\n",
      "epoch 6, val_loss 454.7428894042969\n",
      "epoch 7, train_loss 1277.08447265625\n",
      "epoch 7, val_loss 454.74273681640625\n",
      "epoch 8, train_loss 1277.084228515625\n",
      "epoch 8, val_loss 454.7428283691406\n",
      "epoch 9, train_loss 1277.0843505859375\n",
      "epoch 9, val_loss 454.74273681640625\n",
      "epoch 10, train_loss 1277.0843505859375\n",
      "epoch 10, val_loss 454.7427062988281\n",
      "epoch 11, train_loss 1277.0841064453125\n",
      "epoch 11, val_loss 454.74267578125\n",
      "epoch 12, train_loss 1277.0841064453125\n",
      "epoch 12, val_loss 454.74261474609375\n",
      "epoch 13, train_loss 1277.0841064453125\n",
      "epoch 13, val_loss 454.74261474609375\n",
      "epoch 14, train_loss 1277.0841064453125\n",
      "epoch 14, val_loss 454.7425537109375\n",
      "epoch 15, train_loss 1277.0841064453125\n",
      "epoch 15, val_loss 454.7425537109375\n",
      "epoch 16, train_loss 1277.0841064453125\n",
      "epoch 16, val_loss 454.7424621582031\n",
      "epoch 17, train_loss 1277.0841064453125\n",
      "epoch 17, val_loss 454.7424621582031\n",
      "epoch 18, train_loss 1277.083984375\n",
      "epoch 18, val_loss 454.742431640625\n",
      "epoch 19, train_loss 1277.0838623046875\n",
      "epoch 19, val_loss 454.74237060546875\n",
      "epoch 20, train_loss 1277.0838623046875\n",
      "epoch 20, val_loss 454.7423400878906\n",
      "epoch 21, train_loss 1277.0838623046875\n",
      "epoch 21, val_loss 454.74237060546875\n",
      "epoch 22, train_loss 1277.0838623046875\n",
      "epoch 22, val_loss 454.7423400878906\n",
      "epoch 23, train_loss 1277.0838623046875\n",
      "epoch 23, val_loss 454.74224853515625\n",
      "epoch 24, train_loss 1277.0838623046875\n",
      "epoch 24, val_loss 454.74212646484375\n",
      "epoch 25, train_loss 1277.083740234375\n",
      "epoch 25, val_loss 454.74212646484375\n",
      "epoch 26, train_loss 1277.0836181640625\n",
      "epoch 26, val_loss 454.7420959472656\n",
      "epoch 27, train_loss 1277.0836181640625\n",
      "epoch 27, val_loss 454.7420959472656\n",
      "epoch 28, train_loss 1277.0836181640625\n",
      "epoch 28, val_loss 454.7420959472656\n",
      "epoch 29, train_loss 1277.0836181640625\n",
      "epoch 29, val_loss 454.74200439453125\n",
      "epoch 30, train_loss 1277.0836181640625\n",
      "epoch 30, val_loss 454.74200439453125\n",
      "epoch 31, train_loss 1277.08349609375\n",
      "epoch 31, val_loss 454.741943359375\n",
      "epoch 32, train_loss 1277.08349609375\n",
      "epoch 32, val_loss 454.7419128417969\n",
      "epoch 33, train_loss 1277.0836181640625\n",
      "epoch 33, val_loss 454.74188232421875\n",
      "epoch 34, train_loss 1277.0833740234375\n",
      "epoch 34, val_loss 454.7417907714844\n",
      "epoch 35, train_loss 1277.0833740234375\n",
      "epoch 35, val_loss 454.7417907714844\n",
      "epoch 36, train_loss 1277.0833740234375\n",
      "epoch 36, val_loss 454.7417907714844\n",
      "epoch 37, train_loss 1277.0833740234375\n",
      "epoch 37, val_loss 454.74169921875\n",
      "epoch 38, train_loss 1277.0833740234375\n",
      "epoch 38, val_loss 454.74169921875\n",
      "epoch 39, train_loss 1277.0833740234375\n",
      "epoch 39, val_loss 454.7416687011719\n",
      "epoch 40, train_loss 1277.0833740234375\n",
      "epoch 40, val_loss 454.74163818359375\n",
      "epoch 41, train_loss 1277.0833740234375\n",
      "epoch 41, val_loss 454.7415466308594\n",
      "epoch 42, train_loss 1277.0833740234375\n",
      "epoch 42, val_loss 454.7415466308594\n",
      "epoch 43, train_loss 1277.0831298828125\n",
      "epoch 43, val_loss 454.7415466308594\n",
      "epoch 44, train_loss 1277.0831298828125\n",
      "epoch 44, val_loss 454.7414855957031\n",
      "epoch 45, train_loss 1277.0831298828125\n",
      "epoch 45, val_loss 454.741455078125\n",
      "epoch 46, train_loss 1277.0831298828125\n",
      "epoch 46, val_loss 454.7413635253906\n",
      "epoch 47, train_loss 1277.0831298828125\n",
      "epoch 47, val_loss 454.7413330078125\n",
      "epoch 48, train_loss 1277.0831298828125\n",
      "epoch 48, val_loss 454.7413635253906\n",
      "epoch 49, train_loss 1277.0831298828125\n",
      "epoch 49, val_loss 454.7413330078125\n",
      "epoch 50, train_loss 1277.0830078125\n",
      "epoch 50, val_loss 454.7412414550781\n",
      "epoch 51, train_loss 1277.0830078125\n",
      "epoch 51, val_loss 454.7412414550781\n",
      "epoch 52, train_loss 1277.0828857421875\n",
      "epoch 52, val_loss 454.7412109375\n",
      "epoch 53, train_loss 1277.0828857421875\n",
      "epoch 53, val_loss 454.7412109375\n",
      "epoch 54, train_loss 1277.0828857421875\n",
      "epoch 54, val_loss 454.7410888671875\n",
      "epoch 55, train_loss 1277.0828857421875\n",
      "epoch 55, val_loss 454.7409973144531\n",
      "epoch 56, train_loss 1277.0828857421875\n",
      "epoch 56, val_loss 454.7409973144531\n",
      "epoch 57, train_loss 1277.0828857421875\n",
      "epoch 57, val_loss 454.740966796875\n",
      "epoch 58, train_loss 1277.0828857421875\n",
      "epoch 58, val_loss 454.7409973144531\n",
      "epoch 59, train_loss 1277.082763671875\n",
      "epoch 59, val_loss 454.7408752441406\n",
      "epoch 60, train_loss 1277.08251953125\n",
      "epoch 60, val_loss 454.7408752441406\n",
      "epoch 61, train_loss 1277.08251953125\n",
      "epoch 61, val_loss 454.74090576171875\n",
      "epoch 62, train_loss 1277.08251953125\n",
      "epoch 62, val_loss 454.7408447265625\n",
      "epoch 63, train_loss 1277.08251953125\n",
      "epoch 63, val_loss 454.74078369140625\n",
      "epoch 64, train_loss 1277.0826416015625\n",
      "epoch 64, val_loss 454.7407531738281\n",
      "epoch 65, train_loss 1277.0826416015625\n",
      "epoch 65, val_loss 454.7407531738281\n",
      "epoch 66, train_loss 1277.08251953125\n",
      "epoch 66, val_loss 454.74072265625\n",
      "epoch 67, train_loss 1277.0823974609375\n",
      "epoch 67, val_loss 454.7406311035156\n",
      "epoch 68, train_loss 1277.0823974609375\n",
      "epoch 68, val_loss 454.7405700683594\n",
      "epoch 69, train_loss 1277.0823974609375\n",
      "epoch 69, val_loss 454.74053955078125\n",
      "epoch 70, train_loss 1277.082275390625\n",
      "epoch 70, val_loss 454.7405090332031\n",
      "epoch 71, train_loss 1277.082275390625\n",
      "epoch 71, val_loss 454.7405090332031\n",
      "epoch 72, train_loss 1277.0823974609375\n",
      "epoch 72, val_loss 454.74041748046875\n",
      "epoch 73, train_loss 1277.082275390625\n",
      "epoch 73, val_loss 454.7403869628906\n",
      "epoch 74, train_loss 1277.0821533203125\n",
      "epoch 74, val_loss 454.74041748046875\n",
      "epoch 75, train_loss 1277.0821533203125\n",
      "epoch 75, val_loss 454.74041748046875\n",
      "epoch 76, train_loss 1277.0821533203125\n",
      "epoch 76, val_loss 454.74029541015625\n",
      "epoch 77, train_loss 1277.08203125\n",
      "epoch 77, val_loss 454.7402648925781\n",
      "epoch 78, train_loss 1277.08203125\n",
      "epoch 78, val_loss 454.7402648925781\n",
      "epoch 79, train_loss 1277.0819091796875\n",
      "epoch 79, val_loss 454.74017333984375\n",
      "epoch 80, train_loss 1277.08203125\n",
      "epoch 80, val_loss 454.74017333984375\n",
      "epoch 81, train_loss 1277.0819091796875\n",
      "epoch 81, val_loss 454.7401123046875\n",
      "epoch 82, train_loss 1277.0819091796875\n",
      "epoch 82, val_loss 454.7401123046875\n",
      "epoch 83, train_loss 1277.08203125\n",
      "epoch 83, val_loss 454.7400817871094\n",
      "epoch 84, train_loss 1277.0819091796875\n",
      "epoch 84, val_loss 454.74005126953125\n",
      "epoch 85, train_loss 1277.0819091796875\n",
      "epoch 85, val_loss 454.739990234375\n",
      "epoch 86, train_loss 1277.0819091796875\n",
      "epoch 86, val_loss 454.73992919921875\n",
      "epoch 87, train_loss 1277.081787109375\n",
      "epoch 87, val_loss 454.73992919921875\n",
      "epoch 88, train_loss 1277.0819091796875\n",
      "epoch 88, val_loss 454.7398681640625\n",
      "epoch 89, train_loss 1277.0819091796875\n",
      "epoch 89, val_loss 454.7397766113281\n",
      "epoch 90, train_loss 1277.081787109375\n",
      "epoch 90, val_loss 454.7397766113281\n",
      "epoch 91, train_loss 1277.0816650390625\n",
      "epoch 91, val_loss 454.73974609375\n",
      "epoch 92, train_loss 1277.0816650390625\n",
      "epoch 92, val_loss 454.73974609375\n",
      "epoch 93, train_loss 1277.08154296875\n",
      "epoch 93, val_loss 454.7397155761719\n",
      "epoch 94, train_loss 1277.08154296875\n",
      "epoch 94, val_loss 454.7396545410156\n",
      "epoch 95, train_loss 1277.08154296875\n",
      "epoch 95, val_loss 454.7396240234375\n",
      "epoch 96, train_loss 1277.08154296875\n",
      "epoch 96, val_loss 454.7395935058594\n",
      "epoch 97, train_loss 1277.08154296875\n",
      "epoch 97, val_loss 454.7395324707031\n",
      "epoch 98, train_loss 1277.0814208984375\n",
      "epoch 98, val_loss 454.739501953125\n",
      "epoch 99, train_loss 1277.08154296875\n",
      "epoch 99, val_loss 454.7394714355469\n",
      "Parameter containing:\n",
      "tensor([5.0704e-28], requires_grad=True)\n",
      "iter 152, train_loss_regularization 0.6948814988136292\n",
      "iter 152, val_loss_regularization 0.6948814988136292\n",
      "epoch 0, train_loss 1277.08154296875\n",
      "epoch 0, val_loss 454.7394714355469\n",
      "epoch 1, train_loss 1277.0814208984375\n",
      "epoch 1, val_loss 454.7394104003906\n",
      "epoch 2, train_loss 1277.0814208984375\n",
      "epoch 2, val_loss 454.7393798828125\n",
      "epoch 3, train_loss 1277.081298828125\n",
      "epoch 3, val_loss 454.73931884765625\n",
      "epoch 4, train_loss 1277.081298828125\n",
      "epoch 4, val_loss 454.7392883300781\n",
      "epoch 5, train_loss 1277.081298828125\n",
      "epoch 5, val_loss 454.7392578125\n",
      "epoch 6, train_loss 1277.0811767578125\n",
      "epoch 6, val_loss 454.73919677734375\n",
      "epoch 7, train_loss 1277.0811767578125\n",
      "epoch 7, val_loss 454.73919677734375\n",
      "epoch 8, train_loss 1277.0810546875\n",
      "epoch 8, val_loss 454.7391662597656\n",
      "epoch 9, train_loss 1277.0810546875\n",
      "epoch 9, val_loss 454.7391662597656\n",
      "epoch 10, train_loss 1277.0810546875\n",
      "epoch 10, val_loss 454.73907470703125\n",
      "epoch 11, train_loss 1277.0810546875\n",
      "epoch 11, val_loss 454.739013671875\n",
      "epoch 12, train_loss 1277.0810546875\n",
      "epoch 12, val_loss 454.739013671875\n",
      "epoch 13, train_loss 1277.0809326171875\n",
      "epoch 13, val_loss 454.73895263671875\n",
      "epoch 14, train_loss 1277.080810546875\n",
      "epoch 14, val_loss 454.73895263671875\n",
      "epoch 15, train_loss 1277.0809326171875\n",
      "epoch 15, val_loss 454.73895263671875\n",
      "epoch 16, train_loss 1277.080810546875\n",
      "epoch 16, val_loss 454.7388610839844\n",
      "epoch 17, train_loss 1277.080810546875\n",
      "epoch 17, val_loss 454.7388610839844\n",
      "epoch 18, train_loss 1277.0809326171875\n",
      "epoch 18, val_loss 454.7388610839844\n",
      "epoch 19, train_loss 1277.0809326171875\n",
      "epoch 19, val_loss 454.7387390136719\n",
      "epoch 20, train_loss 1277.080810546875\n",
      "epoch 20, val_loss 454.73870849609375\n",
      "epoch 21, train_loss 1277.080810546875\n",
      "epoch 21, val_loss 454.7386169433594\n",
      "epoch 22, train_loss 1277.08056640625\n",
      "epoch 22, val_loss 454.7386169433594\n",
      "epoch 23, train_loss 1277.08056640625\n",
      "epoch 23, val_loss 454.73858642578125\n",
      "epoch 24, train_loss 1277.08056640625\n",
      "epoch 24, val_loss 454.7385559082031\n",
      "epoch 25, train_loss 1277.08056640625\n",
      "epoch 25, val_loss 454.7385559082031\n",
      "epoch 26, train_loss 1277.08056640625\n",
      "epoch 26, val_loss 454.7384948730469\n",
      "epoch 27, train_loss 1277.0804443359375\n",
      "epoch 27, val_loss 454.7384948730469\n",
      "epoch 28, train_loss 1277.0804443359375\n",
      "epoch 28, val_loss 454.7384033203125\n",
      "epoch 29, train_loss 1277.0804443359375\n",
      "epoch 29, val_loss 454.7383728027344\n",
      "epoch 30, train_loss 1277.0804443359375\n",
      "epoch 30, val_loss 454.73834228515625\n",
      "epoch 31, train_loss 1277.0804443359375\n",
      "epoch 31, val_loss 454.7383728027344\n",
      "epoch 32, train_loss 1277.080322265625\n",
      "epoch 32, val_loss 454.73828125\n",
      "epoch 33, train_loss 1277.080322265625\n",
      "epoch 33, val_loss 454.7382507324219\n",
      "epoch 34, train_loss 1277.0804443359375\n",
      "epoch 34, val_loss 454.7381591796875\n",
      "epoch 35, train_loss 1277.0804443359375\n",
      "epoch 35, val_loss 454.73822021484375\n",
      "epoch 36, train_loss 1277.080322265625\n",
      "epoch 36, val_loss 454.73809814453125\n",
      "epoch 37, train_loss 1277.0802001953125\n",
      "epoch 37, val_loss 454.73809814453125\n",
      "epoch 38, train_loss 1277.0802001953125\n",
      "epoch 38, val_loss 454.73809814453125\n",
      "epoch 39, train_loss 1277.0802001953125\n",
      "epoch 39, val_loss 454.7380065917969\n",
      "epoch 40, train_loss 1277.0802001953125\n",
      "epoch 40, val_loss 454.7380065917969\n",
      "epoch 41, train_loss 1277.0802001953125\n",
      "epoch 41, val_loss 454.7380065917969\n",
      "epoch 42, train_loss 1277.080078125\n",
      "epoch 42, val_loss 454.7379150390625\n",
      "epoch 43, train_loss 1277.080078125\n",
      "epoch 43, val_loss 454.7379150390625\n",
      "epoch 44, train_loss 1277.0799560546875\n",
      "epoch 44, val_loss 454.7378845214844\n",
      "epoch 45, train_loss 1277.0799560546875\n",
      "epoch 45, val_loss 454.7378234863281\n",
      "epoch 46, train_loss 1277.0799560546875\n",
      "epoch 46, val_loss 454.73779296875\n",
      "epoch 47, train_loss 1277.0799560546875\n",
      "epoch 47, val_loss 454.73779296875\n",
      "epoch 48, train_loss 1277.0799560546875\n",
      "epoch 48, val_loss 454.73779296875\n",
      "epoch 49, train_loss 1277.079833984375\n",
      "epoch 49, val_loss 454.7377014160156\n",
      "epoch 50, train_loss 1277.0799560546875\n",
      "epoch 50, val_loss 454.7376708984375\n",
      "epoch 51, train_loss 1277.079833984375\n",
      "epoch 51, val_loss 454.7375793457031\n",
      "epoch 52, train_loss 1277.0797119140625\n",
      "epoch 52, val_loss 454.7375793457031\n",
      "epoch 53, train_loss 1277.0797119140625\n",
      "epoch 53, val_loss 454.737548828125\n",
      "epoch 54, train_loss 1277.0797119140625\n",
      "epoch 54, val_loss 454.7374572753906\n",
      "epoch 55, train_loss 1277.0797119140625\n",
      "epoch 55, val_loss 454.7374267578125\n",
      "epoch 56, train_loss 1277.0797119140625\n",
      "epoch 56, val_loss 454.73736572265625\n",
      "epoch 57, train_loss 1277.0797119140625\n",
      "epoch 57, val_loss 454.73736572265625\n",
      "epoch 58, train_loss 1277.07958984375\n",
      "epoch 58, val_loss 454.7373352050781\n",
      "epoch 59, train_loss 1277.0794677734375\n",
      "epoch 59, val_loss 454.7373046875\n",
      "epoch 60, train_loss 1277.0794677734375\n",
      "epoch 60, val_loss 454.7373046875\n",
      "epoch 61, train_loss 1277.0794677734375\n",
      "epoch 61, val_loss 454.7373046875\n",
      "epoch 62, train_loss 1277.0794677734375\n",
      "epoch 62, val_loss 454.7372131347656\n",
      "epoch 63, train_loss 1277.0794677734375\n",
      "epoch 63, val_loss 454.7371520996094\n",
      "epoch 64, train_loss 1277.079345703125\n",
      "epoch 64, val_loss 454.7371520996094\n",
      "epoch 65, train_loss 1277.079345703125\n",
      "epoch 65, val_loss 454.7370910644531\n",
      "epoch 66, train_loss 1277.079345703125\n",
      "epoch 66, val_loss 454.7370910644531\n",
      "epoch 67, train_loss 1277.0792236328125\n",
      "epoch 67, val_loss 454.73699951171875\n",
      "epoch 68, train_loss 1277.0792236328125\n",
      "epoch 68, val_loss 454.7369689941406\n",
      "epoch 69, train_loss 1277.0792236328125\n",
      "epoch 69, val_loss 454.7369689941406\n",
      "epoch 70, train_loss 1277.0792236328125\n",
      "epoch 70, val_loss 454.7369079589844\n",
      "epoch 71, train_loss 1277.0792236328125\n",
      "epoch 71, val_loss 454.73687744140625\n",
      "epoch 72, train_loss 1277.0792236328125\n",
      "epoch 72, val_loss 454.7368469238281\n",
      "epoch 73, train_loss 1277.0791015625\n",
      "epoch 73, val_loss 454.7368469238281\n",
      "epoch 74, train_loss 1277.0792236328125\n",
      "epoch 74, val_loss 454.7367858886719\n",
      "epoch 75, train_loss 1277.0789794921875\n",
      "epoch 75, val_loss 454.7367858886719\n",
      "epoch 76, train_loss 1277.0789794921875\n",
      "epoch 76, val_loss 454.73675537109375\n",
      "epoch 77, train_loss 1277.0789794921875\n",
      "epoch 77, val_loss 454.7366638183594\n",
      "epoch 78, train_loss 1277.0789794921875\n",
      "epoch 78, val_loss 454.7366638183594\n",
      "epoch 79, train_loss 1277.0789794921875\n",
      "epoch 79, val_loss 454.73663330078125\n",
      "epoch 80, train_loss 1277.078857421875\n",
      "epoch 80, val_loss 454.7365417480469\n",
      "epoch 81, train_loss 1277.078857421875\n",
      "epoch 81, val_loss 454.73651123046875\n",
      "epoch 82, train_loss 1277.0787353515625\n",
      "epoch 82, val_loss 454.73651123046875\n",
      "epoch 83, train_loss 1277.0787353515625\n",
      "epoch 83, val_loss 454.73651123046875\n",
      "epoch 84, train_loss 1277.07861328125\n",
      "epoch 84, val_loss 454.7364196777344\n",
      "epoch 85, train_loss 1277.0787353515625\n",
      "epoch 85, val_loss 454.73638916015625\n",
      "epoch 86, train_loss 1277.0787353515625\n",
      "epoch 86, val_loss 454.736328125\n",
      "epoch 87, train_loss 1277.0787353515625\n",
      "epoch 87, val_loss 454.7362976074219\n",
      "epoch 88, train_loss 1277.0787353515625\n",
      "epoch 88, val_loss 454.7362365722656\n",
      "epoch 89, train_loss 1277.07861328125\n",
      "epoch 89, val_loss 454.7362365722656\n",
      "epoch 90, train_loss 1277.0787353515625\n",
      "epoch 90, val_loss 454.7362060546875\n",
      "epoch 91, train_loss 1277.07861328125\n",
      "epoch 91, val_loss 454.7361755371094\n",
      "epoch 92, train_loss 1277.078369140625\n",
      "epoch 92, val_loss 454.7361145019531\n",
      "epoch 93, train_loss 1277.0784912109375\n",
      "epoch 93, val_loss 454.7361145019531\n",
      "epoch 94, train_loss 1277.078369140625\n",
      "epoch 94, val_loss 454.736083984375\n",
      "epoch 95, train_loss 1277.078369140625\n",
      "epoch 95, val_loss 454.7359924316406\n",
      "epoch 96, train_loss 1277.078369140625\n",
      "epoch 96, val_loss 454.7360534667969\n",
      "epoch 97, train_loss 1277.078369140625\n",
      "epoch 97, val_loss 454.7359313964844\n",
      "epoch 98, train_loss 1277.078369140625\n",
      "epoch 98, val_loss 454.7359313964844\n",
      "epoch 99, train_loss 1277.078369140625\n",
      "epoch 99, val_loss 454.73583984375\n",
      "Parameter containing:\n",
      "tensor([3.5356e-28], requires_grad=True)\n",
      "iter 153, train_loss_regularization 0.6943253874778748\n",
      "iter 153, val_loss_regularization 0.6943253874778748\n",
      "epoch 0, train_loss 1277.0782470703125\n",
      "epoch 0, val_loss 454.73583984375\n",
      "epoch 1, train_loss 1277.078125\n",
      "epoch 1, val_loss 454.7357482910156\n",
      "epoch 2, train_loss 1277.078125\n",
      "epoch 2, val_loss 454.7357482910156\n",
      "epoch 3, train_loss 1277.078125\n",
      "epoch 3, val_loss 454.7357177734375\n",
      "epoch 4, train_loss 1277.078125\n",
      "epoch 4, val_loss 454.73565673828125\n",
      "epoch 5, train_loss 1277.078125\n",
      "epoch 5, val_loss 454.73565673828125\n",
      "epoch 6, train_loss 1277.078125\n",
      "epoch 6, val_loss 454.7356262207031\n",
      "epoch 7, train_loss 1277.078125\n",
      "epoch 7, val_loss 454.735595703125\n",
      "epoch 8, train_loss 1277.077880859375\n",
      "epoch 8, val_loss 454.7356262207031\n",
      "epoch 9, train_loss 1277.077880859375\n",
      "epoch 9, val_loss 454.7355041503906\n",
      "epoch 10, train_loss 1277.077880859375\n",
      "epoch 10, val_loss 454.7354431152344\n",
      "epoch 11, train_loss 1277.077880859375\n",
      "epoch 11, val_loss 454.7354431152344\n",
      "epoch 12, train_loss 1277.077880859375\n",
      "epoch 12, val_loss 454.7354431152344\n",
      "epoch 13, train_loss 1277.0777587890625\n",
      "epoch 13, val_loss 454.73541259765625\n",
      "epoch 14, train_loss 1277.0777587890625\n",
      "epoch 14, val_loss 454.7353210449219\n",
      "epoch 15, train_loss 1277.0777587890625\n",
      "epoch 15, val_loss 454.73529052734375\n",
      "epoch 16, train_loss 1277.077880859375\n",
      "epoch 16, val_loss 454.7352600097656\n",
      "epoch 17, train_loss 1277.07763671875\n",
      "epoch 17, val_loss 454.7352600097656\n",
      "epoch 18, train_loss 1277.07763671875\n",
      "epoch 18, val_loss 454.7352600097656\n",
      "epoch 19, train_loss 1277.07763671875\n",
      "epoch 19, val_loss 454.73516845703125\n",
      "epoch 20, train_loss 1277.07763671875\n",
      "epoch 20, val_loss 454.7351379394531\n",
      "epoch 21, train_loss 1277.07763671875\n",
      "epoch 21, val_loss 454.7350769042969\n",
      "epoch 22, train_loss 1277.07763671875\n",
      "epoch 22, val_loss 454.7350769042969\n",
      "epoch 23, train_loss 1277.07763671875\n",
      "epoch 23, val_loss 454.7349853515625\n",
      "epoch 24, train_loss 1277.0777587890625\n",
      "epoch 24, val_loss 454.7349548339844\n",
      "epoch 25, train_loss 1277.07763671875\n",
      "epoch 25, val_loss 454.7349548339844\n",
      "epoch 26, train_loss 1277.0775146484375\n",
      "epoch 26, val_loss 454.7349548339844\n",
      "epoch 27, train_loss 1277.077392578125\n",
      "epoch 27, val_loss 454.73492431640625\n",
      "epoch 28, train_loss 1277.077392578125\n",
      "epoch 28, val_loss 454.73486328125\n",
      "epoch 29, train_loss 1277.077392578125\n",
      "epoch 29, val_loss 454.7348327636719\n",
      "epoch 30, train_loss 1277.077392578125\n",
      "epoch 30, val_loss 454.7348327636719\n",
      "epoch 31, train_loss 1277.0772705078125\n",
      "epoch 31, val_loss 454.7348327636719\n",
      "epoch 32, train_loss 1277.0771484375\n",
      "epoch 32, val_loss 454.73468017578125\n",
      "epoch 33, train_loss 1277.0771484375\n",
      "epoch 33, val_loss 454.734619140625\n",
      "epoch 34, train_loss 1277.0772705078125\n",
      "epoch 34, val_loss 454.734619140625\n",
      "epoch 35, train_loss 1277.0771484375\n",
      "epoch 35, val_loss 454.7345886230469\n",
      "epoch 36, train_loss 1277.0771484375\n",
      "epoch 36, val_loss 454.7345275878906\n",
      "epoch 37, train_loss 1277.0771484375\n",
      "epoch 37, val_loss 454.7344970703125\n",
      "epoch 38, train_loss 1277.0771484375\n",
      "epoch 38, val_loss 454.7344970703125\n",
      "epoch 39, train_loss 1277.0770263671875\n",
      "epoch 39, val_loss 454.7344970703125\n",
      "epoch 40, train_loss 1277.0770263671875\n",
      "epoch 40, val_loss 454.7344665527344\n",
      "epoch 41, train_loss 1277.0770263671875\n",
      "epoch 41, val_loss 454.7344055175781\n",
      "epoch 42, train_loss 1277.0770263671875\n",
      "epoch 42, val_loss 454.734375\n",
      "epoch 43, train_loss 1277.076904296875\n",
      "epoch 43, val_loss 454.7343444824219\n",
      "epoch 44, train_loss 1277.0767822265625\n",
      "epoch 44, val_loss 454.7342834472656\n",
      "epoch 45, train_loss 1277.0767822265625\n",
      "epoch 45, val_loss 454.7342224121094\n",
      "epoch 46, train_loss 1277.0767822265625\n",
      "epoch 46, val_loss 454.7341613769531\n",
      "epoch 47, train_loss 1277.0767822265625\n",
      "epoch 47, val_loss 454.734130859375\n",
      "epoch 48, train_loss 1277.0767822265625\n",
      "epoch 48, val_loss 454.7341613769531\n",
      "epoch 49, train_loss 1277.0767822265625\n",
      "epoch 49, val_loss 454.7340393066406\n",
      "epoch 50, train_loss 1277.07666015625\n",
      "epoch 50, val_loss 454.7340393066406\n",
      "epoch 51, train_loss 1277.0765380859375\n",
      "epoch 51, val_loss 454.73406982421875\n",
      "epoch 52, train_loss 1277.0767822265625\n",
      "epoch 52, val_loss 454.7340393066406\n",
      "epoch 53, train_loss 1277.07666015625\n",
      "epoch 53, val_loss 454.73394775390625\n",
      "epoch 54, train_loss 1277.07666015625\n",
      "epoch 54, val_loss 454.73388671875\n",
      "epoch 55, train_loss 1277.07666015625\n",
      "epoch 55, val_loss 454.73382568359375\n",
      "epoch 56, train_loss 1277.0765380859375\n",
      "epoch 56, val_loss 454.73388671875\n",
      "epoch 57, train_loss 1277.0765380859375\n",
      "epoch 57, val_loss 454.7337951660156\n",
      "epoch 58, train_loss 1277.0765380859375\n",
      "epoch 58, val_loss 454.7337646484375\n",
      "epoch 59, train_loss 1277.0765380859375\n",
      "epoch 59, val_loss 454.73370361328125\n",
      "epoch 60, train_loss 1277.0765380859375\n",
      "epoch 60, val_loss 454.7337646484375\n",
      "epoch 61, train_loss 1277.0762939453125\n",
      "epoch 61, val_loss 454.73370361328125\n",
      "epoch 62, train_loss 1277.0762939453125\n",
      "epoch 62, val_loss 454.7336730957031\n",
      "epoch 63, train_loss 1277.0762939453125\n",
      "epoch 63, val_loss 454.73358154296875\n",
      "epoch 64, train_loss 1277.0762939453125\n",
      "epoch 64, val_loss 454.73358154296875\n",
      "epoch 65, train_loss 1277.0762939453125\n",
      "epoch 65, val_loss 454.7334899902344\n",
      "epoch 66, train_loss 1277.0762939453125\n",
      "epoch 66, val_loss 454.7334289550781\n",
      "epoch 67, train_loss 1277.0762939453125\n",
      "epoch 67, val_loss 454.7334289550781\n",
      "epoch 68, train_loss 1277.076171875\n",
      "epoch 68, val_loss 454.7333679199219\n",
      "epoch 69, train_loss 1277.0762939453125\n",
      "epoch 69, val_loss 454.7334289550781\n",
      "epoch 70, train_loss 1277.076171875\n",
      "epoch 70, val_loss 454.73333740234375\n",
      "epoch 71, train_loss 1277.076171875\n",
      "epoch 71, val_loss 454.73333740234375\n",
      "epoch 72, train_loss 1277.0760498046875\n",
      "epoch 72, val_loss 454.7333068847656\n",
      "epoch 73, train_loss 1277.0760498046875\n",
      "epoch 73, val_loss 454.7332458496094\n",
      "epoch 74, train_loss 1277.076171875\n",
      "epoch 74, val_loss 454.7332458496094\n",
      "epoch 75, train_loss 1277.0758056640625\n",
      "epoch 75, val_loss 454.73321533203125\n",
      "epoch 76, train_loss 1277.075927734375\n",
      "epoch 76, val_loss 454.7331237792969\n",
      "epoch 77, train_loss 1277.075927734375\n",
      "epoch 77, val_loss 454.733154296875\n",
      "epoch 78, train_loss 1277.075927734375\n",
      "epoch 78, val_loss 454.7330017089844\n",
      "epoch 79, train_loss 1277.0758056640625\n",
      "epoch 79, val_loss 454.7330017089844\n",
      "epoch 80, train_loss 1277.0758056640625\n",
      "epoch 80, val_loss 454.7330017089844\n",
      "epoch 81, train_loss 1277.07568359375\n",
      "epoch 81, val_loss 454.73291015625\n",
      "epoch 82, train_loss 1277.07568359375\n",
      "epoch 82, val_loss 454.73297119140625\n",
      "epoch 83, train_loss 1277.0758056640625\n",
      "epoch 83, val_loss 454.73291015625\n",
      "epoch 84, train_loss 1277.07568359375\n",
      "epoch 84, val_loss 454.7328186035156\n",
      "epoch 85, train_loss 1277.0758056640625\n",
      "epoch 85, val_loss 454.7328186035156\n",
      "epoch 86, train_loss 1277.0758056640625\n",
      "epoch 86, val_loss 454.7328186035156\n",
      "epoch 87, train_loss 1277.07568359375\n",
      "epoch 87, val_loss 454.7327575683594\n",
      "epoch 88, train_loss 1277.075439453125\n",
      "epoch 88, val_loss 454.7326965332031\n",
      "epoch 89, train_loss 1277.075439453125\n",
      "epoch 89, val_loss 454.732666015625\n",
      "epoch 90, train_loss 1277.075439453125\n",
      "epoch 90, val_loss 454.7326354980469\n",
      "epoch 91, train_loss 1277.075439453125\n",
      "epoch 91, val_loss 454.7326354980469\n",
      "epoch 92, train_loss 1277.075439453125\n",
      "epoch 92, val_loss 454.7326354980469\n",
      "epoch 93, train_loss 1277.075439453125\n",
      "epoch 93, val_loss 454.7325439453125\n",
      "epoch 94, train_loss 1277.0753173828125\n",
      "epoch 94, val_loss 454.7325134277344\n",
      "epoch 95, train_loss 1277.0753173828125\n",
      "epoch 95, val_loss 454.7325134277344\n",
      "epoch 96, train_loss 1277.0751953125\n",
      "epoch 96, val_loss 454.7324523925781\n",
      "epoch 97, train_loss 1277.0751953125\n",
      "epoch 97, val_loss 454.732421875\n",
      "epoch 98, train_loss 1277.0751953125\n",
      "epoch 98, val_loss 454.7323303222656\n",
      "epoch 99, train_loss 1277.0753173828125\n",
      "epoch 99, val_loss 454.7323303222656\n",
      "Parameter containing:\n",
      "tensor([2.4659e-28], requires_grad=True)\n",
      "iter 154, train_loss_regularization 0.6937865614891052\n",
      "iter 154, val_loss_regularization 0.6937865614891052\n",
      "epoch 0, train_loss 1277.0751953125\n",
      "epoch 0, val_loss 454.73223876953125\n",
      "epoch 1, train_loss 1277.0750732421875\n",
      "epoch 1, val_loss 454.73223876953125\n",
      "epoch 2, train_loss 1277.0750732421875\n",
      "epoch 2, val_loss 454.7322082519531\n",
      "epoch 3, train_loss 1277.0750732421875\n",
      "epoch 3, val_loss 454.732177734375\n",
      "epoch 4, train_loss 1277.0751953125\n",
      "epoch 4, val_loss 454.73211669921875\n",
      "epoch 5, train_loss 1277.0751953125\n",
      "epoch 5, val_loss 454.73211669921875\n",
      "epoch 6, train_loss 1277.074951171875\n",
      "epoch 6, val_loss 454.73211669921875\n",
      "epoch 7, train_loss 1277.074951171875\n",
      "epoch 7, val_loss 454.7320556640625\n",
      "epoch 8, train_loss 1277.074951171875\n",
      "epoch 8, val_loss 454.7319641113281\n",
      "epoch 9, train_loss 1277.074951171875\n",
      "epoch 9, val_loss 454.7320556640625\n",
      "epoch 10, train_loss 1277.074951171875\n",
      "epoch 10, val_loss 454.7319641113281\n",
      "epoch 11, train_loss 1277.074951171875\n",
      "epoch 11, val_loss 454.73187255859375\n",
      "epoch 12, train_loss 1277.0748291015625\n",
      "epoch 12, val_loss 454.7318420410156\n",
      "epoch 13, train_loss 1277.07470703125\n",
      "epoch 13, val_loss 454.7317810058594\n",
      "epoch 14, train_loss 1277.0748291015625\n",
      "epoch 14, val_loss 454.7317810058594\n",
      "epoch 15, train_loss 1277.0748291015625\n",
      "epoch 15, val_loss 454.7317810058594\n",
      "epoch 16, train_loss 1277.07470703125\n",
      "epoch 16, val_loss 454.7317199707031\n",
      "epoch 17, train_loss 1277.07470703125\n",
      "epoch 17, val_loss 454.7316589355469\n",
      "epoch 18, train_loss 1277.07470703125\n",
      "epoch 18, val_loss 454.7316589355469\n",
      "epoch 19, train_loss 1277.0745849609375\n",
      "epoch 19, val_loss 454.7316589355469\n",
      "epoch 20, train_loss 1277.0745849609375\n",
      "epoch 20, val_loss 454.7315368652344\n",
      "epoch 21, train_loss 1277.074462890625\n",
      "epoch 21, val_loss 454.7315368652344\n",
      "epoch 22, train_loss 1277.0745849609375\n",
      "epoch 22, val_loss 454.73150634765625\n",
      "epoch 23, train_loss 1277.074462890625\n",
      "epoch 23, val_loss 454.73150634765625\n",
      "epoch 24, train_loss 1277.074462890625\n",
      "epoch 24, val_loss 454.7314147949219\n",
      "epoch 25, train_loss 1277.074462890625\n",
      "epoch 25, val_loss 454.7314147949219\n",
      "epoch 26, train_loss 1277.0743408203125\n",
      "epoch 26, val_loss 454.73138427734375\n",
      "epoch 27, train_loss 1277.0743408203125\n",
      "epoch 27, val_loss 454.73138427734375\n",
      "epoch 28, train_loss 1277.0743408203125\n",
      "epoch 28, val_loss 454.7313232421875\n",
      "epoch 29, train_loss 1277.0743408203125\n",
      "epoch 29, val_loss 454.73126220703125\n",
      "epoch 30, train_loss 1277.07421875\n",
      "epoch 30, val_loss 454.731201171875\n",
      "epoch 31, train_loss 1277.07421875\n",
      "epoch 31, val_loss 454.731201171875\n",
      "epoch 32, train_loss 1277.07421875\n",
      "epoch 32, val_loss 454.7311706542969\n",
      "epoch 33, train_loss 1277.07421875\n",
      "epoch 33, val_loss 454.7310791015625\n",
      "epoch 34, train_loss 1277.07421875\n",
      "epoch 34, val_loss 454.7310485839844\n",
      "epoch 35, train_loss 1277.07421875\n",
      "epoch 35, val_loss 454.7310485839844\n",
      "epoch 36, train_loss 1277.0740966796875\n",
      "epoch 36, val_loss 454.7310485839844\n",
      "epoch 37, train_loss 1277.073974609375\n",
      "epoch 37, val_loss 454.73095703125\n",
      "epoch 38, train_loss 1277.073974609375\n",
      "epoch 38, val_loss 454.73095703125\n",
      "epoch 39, train_loss 1277.073974609375\n",
      "epoch 39, val_loss 454.7309265136719\n",
      "epoch 40, train_loss 1277.073974609375\n",
      "epoch 40, val_loss 454.7309265136719\n",
      "epoch 41, train_loss 1277.073974609375\n",
      "epoch 41, val_loss 454.7308654785156\n",
      "epoch 42, train_loss 1277.073974609375\n",
      "epoch 42, val_loss 454.7308349609375\n",
      "epoch 43, train_loss 1277.073974609375\n",
      "epoch 43, val_loss 454.7307434082031\n",
      "epoch 44, train_loss 1277.07373046875\n",
      "epoch 44, val_loss 454.7307434082031\n",
      "epoch 45, train_loss 1277.07373046875\n",
      "epoch 45, val_loss 454.73065185546875\n",
      "epoch 46, train_loss 1277.0738525390625\n",
      "epoch 46, val_loss 454.7306213378906\n",
      "epoch 47, train_loss 1277.07373046875\n",
      "epoch 47, val_loss 454.7305908203125\n",
      "epoch 48, train_loss 1277.07373046875\n",
      "epoch 48, val_loss 454.7305908203125\n",
      "epoch 49, train_loss 1277.07373046875\n",
      "epoch 49, val_loss 454.73052978515625\n",
      "epoch 50, train_loss 1277.07373046875\n",
      "epoch 50, val_loss 454.7304992675781\n",
      "epoch 51, train_loss 1277.0736083984375\n",
      "epoch 51, val_loss 454.73046875\n",
      "epoch 52, train_loss 1277.073486328125\n",
      "epoch 52, val_loss 454.73046875\n",
      "epoch 53, train_loss 1277.073486328125\n",
      "epoch 53, val_loss 454.73040771484375\n",
      "epoch 54, train_loss 1277.0736083984375\n",
      "epoch 54, val_loss 454.73040771484375\n",
      "epoch 55, train_loss 1277.0736083984375\n",
      "epoch 55, val_loss 454.7303771972656\n",
      "epoch 56, train_loss 1277.073486328125\n",
      "epoch 56, val_loss 454.73028564453125\n",
      "epoch 57, train_loss 1277.073486328125\n",
      "epoch 57, val_loss 454.7302551269531\n",
      "epoch 58, train_loss 1277.0733642578125\n",
      "epoch 58, val_loss 454.73028564453125\n",
      "epoch 59, train_loss 1277.0733642578125\n",
      "epoch 59, val_loss 454.73016357421875\n",
      "epoch 60, train_loss 1277.0733642578125\n",
      "epoch 60, val_loss 454.73016357421875\n",
      "epoch 61, train_loss 1277.0733642578125\n",
      "epoch 61, val_loss 454.7300720214844\n",
      "epoch 62, train_loss 1277.0732421875\n",
      "epoch 62, val_loss 454.7301330566406\n",
      "epoch 63, train_loss 1277.0732421875\n",
      "epoch 63, val_loss 454.73004150390625\n",
      "epoch 64, train_loss 1277.0732421875\n",
      "epoch 64, val_loss 454.73004150390625\n",
      "epoch 65, train_loss 1277.0733642578125\n",
      "epoch 65, val_loss 454.7300109863281\n",
      "epoch 66, train_loss 1277.0733642578125\n",
      "epoch 66, val_loss 454.72991943359375\n",
      "epoch 67, train_loss 1277.0731201171875\n",
      "epoch 67, val_loss 454.7299499511719\n",
      "epoch 68, train_loss 1277.0728759765625\n",
      "epoch 68, val_loss 454.7298889160156\n",
      "epoch 69, train_loss 1277.0728759765625\n",
      "epoch 69, val_loss 454.7298278808594\n",
      "epoch 70, train_loss 1277.0728759765625\n",
      "epoch 70, val_loss 454.7298278808594\n",
      "epoch 71, train_loss 1277.072998046875\n",
      "epoch 71, val_loss 454.729736328125\n",
      "epoch 72, train_loss 1277.0731201171875\n",
      "epoch 72, val_loss 454.729736328125\n",
      "epoch 73, train_loss 1277.072998046875\n",
      "epoch 73, val_loss 454.7297058105469\n",
      "epoch 74, train_loss 1277.072998046875\n",
      "epoch 74, val_loss 454.7297058105469\n",
      "epoch 75, train_loss 1277.072998046875\n",
      "epoch 75, val_loss 454.72967529296875\n",
      "epoch 76, train_loss 1277.0728759765625\n",
      "epoch 76, val_loss 454.72967529296875\n",
      "epoch 77, train_loss 1277.0728759765625\n",
      "epoch 77, val_loss 454.7295837402344\n",
      "epoch 78, train_loss 1277.07275390625\n",
      "epoch 78, val_loss 454.7294921875\n",
      "epoch 79, train_loss 1277.07275390625\n",
      "epoch 79, val_loss 454.7294616699219\n",
      "epoch 80, train_loss 1277.07275390625\n",
      "epoch 80, val_loss 454.7294616699219\n",
      "epoch 81, train_loss 1277.07275390625\n",
      "epoch 81, val_loss 454.72943115234375\n",
      "epoch 82, train_loss 1277.0726318359375\n",
      "epoch 82, val_loss 454.7293701171875\n",
      "epoch 83, train_loss 1277.0726318359375\n",
      "epoch 83, val_loss 454.7293701171875\n",
      "epoch 84, train_loss 1277.0726318359375\n",
      "epoch 84, val_loss 454.7292785644531\n",
      "epoch 85, train_loss 1277.072509765625\n",
      "epoch 85, val_loss 454.7292785644531\n",
      "epoch 86, train_loss 1277.0723876953125\n",
      "epoch 86, val_loss 454.7292785644531\n",
      "epoch 87, train_loss 1277.0723876953125\n",
      "epoch 87, val_loss 454.7292175292969\n",
      "epoch 88, train_loss 1277.0723876953125\n",
      "epoch 88, val_loss 454.7292175292969\n",
      "epoch 89, train_loss 1277.0726318359375\n",
      "epoch 89, val_loss 454.7291259765625\n",
      "epoch 90, train_loss 1277.0726318359375\n",
      "epoch 90, val_loss 454.7290954589844\n",
      "epoch 91, train_loss 1277.072509765625\n",
      "epoch 91, val_loss 454.7291259765625\n",
      "epoch 92, train_loss 1277.0726318359375\n",
      "epoch 92, val_loss 454.72900390625\n",
      "epoch 93, train_loss 1277.0726318359375\n",
      "epoch 93, val_loss 454.7289733886719\n",
      "epoch 94, train_loss 1277.0723876953125\n",
      "epoch 94, val_loss 454.72900390625\n",
      "epoch 95, train_loss 1277.0723876953125\n",
      "epoch 95, val_loss 454.7289733886719\n",
      "epoch 96, train_loss 1277.072265625\n",
      "epoch 96, val_loss 454.7288818359375\n",
      "epoch 97, train_loss 1277.0723876953125\n",
      "epoch 97, val_loss 454.7288818359375\n",
      "epoch 98, train_loss 1277.0723876953125\n",
      "epoch 98, val_loss 454.72882080078125\n",
      "epoch 99, train_loss 1277.0721435546875\n",
      "epoch 99, val_loss 454.7287902832031\n",
      "Parameter containing:\n",
      "tensor([1.7201e-28], requires_grad=True)\n",
      "iter 155, train_loss_regularization 0.693264901638031\n",
      "iter 155, val_loss_regularization 0.693264901638031\n",
      "epoch 0, train_loss 1277.0721435546875\n",
      "epoch 0, val_loss 454.728759765625\n",
      "epoch 1, train_loss 1277.0721435546875\n",
      "epoch 1, val_loss 454.72869873046875\n",
      "epoch 2, train_loss 1277.0721435546875\n",
      "epoch 2, val_loss 454.72869873046875\n",
      "epoch 3, train_loss 1277.0721435546875\n",
      "epoch 3, val_loss 454.7286682128906\n",
      "epoch 4, train_loss 1277.072021484375\n",
      "epoch 4, val_loss 454.72857666015625\n",
      "epoch 5, train_loss 1277.072021484375\n",
      "epoch 5, val_loss 454.72857666015625\n",
      "epoch 6, train_loss 1277.0718994140625\n",
      "epoch 6, val_loss 454.72857666015625\n",
      "epoch 7, train_loss 1277.072021484375\n",
      "epoch 7, val_loss 454.72857666015625\n",
      "epoch 8, train_loss 1277.0718994140625\n",
      "epoch 8, val_loss 454.7284851074219\n",
      "epoch 9, train_loss 1277.07177734375\n",
      "epoch 9, val_loss 454.7284851074219\n",
      "epoch 10, train_loss 1277.072021484375\n",
      "epoch 10, val_loss 454.7284240722656\n",
      "epoch 11, train_loss 1277.07177734375\n",
      "epoch 11, val_loss 454.7284240722656\n",
      "epoch 12, train_loss 1277.07177734375\n",
      "epoch 12, val_loss 454.7283630371094\n",
      "epoch 13, train_loss 1277.07177734375\n",
      "epoch 13, val_loss 454.7282409667969\n",
      "epoch 14, train_loss 1277.07177734375\n",
      "epoch 14, val_loss 454.7282409667969\n",
      "epoch 15, train_loss 1277.07177734375\n",
      "epoch 15, val_loss 454.72821044921875\n",
      "epoch 16, train_loss 1277.071533203125\n",
      "epoch 16, val_loss 454.72821044921875\n",
      "epoch 17, train_loss 1277.0716552734375\n",
      "epoch 17, val_loss 454.7281799316406\n",
      "epoch 18, train_loss 1277.071533203125\n",
      "epoch 18, val_loss 454.7281188964844\n",
      "epoch 19, train_loss 1277.071533203125\n",
      "epoch 19, val_loss 454.7281188964844\n",
      "epoch 20, train_loss 1277.071533203125\n",
      "epoch 20, val_loss 454.7281188964844\n",
      "epoch 21, train_loss 1277.071533203125\n",
      "epoch 21, val_loss 454.72808837890625\n",
      "epoch 22, train_loss 1277.071533203125\n",
      "epoch 22, val_loss 454.72802734375\n",
      "epoch 23, train_loss 1277.071533203125\n",
      "epoch 23, val_loss 454.72796630859375\n",
      "epoch 24, train_loss 1277.071533203125\n",
      "epoch 24, val_loss 454.72796630859375\n",
      "epoch 25, train_loss 1277.0712890625\n",
      "epoch 25, val_loss 454.7279052734375\n",
      "epoch 26, train_loss 1277.0712890625\n",
      "epoch 26, val_loss 454.72784423828125\n",
      "epoch 27, train_loss 1277.0714111328125\n",
      "epoch 27, val_loss 454.72784423828125\n",
      "epoch 28, train_loss 1277.0712890625\n",
      "epoch 28, val_loss 454.7277526855469\n",
      "epoch 29, train_loss 1277.0712890625\n",
      "epoch 29, val_loss 454.7277526855469\n",
      "epoch 30, train_loss 1277.0712890625\n",
      "epoch 30, val_loss 454.72772216796875\n",
      "epoch 31, train_loss 1277.071044921875\n",
      "epoch 31, val_loss 454.7276611328125\n",
      "epoch 32, train_loss 1277.071044921875\n",
      "epoch 32, val_loss 454.7276611328125\n",
      "epoch 33, train_loss 1277.0711669921875\n",
      "epoch 33, val_loss 454.7276306152344\n",
      "epoch 34, train_loss 1277.071044921875\n",
      "epoch 34, val_loss 454.7275695800781\n",
      "epoch 35, train_loss 1277.071044921875\n",
      "epoch 35, val_loss 454.7275085449219\n",
      "epoch 36, train_loss 1277.0711669921875\n",
      "epoch 36, val_loss 454.7275085449219\n",
      "epoch 37, train_loss 1277.0711669921875\n",
      "epoch 37, val_loss 454.7275085449219\n",
      "epoch 38, train_loss 1277.071044921875\n",
      "epoch 38, val_loss 454.7274169921875\n",
      "epoch 39, train_loss 1277.071044921875\n",
      "epoch 39, val_loss 454.7273864746094\n",
      "epoch 40, train_loss 1277.071044921875\n",
      "epoch 40, val_loss 454.7273864746094\n",
      "epoch 41, train_loss 1277.0709228515625\n",
      "epoch 41, val_loss 454.7273864746094\n",
      "epoch 42, train_loss 1277.0709228515625\n",
      "epoch 42, val_loss 454.7273254394531\n",
      "epoch 43, train_loss 1277.071044921875\n",
      "epoch 43, val_loss 454.7272644042969\n",
      "epoch 44, train_loss 1277.0709228515625\n",
      "epoch 44, val_loss 454.7272033691406\n",
      "epoch 45, train_loss 1277.071044921875\n",
      "epoch 45, val_loss 454.7272033691406\n",
      "epoch 46, train_loss 1277.07080078125\n",
      "epoch 46, val_loss 454.72711181640625\n",
      "epoch 47, train_loss 1277.07080078125\n",
      "epoch 47, val_loss 454.72711181640625\n",
      "epoch 48, train_loss 1277.07080078125\n",
      "epoch 48, val_loss 454.72705078125\n",
      "epoch 49, train_loss 1277.070556640625\n",
      "epoch 49, val_loss 454.72705078125\n",
      "epoch 50, train_loss 1277.070556640625\n",
      "epoch 50, val_loss 454.72698974609375\n",
      "epoch 51, train_loss 1277.070556640625\n",
      "epoch 51, val_loss 454.7269592285156\n",
      "epoch 52, train_loss 1277.070556640625\n",
      "epoch 52, val_loss 454.7269592285156\n",
      "epoch 53, train_loss 1277.070556640625\n",
      "epoch 53, val_loss 454.7269592285156\n",
      "epoch 54, train_loss 1277.070556640625\n",
      "epoch 54, val_loss 454.7269287109375\n",
      "epoch 55, train_loss 1277.070556640625\n",
      "epoch 55, val_loss 454.72686767578125\n",
      "epoch 56, train_loss 1277.070556640625\n",
      "epoch 56, val_loss 454.72686767578125\n",
      "epoch 57, train_loss 1277.0704345703125\n",
      "epoch 57, val_loss 454.726806640625\n",
      "epoch 58, train_loss 1277.0704345703125\n",
      "epoch 58, val_loss 454.72674560546875\n",
      "epoch 59, train_loss 1277.0704345703125\n",
      "epoch 59, val_loss 454.7266540527344\n",
      "epoch 60, train_loss 1277.0704345703125\n",
      "epoch 60, val_loss 454.7266540527344\n",
      "epoch 61, train_loss 1277.0704345703125\n",
      "epoch 61, val_loss 454.7265319824219\n",
      "epoch 62, train_loss 1277.0703125\n",
      "epoch 62, val_loss 454.72662353515625\n",
      "epoch 63, train_loss 1277.0701904296875\n",
      "epoch 63, val_loss 454.7265319824219\n",
      "epoch 64, train_loss 1277.0701904296875\n",
      "epoch 64, val_loss 454.7265319824219\n",
      "epoch 65, train_loss 1277.070068359375\n",
      "epoch 65, val_loss 454.7265319824219\n",
      "epoch 66, train_loss 1277.070068359375\n",
      "epoch 66, val_loss 454.72650146484375\n",
      "epoch 67, train_loss 1277.070068359375\n",
      "epoch 67, val_loss 454.7264709472656\n",
      "epoch 68, train_loss 1277.070068359375\n",
      "epoch 68, val_loss 454.7264099121094\n",
      "epoch 69, train_loss 1277.0699462890625\n",
      "epoch 69, val_loss 454.726318359375\n",
      "epoch 70, train_loss 1277.0699462890625\n",
      "epoch 70, val_loss 454.726318359375\n",
      "epoch 71, train_loss 1277.0699462890625\n",
      "epoch 71, val_loss 454.7262878417969\n",
      "epoch 72, train_loss 1277.0699462890625\n",
      "epoch 72, val_loss 454.72625732421875\n",
      "epoch 73, train_loss 1277.070068359375\n",
      "epoch 73, val_loss 454.72625732421875\n",
      "epoch 74, train_loss 1277.0701904296875\n",
      "epoch 74, val_loss 454.7261962890625\n",
      "epoch 75, train_loss 1277.06982421875\n",
      "epoch 75, val_loss 454.7261657714844\n",
      "epoch 76, train_loss 1277.06982421875\n",
      "epoch 76, val_loss 454.72607421875\n",
      "epoch 77, train_loss 1277.06982421875\n",
      "epoch 77, val_loss 454.72607421875\n",
      "epoch 78, train_loss 1277.06982421875\n",
      "epoch 78, val_loss 454.72607421875\n",
      "epoch 79, train_loss 1277.0699462890625\n",
      "epoch 79, val_loss 454.7259521484375\n",
      "epoch 80, train_loss 1277.06982421875\n",
      "epoch 80, val_loss 454.7259521484375\n",
      "epoch 81, train_loss 1277.0697021484375\n",
      "epoch 81, val_loss 454.7259216308594\n",
      "epoch 82, train_loss 1277.0697021484375\n",
      "epoch 82, val_loss 454.7259216308594\n",
      "epoch 83, train_loss 1277.0697021484375\n",
      "epoch 83, val_loss 454.7258605957031\n",
      "epoch 84, train_loss 1277.069580078125\n",
      "epoch 84, val_loss 454.725830078125\n",
      "epoch 85, train_loss 1277.069580078125\n",
      "epoch 85, val_loss 454.725830078125\n",
      "epoch 86, train_loss 1277.069580078125\n",
      "epoch 86, val_loss 454.7257995605469\n",
      "epoch 87, train_loss 1277.0697021484375\n",
      "epoch 87, val_loss 454.7257385253906\n",
      "epoch 88, train_loss 1277.0694580078125\n",
      "epoch 88, val_loss 454.7257080078125\n",
      "epoch 89, train_loss 1277.0694580078125\n",
      "epoch 89, val_loss 454.7256774902344\n",
      "epoch 90, train_loss 1277.0694580078125\n",
      "epoch 90, val_loss 454.7256774902344\n",
      "epoch 91, train_loss 1277.0694580078125\n",
      "epoch 91, val_loss 454.7256164550781\n",
      "epoch 92, train_loss 1277.0694580078125\n",
      "epoch 92, val_loss 454.7254943847656\n",
      "epoch 93, train_loss 1277.0694580078125\n",
      "epoch 93, val_loss 454.7254943847656\n",
      "epoch 94, train_loss 1277.0694580078125\n",
      "epoch 94, val_loss 454.7254638671875\n",
      "epoch 95, train_loss 1277.0693359375\n",
      "epoch 95, val_loss 454.72540283203125\n",
      "epoch 96, train_loss 1277.0694580078125\n",
      "epoch 96, val_loss 454.7254638671875\n",
      "epoch 97, train_loss 1277.0693359375\n",
      "epoch 97, val_loss 454.7253723144531\n",
      "epoch 98, train_loss 1277.0692138671875\n",
      "epoch 98, val_loss 454.725341796875\n",
      "epoch 99, train_loss 1277.0692138671875\n",
      "epoch 99, val_loss 454.725341796875\n",
      "Parameter containing:\n",
      "tensor([1.2001e-28], requires_grad=True)\n",
      "iter 156, train_loss_regularization 0.6927601099014282\n",
      "iter 156, val_loss_regularization 0.6927601099014282\n",
      "epoch 0, train_loss 1277.0692138671875\n",
      "epoch 0, val_loss 454.72528076171875\n",
      "epoch 1, train_loss 1277.069091796875\n",
      "epoch 1, val_loss 454.7252502441406\n",
      "epoch 2, train_loss 1277.0689697265625\n",
      "epoch 2, val_loss 454.72515869140625\n",
      "epoch 3, train_loss 1277.069091796875\n",
      "epoch 3, val_loss 454.72515869140625\n",
      "epoch 4, train_loss 1277.069091796875\n",
      "epoch 4, val_loss 454.72515869140625\n",
      "epoch 5, train_loss 1277.069091796875\n",
      "epoch 5, val_loss 454.7251281738281\n",
      "epoch 6, train_loss 1277.0689697265625\n",
      "epoch 6, val_loss 454.72503662109375\n",
      "epoch 7, train_loss 1277.0689697265625\n",
      "epoch 7, val_loss 454.72503662109375\n",
      "epoch 8, train_loss 1277.0689697265625\n",
      "epoch 8, val_loss 454.7250061035156\n",
      "epoch 9, train_loss 1277.0689697265625\n",
      "epoch 9, val_loss 454.7250061035156\n",
      "epoch 10, train_loss 1277.0689697265625\n",
      "epoch 10, val_loss 454.72491455078125\n",
      "epoch 11, train_loss 1277.06884765625\n",
      "epoch 11, val_loss 454.7248840332031\n",
      "epoch 12, train_loss 1277.06884765625\n",
      "epoch 12, val_loss 454.7248840332031\n",
      "epoch 13, train_loss 1277.0687255859375\n",
      "epoch 13, val_loss 454.72479248046875\n",
      "epoch 14, train_loss 1277.0687255859375\n",
      "epoch 14, val_loss 454.72479248046875\n",
      "epoch 15, train_loss 1277.0687255859375\n",
      "epoch 15, val_loss 454.7247009277344\n",
      "epoch 16, train_loss 1277.0687255859375\n",
      "epoch 16, val_loss 454.7247009277344\n",
      "epoch 17, train_loss 1277.0687255859375\n",
      "epoch 17, val_loss 454.7247009277344\n",
      "epoch 18, train_loss 1277.0687255859375\n",
      "epoch 18, val_loss 454.7247009277344\n",
      "epoch 19, train_loss 1277.0687255859375\n",
      "epoch 19, val_loss 454.72467041015625\n",
      "epoch 20, train_loss 1277.0687255859375\n",
      "epoch 20, val_loss 454.7245788574219\n",
      "epoch 21, train_loss 1277.068603515625\n",
      "epoch 21, val_loss 454.7245788574219\n",
      "epoch 22, train_loss 1277.0684814453125\n",
      "epoch 22, val_loss 454.7245788574219\n",
      "epoch 23, train_loss 1277.0684814453125\n",
      "epoch 23, val_loss 454.7244873046875\n",
      "epoch 24, train_loss 1277.068603515625\n",
      "epoch 24, val_loss 454.7244567871094\n",
      "epoch 25, train_loss 1277.068603515625\n",
      "epoch 25, val_loss 454.7244567871094\n",
      "epoch 26, train_loss 1277.0684814453125\n",
      "epoch 26, val_loss 454.7243347167969\n",
      "epoch 27, train_loss 1277.068359375\n",
      "epoch 27, val_loss 454.7243347167969\n",
      "epoch 28, train_loss 1277.068359375\n",
      "epoch 28, val_loss 454.72430419921875\n",
      "epoch 29, train_loss 1277.068359375\n",
      "epoch 29, val_loss 454.7242126464844\n",
      "epoch 30, train_loss 1277.068359375\n",
      "epoch 30, val_loss 454.7242431640625\n",
      "epoch 31, train_loss 1277.068115234375\n",
      "epoch 31, val_loss 454.7242431640625\n",
      "epoch 32, train_loss 1277.068115234375\n",
      "epoch 32, val_loss 454.7241516113281\n",
      "epoch 33, train_loss 1277.068115234375\n",
      "epoch 33, val_loss 454.72412109375\n",
      "epoch 34, train_loss 1277.068115234375\n",
      "epoch 34, val_loss 454.72412109375\n",
      "epoch 35, train_loss 1277.068115234375\n",
      "epoch 35, val_loss 454.7240905761719\n",
      "epoch 36, train_loss 1277.0682373046875\n",
      "epoch 36, val_loss 454.7240295410156\n",
      "epoch 37, train_loss 1277.0682373046875\n",
      "epoch 37, val_loss 454.7239990234375\n",
      "epoch 38, train_loss 1277.0679931640625\n",
      "epoch 38, val_loss 454.7239685058594\n",
      "epoch 39, train_loss 1277.06787109375\n",
      "epoch 39, val_loss 454.7239685058594\n",
      "epoch 40, train_loss 1277.06787109375\n",
      "epoch 40, val_loss 454.7239074707031\n",
      "epoch 41, train_loss 1277.06787109375\n",
      "epoch 41, val_loss 454.7238464355469\n",
      "epoch 42, train_loss 1277.0679931640625\n",
      "epoch 42, val_loss 454.7237548828125\n",
      "epoch 43, train_loss 1277.0679931640625\n",
      "epoch 43, val_loss 454.7237854003906\n",
      "epoch 44, train_loss 1277.06787109375\n",
      "epoch 44, val_loss 454.7237548828125\n",
      "epoch 45, train_loss 1277.06787109375\n",
      "epoch 45, val_loss 454.7237548828125\n",
      "epoch 46, train_loss 1277.06787109375\n",
      "epoch 46, val_loss 454.72369384765625\n",
      "epoch 47, train_loss 1277.0677490234375\n",
      "epoch 47, val_loss 454.7236328125\n",
      "epoch 48, train_loss 1277.0677490234375\n",
      "epoch 48, val_loss 454.7236328125\n",
      "epoch 49, train_loss 1277.067626953125\n",
      "epoch 49, val_loss 454.72357177734375\n",
      "epoch 50, train_loss 1277.067626953125\n",
      "epoch 50, val_loss 454.7235412597656\n",
      "epoch 51, train_loss 1277.067626953125\n",
      "epoch 51, val_loss 454.7235107421875\n",
      "epoch 52, train_loss 1277.067626953125\n",
      "epoch 52, val_loss 454.7235107421875\n",
      "epoch 53, train_loss 1277.067626953125\n",
      "epoch 53, val_loss 454.7234191894531\n",
      "epoch 54, train_loss 1277.067626953125\n",
      "epoch 54, val_loss 454.723388671875\n",
      "epoch 55, train_loss 1277.067626953125\n",
      "epoch 55, val_loss 454.7234191894531\n",
      "epoch 56, train_loss 1277.0673828125\n",
      "epoch 56, val_loss 454.72332763671875\n",
      "epoch 57, train_loss 1277.0675048828125\n",
      "epoch 57, val_loss 454.7232971191406\n",
      "epoch 58, train_loss 1277.0673828125\n",
      "epoch 58, val_loss 454.72332763671875\n",
      "epoch 59, train_loss 1277.0673828125\n",
      "epoch 59, val_loss 454.7232360839844\n",
      "epoch 60, train_loss 1277.0673828125\n",
      "epoch 60, val_loss 454.7231750488281\n",
      "epoch 61, train_loss 1277.0673828125\n",
      "epoch 61, val_loss 454.7231750488281\n",
      "epoch 62, train_loss 1277.0673828125\n",
      "epoch 62, val_loss 454.7231140136719\n",
      "epoch 63, train_loss 1277.0673828125\n",
      "epoch 63, val_loss 454.7231140136719\n",
      "epoch 64, train_loss 1277.0673828125\n",
      "epoch 64, val_loss 454.72308349609375\n",
      "epoch 65, train_loss 1277.0672607421875\n",
      "epoch 65, val_loss 454.7230529785156\n",
      "epoch 66, train_loss 1277.0673828125\n",
      "epoch 66, val_loss 454.7229919433594\n",
      "epoch 67, train_loss 1277.067138671875\n",
      "epoch 67, val_loss 454.72296142578125\n",
      "epoch 68, train_loss 1277.067138671875\n",
      "epoch 68, val_loss 454.72296142578125\n",
      "epoch 69, train_loss 1277.0672607421875\n",
      "epoch 69, val_loss 454.7229309082031\n",
      "epoch 70, train_loss 1277.067138671875\n",
      "epoch 70, val_loss 454.7228698730469\n",
      "epoch 71, train_loss 1277.067138671875\n",
      "epoch 71, val_loss 454.72283935546875\n",
      "epoch 72, train_loss 1277.067138671875\n",
      "epoch 72, val_loss 454.72283935546875\n",
      "epoch 73, train_loss 1277.067138671875\n",
      "epoch 73, val_loss 454.7227783203125\n",
      "epoch 74, train_loss 1277.0670166015625\n",
      "epoch 74, val_loss 454.7227478027344\n",
      "epoch 75, train_loss 1277.067138671875\n",
      "epoch 75, val_loss 454.72271728515625\n",
      "epoch 76, train_loss 1277.0670166015625\n",
      "epoch 76, val_loss 454.72265625\n",
      "epoch 77, train_loss 1277.06689453125\n",
      "epoch 77, val_loss 454.7226257324219\n",
      "epoch 78, train_loss 1277.06689453125\n",
      "epoch 78, val_loss 454.72259521484375\n",
      "epoch 79, train_loss 1277.06689453125\n",
      "epoch 79, val_loss 454.7225341796875\n",
      "epoch 80, train_loss 1277.0667724609375\n",
      "epoch 80, val_loss 454.7225036621094\n",
      "epoch 81, train_loss 1277.0667724609375\n",
      "epoch 81, val_loss 454.7225036621094\n",
      "epoch 82, train_loss 1277.0667724609375\n",
      "epoch 82, val_loss 454.72247314453125\n",
      "epoch 83, train_loss 1277.0667724609375\n",
      "epoch 83, val_loss 454.7223815917969\n",
      "epoch 84, train_loss 1277.066650390625\n",
      "epoch 84, val_loss 454.7223815917969\n",
      "epoch 85, train_loss 1277.066650390625\n",
      "epoch 85, val_loss 454.7223815917969\n",
      "epoch 86, train_loss 1277.066650390625\n",
      "epoch 86, val_loss 454.7223205566406\n",
      "epoch 87, train_loss 1277.0665283203125\n",
      "epoch 87, val_loss 454.7222595214844\n",
      "epoch 88, train_loss 1277.0665283203125\n",
      "epoch 88, val_loss 454.7222595214844\n",
      "epoch 89, train_loss 1277.0665283203125\n",
      "epoch 89, val_loss 454.7222595214844\n",
      "epoch 90, train_loss 1277.06640625\n",
      "epoch 90, val_loss 454.7221984863281\n",
      "epoch 91, train_loss 1277.06640625\n",
      "epoch 91, val_loss 454.7221984863281\n",
      "epoch 92, train_loss 1277.0662841796875\n",
      "epoch 92, val_loss 454.7220764160156\n",
      "epoch 93, train_loss 1277.0662841796875\n",
      "epoch 93, val_loss 454.72198486328125\n",
      "epoch 94, train_loss 1277.06640625\n",
      "epoch 94, val_loss 454.72198486328125\n",
      "epoch 95, train_loss 1277.06640625\n",
      "epoch 95, val_loss 454.72198486328125\n",
      "epoch 96, train_loss 1277.0662841796875\n",
      "epoch 96, val_loss 454.721923828125\n",
      "epoch 97, train_loss 1277.0662841796875\n",
      "epoch 97, val_loss 454.721923828125\n",
      "epoch 98, train_loss 1277.0662841796875\n",
      "epoch 98, val_loss 454.721923828125\n",
      "epoch 99, train_loss 1277.0662841796875\n",
      "epoch 99, val_loss 454.72186279296875\n",
      "Parameter containing:\n",
      "tensor([8.3744e-29], requires_grad=True)\n",
      "iter 157, train_loss_regularization 0.6922719478607178\n",
      "iter 157, val_loss_regularization 0.6922719478607178\n",
      "epoch 0, train_loss 1277.0662841796875\n",
      "epoch 0, val_loss 454.7218017578125\n",
      "epoch 1, train_loss 1277.066162109375\n",
      "epoch 1, val_loss 454.7218017578125\n",
      "epoch 2, train_loss 1277.066162109375\n",
      "epoch 2, val_loss 454.72174072265625\n",
      "epoch 3, train_loss 1277.0660400390625\n",
      "epoch 3, val_loss 454.7218017578125\n",
      "epoch 4, train_loss 1277.0660400390625\n",
      "epoch 4, val_loss 454.72174072265625\n",
      "epoch 5, train_loss 1277.0660400390625\n",
      "epoch 5, val_loss 454.7216796875\n",
      "epoch 6, train_loss 1277.0660400390625\n",
      "epoch 6, val_loss 454.72161865234375\n",
      "epoch 7, train_loss 1277.0660400390625\n",
      "epoch 7, val_loss 454.7215881347656\n",
      "epoch 8, train_loss 1277.0660400390625\n",
      "epoch 8, val_loss 454.7215881347656\n",
      "epoch 9, train_loss 1277.06591796875\n",
      "epoch 9, val_loss 454.72149658203125\n",
      "epoch 10, train_loss 1277.0660400390625\n",
      "epoch 10, val_loss 454.7214050292969\n",
      "epoch 11, train_loss 1277.06591796875\n",
      "epoch 11, val_loss 454.7214050292969\n",
      "epoch 12, train_loss 1277.0657958984375\n",
      "epoch 12, val_loss 454.7214050292969\n",
      "epoch 13, train_loss 1277.06591796875\n",
      "epoch 13, val_loss 454.7214050292969\n",
      "epoch 14, train_loss 1277.06591796875\n",
      "epoch 14, val_loss 454.72137451171875\n",
      "epoch 15, train_loss 1277.0657958984375\n",
      "epoch 15, val_loss 454.7213439941406\n",
      "epoch 16, train_loss 1277.0657958984375\n",
      "epoch 16, val_loss 454.7212829589844\n",
      "epoch 17, train_loss 1277.0657958984375\n",
      "epoch 17, val_loss 454.7212219238281\n",
      "epoch 18, train_loss 1277.0657958984375\n",
      "epoch 18, val_loss 454.72125244140625\n",
      "epoch 19, train_loss 1277.0657958984375\n",
      "epoch 19, val_loss 454.72113037109375\n",
      "epoch 20, train_loss 1277.065673828125\n",
      "epoch 20, val_loss 454.72113037109375\n",
      "epoch 21, train_loss 1277.0655517578125\n",
      "epoch 21, val_loss 454.7211608886719\n",
      "epoch 22, train_loss 1277.0655517578125\n",
      "epoch 22, val_loss 454.7210693359375\n",
      "epoch 23, train_loss 1277.0655517578125\n",
      "epoch 23, val_loss 454.7210388183594\n",
      "epoch 24, train_loss 1277.0655517578125\n",
      "epoch 24, val_loss 454.7210388183594\n",
      "epoch 25, train_loss 1277.0655517578125\n",
      "epoch 25, val_loss 454.720947265625\n",
      "epoch 26, train_loss 1277.0655517578125\n",
      "epoch 26, val_loss 454.72100830078125\n",
      "epoch 27, train_loss 1277.065673828125\n",
      "epoch 27, val_loss 454.72088623046875\n",
      "epoch 28, train_loss 1277.0655517578125\n",
      "epoch 28, val_loss 454.7208251953125\n",
      "epoch 29, train_loss 1277.0653076171875\n",
      "epoch 29, val_loss 454.7207946777344\n",
      "epoch 30, train_loss 1277.0653076171875\n",
      "epoch 30, val_loss 454.7207946777344\n",
      "epoch 31, train_loss 1277.0653076171875\n",
      "epoch 31, val_loss 454.7207946777344\n",
      "epoch 32, train_loss 1277.0653076171875\n",
      "epoch 32, val_loss 454.720703125\n",
      "epoch 33, train_loss 1277.0653076171875\n",
      "epoch 33, val_loss 454.720703125\n",
      "epoch 34, train_loss 1277.0653076171875\n",
      "epoch 34, val_loss 454.7206726074219\n",
      "epoch 35, train_loss 1277.0653076171875\n",
      "epoch 35, val_loss 454.7206115722656\n",
      "epoch 36, train_loss 1277.0653076171875\n",
      "epoch 36, val_loss 454.7205810546875\n",
      "epoch 37, train_loss 1277.065185546875\n",
      "epoch 37, val_loss 454.7205810546875\n",
      "epoch 38, train_loss 1277.0650634765625\n",
      "epoch 38, val_loss 454.7205505371094\n",
      "epoch 39, train_loss 1277.06494140625\n",
      "epoch 39, val_loss 454.7205505371094\n",
      "epoch 40, train_loss 1277.06494140625\n",
      "epoch 40, val_loss 454.7204895019531\n",
      "epoch 41, train_loss 1277.06494140625\n",
      "epoch 41, val_loss 454.7204284667969\n",
      "epoch 42, train_loss 1277.06494140625\n",
      "epoch 42, val_loss 454.7204284667969\n",
      "epoch 43, train_loss 1277.06494140625\n",
      "epoch 43, val_loss 454.7203063964844\n",
      "epoch 44, train_loss 1277.06494140625\n",
      "epoch 44, val_loss 454.7203369140625\n",
      "epoch 45, train_loss 1277.06494140625\n",
      "epoch 45, val_loss 454.7202453613281\n",
      "epoch 46, train_loss 1277.06494140625\n",
      "epoch 46, val_loss 454.72021484375\n",
      "epoch 47, train_loss 1277.064697265625\n",
      "epoch 47, val_loss 454.72021484375\n",
      "epoch 48, train_loss 1277.064697265625\n",
      "epoch 48, val_loss 454.72021484375\n",
      "epoch 49, train_loss 1277.064697265625\n",
      "epoch 49, val_loss 454.72015380859375\n",
      "epoch 50, train_loss 1277.0648193359375\n",
      "epoch 50, val_loss 454.7201232910156\n",
      "epoch 51, train_loss 1277.06494140625\n",
      "epoch 51, val_loss 454.7201232910156\n",
      "epoch 52, train_loss 1277.0648193359375\n",
      "epoch 52, val_loss 454.72003173828125\n",
      "epoch 53, train_loss 1277.064697265625\n",
      "epoch 53, val_loss 454.7200012207031\n",
      "epoch 54, train_loss 1277.064697265625\n",
      "epoch 54, val_loss 454.7200012207031\n",
      "epoch 55, train_loss 1277.0645751953125\n",
      "epoch 55, val_loss 454.719970703125\n",
      "epoch 56, train_loss 1277.064697265625\n",
      "epoch 56, val_loss 454.71990966796875\n",
      "epoch 57, train_loss 1277.064453125\n",
      "epoch 57, val_loss 454.7198791503906\n",
      "epoch 58, train_loss 1277.0645751953125\n",
      "epoch 58, val_loss 454.7198791503906\n",
      "epoch 59, train_loss 1277.064453125\n",
      "epoch 59, val_loss 454.7198181152344\n",
      "epoch 60, train_loss 1277.064453125\n",
      "epoch 60, val_loss 454.7197570800781\n",
      "epoch 61, train_loss 1277.064453125\n",
      "epoch 61, val_loss 454.7196960449219\n",
      "epoch 62, train_loss 1277.0643310546875\n",
      "epoch 62, val_loss 454.7197570800781\n",
      "epoch 63, train_loss 1277.064453125\n",
      "epoch 63, val_loss 454.71966552734375\n",
      "epoch 64, train_loss 1277.064453125\n",
      "epoch 64, val_loss 454.7195739746094\n",
      "epoch 65, train_loss 1277.064453125\n",
      "epoch 65, val_loss 454.7195739746094\n",
      "epoch 66, train_loss 1277.0643310546875\n",
      "epoch 66, val_loss 454.71954345703125\n",
      "epoch 67, train_loss 1277.064208984375\n",
      "epoch 67, val_loss 454.7195739746094\n",
      "epoch 68, train_loss 1277.064208984375\n",
      "epoch 68, val_loss 454.71954345703125\n",
      "epoch 69, train_loss 1277.0643310546875\n",
      "epoch 69, val_loss 454.7194519042969\n",
      "epoch 70, train_loss 1277.064208984375\n",
      "epoch 70, val_loss 454.71942138671875\n",
      "epoch 71, train_loss 1277.0643310546875\n",
      "epoch 71, val_loss 454.7194519042969\n",
      "epoch 72, train_loss 1277.0640869140625\n",
      "epoch 72, val_loss 454.7194519042969\n",
      "epoch 73, train_loss 1277.06396484375\n",
      "epoch 73, val_loss 454.7193298339844\n",
      "epoch 74, train_loss 1277.06396484375\n",
      "epoch 74, val_loss 454.7193298339844\n",
      "epoch 75, train_loss 1277.06396484375\n",
      "epoch 75, val_loss 454.71923828125\n",
      "epoch 76, train_loss 1277.06396484375\n",
      "epoch 76, val_loss 454.71923828125\n",
      "epoch 77, train_loss 1277.06396484375\n",
      "epoch 77, val_loss 454.71917724609375\n",
      "epoch 78, train_loss 1277.06396484375\n",
      "epoch 78, val_loss 454.7191162109375\n",
      "epoch 79, train_loss 1277.0638427734375\n",
      "epoch 79, val_loss 454.7190856933594\n",
      "epoch 80, train_loss 1277.0638427734375\n",
      "epoch 80, val_loss 454.71905517578125\n",
      "epoch 81, train_loss 1277.06396484375\n",
      "epoch 81, val_loss 454.7190856933594\n",
      "epoch 82, train_loss 1277.06396484375\n",
      "epoch 82, val_loss 454.7190856933594\n",
      "epoch 83, train_loss 1277.0638427734375\n",
      "epoch 83, val_loss 454.7189636230469\n",
      "epoch 84, train_loss 1277.0638427734375\n",
      "epoch 84, val_loss 454.718994140625\n",
      "epoch 85, train_loss 1277.063720703125\n",
      "epoch 85, val_loss 454.7189636230469\n",
      "epoch 86, train_loss 1277.063720703125\n",
      "epoch 86, val_loss 454.7188720703125\n",
      "epoch 87, train_loss 1277.063720703125\n",
      "epoch 87, val_loss 454.7188415527344\n",
      "epoch 88, train_loss 1277.063720703125\n",
      "epoch 88, val_loss 454.7188415527344\n",
      "epoch 89, train_loss 1277.0634765625\n",
      "epoch 89, val_loss 454.7187805175781\n",
      "epoch 90, train_loss 1277.063720703125\n",
      "epoch 90, val_loss 454.7187805175781\n",
      "epoch 91, train_loss 1277.0635986328125\n",
      "epoch 91, val_loss 454.7187194824219\n",
      "epoch 92, train_loss 1277.0635986328125\n",
      "epoch 92, val_loss 454.7187194824219\n",
      "epoch 93, train_loss 1277.0634765625\n",
      "epoch 93, val_loss 454.7186584472656\n",
      "epoch 94, train_loss 1277.0634765625\n",
      "epoch 94, val_loss 454.7186584472656\n",
      "epoch 95, train_loss 1277.0634765625\n",
      "epoch 95, val_loss 454.7186279296875\n",
      "epoch 96, train_loss 1277.0634765625\n",
      "epoch 96, val_loss 454.7185363769531\n",
      "epoch 97, train_loss 1277.0634765625\n",
      "epoch 97, val_loss 454.7185363769531\n",
      "epoch 98, train_loss 1277.0634765625\n",
      "epoch 98, val_loss 454.718505859375\n",
      "epoch 99, train_loss 1277.0634765625\n",
      "epoch 99, val_loss 454.71844482421875\n",
      "Parameter containing:\n",
      "tensor([5.8447e-29], requires_grad=True)\n",
      "iter 158, train_loss_regularization 0.6918001174926758\n",
      "iter 158, val_loss_regularization 0.6918001174926758\n",
      "epoch 0, train_loss 1277.0634765625\n",
      "epoch 0, val_loss 454.7183837890625\n",
      "epoch 1, train_loss 1277.0634765625\n",
      "epoch 1, val_loss 454.7183837890625\n",
      "epoch 2, train_loss 1277.0633544921875\n",
      "epoch 2, val_loss 454.7183837890625\n",
      "epoch 3, train_loss 1277.0634765625\n",
      "epoch 3, val_loss 454.71832275390625\n",
      "epoch 4, train_loss 1277.0633544921875\n",
      "epoch 4, val_loss 454.71832275390625\n",
      "epoch 5, train_loss 1277.0631103515625\n",
      "epoch 5, val_loss 454.71826171875\n",
      "epoch 6, train_loss 1277.0631103515625\n",
      "epoch 6, val_loss 454.71820068359375\n",
      "epoch 7, train_loss 1277.0631103515625\n",
      "epoch 7, val_loss 454.71820068359375\n",
      "epoch 8, train_loss 1277.0631103515625\n",
      "epoch 8, val_loss 454.7181701660156\n",
      "epoch 9, train_loss 1277.0631103515625\n",
      "epoch 9, val_loss 454.71807861328125\n",
      "epoch 10, train_loss 1277.0631103515625\n",
      "epoch 10, val_loss 454.7179870605469\n",
      "epoch 11, train_loss 1277.06298828125\n",
      "epoch 11, val_loss 454.7179870605469\n",
      "epoch 12, train_loss 1277.0628662109375\n",
      "epoch 12, val_loss 454.7179870605469\n",
      "epoch 13, train_loss 1277.0628662109375\n",
      "epoch 13, val_loss 454.71795654296875\n",
      "epoch 14, train_loss 1277.0628662109375\n",
      "epoch 14, val_loss 454.7179260253906\n",
      "epoch 15, train_loss 1277.06298828125\n",
      "epoch 15, val_loss 454.7179260253906\n",
      "epoch 16, train_loss 1277.06298828125\n",
      "epoch 16, val_loss 454.7178649902344\n",
      "epoch 17, train_loss 1277.06298828125\n",
      "epoch 17, val_loss 454.7178649902344\n",
      "epoch 18, train_loss 1277.0628662109375\n",
      "epoch 18, val_loss 454.71783447265625\n",
      "epoch 19, train_loss 1277.0628662109375\n",
      "epoch 19, val_loss 454.7177429199219\n",
      "epoch 20, train_loss 1277.0628662109375\n",
      "epoch 20, val_loss 454.7177429199219\n",
      "epoch 21, train_loss 1277.0626220703125\n",
      "epoch 21, val_loss 454.71771240234375\n",
      "epoch 22, train_loss 1277.0626220703125\n",
      "epoch 22, val_loss 454.71771240234375\n",
      "epoch 23, train_loss 1277.0626220703125\n",
      "epoch 23, val_loss 454.7176208496094\n",
      "epoch 24, train_loss 1277.0626220703125\n",
      "epoch 24, val_loss 454.7176208496094\n",
      "epoch 25, train_loss 1277.0625\n",
      "epoch 25, val_loss 454.71759033203125\n",
      "epoch 26, train_loss 1277.0625\n",
      "epoch 26, val_loss 454.7174987792969\n",
      "epoch 27, train_loss 1277.0626220703125\n",
      "epoch 27, val_loss 454.7174987792969\n",
      "epoch 28, train_loss 1277.0625\n",
      "epoch 28, val_loss 454.71746826171875\n",
      "epoch 29, train_loss 1277.0623779296875\n",
      "epoch 29, val_loss 454.71746826171875\n",
      "epoch 30, train_loss 1277.0623779296875\n",
      "epoch 30, val_loss 454.7174072265625\n",
      "epoch 31, train_loss 1277.0623779296875\n",
      "epoch 31, val_loss 454.7174072265625\n",
      "epoch 32, train_loss 1277.0623779296875\n",
      "epoch 32, val_loss 454.71734619140625\n",
      "epoch 33, train_loss 1277.0625\n",
      "epoch 33, val_loss 454.7172546386719\n",
      "epoch 34, train_loss 1277.0625\n",
      "epoch 34, val_loss 454.7172546386719\n",
      "epoch 35, train_loss 1277.0625\n",
      "epoch 35, val_loss 454.7171936035156\n",
      "epoch 36, train_loss 1277.0623779296875\n",
      "epoch 36, val_loss 454.7171936035156\n",
      "epoch 37, train_loss 1277.0623779296875\n",
      "epoch 37, val_loss 454.7171630859375\n",
      "epoch 38, train_loss 1277.0623779296875\n",
      "epoch 38, val_loss 454.7171630859375\n",
      "epoch 39, train_loss 1277.062255859375\n",
      "epoch 39, val_loss 454.7171325683594\n",
      "epoch 40, train_loss 1277.062255859375\n",
      "epoch 40, val_loss 454.7170715332031\n",
      "epoch 41, train_loss 1277.062255859375\n",
      "epoch 41, val_loss 454.717041015625\n",
      "epoch 42, train_loss 1277.0621337890625\n",
      "epoch 42, val_loss 454.717041015625\n",
      "epoch 43, train_loss 1277.0621337890625\n",
      "epoch 43, val_loss 454.7169494628906\n",
      "epoch 44, train_loss 1277.0621337890625\n",
      "epoch 44, val_loss 454.7169494628906\n",
      "epoch 45, train_loss 1277.06201171875\n",
      "epoch 45, val_loss 454.7169189453125\n",
      "epoch 46, train_loss 1277.0621337890625\n",
      "epoch 46, val_loss 454.716796875\n",
      "epoch 47, train_loss 1277.0621337890625\n",
      "epoch 47, val_loss 454.716796875\n",
      "epoch 48, train_loss 1277.0621337890625\n",
      "epoch 48, val_loss 454.71673583984375\n",
      "epoch 49, train_loss 1277.06201171875\n",
      "epoch 49, val_loss 454.71673583984375\n",
      "epoch 50, train_loss 1277.0621337890625\n",
      "epoch 50, val_loss 454.7167053222656\n",
      "epoch 51, train_loss 1277.0618896484375\n",
      "epoch 51, val_loss 454.7167053222656\n",
      "epoch 52, train_loss 1277.0618896484375\n",
      "epoch 52, val_loss 454.71661376953125\n",
      "epoch 53, train_loss 1277.0618896484375\n",
      "epoch 53, val_loss 454.71661376953125\n",
      "epoch 54, train_loss 1277.0618896484375\n",
      "epoch 54, val_loss 454.7166748046875\n",
      "epoch 55, train_loss 1277.0618896484375\n",
      "epoch 55, val_loss 454.716552734375\n",
      "epoch 56, train_loss 1277.0618896484375\n",
      "epoch 56, val_loss 454.716552734375\n",
      "epoch 57, train_loss 1277.061767578125\n",
      "epoch 57, val_loss 454.71649169921875\n",
      "epoch 58, train_loss 1277.0616455078125\n",
      "epoch 58, val_loss 454.7164611816406\n",
      "epoch 59, train_loss 1277.0616455078125\n",
      "epoch 59, val_loss 454.7164306640625\n",
      "epoch 60, train_loss 1277.0616455078125\n",
      "epoch 60, val_loss 454.7164306640625\n",
      "epoch 61, train_loss 1277.0616455078125\n",
      "epoch 61, val_loss 454.7163391113281\n",
      "epoch 62, train_loss 1277.0615234375\n",
      "epoch 62, val_loss 454.71624755859375\n",
      "epoch 63, train_loss 1277.0615234375\n",
      "epoch 63, val_loss 454.71624755859375\n",
      "epoch 64, train_loss 1277.0615234375\n",
      "epoch 64, val_loss 454.71624755859375\n",
      "epoch 65, train_loss 1277.0615234375\n",
      "epoch 65, val_loss 454.7162170410156\n",
      "epoch 66, train_loss 1277.0615234375\n",
      "epoch 66, val_loss 454.7161560058594\n",
      "epoch 67, train_loss 1277.0615234375\n",
      "epoch 67, val_loss 454.7161560058594\n",
      "epoch 68, train_loss 1277.061279296875\n",
      "epoch 68, val_loss 454.7160949707031\n",
      "epoch 69, train_loss 1277.0614013671875\n",
      "epoch 69, val_loss 454.7160949707031\n",
      "epoch 70, train_loss 1277.0611572265625\n",
      "epoch 70, val_loss 454.7160339355469\n",
      "epoch 71, train_loss 1277.061279296875\n",
      "epoch 71, val_loss 454.7160339355469\n",
      "epoch 72, train_loss 1277.061279296875\n",
      "epoch 72, val_loss 454.71600341796875\n",
      "epoch 73, train_loss 1277.061279296875\n",
      "epoch 73, val_loss 454.7159729003906\n",
      "epoch 74, train_loss 1277.061279296875\n",
      "epoch 74, val_loss 454.7159118652344\n",
      "epoch 75, train_loss 1277.061279296875\n",
      "epoch 75, val_loss 454.71588134765625\n",
      "epoch 76, train_loss 1277.061279296875\n",
      "epoch 76, val_loss 454.71588134765625\n",
      "epoch 77, train_loss 1277.0611572265625\n",
      "epoch 77, val_loss 454.7158203125\n",
      "epoch 78, train_loss 1277.06103515625\n",
      "epoch 78, val_loss 454.71575927734375\n",
      "epoch 79, train_loss 1277.06103515625\n",
      "epoch 79, val_loss 454.71575927734375\n",
      "epoch 80, train_loss 1277.06103515625\n",
      "epoch 80, val_loss 454.7156677246094\n",
      "epoch 81, train_loss 1277.06103515625\n",
      "epoch 81, val_loss 454.7156677246094\n",
      "epoch 82, train_loss 1277.06103515625\n",
      "epoch 82, val_loss 454.71563720703125\n",
      "epoch 83, train_loss 1277.06103515625\n",
      "epoch 83, val_loss 454.715576171875\n",
      "epoch 84, train_loss 1277.06103515625\n",
      "epoch 84, val_loss 454.715576171875\n",
      "epoch 85, train_loss 1277.0611572265625\n",
      "epoch 85, val_loss 454.7155456542969\n",
      "epoch 86, train_loss 1277.06103515625\n",
      "epoch 86, val_loss 454.71551513671875\n",
      "epoch 87, train_loss 1277.060791015625\n",
      "epoch 87, val_loss 454.7154541015625\n",
      "epoch 88, train_loss 1277.060791015625\n",
      "epoch 88, val_loss 454.7154541015625\n",
      "epoch 89, train_loss 1277.060791015625\n",
      "epoch 89, val_loss 454.7154235839844\n",
      "epoch 90, train_loss 1277.060791015625\n",
      "epoch 90, val_loss 454.7154235839844\n",
      "epoch 91, train_loss 1277.060791015625\n",
      "epoch 91, val_loss 454.7153625488281\n",
      "epoch 92, train_loss 1277.060791015625\n",
      "epoch 92, val_loss 454.7153015136719\n",
      "epoch 93, train_loss 1277.060791015625\n",
      "epoch 93, val_loss 454.7152404785156\n",
      "epoch 94, train_loss 1277.0606689453125\n",
      "epoch 94, val_loss 454.7152404785156\n",
      "epoch 95, train_loss 1277.0606689453125\n",
      "epoch 95, val_loss 454.7152404785156\n",
      "epoch 96, train_loss 1277.060546875\n",
      "epoch 96, val_loss 454.7151794433594\n",
      "epoch 97, train_loss 1277.060546875\n",
      "epoch 97, val_loss 454.7151184082031\n",
      "epoch 98, train_loss 1277.060546875\n",
      "epoch 98, val_loss 454.7151184082031\n",
      "epoch 99, train_loss 1277.060546875\n",
      "epoch 99, val_loss 454.715087890625\n",
      "Parameter containing:\n",
      "tensor([4.0798e-29], requires_grad=True)\n",
      "iter 159, train_loss_regularization 0.6913446187973022\n",
      "iter 159, val_loss_regularization 0.6913446187973022\n",
      "epoch 0, train_loss 1277.060546875\n",
      "epoch 0, val_loss 454.7149963378906\n",
      "epoch 1, train_loss 1277.060546875\n",
      "epoch 1, val_loss 454.7149963378906\n",
      "epoch 2, train_loss 1277.0604248046875\n",
      "epoch 2, val_loss 454.7149963378906\n",
      "epoch 3, train_loss 1277.060546875\n",
      "epoch 3, val_loss 454.71490478515625\n",
      "epoch 4, train_loss 1277.060302734375\n",
      "epoch 4, val_loss 454.71490478515625\n",
      "epoch 5, train_loss 1277.060302734375\n",
      "epoch 5, val_loss 454.7148742675781\n",
      "epoch 6, train_loss 1277.060302734375\n",
      "epoch 6, val_loss 454.71484375\n",
      "epoch 7, train_loss 1277.060302734375\n",
      "epoch 7, val_loss 454.71478271484375\n",
      "epoch 8, train_loss 1277.0601806640625\n",
      "epoch 8, val_loss 454.71478271484375\n",
      "epoch 9, train_loss 1277.060302734375\n",
      "epoch 9, val_loss 454.7147521972656\n",
      "epoch 10, train_loss 1277.060302734375\n",
      "epoch 10, val_loss 454.7147216796875\n",
      "epoch 11, train_loss 1277.060302734375\n",
      "epoch 11, val_loss 454.71466064453125\n",
      "epoch 12, train_loss 1277.060302734375\n",
      "epoch 12, val_loss 454.71466064453125\n",
      "epoch 13, train_loss 1277.06005859375\n",
      "epoch 13, val_loss 454.71466064453125\n",
      "epoch 14, train_loss 1277.06005859375\n",
      "epoch 14, val_loss 454.71453857421875\n",
      "epoch 15, train_loss 1277.0599365234375\n",
      "epoch 15, val_loss 454.7145080566406\n",
      "epoch 16, train_loss 1277.0599365234375\n",
      "epoch 16, val_loss 454.7145080566406\n",
      "epoch 17, train_loss 1277.06005859375\n",
      "epoch 17, val_loss 454.7144470214844\n",
      "epoch 18, train_loss 1277.06005859375\n",
      "epoch 18, val_loss 454.71441650390625\n",
      "epoch 19, train_loss 1277.06005859375\n",
      "epoch 19, val_loss 454.71441650390625\n",
      "epoch 20, train_loss 1277.06005859375\n",
      "epoch 20, val_loss 454.7143249511719\n",
      "epoch 21, train_loss 1277.0599365234375\n",
      "epoch 21, val_loss 454.7143249511719\n",
      "epoch 22, train_loss 1277.0599365234375\n",
      "epoch 22, val_loss 454.7143249511719\n",
      "epoch 23, train_loss 1277.059814453125\n",
      "epoch 23, val_loss 454.71429443359375\n",
      "epoch 24, train_loss 1277.0596923828125\n",
      "epoch 24, val_loss 454.7142639160156\n",
      "epoch 25, train_loss 1277.059814453125\n",
      "epoch 25, val_loss 454.7142639160156\n",
      "epoch 26, train_loss 1277.059814453125\n",
      "epoch 26, val_loss 454.71417236328125\n",
      "epoch 27, train_loss 1277.059814453125\n",
      "epoch 27, val_loss 454.714111328125\n",
      "epoch 28, train_loss 1277.059814453125\n",
      "epoch 28, val_loss 454.714111328125\n",
      "epoch 29, train_loss 1277.0596923828125\n",
      "epoch 29, val_loss 454.7140808105469\n",
      "epoch 30, train_loss 1277.0596923828125\n",
      "epoch 30, val_loss 454.7139892578125\n",
      "epoch 31, train_loss 1277.0594482421875\n",
      "epoch 31, val_loss 454.7139587402344\n",
      "epoch 32, train_loss 1277.0595703125\n",
      "epoch 32, val_loss 454.7139892578125\n",
      "epoch 33, train_loss 1277.0595703125\n",
      "epoch 33, val_loss 454.7138671875\n",
      "epoch 34, train_loss 1277.0595703125\n",
      "epoch 34, val_loss 454.7138671875\n",
      "epoch 35, train_loss 1277.0595703125\n",
      "epoch 35, val_loss 454.7138671875\n",
      "epoch 36, train_loss 1277.0594482421875\n",
      "epoch 36, val_loss 454.7138366699219\n",
      "epoch 37, train_loss 1277.0594482421875\n",
      "epoch 37, val_loss 454.7137451171875\n",
      "epoch 38, train_loss 1277.0594482421875\n",
      "epoch 38, val_loss 454.7137451171875\n",
      "epoch 39, train_loss 1277.0594482421875\n",
      "epoch 39, val_loss 454.7137451171875\n",
      "epoch 40, train_loss 1277.059326171875\n",
      "epoch 40, val_loss 454.7137145996094\n",
      "epoch 41, train_loss 1277.0592041015625\n",
      "epoch 41, val_loss 454.7136535644531\n",
      "epoch 42, train_loss 1277.0592041015625\n",
      "epoch 42, val_loss 454.713623046875\n",
      "epoch 43, train_loss 1277.0592041015625\n",
      "epoch 43, val_loss 454.713623046875\n",
      "epoch 44, train_loss 1277.0592041015625\n",
      "epoch 44, val_loss 454.7135314941406\n",
      "epoch 45, train_loss 1277.0592041015625\n",
      "epoch 45, val_loss 454.7135314941406\n",
      "epoch 46, train_loss 1277.059326171875\n",
      "epoch 46, val_loss 454.7135009765625\n",
      "epoch 47, train_loss 1277.0592041015625\n",
      "epoch 47, val_loss 454.7135009765625\n",
      "epoch 48, train_loss 1277.0592041015625\n",
      "epoch 48, val_loss 454.7134094238281\n",
      "epoch 49, train_loss 1277.05908203125\n",
      "epoch 49, val_loss 454.7134094238281\n",
      "epoch 50, train_loss 1277.0589599609375\n",
      "epoch 50, val_loss 454.7133483886719\n",
      "epoch 51, train_loss 1277.0589599609375\n",
      "epoch 51, val_loss 454.7132568359375\n",
      "epoch 52, train_loss 1277.0589599609375\n",
      "epoch 52, val_loss 454.7132873535156\n",
      "epoch 53, train_loss 1277.05908203125\n",
      "epoch 53, val_loss 454.7132568359375\n",
      "epoch 54, train_loss 1277.058837890625\n",
      "epoch 54, val_loss 454.71319580078125\n",
      "epoch 55, train_loss 1277.0589599609375\n",
      "epoch 55, val_loss 454.7131652832031\n",
      "epoch 56, train_loss 1277.0589599609375\n",
      "epoch 56, val_loss 454.7131652832031\n",
      "epoch 57, train_loss 1277.0589599609375\n",
      "epoch 57, val_loss 454.713134765625\n",
      "epoch 58, train_loss 1277.058837890625\n",
      "epoch 58, val_loss 454.713134765625\n",
      "epoch 59, train_loss 1277.0587158203125\n",
      "epoch 59, val_loss 454.713134765625\n",
      "epoch 60, train_loss 1277.0587158203125\n",
      "epoch 60, val_loss 454.71307373046875\n",
      "epoch 61, train_loss 1277.0587158203125\n",
      "epoch 61, val_loss 454.7130126953125\n",
      "epoch 62, train_loss 1277.0587158203125\n",
      "epoch 62, val_loss 454.71295166015625\n",
      "epoch 63, train_loss 1277.0587158203125\n",
      "epoch 63, val_loss 454.71295166015625\n",
      "epoch 64, train_loss 1277.0587158203125\n",
      "epoch 64, val_loss 454.7129211425781\n",
      "epoch 65, train_loss 1277.0587158203125\n",
      "epoch 65, val_loss 454.71282958984375\n",
      "epoch 66, train_loss 1277.0587158203125\n",
      "epoch 66, val_loss 454.7127990722656\n",
      "epoch 67, train_loss 1277.0587158203125\n",
      "epoch 67, val_loss 454.7127990722656\n",
      "epoch 68, train_loss 1277.05859375\n",
      "epoch 68, val_loss 454.7127990722656\n",
      "epoch 69, train_loss 1277.0584716796875\n",
      "epoch 69, val_loss 454.7127380371094\n",
      "epoch 70, train_loss 1277.0584716796875\n",
      "epoch 70, val_loss 454.7126770019531\n",
      "epoch 71, train_loss 1277.05859375\n",
      "epoch 71, val_loss 454.7126770019531\n",
      "epoch 72, train_loss 1277.0584716796875\n",
      "epoch 72, val_loss 454.7126770019531\n",
      "epoch 73, train_loss 1277.0584716796875\n",
      "epoch 73, val_loss 454.71258544921875\n",
      "epoch 74, train_loss 1277.0584716796875\n",
      "epoch 74, val_loss 454.7125549316406\n",
      "epoch 75, train_loss 1277.058349609375\n",
      "epoch 75, val_loss 454.7124938964844\n",
      "epoch 76, train_loss 1277.0584716796875\n",
      "epoch 76, val_loss 454.7124938964844\n",
      "epoch 77, train_loss 1277.0584716796875\n",
      "epoch 77, val_loss 454.71246337890625\n",
      "epoch 78, train_loss 1277.0582275390625\n",
      "epoch 78, val_loss 454.71240234375\n",
      "epoch 79, train_loss 1277.0582275390625\n",
      "epoch 79, val_loss 454.7123718261719\n",
      "epoch 80, train_loss 1277.0582275390625\n",
      "epoch 80, val_loss 454.7123718261719\n",
      "epoch 81, train_loss 1277.0582275390625\n",
      "epoch 81, val_loss 454.71234130859375\n",
      "epoch 82, train_loss 1277.0582275390625\n",
      "epoch 82, val_loss 454.71234130859375\n",
      "epoch 83, train_loss 1277.0582275390625\n",
      "epoch 83, val_loss 454.7122497558594\n",
      "epoch 84, train_loss 1277.0582275390625\n",
      "epoch 84, val_loss 454.71221923828125\n",
      "epoch 85, train_loss 1277.0582275390625\n",
      "epoch 85, val_loss 454.7121276855469\n",
      "epoch 86, train_loss 1277.0582275390625\n",
      "epoch 86, val_loss 454.712158203125\n",
      "epoch 87, train_loss 1277.05810546875\n",
      "epoch 87, val_loss 454.7121276855469\n",
      "epoch 88, train_loss 1277.057861328125\n",
      "epoch 88, val_loss 454.7120361328125\n",
      "epoch 89, train_loss 1277.057861328125\n",
      "epoch 89, val_loss 454.7120361328125\n",
      "epoch 90, train_loss 1277.057861328125\n",
      "epoch 90, val_loss 454.7120361328125\n",
      "epoch 91, train_loss 1277.057861328125\n",
      "epoch 91, val_loss 454.7119445800781\n",
      "epoch 92, train_loss 1277.0579833984375\n",
      "epoch 92, val_loss 454.7120056152344\n",
      "epoch 93, train_loss 1277.0579833984375\n",
      "epoch 93, val_loss 454.7119140625\n",
      "epoch 94, train_loss 1277.057861328125\n",
      "epoch 94, val_loss 454.7119140625\n",
      "epoch 95, train_loss 1277.0577392578125\n",
      "epoch 95, val_loss 454.7119140625\n",
      "epoch 96, train_loss 1277.0576171875\n",
      "epoch 96, val_loss 454.7118835449219\n",
      "epoch 97, train_loss 1277.0576171875\n",
      "epoch 97, val_loss 454.7117614746094\n",
      "epoch 98, train_loss 1277.0577392578125\n",
      "epoch 98, val_loss 454.7117614746094\n",
      "epoch 99, train_loss 1277.0576171875\n",
      "epoch 99, val_loss 454.7117614746094\n",
      "Parameter containing:\n",
      "tensor([2.8483e-29], requires_grad=True)\n",
      "iter 160, train_loss_regularization 0.6909049153327942\n",
      "iter 160, val_loss_regularization 0.6909049153327942\n",
      "epoch 0, train_loss 1277.0576171875\n",
      "epoch 0, val_loss 454.711669921875\n",
      "epoch 1, train_loss 1277.0576171875\n",
      "epoch 1, val_loss 454.7116394042969\n",
      "epoch 2, train_loss 1277.0576171875\n",
      "epoch 2, val_loss 454.7115783691406\n",
      "epoch 3, train_loss 1277.0576171875\n",
      "epoch 3, val_loss 454.7116394042969\n",
      "epoch 4, train_loss 1277.0574951171875\n",
      "epoch 4, val_loss 454.7115783691406\n",
      "epoch 5, train_loss 1277.0574951171875\n",
      "epoch 5, val_loss 454.7115478515625\n",
      "epoch 6, train_loss 1277.0574951171875\n",
      "epoch 6, val_loss 454.7114562988281\n",
      "epoch 7, train_loss 1277.057373046875\n",
      "epoch 7, val_loss 454.71148681640625\n",
      "epoch 8, train_loss 1277.057373046875\n",
      "epoch 8, val_loss 454.71142578125\n",
      "epoch 9, train_loss 1277.0574951171875\n",
      "epoch 9, val_loss 454.71136474609375\n",
      "epoch 10, train_loss 1277.0574951171875\n",
      "epoch 10, val_loss 454.71136474609375\n",
      "epoch 11, train_loss 1277.0574951171875\n",
      "epoch 11, val_loss 454.7113342285156\n",
      "epoch 12, train_loss 1277.057373046875\n",
      "epoch 12, val_loss 454.7113037109375\n",
      "epoch 13, train_loss 1277.057373046875\n",
      "epoch 13, val_loss 454.7113037109375\n",
      "epoch 14, train_loss 1277.057373046875\n",
      "epoch 14, val_loss 454.71124267578125\n",
      "epoch 15, train_loss 1277.057373046875\n",
      "epoch 15, val_loss 454.7112121582031\n",
      "epoch 16, train_loss 1277.0572509765625\n",
      "epoch 16, val_loss 454.71112060546875\n",
      "epoch 17, train_loss 1277.0572509765625\n",
      "epoch 17, val_loss 454.71112060546875\n",
      "epoch 18, train_loss 1277.0572509765625\n",
      "epoch 18, val_loss 454.71112060546875\n",
      "epoch 19, train_loss 1277.05712890625\n",
      "epoch 19, val_loss 454.7110290527344\n",
      "epoch 20, train_loss 1277.05712890625\n",
      "epoch 20, val_loss 454.7110290527344\n",
      "epoch 21, train_loss 1277.05712890625\n",
      "epoch 21, val_loss 454.7109680175781\n",
      "epoch 22, train_loss 1277.0570068359375\n",
      "epoch 22, val_loss 454.7109069824219\n",
      "epoch 23, train_loss 1277.056884765625\n",
      "epoch 23, val_loss 454.7109680175781\n",
      "epoch 24, train_loss 1277.056884765625\n",
      "epoch 24, val_loss 454.71087646484375\n",
      "epoch 25, train_loss 1277.0570068359375\n",
      "epoch 25, val_loss 454.7108459472656\n",
      "epoch 26, train_loss 1277.0570068359375\n",
      "epoch 26, val_loss 454.7108459472656\n",
      "epoch 27, train_loss 1277.0570068359375\n",
      "epoch 27, val_loss 454.7108459472656\n",
      "epoch 28, train_loss 1277.056884765625\n",
      "epoch 28, val_loss 454.7107849121094\n",
      "epoch 29, train_loss 1277.056884765625\n",
      "epoch 29, val_loss 454.71075439453125\n",
      "epoch 30, train_loss 1277.056884765625\n",
      "epoch 30, val_loss 454.71075439453125\n",
      "epoch 31, train_loss 1277.056884765625\n",
      "epoch 31, val_loss 454.710693359375\n",
      "epoch 32, train_loss 1277.056884765625\n",
      "epoch 32, val_loss 454.71063232421875\n",
      "epoch 33, train_loss 1277.0567626953125\n",
      "epoch 33, val_loss 454.7105712890625\n",
      "epoch 34, train_loss 1277.0567626953125\n",
      "epoch 34, val_loss 454.71051025390625\n",
      "epoch 35, train_loss 1277.056640625\n",
      "epoch 35, val_loss 454.71051025390625\n",
      "epoch 36, train_loss 1277.056640625\n",
      "epoch 36, val_loss 454.71051025390625\n",
      "epoch 37, train_loss 1277.056640625\n",
      "epoch 37, val_loss 454.71044921875\n",
      "epoch 38, train_loss 1277.056640625\n",
      "epoch 38, val_loss 454.71044921875\n",
      "epoch 39, train_loss 1277.056396484375\n",
      "epoch 39, val_loss 454.71038818359375\n",
      "epoch 40, train_loss 1277.0565185546875\n",
      "epoch 40, val_loss 454.7104187011719\n",
      "epoch 41, train_loss 1277.056396484375\n",
      "epoch 41, val_loss 454.71038818359375\n",
      "epoch 42, train_loss 1277.056396484375\n",
      "epoch 42, val_loss 454.7102966308594\n",
      "epoch 43, train_loss 1277.056640625\n",
      "epoch 43, val_loss 454.7102966308594\n",
      "epoch 44, train_loss 1277.0565185546875\n",
      "epoch 44, val_loss 454.7102355957031\n",
      "epoch 45, train_loss 1277.0565185546875\n",
      "epoch 45, val_loss 454.710205078125\n",
      "epoch 46, train_loss 1277.056396484375\n",
      "epoch 46, val_loss 454.7101745605469\n",
      "epoch 47, train_loss 1277.0562744140625\n",
      "epoch 47, val_loss 454.7101745605469\n",
      "epoch 48, train_loss 1277.0562744140625\n",
      "epoch 48, val_loss 454.7100524902344\n",
      "epoch 49, train_loss 1277.0562744140625\n",
      "epoch 49, val_loss 454.7100830078125\n",
      "epoch 50, train_loss 1277.0562744140625\n",
      "epoch 50, val_loss 454.7100524902344\n",
      "epoch 51, train_loss 1277.0562744140625\n",
      "epoch 51, val_loss 454.7099914550781\n",
      "epoch 52, train_loss 1277.0562744140625\n",
      "epoch 52, val_loss 454.7099914550781\n",
      "epoch 53, train_loss 1277.05615234375\n",
      "epoch 53, val_loss 454.7099304199219\n",
      "epoch 54, train_loss 1277.0562744140625\n",
      "epoch 54, val_loss 454.7099304199219\n",
      "epoch 55, train_loss 1277.05615234375\n",
      "epoch 55, val_loss 454.7098693847656\n",
      "epoch 56, train_loss 1277.05615234375\n",
      "epoch 56, val_loss 454.70977783203125\n",
      "epoch 57, train_loss 1277.0560302734375\n",
      "epoch 57, val_loss 454.70977783203125\n",
      "epoch 58, train_loss 1277.0560302734375\n",
      "epoch 58, val_loss 454.7097473144531\n",
      "epoch 59, train_loss 1277.0560302734375\n",
      "epoch 59, val_loss 454.7097473144531\n",
      "epoch 60, train_loss 1277.0560302734375\n",
      "epoch 60, val_loss 454.70965576171875\n",
      "epoch 61, train_loss 1277.0560302734375\n",
      "epoch 61, val_loss 454.70965576171875\n",
      "epoch 62, train_loss 1277.0560302734375\n",
      "epoch 62, val_loss 454.7096252441406\n",
      "epoch 63, train_loss 1277.0560302734375\n",
      "epoch 63, val_loss 454.7095947265625\n",
      "epoch 64, train_loss 1277.0560302734375\n",
      "epoch 64, val_loss 454.7096252441406\n",
      "epoch 65, train_loss 1277.0560302734375\n",
      "epoch 65, val_loss 454.70953369140625\n",
      "epoch 66, train_loss 1277.0557861328125\n",
      "epoch 66, val_loss 454.70953369140625\n",
      "epoch 67, train_loss 1277.055908203125\n",
      "epoch 67, val_loss 454.70947265625\n",
      "epoch 68, train_loss 1277.0557861328125\n",
      "epoch 68, val_loss 454.70941162109375\n",
      "epoch 69, train_loss 1277.055908203125\n",
      "epoch 69, val_loss 454.70941162109375\n",
      "epoch 70, train_loss 1277.0557861328125\n",
      "epoch 70, val_loss 454.7093200683594\n",
      "epoch 71, train_loss 1277.0557861328125\n",
      "epoch 71, val_loss 454.7093811035156\n",
      "epoch 72, train_loss 1277.0556640625\n",
      "epoch 72, val_loss 454.70928955078125\n",
      "epoch 73, train_loss 1277.0555419921875\n",
      "epoch 73, val_loss 454.7092590332031\n",
      "epoch 74, train_loss 1277.0555419921875\n",
      "epoch 74, val_loss 454.70928955078125\n",
      "epoch 75, train_loss 1277.0555419921875\n",
      "epoch 75, val_loss 454.7091979980469\n",
      "epoch 76, train_loss 1277.0555419921875\n",
      "epoch 76, val_loss 454.70916748046875\n",
      "epoch 77, train_loss 1277.0555419921875\n",
      "epoch 77, val_loss 454.70916748046875\n",
      "epoch 78, train_loss 1277.0555419921875\n",
      "epoch 78, val_loss 454.7091369628906\n",
      "epoch 79, train_loss 1277.0555419921875\n",
      "epoch 79, val_loss 454.7090759277344\n",
      "epoch 80, train_loss 1277.0555419921875\n",
      "epoch 80, val_loss 454.70904541015625\n",
      "epoch 81, train_loss 1277.0555419921875\n",
      "epoch 81, val_loss 454.7089538574219\n",
      "epoch 82, train_loss 1277.0552978515625\n",
      "epoch 82, val_loss 454.7090148925781\n",
      "epoch 83, train_loss 1277.055419921875\n",
      "epoch 83, val_loss 454.7089538574219\n",
      "epoch 84, train_loss 1277.0552978515625\n",
      "epoch 84, val_loss 454.7089538574219\n",
      "epoch 85, train_loss 1277.0552978515625\n",
      "epoch 85, val_loss 454.70892333984375\n",
      "epoch 86, train_loss 1277.0552978515625\n",
      "epoch 86, val_loss 454.7088317871094\n",
      "epoch 87, train_loss 1277.0552978515625\n",
      "epoch 87, val_loss 454.708740234375\n",
      "epoch 88, train_loss 1277.0552978515625\n",
      "epoch 88, val_loss 454.70880126953125\n",
      "epoch 89, train_loss 1277.0552978515625\n",
      "epoch 89, val_loss 454.708740234375\n",
      "epoch 90, train_loss 1277.05517578125\n",
      "epoch 90, val_loss 454.7087097167969\n",
      "epoch 91, train_loss 1277.0550537109375\n",
      "epoch 91, val_loss 454.7086181640625\n",
      "epoch 92, train_loss 1277.05517578125\n",
      "epoch 92, val_loss 454.7086181640625\n",
      "epoch 93, train_loss 1277.05517578125\n",
      "epoch 93, val_loss 454.7086181640625\n",
      "epoch 94, train_loss 1277.0550537109375\n",
      "epoch 94, val_loss 454.7085876464844\n",
      "epoch 95, train_loss 1277.054931640625\n",
      "epoch 95, val_loss 454.70849609375\n",
      "epoch 96, train_loss 1277.0550537109375\n",
      "epoch 96, val_loss 454.7085266113281\n",
      "epoch 97, train_loss 1277.05517578125\n",
      "epoch 97, val_loss 454.7084655761719\n",
      "epoch 98, train_loss 1277.05517578125\n",
      "epoch 98, val_loss 454.70849609375\n",
      "epoch 99, train_loss 1277.05517578125\n",
      "epoch 99, val_loss 454.7084045410156\n",
      "Parameter containing:\n",
      "tensor([1.9888e-29], requires_grad=True)\n",
      "iter 161, train_loss_regularization 0.690475344657898\n",
      "iter 161, val_loss_regularization 0.690475344657898\n",
      "epoch 0, train_loss 1277.054931640625\n",
      "epoch 0, val_loss 454.7083740234375\n",
      "epoch 1, train_loss 1277.0548095703125\n",
      "epoch 1, val_loss 454.7083740234375\n",
      "epoch 2, train_loss 1277.0548095703125\n",
      "epoch 2, val_loss 454.7083740234375\n",
      "epoch 3, train_loss 1277.0548095703125\n",
      "epoch 3, val_loss 454.708251953125\n",
      "epoch 4, train_loss 1277.0548095703125\n",
      "epoch 4, val_loss 454.7081604003906\n",
      "epoch 5, train_loss 1277.0548095703125\n",
      "epoch 5, val_loss 454.7082214355469\n",
      "epoch 6, train_loss 1277.0548095703125\n",
      "epoch 6, val_loss 454.7081604003906\n",
      "epoch 7, train_loss 1277.0548095703125\n",
      "epoch 7, val_loss 454.7081298828125\n",
      "epoch 8, train_loss 1277.0546875\n",
      "epoch 8, val_loss 454.7081604003906\n",
      "epoch 9, train_loss 1277.0546875\n",
      "epoch 9, val_loss 454.70806884765625\n",
      "epoch 10, train_loss 1277.0546875\n",
      "epoch 10, val_loss 454.70806884765625\n",
      "epoch 11, train_loss 1277.0546875\n",
      "epoch 11, val_loss 454.70806884765625\n",
      "epoch 12, train_loss 1277.0545654296875\n",
      "epoch 12, val_loss 454.7080078125\n",
      "epoch 13, train_loss 1277.054443359375\n",
      "epoch 13, val_loss 454.7080078125\n",
      "epoch 14, train_loss 1277.054443359375\n",
      "epoch 14, val_loss 454.7079162597656\n",
      "epoch 15, train_loss 1277.0545654296875\n",
      "epoch 15, val_loss 454.7078857421875\n",
      "epoch 16, train_loss 1277.054443359375\n",
      "epoch 16, val_loss 454.70782470703125\n",
      "epoch 17, train_loss 1277.0545654296875\n",
      "epoch 17, val_loss 454.70782470703125\n",
      "epoch 18, train_loss 1277.0545654296875\n",
      "epoch 18, val_loss 454.7077941894531\n",
      "epoch 19, train_loss 1277.054443359375\n",
      "epoch 19, val_loss 454.707763671875\n",
      "epoch 20, train_loss 1277.054443359375\n",
      "epoch 20, val_loss 454.707763671875\n",
      "epoch 21, train_loss 1277.0543212890625\n",
      "epoch 21, val_loss 454.707763671875\n",
      "epoch 22, train_loss 1277.05419921875\n",
      "epoch 22, val_loss 454.7076721191406\n",
      "epoch 23, train_loss 1277.05419921875\n",
      "epoch 23, val_loss 454.7076110839844\n",
      "epoch 24, train_loss 1277.05419921875\n",
      "epoch 24, val_loss 454.70758056640625\n",
      "epoch 25, train_loss 1277.05419921875\n",
      "epoch 25, val_loss 454.70758056640625\n",
      "epoch 26, train_loss 1277.05419921875\n",
      "epoch 26, val_loss 454.7075500488281\n",
      "epoch 27, train_loss 1277.0540771484375\n",
      "epoch 27, val_loss 454.70745849609375\n",
      "epoch 28, train_loss 1277.05419921875\n",
      "epoch 28, val_loss 454.7074279785156\n",
      "epoch 29, train_loss 1277.05419921875\n",
      "epoch 29, val_loss 454.70745849609375\n",
      "epoch 30, train_loss 1277.0540771484375\n",
      "epoch 30, val_loss 454.7074279785156\n",
      "epoch 31, train_loss 1277.0540771484375\n",
      "epoch 31, val_loss 454.7073669433594\n",
      "epoch 32, train_loss 1277.05419921875\n",
      "epoch 32, val_loss 454.7073059082031\n",
      "epoch 33, train_loss 1277.053955078125\n",
      "epoch 33, val_loss 454.7073059082031\n",
      "epoch 34, train_loss 1277.053955078125\n",
      "epoch 34, val_loss 454.70733642578125\n",
      "epoch 35, train_loss 1277.0540771484375\n",
      "epoch 35, val_loss 454.7073059082031\n",
      "epoch 36, train_loss 1277.0540771484375\n",
      "epoch 36, val_loss 454.7072448730469\n",
      "epoch 37, train_loss 1277.0540771484375\n",
      "epoch 37, val_loss 454.7071533203125\n",
      "epoch 38, train_loss 1277.0538330078125\n",
      "epoch 38, val_loss 454.7071228027344\n",
      "epoch 39, train_loss 1277.0538330078125\n",
      "epoch 39, val_loss 454.7071533203125\n",
      "epoch 40, train_loss 1277.0538330078125\n",
      "epoch 40, val_loss 454.70703125\n",
      "epoch 41, train_loss 1277.0538330078125\n",
      "epoch 41, val_loss 454.70703125\n",
      "epoch 42, train_loss 1277.0537109375\n",
      "epoch 42, val_loss 454.70697021484375\n",
      "epoch 43, train_loss 1277.0537109375\n",
      "epoch 43, val_loss 454.70697021484375\n",
      "epoch 44, train_loss 1277.0537109375\n",
      "epoch 44, val_loss 454.7069091796875\n",
      "epoch 45, train_loss 1277.0537109375\n",
      "epoch 45, val_loss 454.7068786621094\n",
      "epoch 46, train_loss 1277.0537109375\n",
      "epoch 46, val_loss 454.70684814453125\n",
      "epoch 47, train_loss 1277.0535888671875\n",
      "epoch 47, val_loss 454.7068786621094\n",
      "epoch 48, train_loss 1277.0537109375\n",
      "epoch 48, val_loss 454.70684814453125\n",
      "epoch 49, train_loss 1277.053466796875\n",
      "epoch 49, val_loss 454.706787109375\n",
      "epoch 50, train_loss 1277.0537109375\n",
      "epoch 50, val_loss 454.706787109375\n",
      "epoch 51, train_loss 1277.0537109375\n",
      "epoch 51, val_loss 454.7066650390625\n",
      "epoch 52, train_loss 1277.0535888671875\n",
      "epoch 52, val_loss 454.7066955566406\n",
      "epoch 53, train_loss 1277.0537109375\n",
      "epoch 53, val_loss 454.7066650390625\n",
      "epoch 54, train_loss 1277.053466796875\n",
      "epoch 54, val_loss 454.7066345214844\n",
      "epoch 55, train_loss 1277.053466796875\n",
      "epoch 55, val_loss 454.7065734863281\n",
      "epoch 56, train_loss 1277.053466796875\n",
      "epoch 56, val_loss 454.7065124511719\n",
      "epoch 57, train_loss 1277.0533447265625\n",
      "epoch 57, val_loss 454.7064514160156\n",
      "epoch 58, train_loss 1277.0533447265625\n",
      "epoch 58, val_loss 454.7064514160156\n",
      "epoch 59, train_loss 1277.05322265625\n",
      "epoch 59, val_loss 454.7064514160156\n",
      "epoch 60, train_loss 1277.05322265625\n",
      "epoch 60, val_loss 454.70635986328125\n",
      "epoch 61, train_loss 1277.05322265625\n",
      "epoch 61, val_loss 454.70635986328125\n",
      "epoch 62, train_loss 1277.05322265625\n",
      "epoch 62, val_loss 454.706298828125\n",
      "epoch 63, train_loss 1277.05322265625\n",
      "epoch 63, val_loss 454.7063293457031\n",
      "epoch 64, train_loss 1277.05322265625\n",
      "epoch 64, val_loss 454.706298828125\n",
      "epoch 65, train_loss 1277.05322265625\n",
      "epoch 65, val_loss 454.7062072753906\n",
      "epoch 66, train_loss 1277.0531005859375\n",
      "epoch 66, val_loss 454.7061767578125\n",
      "epoch 67, train_loss 1277.0528564453125\n",
      "epoch 67, val_loss 454.7062072753906\n",
      "epoch 68, train_loss 1277.052978515625\n",
      "epoch 68, val_loss 454.7062072753906\n",
      "epoch 69, train_loss 1277.052978515625\n",
      "epoch 69, val_loss 454.70611572265625\n",
      "epoch 70, train_loss 1277.052978515625\n",
      "epoch 70, val_loss 454.7060852050781\n",
      "epoch 71, train_loss 1277.0531005859375\n",
      "epoch 71, val_loss 454.7060546875\n",
      "epoch 72, train_loss 1277.0528564453125\n",
      "epoch 72, val_loss 454.7060546875\n",
      "epoch 73, train_loss 1277.052978515625\n",
      "epoch 73, val_loss 454.7060546875\n",
      "epoch 74, train_loss 1277.0528564453125\n",
      "epoch 74, val_loss 454.70599365234375\n",
      "epoch 75, train_loss 1277.0528564453125\n",
      "epoch 75, val_loss 454.7058410644531\n",
      "epoch 76, train_loss 1277.0526123046875\n",
      "epoch 76, val_loss 454.70587158203125\n",
      "epoch 77, train_loss 1277.052734375\n",
      "epoch 77, val_loss 454.70587158203125\n",
      "epoch 78, train_loss 1277.052734375\n",
      "epoch 78, val_loss 454.7058410644531\n",
      "epoch 79, train_loss 1277.052734375\n",
      "epoch 79, val_loss 454.7057800292969\n",
      "epoch 80, train_loss 1277.052734375\n",
      "epoch 80, val_loss 454.7057189941406\n",
      "epoch 81, train_loss 1277.052734375\n",
      "epoch 81, val_loss 454.7057189941406\n",
      "epoch 82, train_loss 1277.0528564453125\n",
      "epoch 82, val_loss 454.7057189941406\n",
      "epoch 83, train_loss 1277.0528564453125\n",
      "epoch 83, val_loss 454.7056579589844\n",
      "epoch 84, train_loss 1277.0526123046875\n",
      "epoch 84, val_loss 454.70562744140625\n",
      "epoch 85, train_loss 1277.0526123046875\n",
      "epoch 85, val_loss 454.70562744140625\n",
      "epoch 86, train_loss 1277.0526123046875\n",
      "epoch 86, val_loss 454.7055969238281\n",
      "epoch 87, train_loss 1277.0526123046875\n",
      "epoch 87, val_loss 454.7055358886719\n",
      "epoch 88, train_loss 1277.0526123046875\n",
      "epoch 88, val_loss 454.7055358886719\n",
      "epoch 89, train_loss 1277.0523681640625\n",
      "epoch 89, val_loss 454.7054138183594\n",
      "epoch 90, train_loss 1277.052490234375\n",
      "epoch 90, val_loss 454.7054443359375\n",
      "epoch 91, train_loss 1277.052490234375\n",
      "epoch 91, val_loss 454.7054138183594\n",
      "epoch 92, train_loss 1277.0526123046875\n",
      "epoch 92, val_loss 454.7054138183594\n",
      "epoch 93, train_loss 1277.052490234375\n",
      "epoch 93, val_loss 454.7052917480469\n",
      "epoch 94, train_loss 1277.05224609375\n",
      "epoch 94, val_loss 454.70526123046875\n",
      "epoch 95, train_loss 1277.05224609375\n",
      "epoch 95, val_loss 454.7052917480469\n",
      "epoch 96, train_loss 1277.05224609375\n",
      "epoch 96, val_loss 454.70526123046875\n",
      "epoch 97, train_loss 1277.05224609375\n",
      "epoch 97, val_loss 454.7052001953125\n",
      "epoch 98, train_loss 1277.0523681640625\n",
      "epoch 98, val_loss 454.70513916015625\n",
      "epoch 99, train_loss 1277.0523681640625\n",
      "epoch 99, val_loss 454.70513916015625\n",
      "Parameter containing:\n",
      "tensor([1.3889e-29], requires_grad=True)\n",
      "iter 162, train_loss_regularization 0.6900582313537598\n",
      "iter 162, val_loss_regularization 0.6900582313537598\n",
      "epoch 0, train_loss 1277.05224609375\n",
      "epoch 0, val_loss 454.705078125\n",
      "epoch 1, train_loss 1277.0521240234375\n",
      "epoch 1, val_loss 454.705078125\n",
      "epoch 2, train_loss 1277.052001953125\n",
      "epoch 2, val_loss 454.7050476074219\n",
      "epoch 3, train_loss 1277.052001953125\n",
      "epoch 3, val_loss 454.7049560546875\n",
      "epoch 4, train_loss 1277.0521240234375\n",
      "epoch 4, val_loss 454.7049560546875\n",
      "epoch 5, train_loss 1277.0521240234375\n",
      "epoch 5, val_loss 454.7049560546875\n",
      "epoch 6, train_loss 1277.0521240234375\n",
      "epoch 6, val_loss 454.7049560546875\n",
      "epoch 7, train_loss 1277.0521240234375\n",
      "epoch 7, val_loss 454.7048645019531\n",
      "epoch 8, train_loss 1277.0518798828125\n",
      "epoch 8, val_loss 454.704833984375\n",
      "epoch 9, train_loss 1277.0518798828125\n",
      "epoch 9, val_loss 454.704833984375\n",
      "epoch 10, train_loss 1277.0518798828125\n",
      "epoch 10, val_loss 454.7048034667969\n",
      "epoch 11, train_loss 1277.0518798828125\n",
      "epoch 11, val_loss 454.7047424316406\n",
      "epoch 12, train_loss 1277.0518798828125\n",
      "epoch 12, val_loss 454.7046813964844\n",
      "epoch 13, train_loss 1277.0518798828125\n",
      "epoch 13, val_loss 454.7046203613281\n",
      "epoch 14, train_loss 1277.0516357421875\n",
      "epoch 14, val_loss 454.7046203613281\n",
      "epoch 15, train_loss 1277.0516357421875\n",
      "epoch 15, val_loss 454.7046203613281\n",
      "epoch 16, train_loss 1277.0517578125\n",
      "epoch 16, val_loss 454.70452880859375\n",
      "epoch 17, train_loss 1277.0517578125\n",
      "epoch 17, val_loss 454.7044982910156\n",
      "epoch 18, train_loss 1277.0516357421875\n",
      "epoch 18, val_loss 454.7044982910156\n",
      "epoch 19, train_loss 1277.0517578125\n",
      "epoch 19, val_loss 454.7044982910156\n",
      "epoch 20, train_loss 1277.0516357421875\n",
      "epoch 20, val_loss 454.7044982910156\n",
      "epoch 21, train_loss 1277.0516357421875\n",
      "epoch 21, val_loss 454.7044677734375\n",
      "epoch 22, train_loss 1277.0516357421875\n",
      "epoch 22, val_loss 454.7043762207031\n",
      "epoch 23, train_loss 1277.0516357421875\n",
      "epoch 23, val_loss 454.704345703125\n",
      "epoch 24, train_loss 1277.0516357421875\n",
      "epoch 24, val_loss 454.704345703125\n",
      "epoch 25, train_loss 1277.0516357421875\n",
      "epoch 25, val_loss 454.704345703125\n",
      "epoch 26, train_loss 1277.0516357421875\n",
      "epoch 26, val_loss 454.7042541503906\n",
      "epoch 27, train_loss 1277.0516357421875\n",
      "epoch 27, val_loss 454.70416259765625\n",
      "epoch 28, train_loss 1277.0513916015625\n",
      "epoch 28, val_loss 454.7041931152344\n",
      "epoch 29, train_loss 1277.0513916015625\n",
      "epoch 29, val_loss 454.70416259765625\n",
      "epoch 30, train_loss 1277.0513916015625\n",
      "epoch 30, val_loss 454.7041320800781\n",
      "epoch 31, train_loss 1277.0513916015625\n",
      "epoch 31, val_loss 454.7040710449219\n",
      "epoch 32, train_loss 1277.05126953125\n",
      "epoch 32, val_loss 454.70404052734375\n",
      "epoch 33, train_loss 1277.0513916015625\n",
      "epoch 33, val_loss 454.7040710449219\n",
      "epoch 34, train_loss 1277.0513916015625\n",
      "epoch 34, val_loss 454.70404052734375\n",
      "epoch 35, train_loss 1277.05126953125\n",
      "epoch 35, val_loss 454.7040100097656\n",
      "epoch 36, train_loss 1277.05126953125\n",
      "epoch 36, val_loss 454.70391845703125\n",
      "epoch 37, train_loss 1277.05126953125\n",
      "epoch 37, val_loss 454.7039489746094\n",
      "epoch 38, train_loss 1277.05126953125\n",
      "epoch 38, val_loss 454.7038879394531\n",
      "epoch 39, train_loss 1277.05126953125\n",
      "epoch 39, val_loss 454.7038269042969\n",
      "epoch 40, train_loss 1277.05126953125\n",
      "epoch 40, val_loss 454.7038269042969\n",
      "epoch 41, train_loss 1277.051025390625\n",
      "epoch 41, val_loss 454.7037353515625\n",
      "epoch 42, train_loss 1277.051025390625\n",
      "epoch 42, val_loss 454.7037353515625\n",
      "epoch 43, train_loss 1277.051025390625\n",
      "epoch 43, val_loss 454.7037353515625\n",
      "epoch 44, train_loss 1277.051025390625\n",
      "epoch 44, val_loss 454.7037048339844\n",
      "epoch 45, train_loss 1277.051025390625\n",
      "epoch 45, val_loss 454.70367431640625\n",
      "epoch 46, train_loss 1277.051025390625\n",
      "epoch 46, val_loss 454.7035827636719\n",
      "epoch 47, train_loss 1277.051025390625\n",
      "epoch 47, val_loss 454.7035827636719\n",
      "epoch 48, train_loss 1277.05078125\n",
      "epoch 48, val_loss 454.70355224609375\n",
      "epoch 49, train_loss 1277.05078125\n",
      "epoch 49, val_loss 454.7034912109375\n",
      "epoch 50, train_loss 1277.0509033203125\n",
      "epoch 50, val_loss 454.7034606933594\n",
      "epoch 51, train_loss 1277.05078125\n",
      "epoch 51, val_loss 454.70343017578125\n",
      "epoch 52, train_loss 1277.05078125\n",
      "epoch 52, val_loss 454.70343017578125\n",
      "epoch 53, train_loss 1277.05078125\n",
      "epoch 53, val_loss 454.70343017578125\n",
      "epoch 54, train_loss 1277.0506591796875\n",
      "epoch 54, val_loss 454.70343017578125\n",
      "epoch 55, train_loss 1277.050537109375\n",
      "epoch 55, val_loss 454.7032775878906\n",
      "epoch 56, train_loss 1277.0506591796875\n",
      "epoch 56, val_loss 454.7033386230469\n",
      "epoch 57, train_loss 1277.0506591796875\n",
      "epoch 57, val_loss 454.7032775878906\n",
      "epoch 58, train_loss 1277.050537109375\n",
      "epoch 58, val_loss 454.7032470703125\n",
      "epoch 59, train_loss 1277.050537109375\n",
      "epoch 59, val_loss 454.7032165527344\n",
      "epoch 60, train_loss 1277.050537109375\n",
      "epoch 60, val_loss 454.703125\n",
      "epoch 61, train_loss 1277.050537109375\n",
      "epoch 61, val_loss 454.703125\n",
      "epoch 62, train_loss 1277.0504150390625\n",
      "epoch 62, val_loss 454.7030944824219\n",
      "epoch 63, train_loss 1277.0504150390625\n",
      "epoch 63, val_loss 454.7030944824219\n",
      "epoch 64, train_loss 1277.050537109375\n",
      "epoch 64, val_loss 454.7030029296875\n",
      "epoch 65, train_loss 1277.0504150390625\n",
      "epoch 65, val_loss 454.7030029296875\n",
      "epoch 66, train_loss 1277.0504150390625\n",
      "epoch 66, val_loss 454.7030029296875\n",
      "epoch 67, train_loss 1277.050537109375\n",
      "epoch 67, val_loss 454.7029724121094\n",
      "epoch 68, train_loss 1277.050537109375\n",
      "epoch 68, val_loss 454.7029724121094\n",
      "epoch 69, train_loss 1277.0504150390625\n",
      "epoch 69, val_loss 454.702880859375\n",
      "epoch 70, train_loss 1277.0504150390625\n",
      "epoch 70, val_loss 454.70281982421875\n",
      "epoch 71, train_loss 1277.05029296875\n",
      "epoch 71, val_loss 454.7027893066406\n",
      "epoch 72, train_loss 1277.05029296875\n",
      "epoch 72, val_loss 454.7027587890625\n",
      "epoch 73, train_loss 1277.0504150390625\n",
      "epoch 73, val_loss 454.70269775390625\n",
      "epoch 74, train_loss 1277.0501708984375\n",
      "epoch 74, val_loss 454.7026672363281\n",
      "epoch 75, train_loss 1277.0501708984375\n",
      "epoch 75, val_loss 454.7026672363281\n",
      "epoch 76, train_loss 1277.0501708984375\n",
      "epoch 76, val_loss 454.7026672363281\n",
      "epoch 77, train_loss 1277.050048828125\n",
      "epoch 77, val_loss 454.70263671875\n",
      "epoch 78, train_loss 1277.0501708984375\n",
      "epoch 78, val_loss 454.70263671875\n",
      "epoch 79, train_loss 1277.050048828125\n",
      "epoch 79, val_loss 454.70257568359375\n",
      "epoch 80, train_loss 1277.050048828125\n",
      "epoch 80, val_loss 454.7025451660156\n",
      "epoch 81, train_loss 1277.050048828125\n",
      "epoch 81, val_loss 454.7025146484375\n",
      "epoch 82, train_loss 1277.050048828125\n",
      "epoch 82, val_loss 454.70245361328125\n",
      "epoch 83, train_loss 1277.0499267578125\n",
      "epoch 83, val_loss 454.7023620605469\n",
      "epoch 84, train_loss 1277.050048828125\n",
      "epoch 84, val_loss 454.70233154296875\n",
      "epoch 85, train_loss 1277.050048828125\n",
      "epoch 85, val_loss 454.70233154296875\n",
      "epoch 86, train_loss 1277.0499267578125\n",
      "epoch 86, val_loss 454.7023010253906\n",
      "epoch 87, train_loss 1277.0499267578125\n",
      "epoch 87, val_loss 454.7023010253906\n",
      "epoch 88, train_loss 1277.0499267578125\n",
      "epoch 88, val_loss 454.7022399902344\n",
      "epoch 89, train_loss 1277.0498046875\n",
      "epoch 89, val_loss 454.70220947265625\n",
      "epoch 90, train_loss 1277.0496826171875\n",
      "epoch 90, val_loss 454.7021789550781\n",
      "epoch 91, train_loss 1277.0496826171875\n",
      "epoch 91, val_loss 454.70220947265625\n",
      "epoch 92, train_loss 1277.0498046875\n",
      "epoch 92, val_loss 454.7021789550781\n",
      "epoch 93, train_loss 1277.0498046875\n",
      "epoch 93, val_loss 454.70208740234375\n",
      "epoch 94, train_loss 1277.0496826171875\n",
      "epoch 94, val_loss 454.70208740234375\n",
      "epoch 95, train_loss 1277.0496826171875\n",
      "epoch 95, val_loss 454.7020568847656\n",
      "epoch 96, train_loss 1277.049560546875\n",
      "epoch 96, val_loss 454.7019958496094\n",
      "epoch 97, train_loss 1277.049560546875\n",
      "epoch 97, val_loss 454.70196533203125\n",
      "epoch 98, train_loss 1277.049560546875\n",
      "epoch 98, val_loss 454.701904296875\n",
      "epoch 99, train_loss 1277.049560546875\n",
      "epoch 99, val_loss 454.70184326171875\n",
      "Parameter containing:\n",
      "tensor([9.7009e-30], requires_grad=True)\n",
      "iter 163, train_loss_regularization 0.6896564364433289\n",
      "iter 163, val_loss_regularization 0.6896564364433289\n",
      "epoch 0, train_loss 1277.049560546875\n",
      "epoch 0, val_loss 454.70184326171875\n",
      "epoch 1, train_loss 1277.049560546875\n",
      "epoch 1, val_loss 454.70184326171875\n",
      "epoch 2, train_loss 1277.0494384765625\n",
      "epoch 2, val_loss 454.7017517089844\n",
      "epoch 3, train_loss 1277.04931640625\n",
      "epoch 3, val_loss 454.7017517089844\n",
      "epoch 4, train_loss 1277.04931640625\n",
      "epoch 4, val_loss 454.7017517089844\n",
      "epoch 5, train_loss 1277.0494384765625\n",
      "epoch 5, val_loss 454.7017517089844\n",
      "epoch 6, train_loss 1277.0491943359375\n",
      "epoch 6, val_loss 454.70172119140625\n",
      "epoch 7, train_loss 1277.04931640625\n",
      "epoch 7, val_loss 454.7016296386719\n",
      "epoch 8, train_loss 1277.04931640625\n",
      "epoch 8, val_loss 454.7015380859375\n",
      "epoch 9, train_loss 1277.04931640625\n",
      "epoch 9, val_loss 454.7015686035156\n",
      "epoch 10, train_loss 1277.0491943359375\n",
      "epoch 10, val_loss 454.7015380859375\n",
      "epoch 11, train_loss 1277.04931640625\n",
      "epoch 11, val_loss 454.7015686035156\n",
      "epoch 12, train_loss 1277.0491943359375\n",
      "epoch 12, val_loss 454.7014465332031\n",
      "epoch 13, train_loss 1277.04931640625\n",
      "epoch 13, val_loss 454.7014465332031\n",
      "epoch 14, train_loss 1277.0491943359375\n",
      "epoch 14, val_loss 454.7014465332031\n",
      "epoch 15, train_loss 1277.0491943359375\n",
      "epoch 15, val_loss 454.7014465332031\n",
      "epoch 16, train_loss 1277.049072265625\n",
      "epoch 16, val_loss 454.7013854980469\n",
      "epoch 17, train_loss 1277.049072265625\n",
      "epoch 17, val_loss 454.7012939453125\n",
      "epoch 18, train_loss 1277.0489501953125\n",
      "epoch 18, val_loss 454.7012939453125\n",
      "epoch 19, train_loss 1277.0489501953125\n",
      "epoch 19, val_loss 454.7012634277344\n",
      "epoch 20, train_loss 1277.049072265625\n",
      "epoch 20, val_loss 454.7012023925781\n",
      "epoch 21, train_loss 1277.049072265625\n",
      "epoch 21, val_loss 454.70111083984375\n",
      "epoch 22, train_loss 1277.0489501953125\n",
      "epoch 22, val_loss 454.70111083984375\n",
      "epoch 23, train_loss 1277.0489501953125\n",
      "epoch 23, val_loss 454.70111083984375\n",
      "epoch 24, train_loss 1277.0489501953125\n",
      "epoch 24, val_loss 454.70111083984375\n",
      "epoch 25, train_loss 1277.0489501953125\n",
      "epoch 25, val_loss 454.7010803222656\n",
      "epoch 26, train_loss 1277.0487060546875\n",
      "epoch 26, val_loss 454.70098876953125\n",
      "epoch 27, train_loss 1277.048828125\n",
      "epoch 27, val_loss 454.70098876953125\n",
      "epoch 28, train_loss 1277.048828125\n",
      "epoch 28, val_loss 454.70098876953125\n",
      "epoch 29, train_loss 1277.0487060546875\n",
      "epoch 29, val_loss 454.7009582519531\n",
      "epoch 30, train_loss 1277.0487060546875\n",
      "epoch 30, val_loss 454.700927734375\n",
      "epoch 31, train_loss 1277.0487060546875\n",
      "epoch 31, val_loss 454.7008361816406\n",
      "epoch 32, train_loss 1277.0487060546875\n",
      "epoch 32, val_loss 454.7008361816406\n",
      "epoch 33, train_loss 1277.048583984375\n",
      "epoch 33, val_loss 454.7008361816406\n",
      "epoch 34, train_loss 1277.048583984375\n",
      "epoch 34, val_loss 454.7008361816406\n",
      "epoch 35, train_loss 1277.048583984375\n",
      "epoch 35, val_loss 454.70074462890625\n",
      "epoch 36, train_loss 1277.0487060546875\n",
      "epoch 36, val_loss 454.7007141113281\n",
      "epoch 37, train_loss 1277.0484619140625\n",
      "epoch 37, val_loss 454.7006530761719\n",
      "epoch 38, train_loss 1277.0484619140625\n",
      "epoch 38, val_loss 454.7006530761719\n",
      "epoch 39, train_loss 1277.0484619140625\n",
      "epoch 39, val_loss 454.70062255859375\n",
      "epoch 40, train_loss 1277.048583984375\n",
      "epoch 40, val_loss 454.7005310058594\n",
      "epoch 41, train_loss 1277.04833984375\n",
      "epoch 41, val_loss 454.7005310058594\n",
      "epoch 42, train_loss 1277.0484619140625\n",
      "epoch 42, val_loss 454.7005920410156\n",
      "epoch 43, train_loss 1277.04833984375\n",
      "epoch 43, val_loss 454.70050048828125\n",
      "epoch 44, train_loss 1277.04833984375\n",
      "epoch 44, val_loss 454.7004089355469\n",
      "epoch 45, train_loss 1277.0482177734375\n",
      "epoch 45, val_loss 454.70037841796875\n",
      "epoch 46, train_loss 1277.0484619140625\n",
      "epoch 46, val_loss 454.70037841796875\n",
      "epoch 47, train_loss 1277.0482177734375\n",
      "epoch 47, val_loss 454.70037841796875\n",
      "epoch 48, train_loss 1277.04833984375\n",
      "epoch 48, val_loss 454.7003479003906\n",
      "epoch 49, train_loss 1277.04833984375\n",
      "epoch 49, val_loss 454.7003479003906\n",
      "epoch 50, train_loss 1277.04833984375\n",
      "epoch 50, val_loss 454.70025634765625\n",
      "epoch 51, train_loss 1277.0479736328125\n",
      "epoch 51, val_loss 454.70025634765625\n",
      "epoch 52, train_loss 1277.048095703125\n",
      "epoch 52, val_loss 454.70025634765625\n",
      "epoch 53, train_loss 1277.0482177734375\n",
      "epoch 53, val_loss 454.70025634765625\n",
      "epoch 54, train_loss 1277.0482177734375\n",
      "epoch 54, val_loss 454.7000732421875\n",
      "epoch 55, train_loss 1277.0482177734375\n",
      "epoch 55, val_loss 454.7000427246094\n",
      "epoch 56, train_loss 1277.0482177734375\n",
      "epoch 56, val_loss 454.7000427246094\n",
      "epoch 57, train_loss 1277.0479736328125\n",
      "epoch 57, val_loss 454.70001220703125\n",
      "epoch 58, train_loss 1277.0479736328125\n",
      "epoch 58, val_loss 454.70001220703125\n",
      "epoch 59, train_loss 1277.0479736328125\n",
      "epoch 59, val_loss 454.6999206542969\n",
      "epoch 60, train_loss 1277.0479736328125\n",
      "epoch 60, val_loss 454.6999206542969\n",
      "epoch 61, train_loss 1277.0478515625\n",
      "epoch 61, val_loss 454.699951171875\n",
      "epoch 62, train_loss 1277.0479736328125\n",
      "epoch 62, val_loss 454.69989013671875\n",
      "epoch 63, train_loss 1277.0479736328125\n",
      "epoch 63, val_loss 454.69989013671875\n",
      "epoch 64, train_loss 1277.0478515625\n",
      "epoch 64, val_loss 454.6998291015625\n",
      "epoch 65, train_loss 1277.0479736328125\n",
      "epoch 65, val_loss 454.6997985839844\n",
      "epoch 66, train_loss 1277.0479736328125\n",
      "epoch 66, val_loss 454.6997375488281\n",
      "epoch 67, train_loss 1277.0478515625\n",
      "epoch 67, val_loss 454.6996765136719\n",
      "epoch 68, train_loss 1277.0477294921875\n",
      "epoch 68, val_loss 454.6996765136719\n",
      "epoch 69, train_loss 1277.047607421875\n",
      "epoch 69, val_loss 454.6996154785156\n",
      "epoch 70, train_loss 1277.047607421875\n",
      "epoch 70, val_loss 454.6995849609375\n",
      "epoch 71, train_loss 1277.047607421875\n",
      "epoch 71, val_loss 454.6995849609375\n",
      "epoch 72, train_loss 1277.047607421875\n",
      "epoch 72, val_loss 454.6995544433594\n",
      "epoch 73, train_loss 1277.047607421875\n",
      "epoch 73, val_loss 454.699462890625\n",
      "epoch 74, train_loss 1277.047607421875\n",
      "epoch 74, val_loss 454.699462890625\n",
      "epoch 75, train_loss 1277.047607421875\n",
      "epoch 75, val_loss 454.699462890625\n",
      "epoch 76, train_loss 1277.0474853515625\n",
      "epoch 76, val_loss 454.69940185546875\n",
      "epoch 77, train_loss 1277.04736328125\n",
      "epoch 77, val_loss 454.69940185546875\n",
      "epoch 78, train_loss 1277.04736328125\n",
      "epoch 78, val_loss 454.6993713378906\n",
      "epoch 79, train_loss 1277.04736328125\n",
      "epoch 79, val_loss 454.69927978515625\n",
      "epoch 80, train_loss 1277.04736328125\n",
      "epoch 80, val_loss 454.6992492675781\n",
      "epoch 81, train_loss 1277.0472412109375\n",
      "epoch 81, val_loss 454.6992492675781\n",
      "epoch 82, train_loss 1277.04736328125\n",
      "epoch 82, val_loss 454.6992492675781\n",
      "epoch 83, train_loss 1277.04736328125\n",
      "epoch 83, val_loss 454.69915771484375\n",
      "epoch 84, train_loss 1277.04736328125\n",
      "epoch 84, val_loss 454.69915771484375\n",
      "epoch 85, train_loss 1277.04736328125\n",
      "epoch 85, val_loss 454.6991271972656\n",
      "epoch 86, train_loss 1277.047119140625\n",
      "epoch 86, val_loss 454.6991271972656\n",
      "epoch 87, train_loss 1277.047119140625\n",
      "epoch 87, val_loss 454.6991271972656\n",
      "epoch 88, train_loss 1277.047119140625\n",
      "epoch 88, val_loss 454.6990966796875\n",
      "epoch 89, train_loss 1277.047119140625\n",
      "epoch 89, val_loss 454.6990051269531\n",
      "epoch 90, train_loss 1277.047119140625\n",
      "epoch 90, val_loss 454.6990051269531\n",
      "epoch 91, train_loss 1277.047119140625\n",
      "epoch 91, val_loss 454.6990051269531\n",
      "epoch 92, train_loss 1277.047119140625\n",
      "epoch 92, val_loss 454.69891357421875\n",
      "epoch 93, train_loss 1277.047119140625\n",
      "epoch 93, val_loss 454.6988220214844\n",
      "epoch 94, train_loss 1277.0469970703125\n",
      "epoch 94, val_loss 454.6988220214844\n",
      "epoch 95, train_loss 1277.047119140625\n",
      "epoch 95, val_loss 454.6988220214844\n",
      "epoch 96, train_loss 1277.047119140625\n",
      "epoch 96, val_loss 454.69879150390625\n",
      "epoch 97, train_loss 1277.0469970703125\n",
      "epoch 97, val_loss 454.6987609863281\n",
      "epoch 98, train_loss 1277.046875\n",
      "epoch 98, val_loss 454.6986999511719\n",
      "epoch 99, train_loss 1277.046875\n",
      "epoch 99, val_loss 454.69866943359375\n",
      "Parameter containing:\n",
      "tensor([6.7766e-30], requires_grad=True)\n",
      "iter 164, train_loss_regularization 0.6892699003219604\n",
      "iter 164, val_loss_regularization 0.6892699003219604\n",
      "epoch 0, train_loss 1277.046875\n",
      "epoch 0, val_loss 454.6986999511719\n",
      "epoch 1, train_loss 1277.046875\n",
      "epoch 1, val_loss 454.69866943359375\n",
      "epoch 2, train_loss 1277.0469970703125\n",
      "epoch 2, val_loss 454.6986389160156\n",
      "epoch 3, train_loss 1277.046875\n",
      "epoch 3, val_loss 454.6985778808594\n",
      "epoch 4, train_loss 1277.046875\n",
      "epoch 4, val_loss 454.698486328125\n",
      "epoch 5, train_loss 1277.046875\n",
      "epoch 5, val_loss 454.698486328125\n",
      "epoch 6, train_loss 1277.046630859375\n",
      "epoch 6, val_loss 454.698486328125\n",
      "epoch 7, train_loss 1277.0467529296875\n",
      "epoch 7, val_loss 454.69842529296875\n",
      "epoch 8, train_loss 1277.0467529296875\n",
      "epoch 8, val_loss 454.6983337402344\n",
      "epoch 9, train_loss 1277.046630859375\n",
      "epoch 9, val_loss 454.6983642578125\n",
      "epoch 10, train_loss 1277.046630859375\n",
      "epoch 10, val_loss 454.6983337402344\n",
      "epoch 11, train_loss 1277.046630859375\n",
      "epoch 11, val_loss 454.6983337402344\n",
      "epoch 12, train_loss 1277.046630859375\n",
      "epoch 12, val_loss 454.6982421875\n",
      "epoch 13, train_loss 1277.046630859375\n",
      "epoch 13, val_loss 454.6982421875\n",
      "epoch 14, train_loss 1277.0465087890625\n",
      "epoch 14, val_loss 454.6982421875\n",
      "epoch 15, train_loss 1277.04638671875\n",
      "epoch 15, val_loss 454.69818115234375\n",
      "epoch 16, train_loss 1277.04638671875\n",
      "epoch 16, val_loss 454.6981201171875\n",
      "epoch 17, train_loss 1277.04638671875\n",
      "epoch 17, val_loss 454.6980285644531\n",
      "epoch 18, train_loss 1277.0465087890625\n",
      "epoch 18, val_loss 454.6980285644531\n",
      "epoch 19, train_loss 1277.04638671875\n",
      "epoch 19, val_loss 454.6980285644531\n",
      "epoch 20, train_loss 1277.04638671875\n",
      "epoch 20, val_loss 454.6980285644531\n",
      "epoch 21, train_loss 1277.04638671875\n",
      "epoch 21, val_loss 454.6979675292969\n",
      "epoch 22, train_loss 1277.04638671875\n",
      "epoch 22, val_loss 454.6979675292969\n",
      "epoch 23, train_loss 1277.04638671875\n",
      "epoch 23, val_loss 454.6979675292969\n",
      "epoch 24, train_loss 1277.0462646484375\n",
      "epoch 24, val_loss 454.6979064941406\n",
      "epoch 25, train_loss 1277.046142578125\n",
      "epoch 25, val_loss 454.6979064941406\n",
      "epoch 26, train_loss 1277.046142578125\n",
      "epoch 26, val_loss 454.6978454589844\n",
      "epoch 27, train_loss 1277.0462646484375\n",
      "epoch 27, val_loss 454.69775390625\n",
      "epoch 28, train_loss 1277.0462646484375\n",
      "epoch 28, val_loss 454.69775390625\n",
      "epoch 29, train_loss 1277.046142578125\n",
      "epoch 29, val_loss 454.6977233886719\n",
      "epoch 30, train_loss 1277.046142578125\n",
      "epoch 30, val_loss 454.6977233886719\n",
      "epoch 31, train_loss 1277.046142578125\n",
      "epoch 31, val_loss 454.6976318359375\n",
      "epoch 32, train_loss 1277.0460205078125\n",
      "epoch 32, val_loss 454.69757080078125\n",
      "epoch 33, train_loss 1277.0458984375\n",
      "epoch 33, val_loss 454.6976318359375\n",
      "epoch 34, train_loss 1277.0457763671875\n",
      "epoch 34, val_loss 454.6976318359375\n",
      "epoch 35, train_loss 1277.0457763671875\n",
      "epoch 35, val_loss 454.69757080078125\n",
      "epoch 36, train_loss 1277.0458984375\n",
      "epoch 36, val_loss 454.697509765625\n",
      "epoch 37, train_loss 1277.0460205078125\n",
      "epoch 37, val_loss 454.69744873046875\n",
      "epoch 38, train_loss 1277.0457763671875\n",
      "epoch 38, val_loss 454.6974182128906\n",
      "epoch 39, train_loss 1277.0457763671875\n",
      "epoch 39, val_loss 454.69744873046875\n",
      "epoch 40, train_loss 1277.0457763671875\n",
      "epoch 40, val_loss 454.6973876953125\n",
      "epoch 41, train_loss 1277.0458984375\n",
      "epoch 41, val_loss 454.69732666015625\n",
      "epoch 42, train_loss 1277.0458984375\n",
      "epoch 42, val_loss 454.69732666015625\n",
      "epoch 43, train_loss 1277.0458984375\n",
      "epoch 43, val_loss 454.6972351074219\n",
      "epoch 44, train_loss 1277.0457763671875\n",
      "epoch 44, val_loss 454.6972351074219\n",
      "epoch 45, train_loss 1277.0457763671875\n",
      "epoch 45, val_loss 454.69720458984375\n",
      "epoch 46, train_loss 1277.0457763671875\n",
      "epoch 46, val_loss 454.6971740722656\n",
      "epoch 47, train_loss 1277.0455322265625\n",
      "epoch 47, val_loss 454.6971740722656\n",
      "epoch 48, train_loss 1277.0455322265625\n",
      "epoch 48, val_loss 454.6971740722656\n",
      "epoch 49, train_loss 1277.0457763671875\n",
      "epoch 49, val_loss 454.6971130371094\n",
      "epoch 50, train_loss 1277.0455322265625\n",
      "epoch 50, val_loss 454.6970520019531\n",
      "epoch 51, train_loss 1277.0455322265625\n",
      "epoch 51, val_loss 454.6969909667969\n",
      "epoch 52, train_loss 1277.045654296875\n",
      "epoch 52, val_loss 454.69696044921875\n",
      "epoch 53, train_loss 1277.0452880859375\n",
      "epoch 53, val_loss 454.69696044921875\n",
      "epoch 54, train_loss 1277.0455322265625\n",
      "epoch 54, val_loss 454.6969299316406\n",
      "epoch 55, train_loss 1277.0455322265625\n",
      "epoch 55, val_loss 454.6968688964844\n",
      "epoch 56, train_loss 1277.0452880859375\n",
      "epoch 56, val_loss 454.6968688964844\n",
      "epoch 57, train_loss 1277.04541015625\n",
      "epoch 57, val_loss 454.6968688964844\n",
      "epoch 58, train_loss 1277.0455322265625\n",
      "epoch 58, val_loss 454.69683837890625\n",
      "epoch 59, train_loss 1277.0452880859375\n",
      "epoch 59, val_loss 454.69677734375\n",
      "epoch 60, train_loss 1277.0452880859375\n",
      "epoch 60, val_loss 454.69671630859375\n",
      "epoch 61, train_loss 1277.0452880859375\n",
      "epoch 61, val_loss 454.6966552734375\n",
      "epoch 62, train_loss 1277.0452880859375\n",
      "epoch 62, val_loss 454.6966552734375\n",
      "epoch 63, train_loss 1277.0452880859375\n",
      "epoch 63, val_loss 454.6966552734375\n",
      "epoch 64, train_loss 1277.045166015625\n",
      "epoch 64, val_loss 454.69659423828125\n",
      "epoch 65, train_loss 1277.0450439453125\n",
      "epoch 65, val_loss 454.696533203125\n",
      "epoch 66, train_loss 1277.0450439453125\n",
      "epoch 66, val_loss 454.696533203125\n",
      "epoch 67, train_loss 1277.0450439453125\n",
      "epoch 67, val_loss 454.696533203125\n",
      "epoch 68, train_loss 1277.0450439453125\n",
      "epoch 68, val_loss 454.6965026855469\n",
      "epoch 69, train_loss 1277.0450439453125\n",
      "epoch 69, val_loss 454.6965026855469\n",
      "epoch 70, train_loss 1277.0450439453125\n",
      "epoch 70, val_loss 454.6964111328125\n",
      "epoch 71, train_loss 1277.0450439453125\n",
      "epoch 71, val_loss 454.6963806152344\n",
      "epoch 72, train_loss 1277.045166015625\n",
      "epoch 72, val_loss 454.6963806152344\n",
      "epoch 73, train_loss 1277.0450439453125\n",
      "epoch 73, val_loss 454.6963195800781\n",
      "epoch 74, train_loss 1277.0447998046875\n",
      "epoch 74, val_loss 454.6963195800781\n",
      "epoch 75, train_loss 1277.044921875\n",
      "epoch 75, val_loss 454.6962890625\n",
      "epoch 76, train_loss 1277.044921875\n",
      "epoch 76, val_loss 454.6962585449219\n",
      "epoch 77, train_loss 1277.0447998046875\n",
      "epoch 77, val_loss 454.6962585449219\n",
      "epoch 78, train_loss 1277.0447998046875\n",
      "epoch 78, val_loss 454.6961975097656\n",
      "epoch 79, train_loss 1277.044921875\n",
      "epoch 79, val_loss 454.6961364746094\n",
      "epoch 80, train_loss 1277.044921875\n",
      "epoch 80, val_loss 454.6960754394531\n",
      "epoch 81, train_loss 1277.0447998046875\n",
      "epoch 81, val_loss 454.6960754394531\n",
      "epoch 82, train_loss 1277.0447998046875\n",
      "epoch 82, val_loss 454.696044921875\n",
      "epoch 83, train_loss 1277.0447998046875\n",
      "epoch 83, val_loss 454.696044921875\n",
      "epoch 84, train_loss 1277.0445556640625\n",
      "epoch 84, val_loss 454.6959533691406\n",
      "epoch 85, train_loss 1277.0445556640625\n",
      "epoch 85, val_loss 454.6959228515625\n",
      "epoch 86, train_loss 1277.044677734375\n",
      "epoch 86, val_loss 454.6959228515625\n",
      "epoch 87, train_loss 1277.0445556640625\n",
      "epoch 87, val_loss 454.6959228515625\n",
      "epoch 88, train_loss 1277.0445556640625\n",
      "epoch 88, val_loss 454.69586181640625\n",
      "epoch 89, train_loss 1277.0445556640625\n",
      "epoch 89, val_loss 454.69580078125\n",
      "epoch 90, train_loss 1277.044677734375\n",
      "epoch 90, val_loss 454.69580078125\n",
      "epoch 91, train_loss 1277.0445556640625\n",
      "epoch 91, val_loss 454.69580078125\n",
      "epoch 92, train_loss 1277.0445556640625\n",
      "epoch 92, val_loss 454.69573974609375\n",
      "epoch 93, train_loss 1277.0445556640625\n",
      "epoch 93, val_loss 454.6957092285156\n",
      "epoch 94, train_loss 1277.0445556640625\n",
      "epoch 94, val_loss 454.6956787109375\n",
      "epoch 95, train_loss 1277.04443359375\n",
      "epoch 95, val_loss 454.69561767578125\n",
      "epoch 96, train_loss 1277.04443359375\n",
      "epoch 96, val_loss 454.69561767578125\n",
      "epoch 97, train_loss 1277.04443359375\n",
      "epoch 97, val_loss 454.69561767578125\n",
      "epoch 98, train_loss 1277.0445556640625\n",
      "epoch 98, val_loss 454.6955871582031\n",
      "epoch 99, train_loss 1277.0445556640625\n",
      "epoch 99, val_loss 454.69549560546875\n",
      "Parameter containing:\n",
      "tensor([4.7345e-30], requires_grad=True)\n",
      "iter 165, train_loss_regularization 0.6888980865478516\n",
      "iter 165, val_loss_regularization 0.6888980865478516\n",
      "epoch 0, train_loss 1277.0443115234375\n",
      "epoch 0, val_loss 454.6954650878906\n",
      "epoch 1, train_loss 1277.044189453125\n",
      "epoch 1, val_loss 454.6954040527344\n",
      "epoch 2, train_loss 1277.0440673828125\n",
      "epoch 2, val_loss 454.69537353515625\n",
      "epoch 3, train_loss 1277.0440673828125\n",
      "epoch 3, val_loss 454.69537353515625\n",
      "epoch 4, train_loss 1277.0440673828125\n",
      "epoch 4, val_loss 454.6952819824219\n",
      "epoch 5, train_loss 1277.0440673828125\n",
      "epoch 5, val_loss 454.6952819824219\n",
      "epoch 6, train_loss 1277.044189453125\n",
      "epoch 6, val_loss 454.6953430175781\n",
      "epoch 7, train_loss 1277.0440673828125\n",
      "epoch 7, val_loss 454.6952819824219\n",
      "epoch 8, train_loss 1277.0440673828125\n",
      "epoch 8, val_loss 454.6952209472656\n",
      "epoch 9, train_loss 1277.0440673828125\n",
      "epoch 9, val_loss 454.6951599121094\n",
      "epoch 10, train_loss 1277.0439453125\n",
      "epoch 10, val_loss 454.6951599121094\n",
      "epoch 11, train_loss 1277.0439453125\n",
      "epoch 11, val_loss 454.6951599121094\n",
      "epoch 12, train_loss 1277.0439453125\n",
      "epoch 12, val_loss 454.6951599121094\n",
      "epoch 13, train_loss 1277.0439453125\n",
      "epoch 13, val_loss 454.695068359375\n",
      "epoch 14, train_loss 1277.0439453125\n",
      "epoch 14, val_loss 454.6950378417969\n",
      "epoch 15, train_loss 1277.0439453125\n",
      "epoch 15, val_loss 454.69500732421875\n",
      "epoch 16, train_loss 1277.0439453125\n",
      "epoch 16, val_loss 454.69500732421875\n",
      "epoch 17, train_loss 1277.0438232421875\n",
      "epoch 17, val_loss 454.6949462890625\n",
      "epoch 18, train_loss 1277.0438232421875\n",
      "epoch 18, val_loss 454.69488525390625\n",
      "epoch 19, train_loss 1277.0438232421875\n",
      "epoch 19, val_loss 454.69482421875\n",
      "epoch 20, train_loss 1277.043701171875\n",
      "epoch 20, val_loss 454.69482421875\n",
      "epoch 21, train_loss 1277.043701171875\n",
      "epoch 21, val_loss 454.6947937011719\n",
      "epoch 22, train_loss 1277.043701171875\n",
      "epoch 22, val_loss 454.6947937011719\n",
      "epoch 23, train_loss 1277.043701171875\n",
      "epoch 23, val_loss 454.69476318359375\n",
      "epoch 24, train_loss 1277.043701171875\n",
      "epoch 24, val_loss 454.6947021484375\n",
      "epoch 25, train_loss 1277.043701171875\n",
      "epoch 25, val_loss 454.6947021484375\n",
      "epoch 26, train_loss 1277.043701171875\n",
      "epoch 26, val_loss 454.6946716308594\n",
      "epoch 27, train_loss 1277.043701171875\n",
      "epoch 27, val_loss 454.6946716308594\n",
      "epoch 28, train_loss 1277.043701171875\n",
      "epoch 28, val_loss 454.6945495605469\n",
      "epoch 29, train_loss 1277.0435791015625\n",
      "epoch 29, val_loss 454.694580078125\n",
      "epoch 30, train_loss 1277.0435791015625\n",
      "epoch 30, val_loss 454.6945495605469\n",
      "epoch 31, train_loss 1277.04345703125\n",
      "epoch 31, val_loss 454.6945495605469\n",
      "epoch 32, train_loss 1277.04345703125\n",
      "epoch 32, val_loss 454.6944885253906\n",
      "epoch 33, train_loss 1277.04345703125\n",
      "epoch 33, val_loss 454.6944274902344\n",
      "epoch 34, train_loss 1277.04345703125\n",
      "epoch 34, val_loss 454.6944274902344\n",
      "epoch 35, train_loss 1277.04345703125\n",
      "epoch 35, val_loss 454.6944274902344\n",
      "epoch 36, train_loss 1277.04345703125\n",
      "epoch 36, val_loss 454.6943664550781\n",
      "epoch 37, train_loss 1277.0433349609375\n",
      "epoch 37, val_loss 454.6943054199219\n",
      "epoch 38, train_loss 1277.043212890625\n",
      "epoch 38, val_loss 454.6942443847656\n",
      "epoch 39, train_loss 1277.043212890625\n",
      "epoch 39, val_loss 454.6942443847656\n",
      "epoch 40, train_loss 1277.043212890625\n",
      "epoch 40, val_loss 454.6942138671875\n",
      "epoch 41, train_loss 1277.0433349609375\n",
      "epoch 41, val_loss 454.6942138671875\n",
      "epoch 42, train_loss 1277.043212890625\n",
      "epoch 42, val_loss 454.6941223144531\n",
      "epoch 43, train_loss 1277.0430908203125\n",
      "epoch 43, val_loss 454.6941223144531\n",
      "epoch 44, train_loss 1277.0430908203125\n",
      "epoch 44, val_loss 454.694091796875\n",
      "epoch 45, train_loss 1277.0430908203125\n",
      "epoch 45, val_loss 454.694091796875\n",
      "epoch 46, train_loss 1277.043212890625\n",
      "epoch 46, val_loss 454.69403076171875\n",
      "epoch 47, train_loss 1277.043212890625\n",
      "epoch 47, val_loss 454.6940002441406\n",
      "epoch 48, train_loss 1277.043212890625\n",
      "epoch 48, val_loss 454.6939697265625\n",
      "epoch 49, train_loss 1277.043212890625\n",
      "epoch 49, val_loss 454.6939697265625\n",
      "epoch 50, train_loss 1277.04296875\n",
      "epoch 50, val_loss 454.6940002441406\n",
      "epoch 51, train_loss 1277.04296875\n",
      "epoch 51, val_loss 454.69390869140625\n",
      "epoch 52, train_loss 1277.04296875\n",
      "epoch 52, val_loss 454.69384765625\n",
      "epoch 53, train_loss 1277.0428466796875\n",
      "epoch 53, val_loss 454.69384765625\n",
      "epoch 54, train_loss 1277.0428466796875\n",
      "epoch 54, val_loss 454.69378662109375\n",
      "epoch 55, train_loss 1277.0428466796875\n",
      "epoch 55, val_loss 454.6937561035156\n",
      "epoch 56, train_loss 1277.0428466796875\n",
      "epoch 56, val_loss 454.6936950683594\n",
      "epoch 57, train_loss 1277.0428466796875\n",
      "epoch 57, val_loss 454.69366455078125\n",
      "epoch 58, train_loss 1277.042724609375\n",
      "epoch 58, val_loss 454.6936340332031\n",
      "epoch 59, train_loss 1277.042724609375\n",
      "epoch 59, val_loss 454.6936340332031\n",
      "epoch 60, train_loss 1277.042724609375\n",
      "epoch 60, val_loss 454.6935729980469\n",
      "epoch 61, train_loss 1277.042724609375\n",
      "epoch 61, val_loss 454.6935729980469\n",
      "epoch 62, train_loss 1277.042724609375\n",
      "epoch 62, val_loss 454.6934509277344\n",
      "epoch 63, train_loss 1277.042724609375\n",
      "epoch 63, val_loss 454.69354248046875\n",
      "epoch 64, train_loss 1277.0426025390625\n",
      "epoch 64, val_loss 454.6934509277344\n",
      "epoch 65, train_loss 1277.042724609375\n",
      "epoch 65, val_loss 454.6934509277344\n",
      "epoch 66, train_loss 1277.042724609375\n",
      "epoch 66, val_loss 454.6933898925781\n",
      "epoch 67, train_loss 1277.042724609375\n",
      "epoch 67, val_loss 454.6933898925781\n",
      "epoch 68, train_loss 1277.0426025390625\n",
      "epoch 68, val_loss 454.6933288574219\n",
      "epoch 69, train_loss 1277.042724609375\n",
      "epoch 69, val_loss 454.6933288574219\n",
      "epoch 70, train_loss 1277.0426025390625\n",
      "epoch 70, val_loss 454.69329833984375\n",
      "epoch 71, train_loss 1277.0423583984375\n",
      "epoch 71, val_loss 454.6932373046875\n",
      "epoch 72, train_loss 1277.0423583984375\n",
      "epoch 72, val_loss 454.69317626953125\n",
      "epoch 73, train_loss 1277.0423583984375\n",
      "epoch 73, val_loss 454.6932373046875\n",
      "epoch 74, train_loss 1277.0423583984375\n",
      "epoch 74, val_loss 454.69317626953125\n",
      "epoch 75, train_loss 1277.0423583984375\n",
      "epoch 75, val_loss 454.6930847167969\n",
      "epoch 76, train_loss 1277.0423583984375\n",
      "epoch 76, val_loss 454.6930847167969\n",
      "epoch 77, train_loss 1277.042236328125\n",
      "epoch 77, val_loss 454.6929931640625\n",
      "epoch 78, train_loss 1277.0423583984375\n",
      "epoch 78, val_loss 454.69305419921875\n",
      "epoch 79, train_loss 1277.0423583984375\n",
      "epoch 79, val_loss 454.6929931640625\n",
      "epoch 80, train_loss 1277.042236328125\n",
      "epoch 80, val_loss 454.6929626464844\n",
      "epoch 81, train_loss 1277.042236328125\n",
      "epoch 81, val_loss 454.6929016113281\n",
      "epoch 82, train_loss 1277.042236328125\n",
      "epoch 82, val_loss 454.69287109375\n",
      "epoch 83, train_loss 1277.042236328125\n",
      "epoch 83, val_loss 454.69287109375\n",
      "epoch 84, train_loss 1277.042236328125\n",
      "epoch 84, val_loss 454.6928405761719\n",
      "epoch 85, train_loss 1277.042236328125\n",
      "epoch 85, val_loss 454.6928405761719\n",
      "epoch 86, train_loss 1277.0421142578125\n",
      "epoch 86, val_loss 454.6927490234375\n",
      "epoch 87, train_loss 1277.0421142578125\n",
      "epoch 87, val_loss 454.6927490234375\n",
      "epoch 88, train_loss 1277.0421142578125\n",
      "epoch 88, val_loss 454.6927490234375\n",
      "epoch 89, train_loss 1277.0419921875\n",
      "epoch 89, val_loss 454.6926574707031\n",
      "epoch 90, train_loss 1277.0418701171875\n",
      "epoch 90, val_loss 454.6926574707031\n",
      "epoch 91, train_loss 1277.0418701171875\n",
      "epoch 91, val_loss 454.6925964355469\n",
      "epoch 92, train_loss 1277.0418701171875\n",
      "epoch 92, val_loss 454.6925964355469\n",
      "epoch 93, train_loss 1277.0418701171875\n",
      "epoch 93, val_loss 454.6925354003906\n",
      "epoch 94, train_loss 1277.0418701171875\n",
      "epoch 94, val_loss 454.6925048828125\n",
      "epoch 95, train_loss 1277.0418701171875\n",
      "epoch 95, val_loss 454.69244384765625\n",
      "epoch 96, train_loss 1277.041748046875\n",
      "epoch 96, val_loss 454.69244384765625\n",
      "epoch 97, train_loss 1277.041748046875\n",
      "epoch 97, val_loss 454.6924133300781\n",
      "epoch 98, train_loss 1277.0418701171875\n",
      "epoch 98, val_loss 454.6924133300781\n",
      "epoch 99, train_loss 1277.0416259765625\n",
      "epoch 99, val_loss 454.6923828125\n",
      "Parameter containing:\n",
      "tensor([3.3082e-30], requires_grad=True)\n",
      "iter 166, train_loss_regularization 0.688541054725647\n",
      "iter 166, val_loss_regularization 0.688541054725647\n",
      "epoch 0, train_loss 1277.0416259765625\n",
      "epoch 0, val_loss 454.69232177734375\n",
      "epoch 1, train_loss 1277.0416259765625\n",
      "epoch 1, val_loss 454.6922912597656\n",
      "epoch 2, train_loss 1277.041748046875\n",
      "epoch 2, val_loss 454.6922912597656\n",
      "epoch 3, train_loss 1277.0416259765625\n",
      "epoch 3, val_loss 454.69219970703125\n",
      "epoch 4, train_loss 1277.041748046875\n",
      "epoch 4, val_loss 454.69219970703125\n",
      "epoch 5, train_loss 1277.0416259765625\n",
      "epoch 5, val_loss 454.6921691894531\n",
      "epoch 6, train_loss 1277.0416259765625\n",
      "epoch 6, val_loss 454.692138671875\n",
      "epoch 7, train_loss 1277.04150390625\n",
      "epoch 7, val_loss 454.692138671875\n",
      "epoch 8, train_loss 1277.04150390625\n",
      "epoch 8, val_loss 454.692138671875\n",
      "epoch 9, train_loss 1277.04150390625\n",
      "epoch 9, val_loss 454.69207763671875\n",
      "epoch 10, train_loss 1277.0413818359375\n",
      "epoch 10, val_loss 454.69207763671875\n",
      "epoch 11, train_loss 1277.04150390625\n",
      "epoch 11, val_loss 454.6919860839844\n",
      "epoch 12, train_loss 1277.04150390625\n",
      "epoch 12, val_loss 454.6919250488281\n",
      "epoch 13, train_loss 1277.0413818359375\n",
      "epoch 13, val_loss 454.6919250488281\n",
      "epoch 14, train_loss 1277.0413818359375\n",
      "epoch 14, val_loss 454.6918640136719\n",
      "epoch 15, train_loss 1277.0413818359375\n",
      "epoch 15, val_loss 454.69183349609375\n",
      "epoch 16, train_loss 1277.0413818359375\n",
      "epoch 16, val_loss 454.6918029785156\n",
      "epoch 17, train_loss 1277.0413818359375\n",
      "epoch 17, val_loss 454.6918029785156\n",
      "epoch 18, train_loss 1277.0413818359375\n",
      "epoch 18, val_loss 454.6918029785156\n",
      "epoch 19, train_loss 1277.0413818359375\n",
      "epoch 19, val_loss 454.6918029785156\n",
      "epoch 20, train_loss 1277.0411376953125\n",
      "epoch 20, val_loss 454.69171142578125\n",
      "epoch 21, train_loss 1277.041259765625\n",
      "epoch 21, val_loss 454.69171142578125\n",
      "epoch 22, train_loss 1277.041259765625\n",
      "epoch 22, val_loss 454.69171142578125\n",
      "epoch 23, train_loss 1277.041259765625\n",
      "epoch 23, val_loss 454.6916198730469\n",
      "epoch 24, train_loss 1277.0411376953125\n",
      "epoch 24, val_loss 454.6916809082031\n",
      "epoch 25, train_loss 1277.0411376953125\n",
      "epoch 25, val_loss 454.6915283203125\n",
      "epoch 26, train_loss 1277.0411376953125\n",
      "epoch 26, val_loss 454.6914978027344\n",
      "epoch 27, train_loss 1277.0411376953125\n",
      "epoch 27, val_loss 454.6914978027344\n",
      "epoch 28, train_loss 1277.041015625\n",
      "epoch 28, val_loss 454.69140625\n",
      "epoch 29, train_loss 1277.041015625\n",
      "epoch 29, val_loss 454.69140625\n",
      "epoch 30, train_loss 1277.0408935546875\n",
      "epoch 30, val_loss 454.69134521484375\n",
      "epoch 31, train_loss 1277.0408935546875\n",
      "epoch 31, val_loss 454.69134521484375\n",
      "epoch 32, train_loss 1277.0408935546875\n",
      "epoch 32, val_loss 454.6913757324219\n",
      "epoch 33, train_loss 1277.0408935546875\n",
      "epoch 33, val_loss 454.6912841796875\n",
      "epoch 34, train_loss 1277.0408935546875\n",
      "epoch 34, val_loss 454.6912536621094\n",
      "epoch 35, train_loss 1277.0408935546875\n",
      "epoch 35, val_loss 454.69122314453125\n",
      "epoch 36, train_loss 1277.0408935546875\n",
      "epoch 36, val_loss 454.69122314453125\n",
      "epoch 37, train_loss 1277.0408935546875\n",
      "epoch 37, val_loss 454.69122314453125\n",
      "epoch 38, train_loss 1277.0408935546875\n",
      "epoch 38, val_loss 454.691162109375\n",
      "epoch 39, train_loss 1277.040771484375\n",
      "epoch 39, val_loss 454.6911315917969\n",
      "epoch 40, train_loss 1277.040771484375\n",
      "epoch 40, val_loss 454.6910400390625\n",
      "epoch 41, train_loss 1277.040771484375\n",
      "epoch 41, val_loss 454.6910400390625\n",
      "epoch 42, train_loss 1277.040771484375\n",
      "epoch 42, val_loss 454.6910095214844\n",
      "epoch 43, train_loss 1277.0406494140625\n",
      "epoch 43, val_loss 454.6910095214844\n",
      "epoch 44, train_loss 1277.04052734375\n",
      "epoch 44, val_loss 454.6909484863281\n",
      "epoch 45, train_loss 1277.04052734375\n",
      "epoch 45, val_loss 454.69091796875\n",
      "epoch 46, train_loss 1277.04052734375\n",
      "epoch 46, val_loss 454.69091796875\n",
      "epoch 47, train_loss 1277.04052734375\n",
      "epoch 47, val_loss 454.6908874511719\n",
      "epoch 48, train_loss 1277.04052734375\n",
      "epoch 48, val_loss 454.6908874511719\n",
      "epoch 49, train_loss 1277.0404052734375\n",
      "epoch 49, val_loss 454.69073486328125\n",
      "epoch 50, train_loss 1277.0404052734375\n",
      "epoch 50, val_loss 454.69073486328125\n",
      "epoch 51, train_loss 1277.0404052734375\n",
      "epoch 51, val_loss 454.6907043457031\n",
      "epoch 52, train_loss 1277.0404052734375\n",
      "epoch 52, val_loss 454.6907043457031\n",
      "epoch 53, train_loss 1277.040283203125\n",
      "epoch 53, val_loss 454.690673828125\n",
      "epoch 54, train_loss 1277.0404052734375\n",
      "epoch 54, val_loss 454.69061279296875\n",
      "epoch 55, train_loss 1277.040283203125\n",
      "epoch 55, val_loss 454.6905822753906\n",
      "epoch 56, train_loss 1277.0404052734375\n",
      "epoch 56, val_loss 454.6905822753906\n",
      "epoch 57, train_loss 1277.0404052734375\n",
      "epoch 57, val_loss 454.6905822753906\n",
      "epoch 58, train_loss 1277.0404052734375\n",
      "epoch 58, val_loss 454.6905517578125\n",
      "epoch 59, train_loss 1277.040283203125\n",
      "epoch 59, val_loss 454.6904602050781\n",
      "epoch 60, train_loss 1277.040283203125\n",
      "epoch 60, val_loss 454.69049072265625\n",
      "epoch 61, train_loss 1277.040283203125\n",
      "epoch 61, val_loss 454.69049072265625\n",
      "epoch 62, train_loss 1277.0401611328125\n",
      "epoch 62, val_loss 454.6904296875\n",
      "epoch 63, train_loss 1277.0400390625\n",
      "epoch 63, val_loss 454.69036865234375\n",
      "epoch 64, train_loss 1277.0400390625\n",
      "epoch 64, val_loss 454.6903381347656\n",
      "epoch 65, train_loss 1277.0400390625\n",
      "epoch 65, val_loss 454.69024658203125\n",
      "epoch 66, train_loss 1277.0401611328125\n",
      "epoch 66, val_loss 454.69024658203125\n",
      "epoch 67, train_loss 1277.0400390625\n",
      "epoch 67, val_loss 454.69024658203125\n",
      "epoch 68, train_loss 1277.0400390625\n",
      "epoch 68, val_loss 454.6902160644531\n",
      "epoch 69, train_loss 1277.0400390625\n",
      "epoch 69, val_loss 454.6901550292969\n",
      "epoch 70, train_loss 1277.0399169921875\n",
      "epoch 70, val_loss 454.69012451171875\n",
      "epoch 71, train_loss 1277.039794921875\n",
      "epoch 71, val_loss 454.69012451171875\n",
      "epoch 72, train_loss 1277.039794921875\n",
      "epoch 72, val_loss 454.69012451171875\n",
      "epoch 73, train_loss 1277.039794921875\n",
      "epoch 73, val_loss 454.6900939941406\n",
      "epoch 74, train_loss 1277.039794921875\n",
      "epoch 74, val_loss 454.6900329589844\n",
      "epoch 75, train_loss 1277.0399169921875\n",
      "epoch 75, val_loss 454.69000244140625\n",
      "epoch 76, train_loss 1277.0399169921875\n",
      "epoch 76, val_loss 454.6899108886719\n",
      "epoch 77, train_loss 1277.039794921875\n",
      "epoch 77, val_loss 454.6899719238281\n",
      "epoch 78, train_loss 1277.039794921875\n",
      "epoch 78, val_loss 454.6899108886719\n",
      "epoch 79, train_loss 1277.039794921875\n",
      "epoch 79, val_loss 454.68988037109375\n",
      "epoch 80, train_loss 1277.039794921875\n",
      "epoch 80, val_loss 454.6898193359375\n",
      "epoch 81, train_loss 1277.0396728515625\n",
      "epoch 81, val_loss 454.68988037109375\n",
      "epoch 82, train_loss 1277.03955078125\n",
      "epoch 82, val_loss 454.6898193359375\n",
      "epoch 83, train_loss 1277.03955078125\n",
      "epoch 83, val_loss 454.68975830078125\n",
      "epoch 84, train_loss 1277.03955078125\n",
      "epoch 84, val_loss 454.6896667480469\n",
      "epoch 85, train_loss 1277.03955078125\n",
      "epoch 85, val_loss 454.689697265625\n",
      "epoch 86, train_loss 1277.03955078125\n",
      "epoch 86, val_loss 454.6896667480469\n",
      "epoch 87, train_loss 1277.03955078125\n",
      "epoch 87, val_loss 454.6895751953125\n",
      "epoch 88, train_loss 1277.03955078125\n",
      "epoch 88, val_loss 454.68951416015625\n",
      "epoch 89, train_loss 1277.03955078125\n",
      "epoch 89, val_loss 454.68951416015625\n",
      "epoch 90, train_loss 1277.0394287109375\n",
      "epoch 90, val_loss 454.68951416015625\n",
      "epoch 91, train_loss 1277.039306640625\n",
      "epoch 91, val_loss 454.68951416015625\n",
      "epoch 92, train_loss 1277.039306640625\n",
      "epoch 92, val_loss 454.689453125\n",
      "epoch 93, train_loss 1277.039306640625\n",
      "epoch 93, val_loss 454.6894226074219\n",
      "epoch 94, train_loss 1277.0394287109375\n",
      "epoch 94, val_loss 454.6893615722656\n",
      "epoch 95, train_loss 1277.039306640625\n",
      "epoch 95, val_loss 454.6894226074219\n",
      "epoch 96, train_loss 1277.0394287109375\n",
      "epoch 96, val_loss 454.6893310546875\n",
      "epoch 97, train_loss 1277.0394287109375\n",
      "epoch 97, val_loss 454.6893310546875\n",
      "epoch 98, train_loss 1277.0394287109375\n",
      "epoch 98, val_loss 454.6893005371094\n",
      "epoch 99, train_loss 1277.0391845703125\n",
      "epoch 99, val_loss 454.6892395019531\n",
      "Parameter containing:\n",
      "tensor([2.3119e-30], requires_grad=True)\n",
      "iter 167, train_loss_regularization 0.6881985664367676\n",
      "iter 167, val_loss_regularization 0.6881985664367676\n",
      "epoch 0, train_loss 1277.039306640625\n",
      "epoch 0, val_loss 454.6892395019531\n",
      "epoch 1, train_loss 1277.0391845703125\n",
      "epoch 1, val_loss 454.6891784667969\n",
      "epoch 2, train_loss 1277.0390625\n",
      "epoch 2, val_loss 454.6891784667969\n",
      "epoch 3, train_loss 1277.0391845703125\n",
      "epoch 3, val_loss 454.6890869140625\n",
      "epoch 4, train_loss 1277.0391845703125\n",
      "epoch 4, val_loss 454.6890869140625\n",
      "epoch 5, train_loss 1277.0390625\n",
      "epoch 5, val_loss 454.6890563964844\n",
      "epoch 6, train_loss 1277.0390625\n",
      "epoch 6, val_loss 454.6890563964844\n",
      "epoch 7, train_loss 1277.0389404296875\n",
      "epoch 7, val_loss 454.6890563964844\n",
      "epoch 8, train_loss 1277.0389404296875\n",
      "epoch 8, val_loss 454.68890380859375\n",
      "epoch 9, train_loss 1277.0389404296875\n",
      "epoch 9, val_loss 454.68896484375\n",
      "epoch 10, train_loss 1277.0390625\n",
      "epoch 10, val_loss 454.68896484375\n",
      "epoch 11, train_loss 1277.0389404296875\n",
      "epoch 11, val_loss 454.6888732910156\n",
      "epoch 12, train_loss 1277.0389404296875\n",
      "epoch 12, val_loss 454.6888732910156\n",
      "epoch 13, train_loss 1277.0389404296875\n",
      "epoch 13, val_loss 454.68878173828125\n",
      "epoch 14, train_loss 1277.0389404296875\n",
      "epoch 14, val_loss 454.68878173828125\n",
      "epoch 15, train_loss 1277.0389404296875\n",
      "epoch 15, val_loss 454.68878173828125\n",
      "epoch 16, train_loss 1277.038818359375\n",
      "epoch 16, val_loss 454.688720703125\n",
      "epoch 17, train_loss 1277.038818359375\n",
      "epoch 17, val_loss 454.688720703125\n",
      "epoch 18, train_loss 1277.0386962890625\n",
      "epoch 18, val_loss 454.68865966796875\n",
      "epoch 19, train_loss 1277.03857421875\n",
      "epoch 19, val_loss 454.6886291503906\n",
      "epoch 20, train_loss 1277.0386962890625\n",
      "epoch 20, val_loss 454.6886291503906\n",
      "epoch 21, train_loss 1277.03857421875\n",
      "epoch 21, val_loss 454.6885681152344\n",
      "epoch 22, train_loss 1277.0386962890625\n",
      "epoch 22, val_loss 454.68853759765625\n",
      "epoch 23, train_loss 1277.0386962890625\n",
      "epoch 23, val_loss 454.6885070800781\n",
      "epoch 24, train_loss 1277.03857421875\n",
      "epoch 24, val_loss 454.6885070800781\n",
      "epoch 25, train_loss 1277.0384521484375\n",
      "epoch 25, val_loss 454.6884460449219\n",
      "epoch 26, train_loss 1277.03857421875\n",
      "epoch 26, val_loss 454.6883850097656\n",
      "epoch 27, train_loss 1277.0384521484375\n",
      "epoch 27, val_loss 454.6883850097656\n",
      "epoch 28, train_loss 1277.0384521484375\n",
      "epoch 28, val_loss 454.6883239746094\n",
      "epoch 29, train_loss 1277.0384521484375\n",
      "epoch 29, val_loss 454.68829345703125\n",
      "epoch 30, train_loss 1277.0384521484375\n",
      "epoch 30, val_loss 454.68829345703125\n",
      "epoch 31, train_loss 1277.0384521484375\n",
      "epoch 31, val_loss 454.6882629394531\n",
      "epoch 32, train_loss 1277.0384521484375\n",
      "epoch 32, val_loss 454.6882629394531\n",
      "epoch 33, train_loss 1277.0384521484375\n",
      "epoch 33, val_loss 454.6882019042969\n",
      "epoch 34, train_loss 1277.0382080078125\n",
      "epoch 34, val_loss 454.68817138671875\n",
      "epoch 35, train_loss 1277.0382080078125\n",
      "epoch 35, val_loss 454.68817138671875\n",
      "epoch 36, train_loss 1277.0382080078125\n",
      "epoch 36, val_loss 454.68817138671875\n",
      "epoch 37, train_loss 1277.0382080078125\n",
      "epoch 37, val_loss 454.6880798339844\n",
      "epoch 38, train_loss 1277.0382080078125\n",
      "epoch 38, val_loss 454.68804931640625\n",
      "epoch 39, train_loss 1277.038330078125\n",
      "epoch 39, val_loss 454.68798828125\n",
      "epoch 40, train_loss 1277.038330078125\n",
      "epoch 40, val_loss 454.68798828125\n",
      "epoch 41, train_loss 1277.0380859375\n",
      "epoch 41, val_loss 454.6879577636719\n",
      "epoch 42, train_loss 1277.0382080078125\n",
      "epoch 42, val_loss 454.6878662109375\n",
      "epoch 43, train_loss 1277.0380859375\n",
      "epoch 43, val_loss 454.6878662109375\n",
      "epoch 44, train_loss 1277.0380859375\n",
      "epoch 44, val_loss 454.6878356933594\n",
      "epoch 45, train_loss 1277.0380859375\n",
      "epoch 45, val_loss 454.68780517578125\n",
      "epoch 46, train_loss 1277.0382080078125\n",
      "epoch 46, val_loss 454.6878356933594\n",
      "epoch 47, train_loss 1277.0380859375\n",
      "epoch 47, val_loss 454.68780517578125\n",
      "epoch 48, train_loss 1277.0380859375\n",
      "epoch 48, val_loss 454.687744140625\n",
      "epoch 49, train_loss 1277.0379638671875\n",
      "epoch 49, val_loss 454.6877136230469\n",
      "epoch 50, train_loss 1277.0379638671875\n",
      "epoch 50, val_loss 454.6876525878906\n",
      "epoch 51, train_loss 1277.0379638671875\n",
      "epoch 51, val_loss 454.6876220703125\n",
      "epoch 52, train_loss 1277.0379638671875\n",
      "epoch 52, val_loss 454.6875915527344\n",
      "epoch 53, train_loss 1277.0379638671875\n",
      "epoch 53, val_loss 454.6875915527344\n",
      "epoch 54, train_loss 1277.0379638671875\n",
      "epoch 54, val_loss 454.6875305175781\n",
      "epoch 55, train_loss 1277.0377197265625\n",
      "epoch 55, val_loss 454.6875305175781\n",
      "epoch 56, train_loss 1277.037841796875\n",
      "epoch 56, val_loss 454.6875305175781\n",
      "epoch 57, train_loss 1277.037841796875\n",
      "epoch 57, val_loss 454.6875\n",
      "epoch 58, train_loss 1277.0377197265625\n",
      "epoch 58, val_loss 454.6874084472656\n",
      "epoch 59, train_loss 1277.0377197265625\n",
      "epoch 59, val_loss 454.6873779296875\n",
      "epoch 60, train_loss 1277.0377197265625\n",
      "epoch 60, val_loss 454.6873779296875\n",
      "epoch 61, train_loss 1277.03759765625\n",
      "epoch 61, val_loss 454.6873474121094\n",
      "epoch 62, train_loss 1277.03759765625\n",
      "epoch 62, val_loss 454.687255859375\n",
      "epoch 63, train_loss 1277.03759765625\n",
      "epoch 63, val_loss 454.68719482421875\n",
      "epoch 64, train_loss 1277.03759765625\n",
      "epoch 64, val_loss 454.687255859375\n",
      "epoch 65, train_loss 1277.0377197265625\n",
      "epoch 65, val_loss 454.68719482421875\n",
      "epoch 66, train_loss 1277.0377197265625\n",
      "epoch 66, val_loss 454.6871643066406\n",
      "epoch 67, train_loss 1277.0377197265625\n",
      "epoch 67, val_loss 454.6871643066406\n",
      "epoch 68, train_loss 1277.0377197265625\n",
      "epoch 68, val_loss 454.6871337890625\n",
      "epoch 69, train_loss 1277.0374755859375\n",
      "epoch 69, val_loss 454.6871337890625\n",
      "epoch 70, train_loss 1277.0374755859375\n",
      "epoch 70, val_loss 454.68707275390625\n",
      "epoch 71, train_loss 1277.037353515625\n",
      "epoch 71, val_loss 454.68707275390625\n",
      "epoch 72, train_loss 1277.0372314453125\n",
      "epoch 72, val_loss 454.68701171875\n",
      "epoch 73, train_loss 1277.037353515625\n",
      "epoch 73, val_loss 454.68701171875\n",
      "epoch 74, train_loss 1277.037353515625\n",
      "epoch 74, val_loss 454.68701171875\n",
      "epoch 75, train_loss 1277.0374755859375\n",
      "epoch 75, val_loss 454.6869201660156\n",
      "epoch 76, train_loss 1277.037353515625\n",
      "epoch 76, val_loss 454.6868896484375\n",
      "epoch 77, train_loss 1277.037353515625\n",
      "epoch 77, val_loss 454.6867370605469\n",
      "epoch 78, train_loss 1277.037109375\n",
      "epoch 78, val_loss 454.6867370605469\n",
      "epoch 79, train_loss 1277.037109375\n",
      "epoch 79, val_loss 454.6867370605469\n",
      "epoch 80, train_loss 1277.037109375\n",
      "epoch 80, val_loss 454.6867370605469\n",
      "epoch 81, train_loss 1277.037109375\n",
      "epoch 81, val_loss 454.6867370605469\n",
      "epoch 82, train_loss 1277.037109375\n",
      "epoch 82, val_loss 454.6866760253906\n",
      "epoch 83, train_loss 1277.037109375\n",
      "epoch 83, val_loss 454.6866149902344\n",
      "epoch 84, train_loss 1277.0372314453125\n",
      "epoch 84, val_loss 454.6866149902344\n",
      "epoch 85, train_loss 1277.037109375\n",
      "epoch 85, val_loss 454.6866149902344\n",
      "epoch 86, train_loss 1277.0372314453125\n",
      "epoch 86, val_loss 454.6866149902344\n",
      "epoch 87, train_loss 1277.0372314453125\n",
      "epoch 87, val_loss 454.6865539550781\n",
      "epoch 88, train_loss 1277.037109375\n",
      "epoch 88, val_loss 454.68646240234375\n",
      "epoch 89, train_loss 1277.0369873046875\n",
      "epoch 89, val_loss 454.68646240234375\n",
      "epoch 90, train_loss 1277.037109375\n",
      "epoch 90, val_loss 454.68646240234375\n",
      "epoch 91, train_loss 1277.036865234375\n",
      "epoch 91, val_loss 454.6864318847656\n",
      "epoch 92, train_loss 1277.036865234375\n",
      "epoch 92, val_loss 454.68634033203125\n",
      "epoch 93, train_loss 1277.036865234375\n",
      "epoch 93, val_loss 454.6863708496094\n",
      "epoch 94, train_loss 1277.036865234375\n",
      "epoch 94, val_loss 454.68634033203125\n",
      "epoch 95, train_loss 1277.036865234375\n",
      "epoch 95, val_loss 454.68634033203125\n",
      "epoch 96, train_loss 1277.036865234375\n",
      "epoch 96, val_loss 454.6862487792969\n",
      "epoch 97, train_loss 1277.036865234375\n",
      "epoch 97, val_loss 454.6862487792969\n",
      "epoch 98, train_loss 1277.03662109375\n",
      "epoch 98, val_loss 454.68621826171875\n",
      "epoch 99, train_loss 1277.03662109375\n",
      "epoch 99, val_loss 454.6861572265625\n",
      "Parameter containing:\n",
      "tensor([1.6158e-30], requires_grad=True)\n",
      "iter 168, train_loss_regularization 0.687870442867279\n",
      "iter 168, val_loss_regularization 0.687870442867279\n",
      "epoch 0, train_loss 1277.03662109375\n",
      "epoch 0, val_loss 454.6861267089844\n",
      "epoch 1, train_loss 1277.03662109375\n",
      "epoch 1, val_loss 454.68609619140625\n",
      "epoch 2, train_loss 1277.03662109375\n",
      "epoch 2, val_loss 454.6860046386719\n",
      "epoch 3, train_loss 1277.03662109375\n",
      "epoch 3, val_loss 454.6860046386719\n",
      "epoch 4, train_loss 1277.0367431640625\n",
      "epoch 4, val_loss 454.6860046386719\n",
      "epoch 5, train_loss 1277.0367431640625\n",
      "epoch 5, val_loss 454.6860046386719\n",
      "epoch 6, train_loss 1277.0367431640625\n",
      "epoch 6, val_loss 454.6859436035156\n",
      "epoch 7, train_loss 1277.03662109375\n",
      "epoch 7, val_loss 454.6859130859375\n",
      "epoch 8, train_loss 1277.036376953125\n",
      "epoch 8, val_loss 454.6859130859375\n",
      "epoch 9, train_loss 1277.036376953125\n",
      "epoch 9, val_loss 454.6858825683594\n",
      "epoch 10, train_loss 1277.036376953125\n",
      "epoch 10, val_loss 454.6858825683594\n",
      "epoch 11, train_loss 1277.036376953125\n",
      "epoch 11, val_loss 454.6858215332031\n",
      "epoch 12, train_loss 1277.036376953125\n",
      "epoch 12, val_loss 454.6857604980469\n",
      "epoch 13, train_loss 1277.036376953125\n",
      "epoch 13, val_loss 454.6857604980469\n",
      "epoch 14, train_loss 1277.036376953125\n",
      "epoch 14, val_loss 454.6856994628906\n",
      "epoch 15, train_loss 1277.036376953125\n",
      "epoch 15, val_loss 454.6856689453125\n",
      "epoch 16, train_loss 1277.036376953125\n",
      "epoch 16, val_loss 454.6856689453125\n",
      "epoch 17, train_loss 1277.0362548828125\n",
      "epoch 17, val_loss 454.685546875\n",
      "epoch 18, train_loss 1277.0361328125\n",
      "epoch 18, val_loss 454.6855773925781\n",
      "epoch 19, train_loss 1277.0361328125\n",
      "epoch 19, val_loss 454.685546875\n",
      "epoch 20, train_loss 1277.0361328125\n",
      "epoch 20, val_loss 454.68548583984375\n",
      "epoch 21, train_loss 1277.0361328125\n",
      "epoch 21, val_loss 454.68548583984375\n",
      "epoch 22, train_loss 1277.0362548828125\n",
      "epoch 22, val_loss 454.6854553222656\n",
      "epoch 23, train_loss 1277.0362548828125\n",
      "epoch 23, val_loss 454.6854553222656\n",
      "epoch 24, train_loss 1277.0361328125\n",
      "epoch 24, val_loss 454.6854553222656\n",
      "epoch 25, train_loss 1277.0361328125\n",
      "epoch 25, val_loss 454.68536376953125\n",
      "epoch 26, train_loss 1277.0360107421875\n",
      "epoch 26, val_loss 454.6853332519531\n",
      "epoch 27, train_loss 1277.0360107421875\n",
      "epoch 27, val_loss 454.685302734375\n",
      "epoch 28, train_loss 1277.0360107421875\n",
      "epoch 28, val_loss 454.685302734375\n",
      "epoch 29, train_loss 1277.0360107421875\n",
      "epoch 29, val_loss 454.68524169921875\n",
      "epoch 30, train_loss 1277.0360107421875\n",
      "epoch 30, val_loss 454.68524169921875\n",
      "epoch 31, train_loss 1277.0360107421875\n",
      "epoch 31, val_loss 454.68524169921875\n",
      "epoch 32, train_loss 1277.035888671875\n",
      "epoch 32, val_loss 454.6851806640625\n",
      "epoch 33, train_loss 1277.0360107421875\n",
      "epoch 33, val_loss 454.6851806640625\n",
      "epoch 34, train_loss 1277.035888671875\n",
      "epoch 34, val_loss 454.6851806640625\n",
      "epoch 35, train_loss 1277.035888671875\n",
      "epoch 35, val_loss 454.6850280761719\n",
      "epoch 36, train_loss 1277.035888671875\n",
      "epoch 36, val_loss 454.6850280761719\n",
      "epoch 37, train_loss 1277.035888671875\n",
      "epoch 37, val_loss 454.68499755859375\n",
      "epoch 38, train_loss 1277.035888671875\n",
      "epoch 38, val_loss 454.6849670410156\n",
      "epoch 39, train_loss 1277.0357666015625\n",
      "epoch 39, val_loss 454.6849060058594\n",
      "epoch 40, train_loss 1277.0357666015625\n",
      "epoch 40, val_loss 454.6849060058594\n",
      "epoch 41, train_loss 1277.0357666015625\n",
      "epoch 41, val_loss 454.68487548828125\n",
      "epoch 42, train_loss 1277.03564453125\n",
      "epoch 42, val_loss 454.6848449707031\n",
      "epoch 43, train_loss 1277.035888671875\n",
      "epoch 43, val_loss 454.6848449707031\n",
      "epoch 44, train_loss 1277.03564453125\n",
      "epoch 44, val_loss 454.6847839355469\n",
      "epoch 45, train_loss 1277.03564453125\n",
      "epoch 45, val_loss 454.6847839355469\n",
      "epoch 46, train_loss 1277.03564453125\n",
      "epoch 46, val_loss 454.68475341796875\n",
      "epoch 47, train_loss 1277.03564453125\n",
      "epoch 47, val_loss 454.6847229003906\n",
      "epoch 48, train_loss 1277.0355224609375\n",
      "epoch 48, val_loss 454.6847229003906\n",
      "epoch 49, train_loss 1277.0355224609375\n",
      "epoch 49, val_loss 454.6846618652344\n",
      "epoch 50, train_loss 1277.0355224609375\n",
      "epoch 50, val_loss 454.6845703125\n",
      "epoch 51, train_loss 1277.035400390625\n",
      "epoch 51, val_loss 454.6845703125\n",
      "epoch 52, train_loss 1277.0355224609375\n",
      "epoch 52, val_loss 454.6845703125\n",
      "epoch 53, train_loss 1277.0352783203125\n",
      "epoch 53, val_loss 454.68450927734375\n",
      "epoch 54, train_loss 1277.035400390625\n",
      "epoch 54, val_loss 454.68450927734375\n",
      "epoch 55, train_loss 1277.0352783203125\n",
      "epoch 55, val_loss 454.6844482421875\n",
      "epoch 56, train_loss 1277.035400390625\n",
      "epoch 56, val_loss 454.6844177246094\n",
      "epoch 57, train_loss 1277.0352783203125\n",
      "epoch 57, val_loss 454.68438720703125\n",
      "epoch 58, train_loss 1277.0352783203125\n",
      "epoch 58, val_loss 454.68438720703125\n",
      "epoch 59, train_loss 1277.0352783203125\n",
      "epoch 59, val_loss 454.684326171875\n",
      "epoch 60, train_loss 1277.0352783203125\n",
      "epoch 60, val_loss 454.6842956542969\n",
      "epoch 61, train_loss 1277.0352783203125\n",
      "epoch 61, val_loss 454.68426513671875\n",
      "epoch 62, train_loss 1277.03515625\n",
      "epoch 62, val_loss 454.6842041015625\n",
      "epoch 63, train_loss 1277.0350341796875\n",
      "epoch 63, val_loss 454.6841735839844\n",
      "epoch 64, train_loss 1277.03515625\n",
      "epoch 64, val_loss 454.6841735839844\n",
      "epoch 65, train_loss 1277.03515625\n",
      "epoch 65, val_loss 454.6841735839844\n",
      "epoch 66, train_loss 1277.03515625\n",
      "epoch 66, val_loss 454.6841125488281\n",
      "epoch 67, train_loss 1277.0350341796875\n",
      "epoch 67, val_loss 454.68408203125\n",
      "epoch 68, train_loss 1277.0350341796875\n",
      "epoch 68, val_loss 454.68408203125\n",
      "epoch 69, train_loss 1277.0350341796875\n",
      "epoch 69, val_loss 454.68408203125\n",
      "epoch 70, train_loss 1277.034912109375\n",
      "epoch 70, val_loss 454.6840515136719\n",
      "epoch 71, train_loss 1277.034912109375\n",
      "epoch 71, val_loss 454.6839904785156\n",
      "epoch 72, train_loss 1277.0350341796875\n",
      "epoch 72, val_loss 454.6839294433594\n",
      "epoch 73, train_loss 1277.0350341796875\n",
      "epoch 73, val_loss 454.6839294433594\n",
      "epoch 74, train_loss 1277.0350341796875\n",
      "epoch 74, val_loss 454.6838684082031\n",
      "epoch 75, train_loss 1277.034912109375\n",
      "epoch 75, val_loss 454.683837890625\n",
      "epoch 76, train_loss 1277.0350341796875\n",
      "epoch 76, val_loss 454.68377685546875\n",
      "epoch 77, train_loss 1277.0347900390625\n",
      "epoch 77, val_loss 454.6837463378906\n",
      "epoch 78, train_loss 1277.034912109375\n",
      "epoch 78, val_loss 454.6837158203125\n",
      "epoch 79, train_loss 1277.0347900390625\n",
      "epoch 79, val_loss 454.6837463378906\n",
      "epoch 80, train_loss 1277.0347900390625\n",
      "epoch 80, val_loss 454.68365478515625\n",
      "epoch 81, train_loss 1277.0347900390625\n",
      "epoch 81, val_loss 454.6836242675781\n",
      "epoch 82, train_loss 1277.0347900390625\n",
      "epoch 82, val_loss 454.68365478515625\n",
      "epoch 83, train_loss 1277.0347900390625\n",
      "epoch 83, val_loss 454.6836242675781\n",
      "epoch 84, train_loss 1277.0347900390625\n",
      "epoch 84, val_loss 454.6836242675781\n",
      "epoch 85, train_loss 1277.03466796875\n",
      "epoch 85, val_loss 454.68359375\n",
      "epoch 86, train_loss 1277.03466796875\n",
      "epoch 86, val_loss 454.68353271484375\n",
      "epoch 87, train_loss 1277.0345458984375\n",
      "epoch 87, val_loss 454.6835021972656\n",
      "epoch 88, train_loss 1277.0345458984375\n",
      "epoch 88, val_loss 454.6834716796875\n",
      "epoch 89, train_loss 1277.0345458984375\n",
      "epoch 89, val_loss 454.68341064453125\n",
      "epoch 90, train_loss 1277.0345458984375\n",
      "epoch 90, val_loss 454.6833801269531\n",
      "epoch 91, train_loss 1277.0345458984375\n",
      "epoch 91, val_loss 454.6833190917969\n",
      "epoch 92, train_loss 1277.034423828125\n",
      "epoch 92, val_loss 454.68328857421875\n",
      "epoch 93, train_loss 1277.034423828125\n",
      "epoch 93, val_loss 454.6832580566406\n",
      "epoch 94, train_loss 1277.0345458984375\n",
      "epoch 94, val_loss 454.6832580566406\n",
      "epoch 95, train_loss 1277.0345458984375\n",
      "epoch 95, val_loss 454.6832580566406\n",
      "epoch 96, train_loss 1277.034423828125\n",
      "epoch 96, val_loss 454.6831970214844\n",
      "epoch 97, train_loss 1277.0345458984375\n",
      "epoch 97, val_loss 454.68316650390625\n",
      "epoch 98, train_loss 1277.0343017578125\n",
      "epoch 98, val_loss 454.6831359863281\n",
      "epoch 99, train_loss 1277.0341796875\n",
      "epoch 99, val_loss 454.68316650390625\n",
      "Parameter containing:\n",
      "tensor([1.1294e-30], requires_grad=True)\n",
      "iter 169, train_loss_regularization 0.6875563859939575\n",
      "iter 169, val_loss_regularization 0.6875563859939575\n",
      "epoch 0, train_loss 1277.0341796875\n",
      "epoch 0, val_loss 454.6831359863281\n",
      "epoch 1, train_loss 1277.0341796875\n",
      "epoch 1, val_loss 454.6830139160156\n",
      "epoch 2, train_loss 1277.0341796875\n",
      "epoch 2, val_loss 454.6830139160156\n",
      "epoch 3, train_loss 1277.0343017578125\n",
      "epoch 3, val_loss 454.6830139160156\n",
      "epoch 4, train_loss 1277.0343017578125\n",
      "epoch 4, val_loss 454.6829528808594\n",
      "epoch 5, train_loss 1277.0341796875\n",
      "epoch 5, val_loss 454.6829528808594\n",
      "epoch 6, train_loss 1277.0340576171875\n",
      "epoch 6, val_loss 454.68292236328125\n",
      "epoch 7, train_loss 1277.0340576171875\n",
      "epoch 7, val_loss 454.682861328125\n",
      "epoch 8, train_loss 1277.0340576171875\n",
      "epoch 8, val_loss 454.682861328125\n",
      "epoch 9, train_loss 1277.0340576171875\n",
      "epoch 9, val_loss 454.682861328125\n",
      "epoch 10, train_loss 1277.0340576171875\n",
      "epoch 10, val_loss 454.6828308105469\n",
      "epoch 11, train_loss 1277.0340576171875\n",
      "epoch 11, val_loss 454.6827087402344\n",
      "epoch 12, train_loss 1277.0340576171875\n",
      "epoch 12, val_loss 454.6827087402344\n",
      "epoch 13, train_loss 1277.033935546875\n",
      "epoch 13, val_loss 454.68267822265625\n",
      "epoch 14, train_loss 1277.033935546875\n",
      "epoch 14, val_loss 454.6826171875\n",
      "epoch 15, train_loss 1277.0340576171875\n",
      "epoch 15, val_loss 454.6825866699219\n",
      "epoch 16, train_loss 1277.0340576171875\n",
      "epoch 16, val_loss 454.6825866699219\n",
      "epoch 17, train_loss 1277.0340576171875\n",
      "epoch 17, val_loss 454.6825866699219\n",
      "epoch 18, train_loss 1277.03369140625\n",
      "epoch 18, val_loss 454.68255615234375\n",
      "epoch 19, train_loss 1277.03369140625\n",
      "epoch 19, val_loss 454.6824951171875\n",
      "epoch 20, train_loss 1277.0338134765625\n",
      "epoch 20, val_loss 454.6824951171875\n",
      "epoch 21, train_loss 1277.03369140625\n",
      "epoch 21, val_loss 454.6824951171875\n",
      "epoch 22, train_loss 1277.03369140625\n",
      "epoch 22, val_loss 454.6824035644531\n",
      "epoch 23, train_loss 1277.03369140625\n",
      "epoch 23, val_loss 454.6824035644531\n",
      "epoch 24, train_loss 1277.03369140625\n",
      "epoch 24, val_loss 454.6823425292969\n",
      "epoch 25, train_loss 1277.03369140625\n",
      "epoch 25, val_loss 454.682373046875\n",
      "epoch 26, train_loss 1277.03369140625\n",
      "epoch 26, val_loss 454.6822814941406\n",
      "epoch 27, train_loss 1277.0335693359375\n",
      "epoch 27, val_loss 454.6822509765625\n",
      "epoch 28, train_loss 1277.0335693359375\n",
      "epoch 28, val_loss 454.6822509765625\n",
      "epoch 29, train_loss 1277.033447265625\n",
      "epoch 29, val_loss 454.6822509765625\n",
      "epoch 30, train_loss 1277.033447265625\n",
      "epoch 30, val_loss 454.6822204589844\n",
      "epoch 31, train_loss 1277.033447265625\n",
      "epoch 31, val_loss 454.6820983886719\n",
      "epoch 32, train_loss 1277.033447265625\n",
      "epoch 32, val_loss 454.6820983886719\n",
      "epoch 33, train_loss 1277.033447265625\n",
      "epoch 33, val_loss 454.6820983886719\n",
      "epoch 34, train_loss 1277.03369140625\n",
      "epoch 34, val_loss 454.6820983886719\n",
      "epoch 35, train_loss 1277.033447265625\n",
      "epoch 35, val_loss 454.6820068359375\n",
      "epoch 36, train_loss 1277.0333251953125\n",
      "epoch 36, val_loss 454.6820068359375\n",
      "epoch 37, train_loss 1277.0333251953125\n",
      "epoch 37, val_loss 454.68194580078125\n",
      "epoch 38, train_loss 1277.0333251953125\n",
      "epoch 38, val_loss 454.68194580078125\n",
      "epoch 39, train_loss 1277.0333251953125\n",
      "epoch 39, val_loss 454.681884765625\n",
      "epoch 40, train_loss 1277.0333251953125\n",
      "epoch 40, val_loss 454.681884765625\n",
      "epoch 41, train_loss 1277.033203125\n",
      "epoch 41, val_loss 454.681884765625\n",
      "epoch 42, train_loss 1277.033203125\n",
      "epoch 42, val_loss 454.6817932128906\n",
      "epoch 43, train_loss 1277.033203125\n",
      "epoch 43, val_loss 454.6817932128906\n",
      "epoch 44, train_loss 1277.033203125\n",
      "epoch 44, val_loss 454.6817932128906\n",
      "epoch 45, train_loss 1277.033203125\n",
      "epoch 45, val_loss 454.6817626953125\n",
      "epoch 46, train_loss 1277.0330810546875\n",
      "epoch 46, val_loss 454.68170166015625\n",
      "epoch 47, train_loss 1277.0330810546875\n",
      "epoch 47, val_loss 454.6816711425781\n",
      "epoch 48, train_loss 1277.033203125\n",
      "epoch 48, val_loss 454.6816711425781\n",
      "epoch 49, train_loss 1277.0330810546875\n",
      "epoch 49, val_loss 454.6816711425781\n",
      "epoch 50, train_loss 1277.032958984375\n",
      "epoch 50, val_loss 454.6815490722656\n",
      "epoch 51, train_loss 1277.032958984375\n",
      "epoch 51, val_loss 454.6814880371094\n",
      "epoch 52, train_loss 1277.032958984375\n",
      "epoch 52, val_loss 454.6814880371094\n",
      "epoch 53, train_loss 1277.0330810546875\n",
      "epoch 53, val_loss 454.68145751953125\n",
      "epoch 54, train_loss 1277.0330810546875\n",
      "epoch 54, val_loss 454.6814270019531\n",
      "epoch 55, train_loss 1277.032958984375\n",
      "epoch 55, val_loss 454.6814270019531\n",
      "epoch 56, train_loss 1277.032958984375\n",
      "epoch 56, val_loss 454.6814270019531\n",
      "epoch 57, train_loss 1277.032958984375\n",
      "epoch 57, val_loss 454.6813659667969\n",
      "epoch 58, train_loss 1277.032958984375\n",
      "epoch 58, val_loss 454.68133544921875\n",
      "epoch 59, train_loss 1277.03271484375\n",
      "epoch 59, val_loss 454.6813049316406\n",
      "epoch 60, train_loss 1277.0328369140625\n",
      "epoch 60, val_loss 454.6813049316406\n",
      "epoch 61, train_loss 1277.0328369140625\n",
      "epoch 61, val_loss 454.6812438964844\n",
      "epoch 62, train_loss 1277.032958984375\n",
      "epoch 62, val_loss 454.68121337890625\n",
      "epoch 63, train_loss 1277.032958984375\n",
      "epoch 63, val_loss 454.68115234375\n",
      "epoch 64, train_loss 1277.0328369140625\n",
      "epoch 64, val_loss 454.68115234375\n",
      "epoch 65, train_loss 1277.03271484375\n",
      "epoch 65, val_loss 454.68109130859375\n",
      "epoch 66, train_loss 1277.03271484375\n",
      "epoch 66, val_loss 454.68109130859375\n",
      "epoch 67, train_loss 1277.03271484375\n",
      "epoch 67, val_loss 454.68109130859375\n",
      "epoch 68, train_loss 1277.03271484375\n",
      "epoch 68, val_loss 454.6809997558594\n",
      "epoch 69, train_loss 1277.0325927734375\n",
      "epoch 69, val_loss 454.6809997558594\n",
      "epoch 70, train_loss 1277.03271484375\n",
      "epoch 70, val_loss 454.6809997558594\n",
      "epoch 71, train_loss 1277.03271484375\n",
      "epoch 71, val_loss 454.68096923828125\n",
      "epoch 72, train_loss 1277.03271484375\n",
      "epoch 72, val_loss 454.680908203125\n",
      "epoch 73, train_loss 1277.0325927734375\n",
      "epoch 73, val_loss 454.680908203125\n",
      "epoch 74, train_loss 1277.032470703125\n",
      "epoch 74, val_loss 454.68084716796875\n",
      "epoch 75, train_loss 1277.032470703125\n",
      "epoch 75, val_loss 454.68084716796875\n",
      "epoch 76, train_loss 1277.032470703125\n",
      "epoch 76, val_loss 454.6807861328125\n",
      "epoch 77, train_loss 1277.032470703125\n",
      "epoch 77, val_loss 454.6807861328125\n",
      "epoch 78, train_loss 1277.032470703125\n",
      "epoch 78, val_loss 454.6806945800781\n",
      "epoch 79, train_loss 1277.032470703125\n",
      "epoch 79, val_loss 454.6806640625\n",
      "epoch 80, train_loss 1277.0323486328125\n",
      "epoch 80, val_loss 454.6806640625\n",
      "epoch 81, train_loss 1277.0323486328125\n",
      "epoch 81, val_loss 454.6806335449219\n",
      "epoch 82, train_loss 1277.0322265625\n",
      "epoch 82, val_loss 454.6805725097656\n",
      "epoch 83, train_loss 1277.0322265625\n",
      "epoch 83, val_loss 454.6805725097656\n",
      "epoch 84, train_loss 1277.0322265625\n",
      "epoch 84, val_loss 454.6805725097656\n",
      "epoch 85, train_loss 1277.0322265625\n",
      "epoch 85, val_loss 454.6805114746094\n",
      "epoch 86, train_loss 1277.0322265625\n",
      "epoch 86, val_loss 454.6805419921875\n",
      "epoch 87, train_loss 1277.0322265625\n",
      "epoch 87, val_loss 454.6805114746094\n",
      "epoch 88, train_loss 1277.0322265625\n",
      "epoch 88, val_loss 454.680419921875\n",
      "epoch 89, train_loss 1277.0322265625\n",
      "epoch 89, val_loss 454.6803894042969\n",
      "epoch 90, train_loss 1277.0322265625\n",
      "epoch 90, val_loss 454.6803283691406\n",
      "epoch 91, train_loss 1277.0321044921875\n",
      "epoch 91, val_loss 454.6803283691406\n",
      "epoch 92, train_loss 1277.031982421875\n",
      "epoch 92, val_loss 454.6802978515625\n",
      "epoch 93, train_loss 1277.0318603515625\n",
      "epoch 93, val_loss 454.68023681640625\n",
      "epoch 94, train_loss 1277.0318603515625\n",
      "epoch 94, val_loss 454.68023681640625\n",
      "epoch 95, train_loss 1277.031982421875\n",
      "epoch 95, val_loss 454.6802062988281\n",
      "epoch 96, train_loss 1277.0318603515625\n",
      "epoch 96, val_loss 454.68023681640625\n",
      "epoch 97, train_loss 1277.0318603515625\n",
      "epoch 97, val_loss 454.68011474609375\n",
      "epoch 98, train_loss 1277.0318603515625\n",
      "epoch 98, val_loss 454.6800842285156\n",
      "epoch 99, train_loss 1277.031982421875\n",
      "epoch 99, val_loss 454.68011474609375\n",
      "Parameter containing:\n",
      "tensor([7.8958e-31], requires_grad=True)\n",
      "iter 170, train_loss_regularization 0.6872563362121582\n",
      "iter 170, val_loss_regularization 0.6872563362121582\n",
      "epoch 0, train_loss 1277.0318603515625\n",
      "epoch 0, val_loss 454.68011474609375\n",
      "epoch 1, train_loss 1277.03173828125\n",
      "epoch 1, val_loss 454.6800842285156\n",
      "epoch 2, train_loss 1277.0318603515625\n",
      "epoch 2, val_loss 454.67999267578125\n",
      "epoch 3, train_loss 1277.0318603515625\n",
      "epoch 3, val_loss 454.6799621582031\n",
      "epoch 4, train_loss 1277.0318603515625\n",
      "epoch 4, val_loss 454.6799621582031\n",
      "epoch 5, train_loss 1277.0318603515625\n",
      "epoch 5, val_loss 454.679931640625\n",
      "epoch 6, train_loss 1277.03173828125\n",
      "epoch 6, val_loss 454.679931640625\n",
      "epoch 7, train_loss 1277.03173828125\n",
      "epoch 7, val_loss 454.6798400878906\n",
      "epoch 8, train_loss 1277.0316162109375\n",
      "epoch 8, val_loss 454.6797790527344\n",
      "epoch 9, train_loss 1277.03173828125\n",
      "epoch 9, val_loss 454.67974853515625\n",
      "epoch 10, train_loss 1277.0316162109375\n",
      "epoch 10, val_loss 454.67974853515625\n",
      "epoch 11, train_loss 1277.0316162109375\n",
      "epoch 11, val_loss 454.6797180175781\n",
      "epoch 12, train_loss 1277.0316162109375\n",
      "epoch 12, val_loss 454.6796569824219\n",
      "epoch 13, train_loss 1277.0316162109375\n",
      "epoch 13, val_loss 454.6796569824219\n",
      "epoch 14, train_loss 1277.0316162109375\n",
      "epoch 14, val_loss 454.6796569824219\n",
      "epoch 15, train_loss 1277.031494140625\n",
      "epoch 15, val_loss 454.67962646484375\n",
      "epoch 16, train_loss 1277.031494140625\n",
      "epoch 16, val_loss 454.6795959472656\n",
      "epoch 17, train_loss 1277.031494140625\n",
      "epoch 17, val_loss 454.6795959472656\n",
      "epoch 18, train_loss 1277.031494140625\n",
      "epoch 18, val_loss 454.67950439453125\n",
      "epoch 19, train_loss 1277.031494140625\n",
      "epoch 19, val_loss 454.67950439453125\n",
      "epoch 20, train_loss 1277.031494140625\n",
      "epoch 20, val_loss 454.67950439453125\n",
      "epoch 21, train_loss 1277.0313720703125\n",
      "epoch 21, val_loss 454.67950439453125\n",
      "epoch 22, train_loss 1277.0313720703125\n",
      "epoch 22, val_loss 454.6794128417969\n",
      "epoch 23, train_loss 1277.03125\n",
      "epoch 23, val_loss 454.67938232421875\n",
      "epoch 24, train_loss 1277.03125\n",
      "epoch 24, val_loss 454.6793212890625\n",
      "epoch 25, train_loss 1277.0313720703125\n",
      "epoch 25, val_loss 454.6793212890625\n",
      "epoch 26, train_loss 1277.03125\n",
      "epoch 26, val_loss 454.6792907714844\n",
      "epoch 27, train_loss 1277.0313720703125\n",
      "epoch 27, val_loss 454.67926025390625\n",
      "epoch 28, train_loss 1277.0313720703125\n",
      "epoch 28, val_loss 454.6791687011719\n",
      "epoch 29, train_loss 1277.03125\n",
      "epoch 29, val_loss 454.6791687011719\n",
      "epoch 30, train_loss 1277.03125\n",
      "epoch 30, val_loss 454.6791687011719\n",
      "epoch 31, train_loss 1277.0311279296875\n",
      "epoch 31, val_loss 454.6791687011719\n",
      "epoch 32, train_loss 1277.0311279296875\n",
      "epoch 32, val_loss 454.6790466308594\n",
      "epoch 33, train_loss 1277.031005859375\n",
      "epoch 33, val_loss 454.6790466308594\n",
      "epoch 34, train_loss 1277.031005859375\n",
      "epoch 34, val_loss 454.6790466308594\n",
      "epoch 35, train_loss 1277.031005859375\n",
      "epoch 35, val_loss 454.6789855957031\n",
      "epoch 36, train_loss 1277.031005859375\n",
      "epoch 36, val_loss 454.6789855957031\n",
      "epoch 37, train_loss 1277.0308837890625\n",
      "epoch 37, val_loss 454.678955078125\n",
      "epoch 38, train_loss 1277.0308837890625\n",
      "epoch 38, val_loss 454.6789245605469\n",
      "epoch 39, train_loss 1277.0308837890625\n",
      "epoch 39, val_loss 454.6789245605469\n",
      "epoch 40, train_loss 1277.03076171875\n",
      "epoch 40, val_loss 454.6788330078125\n",
      "epoch 41, train_loss 1277.03076171875\n",
      "epoch 41, val_loss 454.6788330078125\n",
      "epoch 42, train_loss 1277.0308837890625\n",
      "epoch 42, val_loss 454.6788330078125\n",
      "epoch 43, train_loss 1277.0308837890625\n",
      "epoch 43, val_loss 454.6787414550781\n",
      "epoch 44, train_loss 1277.0308837890625\n",
      "epoch 44, val_loss 454.6787414550781\n",
      "epoch 45, train_loss 1277.0308837890625\n",
      "epoch 45, val_loss 454.6787414550781\n",
      "epoch 46, train_loss 1277.0308837890625\n",
      "epoch 46, val_loss 454.6786804199219\n",
      "epoch 47, train_loss 1277.0308837890625\n",
      "epoch 47, val_loss 454.6786193847656\n",
      "epoch 48, train_loss 1277.03076171875\n",
      "epoch 48, val_loss 454.6785888671875\n",
      "epoch 49, train_loss 1277.03076171875\n",
      "epoch 49, val_loss 454.6785888671875\n",
      "epoch 50, train_loss 1277.03076171875\n",
      "epoch 50, val_loss 454.67852783203125\n",
      "epoch 51, train_loss 1277.0306396484375\n",
      "epoch 51, val_loss 454.67852783203125\n",
      "epoch 52, train_loss 1277.0306396484375\n",
      "epoch 52, val_loss 454.67852783203125\n",
      "epoch 53, train_loss 1277.0306396484375\n",
      "epoch 53, val_loss 454.678466796875\n",
      "epoch 54, train_loss 1277.0306396484375\n",
      "epoch 54, val_loss 454.67840576171875\n",
      "epoch 55, train_loss 1277.0306396484375\n",
      "epoch 55, val_loss 454.67840576171875\n",
      "epoch 56, train_loss 1277.0306396484375\n",
      "epoch 56, val_loss 454.67840576171875\n",
      "epoch 57, train_loss 1277.0306396484375\n",
      "epoch 57, val_loss 454.6783447265625\n",
      "epoch 58, train_loss 1277.030517578125\n",
      "epoch 58, val_loss 454.6783447265625\n",
      "epoch 59, train_loss 1277.030517578125\n",
      "epoch 59, val_loss 454.6783447265625\n",
      "epoch 60, train_loss 1277.0303955078125\n",
      "epoch 60, val_loss 454.67828369140625\n",
      "epoch 61, train_loss 1277.0303955078125\n",
      "epoch 61, val_loss 454.6782531738281\n",
      "epoch 62, train_loss 1277.0302734375\n",
      "epoch 62, val_loss 454.6782531738281\n",
      "epoch 63, train_loss 1277.0302734375\n",
      "epoch 63, val_loss 454.67822265625\n",
      "epoch 64, train_loss 1277.0303955078125\n",
      "epoch 64, val_loss 454.67816162109375\n",
      "epoch 65, train_loss 1277.0302734375\n",
      "epoch 65, val_loss 454.6780700683594\n",
      "epoch 66, train_loss 1277.0303955078125\n",
      "epoch 66, val_loss 454.6780700683594\n",
      "epoch 67, train_loss 1277.0302734375\n",
      "epoch 67, val_loss 454.6780090332031\n",
      "epoch 68, train_loss 1277.0302734375\n",
      "epoch 68, val_loss 454.6779479980469\n",
      "epoch 69, train_loss 1277.0302734375\n",
      "epoch 69, val_loss 454.6779479980469\n",
      "epoch 70, train_loss 1277.0302734375\n",
      "epoch 70, val_loss 454.6779479980469\n",
      "epoch 71, train_loss 1277.0302734375\n",
      "epoch 71, val_loss 454.67791748046875\n",
      "epoch 72, train_loss 1277.0301513671875\n",
      "epoch 72, val_loss 454.6779479980469\n",
      "epoch 73, train_loss 1277.0301513671875\n",
      "epoch 73, val_loss 454.6778869628906\n",
      "epoch 74, train_loss 1277.030029296875\n",
      "epoch 74, val_loss 454.6778259277344\n",
      "epoch 75, train_loss 1277.030029296875\n",
      "epoch 75, val_loss 454.6778259277344\n",
      "epoch 76, train_loss 1277.030029296875\n",
      "epoch 76, val_loss 454.6778259277344\n",
      "epoch 77, train_loss 1277.0299072265625\n",
      "epoch 77, val_loss 454.67779541015625\n",
      "epoch 78, train_loss 1277.030029296875\n",
      "epoch 78, val_loss 454.6777648925781\n",
      "epoch 79, train_loss 1277.030029296875\n",
      "epoch 79, val_loss 454.67767333984375\n",
      "epoch 80, train_loss 1277.030029296875\n",
      "epoch 80, val_loss 454.67767333984375\n",
      "epoch 81, train_loss 1277.030029296875\n",
      "epoch 81, val_loss 454.6776123046875\n",
      "epoch 82, train_loss 1277.02978515625\n",
      "epoch 82, val_loss 454.6776123046875\n",
      "epoch 83, train_loss 1277.02978515625\n",
      "epoch 83, val_loss 454.6775817871094\n",
      "epoch 84, train_loss 1277.0299072265625\n",
      "epoch 84, val_loss 454.6775817871094\n",
      "epoch 85, train_loss 1277.02978515625\n",
      "epoch 85, val_loss 454.677490234375\n",
      "epoch 86, train_loss 1277.02978515625\n",
      "epoch 86, val_loss 454.67742919921875\n",
      "epoch 87, train_loss 1277.02978515625\n",
      "epoch 87, val_loss 454.677490234375\n",
      "epoch 88, train_loss 1277.02978515625\n",
      "epoch 88, val_loss 454.67742919921875\n",
      "epoch 89, train_loss 1277.02978515625\n",
      "epoch 89, val_loss 454.6773681640625\n",
      "epoch 90, train_loss 1277.02978515625\n",
      "epoch 90, val_loss 454.6773681640625\n",
      "epoch 91, train_loss 1277.02978515625\n",
      "epoch 91, val_loss 454.6773681640625\n",
      "epoch 92, train_loss 1277.02978515625\n",
      "epoch 92, val_loss 454.6772766113281\n",
      "epoch 93, train_loss 1277.02978515625\n",
      "epoch 93, val_loss 454.67724609375\n",
      "epoch 94, train_loss 1277.02978515625\n",
      "epoch 94, val_loss 454.67724609375\n",
      "epoch 95, train_loss 1277.0296630859375\n",
      "epoch 95, val_loss 454.6772155761719\n",
      "epoch 96, train_loss 1277.029541015625\n",
      "epoch 96, val_loss 454.6772155761719\n",
      "epoch 97, train_loss 1277.029541015625\n",
      "epoch 97, val_loss 454.6772155761719\n",
      "epoch 98, train_loss 1277.029541015625\n",
      "epoch 98, val_loss 454.6771240234375\n",
      "epoch 99, train_loss 1277.029541015625\n",
      "epoch 99, val_loss 454.6770935058594\n",
      "Parameter containing:\n",
      "tensor([5.5204e-31], requires_grad=True)\n",
      "iter 171, train_loss_regularization 0.6869699954986572\n",
      "iter 171, val_loss_regularization 0.6869699954986572\n",
      "epoch 0, train_loss 1277.029541015625\n",
      "epoch 0, val_loss 454.6770935058594\n",
      "epoch 1, train_loss 1277.029541015625\n",
      "epoch 1, val_loss 454.6770324707031\n",
      "epoch 2, train_loss 1277.029541015625\n",
      "epoch 2, val_loss 454.6770935058594\n",
      "epoch 3, train_loss 1277.029541015625\n",
      "epoch 3, val_loss 454.677001953125\n",
      "epoch 4, train_loss 1277.0294189453125\n",
      "epoch 4, val_loss 454.6769104003906\n",
      "epoch 5, train_loss 1277.029296875\n",
      "epoch 5, val_loss 454.6769104003906\n",
      "epoch 6, train_loss 1277.029296875\n",
      "epoch 6, val_loss 454.6768798828125\n",
      "epoch 7, train_loss 1277.029296875\n",
      "epoch 7, val_loss 454.6768798828125\n",
      "epoch 8, train_loss 1277.029296875\n",
      "epoch 8, val_loss 454.6768798828125\n",
      "epoch 9, train_loss 1277.029296875\n",
      "epoch 9, val_loss 454.6767883300781\n",
      "epoch 10, train_loss 1277.029296875\n",
      "epoch 10, val_loss 454.6767578125\n",
      "epoch 11, train_loss 1277.029296875\n",
      "epoch 11, val_loss 454.6767578125\n",
      "epoch 12, train_loss 1277.0291748046875\n",
      "epoch 12, val_loss 454.67669677734375\n",
      "epoch 13, train_loss 1277.0291748046875\n",
      "epoch 13, val_loss 454.67669677734375\n",
      "epoch 14, train_loss 1277.0291748046875\n",
      "epoch 14, val_loss 454.6766662597656\n",
      "epoch 15, train_loss 1277.0291748046875\n",
      "epoch 15, val_loss 454.6766357421875\n",
      "epoch 16, train_loss 1277.029052734375\n",
      "epoch 16, val_loss 454.6766357421875\n",
      "epoch 17, train_loss 1277.029052734375\n",
      "epoch 17, val_loss 454.67657470703125\n",
      "epoch 18, train_loss 1277.029052734375\n",
      "epoch 18, val_loss 454.67657470703125\n",
      "epoch 19, train_loss 1277.029052734375\n",
      "epoch 19, val_loss 454.676513671875\n",
      "epoch 20, train_loss 1277.029052734375\n",
      "epoch 20, val_loss 454.67645263671875\n",
      "epoch 21, train_loss 1277.029052734375\n",
      "epoch 21, val_loss 454.67645263671875\n",
      "epoch 22, train_loss 1277.029052734375\n",
      "epoch 22, val_loss 454.67645263671875\n",
      "epoch 23, train_loss 1277.029052734375\n",
      "epoch 23, val_loss 454.6763610839844\n",
      "epoch 24, train_loss 1277.029052734375\n",
      "epoch 24, val_loss 454.6764221191406\n",
      "epoch 25, train_loss 1277.029052734375\n",
      "epoch 25, val_loss 454.6763000488281\n",
      "epoch 26, train_loss 1277.02880859375\n",
      "epoch 26, val_loss 454.6762390136719\n",
      "epoch 27, train_loss 1277.02880859375\n",
      "epoch 27, val_loss 454.6762390136719\n",
      "epoch 28, train_loss 1277.02880859375\n",
      "epoch 28, val_loss 454.6763000488281\n",
      "epoch 29, train_loss 1277.02880859375\n",
      "epoch 29, val_loss 454.67620849609375\n",
      "epoch 30, train_loss 1277.0289306640625\n",
      "epoch 30, val_loss 454.6761779785156\n",
      "epoch 31, train_loss 1277.0289306640625\n",
      "epoch 31, val_loss 454.6761169433594\n",
      "epoch 32, train_loss 1277.02880859375\n",
      "epoch 32, val_loss 454.6761169433594\n",
      "epoch 33, train_loss 1277.0286865234375\n",
      "epoch 33, val_loss 454.6761169433594\n",
      "epoch 34, train_loss 1277.0286865234375\n",
      "epoch 34, val_loss 454.6760559082031\n",
      "epoch 35, train_loss 1277.0286865234375\n",
      "epoch 35, val_loss 454.6760559082031\n",
      "epoch 36, train_loss 1277.028564453125\n",
      "epoch 36, val_loss 454.6759948730469\n",
      "epoch 37, train_loss 1277.0286865234375\n",
      "epoch 37, val_loss 454.6759948730469\n",
      "epoch 38, train_loss 1277.0286865234375\n",
      "epoch 38, val_loss 454.67596435546875\n",
      "epoch 39, train_loss 1277.02880859375\n",
      "epoch 39, val_loss 454.6759033203125\n",
      "epoch 40, train_loss 1277.0286865234375\n",
      "epoch 40, val_loss 454.6758728027344\n",
      "epoch 41, train_loss 1277.0286865234375\n",
      "epoch 41, val_loss 454.6758728027344\n",
      "epoch 42, train_loss 1277.0284423828125\n",
      "epoch 42, val_loss 454.6758728027344\n",
      "epoch 43, train_loss 1277.0284423828125\n",
      "epoch 43, val_loss 454.6757507324219\n",
      "epoch 44, train_loss 1277.0284423828125\n",
      "epoch 44, val_loss 454.6757507324219\n",
      "epoch 45, train_loss 1277.0283203125\n",
      "epoch 45, val_loss 454.67572021484375\n",
      "epoch 46, train_loss 1277.0283203125\n",
      "epoch 46, val_loss 454.67572021484375\n",
      "epoch 47, train_loss 1277.0284423828125\n",
      "epoch 47, val_loss 454.67572021484375\n",
      "epoch 48, train_loss 1277.0283203125\n",
      "epoch 48, val_loss 454.6756591796875\n",
      "epoch 49, train_loss 1277.0283203125\n",
      "epoch 49, val_loss 454.67559814453125\n",
      "epoch 50, train_loss 1277.0283203125\n",
      "epoch 50, val_loss 454.67559814453125\n",
      "epoch 51, train_loss 1277.0283203125\n",
      "epoch 51, val_loss 454.675537109375\n",
      "epoch 52, train_loss 1277.0283203125\n",
      "epoch 52, val_loss 454.675537109375\n",
      "epoch 53, train_loss 1277.0281982421875\n",
      "epoch 53, val_loss 454.675537109375\n",
      "epoch 54, train_loss 1277.0281982421875\n",
      "epoch 54, val_loss 454.6754455566406\n",
      "epoch 55, train_loss 1277.0281982421875\n",
      "epoch 55, val_loss 454.6754455566406\n",
      "epoch 56, train_loss 1277.028076171875\n",
      "epoch 56, val_loss 454.6754150390625\n",
      "epoch 57, train_loss 1277.028076171875\n",
      "epoch 57, val_loss 454.6753845214844\n",
      "epoch 58, train_loss 1277.028076171875\n",
      "epoch 58, val_loss 454.6753845214844\n",
      "epoch 59, train_loss 1277.028076171875\n",
      "epoch 59, val_loss 454.6753845214844\n",
      "epoch 60, train_loss 1277.0281982421875\n",
      "epoch 60, val_loss 454.6752624511719\n",
      "epoch 61, train_loss 1277.0281982421875\n",
      "epoch 61, val_loss 454.67529296875\n",
      "epoch 62, train_loss 1277.028076171875\n",
      "epoch 62, val_loss 454.6752624511719\n",
      "epoch 63, train_loss 1277.0279541015625\n",
      "epoch 63, val_loss 454.6752624511719\n",
      "epoch 64, train_loss 1277.0279541015625\n",
      "epoch 64, val_loss 454.6751708984375\n",
      "epoch 65, train_loss 1277.0279541015625\n",
      "epoch 65, val_loss 454.67510986328125\n",
      "epoch 66, train_loss 1277.02783203125\n",
      "epoch 66, val_loss 454.6750793457031\n",
      "epoch 67, train_loss 1277.02783203125\n",
      "epoch 67, val_loss 454.6750793457031\n",
      "epoch 68, train_loss 1277.02783203125\n",
      "epoch 68, val_loss 454.675048828125\n",
      "epoch 69, train_loss 1277.0279541015625\n",
      "epoch 69, val_loss 454.675048828125\n",
      "epoch 70, train_loss 1277.02783203125\n",
      "epoch 70, val_loss 454.6749572753906\n",
      "epoch 71, train_loss 1277.0277099609375\n",
      "epoch 71, val_loss 454.6749267578125\n",
      "epoch 72, train_loss 1277.02783203125\n",
      "epoch 72, val_loss 454.6749267578125\n",
      "epoch 73, train_loss 1277.0277099609375\n",
      "epoch 73, val_loss 454.6749267578125\n",
      "epoch 74, train_loss 1277.0279541015625\n",
      "epoch 74, val_loss 454.67486572265625\n",
      "epoch 75, train_loss 1277.0277099609375\n",
      "epoch 75, val_loss 454.6748352050781\n",
      "epoch 76, train_loss 1277.0277099609375\n",
      "epoch 76, val_loss 454.6748352050781\n",
      "epoch 77, train_loss 1277.0277099609375\n",
      "epoch 77, val_loss 454.6748352050781\n",
      "epoch 78, train_loss 1277.0277099609375\n",
      "epoch 78, val_loss 454.6748046875\n",
      "epoch 79, train_loss 1277.0277099609375\n",
      "epoch 79, val_loss 454.6748046875\n",
      "epoch 80, train_loss 1277.027587890625\n",
      "epoch 80, val_loss 454.67474365234375\n",
      "epoch 81, train_loss 1277.0277099609375\n",
      "epoch 81, val_loss 454.6746520996094\n",
      "epoch 82, train_loss 1277.0277099609375\n",
      "epoch 82, val_loss 454.6746520996094\n",
      "epoch 83, train_loss 1277.0277099609375\n",
      "epoch 83, val_loss 454.6745910644531\n",
      "epoch 84, train_loss 1277.0277099609375\n",
      "epoch 84, val_loss 454.6745910644531\n",
      "epoch 85, train_loss 1277.027587890625\n",
      "epoch 85, val_loss 454.6745300292969\n",
      "epoch 86, train_loss 1277.0274658203125\n",
      "epoch 86, val_loss 454.6744689941406\n",
      "epoch 87, train_loss 1277.0274658203125\n",
      "epoch 87, val_loss 454.67449951171875\n",
      "epoch 88, train_loss 1277.0274658203125\n",
      "epoch 88, val_loss 454.6744689941406\n",
      "epoch 89, train_loss 1277.0274658203125\n",
      "epoch 89, val_loss 454.6744079589844\n",
      "epoch 90, train_loss 1277.0274658203125\n",
      "epoch 90, val_loss 454.67437744140625\n",
      "epoch 91, train_loss 1277.0274658203125\n",
      "epoch 91, val_loss 454.67437744140625\n",
      "epoch 92, train_loss 1277.02734375\n",
      "epoch 92, val_loss 454.6743469238281\n",
      "epoch 93, train_loss 1277.0274658203125\n",
      "epoch 93, val_loss 454.67437744140625\n",
      "epoch 94, train_loss 1277.02734375\n",
      "epoch 94, val_loss 454.6743469238281\n",
      "epoch 95, train_loss 1277.0272216796875\n",
      "epoch 95, val_loss 454.67425537109375\n",
      "epoch 96, train_loss 1277.0272216796875\n",
      "epoch 96, val_loss 454.6741943359375\n",
      "epoch 97, train_loss 1277.0272216796875\n",
      "epoch 97, val_loss 454.6741943359375\n",
      "epoch 98, train_loss 1277.0272216796875\n",
      "epoch 98, val_loss 454.6741943359375\n",
      "epoch 99, train_loss 1277.0272216796875\n",
      "epoch 99, val_loss 454.6741638183594\n",
      "Parameter containing:\n",
      "tensor([3.8601e-31], requires_grad=True)\n",
      "iter 172, train_loss_regularization 0.6866971850395203\n",
      "iter 172, val_loss_regularization 0.6866971850395203\n",
      "epoch 0, train_loss 1277.0272216796875\n",
      "epoch 0, val_loss 454.674072265625\n",
      "epoch 1, train_loss 1277.0272216796875\n",
      "epoch 1, val_loss 454.674072265625\n",
      "epoch 2, train_loss 1277.027099609375\n",
      "epoch 2, val_loss 454.6740417480469\n",
      "epoch 3, train_loss 1277.027099609375\n",
      "epoch 3, val_loss 454.6740417480469\n",
      "epoch 4, train_loss 1277.027099609375\n",
      "epoch 4, val_loss 454.6739501953125\n",
      "epoch 5, train_loss 1277.0272216796875\n",
      "epoch 5, val_loss 454.6739501953125\n",
      "epoch 6, train_loss 1277.0272216796875\n",
      "epoch 6, val_loss 454.6739196777344\n",
      "epoch 7, train_loss 1277.0269775390625\n",
      "epoch 7, val_loss 454.6739196777344\n",
      "epoch 8, train_loss 1277.02685546875\n",
      "epoch 8, val_loss 454.67388916015625\n",
      "epoch 9, train_loss 1277.02685546875\n",
      "epoch 9, val_loss 454.67388916015625\n",
      "epoch 10, train_loss 1277.0269775390625\n",
      "epoch 10, val_loss 454.673828125\n",
      "epoch 11, train_loss 1277.0269775390625\n",
      "epoch 11, val_loss 454.6737365722656\n",
      "epoch 12, train_loss 1277.0269775390625\n",
      "epoch 12, val_loss 454.6737365722656\n",
      "epoch 13, train_loss 1277.02685546875\n",
      "epoch 13, val_loss 454.6737365722656\n",
      "epoch 14, train_loss 1277.02685546875\n",
      "epoch 14, val_loss 454.6737060546875\n",
      "epoch 15, train_loss 1277.0269775390625\n",
      "epoch 15, val_loss 454.6736755371094\n",
      "epoch 16, train_loss 1277.02685546875\n",
      "epoch 16, val_loss 454.6736145019531\n",
      "epoch 17, train_loss 1277.026611328125\n",
      "epoch 17, val_loss 454.6736145019531\n",
      "epoch 18, train_loss 1277.026611328125\n",
      "epoch 18, val_loss 454.6736145019531\n",
      "epoch 19, train_loss 1277.026611328125\n",
      "epoch 19, val_loss 454.673583984375\n",
      "epoch 20, train_loss 1277.026611328125\n",
      "epoch 20, val_loss 454.673583984375\n",
      "epoch 21, train_loss 1277.026611328125\n",
      "epoch 21, val_loss 454.6734924316406\n",
      "epoch 22, train_loss 1277.026611328125\n",
      "epoch 22, val_loss 454.6734313964844\n",
      "epoch 23, train_loss 1277.026611328125\n",
      "epoch 23, val_loss 454.6734313964844\n",
      "epoch 24, train_loss 1277.026611328125\n",
      "epoch 24, val_loss 454.6734313964844\n",
      "epoch 25, train_loss 1277.026611328125\n",
      "epoch 25, val_loss 454.6733703613281\n",
      "epoch 26, train_loss 1277.026611328125\n",
      "epoch 26, val_loss 454.67327880859375\n",
      "epoch 27, train_loss 1277.0264892578125\n",
      "epoch 27, val_loss 454.6732482910156\n",
      "epoch 28, train_loss 1277.0264892578125\n",
      "epoch 28, val_loss 454.67327880859375\n",
      "epoch 29, train_loss 1277.0264892578125\n",
      "epoch 29, val_loss 454.6732482910156\n",
      "epoch 30, train_loss 1277.0263671875\n",
      "epoch 30, val_loss 454.6732177734375\n",
      "epoch 31, train_loss 1277.0263671875\n",
      "epoch 31, val_loss 454.67315673828125\n",
      "epoch 32, train_loss 1277.0263671875\n",
      "epoch 32, val_loss 454.67315673828125\n",
      "epoch 33, train_loss 1277.0264892578125\n",
      "epoch 33, val_loss 454.67315673828125\n",
      "epoch 34, train_loss 1277.0264892578125\n",
      "epoch 34, val_loss 454.673095703125\n",
      "epoch 35, train_loss 1277.0263671875\n",
      "epoch 35, val_loss 454.673095703125\n",
      "epoch 36, train_loss 1277.0264892578125\n",
      "epoch 36, val_loss 454.6731262207031\n",
      "epoch 37, train_loss 1277.0263671875\n",
      "epoch 37, val_loss 454.67303466796875\n",
      "epoch 38, train_loss 1277.026123046875\n",
      "epoch 38, val_loss 454.6730041503906\n",
      "epoch 39, train_loss 1277.026123046875\n",
      "epoch 39, val_loss 454.6730041503906\n",
      "epoch 40, train_loss 1277.026123046875\n",
      "epoch 40, val_loss 454.6729431152344\n",
      "epoch 41, train_loss 1277.0262451171875\n",
      "epoch 41, val_loss 454.6728210449219\n",
      "epoch 42, train_loss 1277.0262451171875\n",
      "epoch 42, val_loss 454.67291259765625\n",
      "epoch 43, train_loss 1277.0262451171875\n",
      "epoch 43, val_loss 454.6728820800781\n",
      "epoch 44, train_loss 1277.0262451171875\n",
      "epoch 44, val_loss 454.67279052734375\n",
      "epoch 45, train_loss 1277.0262451171875\n",
      "epoch 45, val_loss 454.67279052734375\n",
      "epoch 46, train_loss 1277.0260009765625\n",
      "epoch 46, val_loss 454.6727600097656\n",
      "epoch 47, train_loss 1277.0260009765625\n",
      "epoch 47, val_loss 454.6726989746094\n",
      "epoch 48, train_loss 1277.02587890625\n",
      "epoch 48, val_loss 454.67266845703125\n",
      "epoch 49, train_loss 1277.02587890625\n",
      "epoch 49, val_loss 454.6726379394531\n",
      "epoch 50, train_loss 1277.02587890625\n",
      "epoch 50, val_loss 454.67266845703125\n",
      "epoch 51, train_loss 1277.02587890625\n",
      "epoch 51, val_loss 454.6726379394531\n",
      "epoch 52, train_loss 1277.0260009765625\n",
      "epoch 52, val_loss 454.67254638671875\n",
      "epoch 53, train_loss 1277.02587890625\n",
      "epoch 53, val_loss 454.6724853515625\n",
      "epoch 54, train_loss 1277.02587890625\n",
      "epoch 54, val_loss 454.67254638671875\n",
      "epoch 55, train_loss 1277.02587890625\n",
      "epoch 55, val_loss 454.6724853515625\n",
      "epoch 56, train_loss 1277.02587890625\n",
      "epoch 56, val_loss 454.6724853515625\n",
      "epoch 57, train_loss 1277.02587890625\n",
      "epoch 57, val_loss 454.6724548339844\n",
      "epoch 58, train_loss 1277.025634765625\n",
      "epoch 58, val_loss 454.6724548339844\n",
      "epoch 59, train_loss 1277.0257568359375\n",
      "epoch 59, val_loss 454.67242431640625\n",
      "epoch 60, train_loss 1277.025634765625\n",
      "epoch 60, val_loss 454.6723327636719\n",
      "epoch 61, train_loss 1277.0257568359375\n",
      "epoch 61, val_loss 454.6723327636719\n",
      "epoch 62, train_loss 1277.0257568359375\n",
      "epoch 62, val_loss 454.6722412109375\n",
      "epoch 63, train_loss 1277.02587890625\n",
      "epoch 63, val_loss 454.6722106933594\n",
      "epoch 64, train_loss 1277.0257568359375\n",
      "epoch 64, val_loss 454.67218017578125\n",
      "epoch 65, train_loss 1277.0257568359375\n",
      "epoch 65, val_loss 454.6722106933594\n",
      "epoch 66, train_loss 1277.0257568359375\n",
      "epoch 66, val_loss 454.67218017578125\n",
      "epoch 67, train_loss 1277.0255126953125\n",
      "epoch 67, val_loss 454.6720886230469\n",
      "epoch 68, train_loss 1277.0255126953125\n",
      "epoch 68, val_loss 454.672119140625\n",
      "epoch 69, train_loss 1277.0255126953125\n",
      "epoch 69, val_loss 454.6720886230469\n",
      "epoch 70, train_loss 1277.0255126953125\n",
      "epoch 70, val_loss 454.6720275878906\n",
      "epoch 71, train_loss 1277.0255126953125\n",
      "epoch 71, val_loss 454.6720275878906\n",
      "epoch 72, train_loss 1277.0255126953125\n",
      "epoch 72, val_loss 454.6719970703125\n",
      "epoch 73, train_loss 1277.025634765625\n",
      "epoch 73, val_loss 454.6719665527344\n",
      "epoch 74, train_loss 1277.025634765625\n",
      "epoch 74, val_loss 454.6719055175781\n",
      "epoch 75, train_loss 1277.025390625\n",
      "epoch 75, val_loss 454.6719055175781\n",
      "epoch 76, train_loss 1277.0255126953125\n",
      "epoch 76, val_loss 454.671875\n",
      "epoch 77, train_loss 1277.025390625\n",
      "epoch 77, val_loss 454.6719055175781\n",
      "epoch 78, train_loss 1277.025390625\n",
      "epoch 78, val_loss 454.6718444824219\n",
      "epoch 79, train_loss 1277.0252685546875\n",
      "epoch 79, val_loss 454.6717834472656\n",
      "epoch 80, train_loss 1277.025390625\n",
      "epoch 80, val_loss 454.6717529296875\n",
      "epoch 81, train_loss 1277.025390625\n",
      "epoch 81, val_loss 454.6717529296875\n",
      "epoch 82, train_loss 1277.025390625\n",
      "epoch 82, val_loss 454.6717224121094\n",
      "epoch 83, train_loss 1277.025146484375\n",
      "epoch 83, val_loss 454.671630859375\n",
      "epoch 84, train_loss 1277.025146484375\n",
      "epoch 84, val_loss 454.6716613769531\n",
      "epoch 85, train_loss 1277.025146484375\n",
      "epoch 85, val_loss 454.671630859375\n",
      "epoch 86, train_loss 1277.025146484375\n",
      "epoch 86, val_loss 454.671630859375\n",
      "epoch 87, train_loss 1277.0252685546875\n",
      "epoch 87, val_loss 454.6715393066406\n",
      "epoch 88, train_loss 1277.0250244140625\n",
      "epoch 88, val_loss 454.6715087890625\n",
      "epoch 89, train_loss 1277.0250244140625\n",
      "epoch 89, val_loss 454.6715087890625\n",
      "epoch 90, train_loss 1277.02490234375\n",
      "epoch 90, val_loss 454.6714172363281\n",
      "epoch 91, train_loss 1277.0250244140625\n",
      "epoch 91, val_loss 454.6714172363281\n",
      "epoch 92, train_loss 1277.0250244140625\n",
      "epoch 92, val_loss 454.6714172363281\n",
      "epoch 93, train_loss 1277.0250244140625\n",
      "epoch 93, val_loss 454.67132568359375\n",
      "epoch 94, train_loss 1277.0250244140625\n",
      "epoch 94, val_loss 454.67132568359375\n",
      "epoch 95, train_loss 1277.0250244140625\n",
      "epoch 95, val_loss 454.67138671875\n",
      "epoch 96, train_loss 1277.0250244140625\n",
      "epoch 96, val_loss 454.67132568359375\n",
      "epoch 97, train_loss 1277.0247802734375\n",
      "epoch 97, val_loss 454.6712951660156\n",
      "epoch 98, train_loss 1277.0247802734375\n",
      "epoch 98, val_loss 454.6712646484375\n",
      "epoch 99, train_loss 1277.0247802734375\n",
      "epoch 99, val_loss 454.67120361328125\n",
      "Parameter containing:\n",
      "tensor([2.6994e-31], requires_grad=True)\n",
      "iter 173, train_loss_regularization 0.6864379048347473\n",
      "iter 173, val_loss_regularization 0.6864379048347473\n",
      "epoch 0, train_loss 1277.0247802734375\n",
      "epoch 0, val_loss 454.6711730957031\n",
      "epoch 1, train_loss 1277.0247802734375\n",
      "epoch 1, val_loss 454.6711730957031\n",
      "epoch 2, train_loss 1277.0247802734375\n",
      "epoch 2, val_loss 454.6711120605469\n",
      "epoch 3, train_loss 1277.0247802734375\n",
      "epoch 3, val_loss 454.67108154296875\n",
      "epoch 4, train_loss 1277.024658203125\n",
      "epoch 4, val_loss 454.6709899902344\n",
      "epoch 5, train_loss 1277.0247802734375\n",
      "epoch 5, val_loss 454.6709899902344\n",
      "epoch 6, train_loss 1277.0247802734375\n",
      "epoch 6, val_loss 454.6709899902344\n",
      "epoch 7, train_loss 1277.0247802734375\n",
      "epoch 7, val_loss 454.6709899902344\n",
      "epoch 8, train_loss 1277.024658203125\n",
      "epoch 8, val_loss 454.6709289550781\n",
      "epoch 9, train_loss 1277.0245361328125\n",
      "epoch 9, val_loss 454.6709289550781\n",
      "epoch 10, train_loss 1277.0245361328125\n",
      "epoch 10, val_loss 454.6709289550781\n",
      "epoch 11, train_loss 1277.024658203125\n",
      "epoch 11, val_loss 454.6709289550781\n",
      "epoch 12, train_loss 1277.0247802734375\n",
      "epoch 12, val_loss 454.67083740234375\n",
      "epoch 13, train_loss 1277.024658203125\n",
      "epoch 13, val_loss 454.6708068847656\n",
      "epoch 14, train_loss 1277.0245361328125\n",
      "epoch 14, val_loss 454.6708068847656\n",
      "epoch 15, train_loss 1277.0245361328125\n",
      "epoch 15, val_loss 454.6707458496094\n",
      "epoch 16, train_loss 1277.0245361328125\n",
      "epoch 16, val_loss 454.6707458496094\n",
      "epoch 17, train_loss 1277.0245361328125\n",
      "epoch 17, val_loss 454.67071533203125\n",
      "epoch 18, train_loss 1277.0245361328125\n",
      "epoch 18, val_loss 454.670654296875\n",
      "epoch 19, train_loss 1277.0244140625\n",
      "epoch 19, val_loss 454.67059326171875\n",
      "epoch 20, train_loss 1277.0244140625\n",
      "epoch 20, val_loss 454.6706237792969\n",
      "epoch 21, train_loss 1277.0242919921875\n",
      "epoch 21, val_loss 454.67059326171875\n",
      "epoch 22, train_loss 1277.0242919921875\n",
      "epoch 22, val_loss 454.6705322265625\n",
      "epoch 23, train_loss 1277.0242919921875\n",
      "epoch 23, val_loss 454.6705017089844\n",
      "epoch 24, train_loss 1277.0242919921875\n",
      "epoch 24, val_loss 454.67047119140625\n",
      "epoch 25, train_loss 1277.0242919921875\n",
      "epoch 25, val_loss 454.67047119140625\n",
      "epoch 26, train_loss 1277.0242919921875\n",
      "epoch 26, val_loss 454.67041015625\n",
      "epoch 27, train_loss 1277.0242919921875\n",
      "epoch 27, val_loss 454.67041015625\n",
      "epoch 28, train_loss 1277.0242919921875\n",
      "epoch 28, val_loss 454.6703186035156\n",
      "epoch 29, train_loss 1277.024169921875\n",
      "epoch 29, val_loss 454.6702880859375\n",
      "epoch 30, train_loss 1277.024169921875\n",
      "epoch 30, val_loss 454.6702880859375\n",
      "epoch 31, train_loss 1277.024169921875\n",
      "epoch 31, val_loss 454.6702575683594\n",
      "epoch 32, train_loss 1277.0242919921875\n",
      "epoch 32, val_loss 454.6702575683594\n",
      "epoch 33, train_loss 1277.024169921875\n",
      "epoch 33, val_loss 454.6701965332031\n",
      "epoch 34, train_loss 1277.0240478515625\n",
      "epoch 34, val_loss 454.6701965332031\n",
      "epoch 35, train_loss 1277.0240478515625\n",
      "epoch 35, val_loss 454.670166015625\n",
      "epoch 36, train_loss 1277.0240478515625\n",
      "epoch 36, val_loss 454.670166015625\n",
      "epoch 37, train_loss 1277.0240478515625\n",
      "epoch 37, val_loss 454.670166015625\n",
      "epoch 38, train_loss 1277.0240478515625\n",
      "epoch 38, val_loss 454.6701354980469\n",
      "epoch 39, train_loss 1277.02392578125\n",
      "epoch 39, val_loss 454.6700134277344\n",
      "epoch 40, train_loss 1277.02392578125\n",
      "epoch 40, val_loss 454.6700134277344\n",
      "epoch 41, train_loss 1277.0238037109375\n",
      "epoch 41, val_loss 454.6700134277344\n",
      "epoch 42, train_loss 1277.0238037109375\n",
      "epoch 42, val_loss 454.6699523925781\n",
      "epoch 43, train_loss 1277.0238037109375\n",
      "epoch 43, val_loss 454.66986083984375\n",
      "epoch 44, train_loss 1277.0238037109375\n",
      "epoch 44, val_loss 454.66986083984375\n",
      "epoch 45, train_loss 1277.0238037109375\n",
      "epoch 45, val_loss 454.66986083984375\n",
      "epoch 46, train_loss 1277.0238037109375\n",
      "epoch 46, val_loss 454.6698303222656\n",
      "epoch 47, train_loss 1277.0238037109375\n",
      "epoch 47, val_loss 454.6698303222656\n",
      "epoch 48, train_loss 1277.02392578125\n",
      "epoch 48, val_loss 454.6697998046875\n",
      "epoch 49, train_loss 1277.0238037109375\n",
      "epoch 49, val_loss 454.66973876953125\n",
      "epoch 50, train_loss 1277.023681640625\n",
      "epoch 50, val_loss 454.66973876953125\n",
      "epoch 51, train_loss 1277.023681640625\n",
      "epoch 51, val_loss 454.6697082519531\n",
      "epoch 52, train_loss 1277.0235595703125\n",
      "epoch 52, val_loss 454.66973876953125\n",
      "epoch 53, train_loss 1277.023681640625\n",
      "epoch 53, val_loss 454.66961669921875\n",
      "epoch 54, train_loss 1277.023681640625\n",
      "epoch 54, val_loss 454.66961669921875\n",
      "epoch 55, train_loss 1277.023681640625\n",
      "epoch 55, val_loss 454.6695556640625\n",
      "epoch 56, train_loss 1277.0238037109375\n",
      "epoch 56, val_loss 454.6695861816406\n",
      "epoch 57, train_loss 1277.0238037109375\n",
      "epoch 57, val_loss 454.6695556640625\n",
      "epoch 58, train_loss 1277.0235595703125\n",
      "epoch 58, val_loss 454.66949462890625\n",
      "epoch 59, train_loss 1277.0235595703125\n",
      "epoch 59, val_loss 454.66949462890625\n",
      "epoch 60, train_loss 1277.0235595703125\n",
      "epoch 60, val_loss 454.6694641113281\n",
      "epoch 61, train_loss 1277.0235595703125\n",
      "epoch 61, val_loss 454.66937255859375\n",
      "epoch 62, train_loss 1277.0233154296875\n",
      "epoch 62, val_loss 454.66937255859375\n",
      "epoch 63, train_loss 1277.0233154296875\n",
      "epoch 63, val_loss 454.66937255859375\n",
      "epoch 64, train_loss 1277.0233154296875\n",
      "epoch 64, val_loss 454.6693420410156\n",
      "epoch 65, train_loss 1277.023193359375\n",
      "epoch 65, val_loss 454.6692810058594\n",
      "epoch 66, train_loss 1277.0233154296875\n",
      "epoch 66, val_loss 454.6692810058594\n",
      "epoch 67, train_loss 1277.0234375\n",
      "epoch 67, val_loss 454.66925048828125\n",
      "epoch 68, train_loss 1277.0233154296875\n",
      "epoch 68, val_loss 454.6692199707031\n",
      "epoch 69, train_loss 1277.0233154296875\n",
      "epoch 69, val_loss 454.6692199707031\n",
      "epoch 70, train_loss 1277.0233154296875\n",
      "epoch 70, val_loss 454.6691589355469\n",
      "epoch 71, train_loss 1277.0233154296875\n",
      "epoch 71, val_loss 454.66912841796875\n",
      "epoch 72, train_loss 1277.023193359375\n",
      "epoch 72, val_loss 454.6690979003906\n",
      "epoch 73, train_loss 1277.023193359375\n",
      "epoch 73, val_loss 454.6690368652344\n",
      "epoch 74, train_loss 1277.023193359375\n",
      "epoch 74, val_loss 454.6690368652344\n",
      "epoch 75, train_loss 1277.023193359375\n",
      "epoch 75, val_loss 454.66900634765625\n",
      "epoch 76, train_loss 1277.023193359375\n",
      "epoch 76, val_loss 454.66900634765625\n",
      "epoch 77, train_loss 1277.023193359375\n",
      "epoch 77, val_loss 454.6689453125\n",
      "epoch 78, train_loss 1277.0230712890625\n",
      "epoch 78, val_loss 454.6689453125\n",
      "epoch 79, train_loss 1277.0230712890625\n",
      "epoch 79, val_loss 454.66888427734375\n",
      "epoch 80, train_loss 1277.02294921875\n",
      "epoch 80, val_loss 454.6688232421875\n",
      "epoch 81, train_loss 1277.02294921875\n",
      "epoch 81, val_loss 454.6688232421875\n",
      "epoch 82, train_loss 1277.02294921875\n",
      "epoch 82, val_loss 454.66876220703125\n",
      "epoch 83, train_loss 1277.02294921875\n",
      "epoch 83, val_loss 454.66876220703125\n",
      "epoch 84, train_loss 1277.0228271484375\n",
      "epoch 84, val_loss 454.668701171875\n",
      "epoch 85, train_loss 1277.0228271484375\n",
      "epoch 85, val_loss 454.6686706542969\n",
      "epoch 86, train_loss 1277.02294921875\n",
      "epoch 86, val_loss 454.6686706542969\n",
      "epoch 87, train_loss 1277.02294921875\n",
      "epoch 87, val_loss 454.66864013671875\n",
      "epoch 88, train_loss 1277.0228271484375\n",
      "epoch 88, val_loss 454.66864013671875\n",
      "epoch 89, train_loss 1277.02294921875\n",
      "epoch 89, val_loss 454.66864013671875\n",
      "epoch 90, train_loss 1277.02294921875\n",
      "epoch 90, val_loss 454.66864013671875\n",
      "epoch 91, train_loss 1277.022705078125\n",
      "epoch 91, val_loss 454.6685791015625\n",
      "epoch 92, train_loss 1277.0228271484375\n",
      "epoch 92, val_loss 454.6684875488281\n",
      "epoch 93, train_loss 1277.0228271484375\n",
      "epoch 93, val_loss 454.66845703125\n",
      "epoch 94, train_loss 1277.0228271484375\n",
      "epoch 94, val_loss 454.66845703125\n",
      "epoch 95, train_loss 1277.0228271484375\n",
      "epoch 95, val_loss 454.66845703125\n",
      "epoch 96, train_loss 1277.0228271484375\n",
      "epoch 96, val_loss 454.66845703125\n",
      "epoch 97, train_loss 1277.0225830078125\n",
      "epoch 97, val_loss 454.6683654785156\n",
      "epoch 98, train_loss 1277.0225830078125\n",
      "epoch 98, val_loss 454.6683349609375\n",
      "epoch 99, train_loss 1277.022705078125\n",
      "epoch 99, val_loss 454.6683349609375\n",
      "Parameter containing:\n",
      "tensor([1.8879e-31], requires_grad=True)\n",
      "iter 174, train_loss_regularization 0.6861913800239563\n",
      "iter 174, val_loss_regularization 0.6861913800239563\n",
      "epoch 0, train_loss 1277.0224609375\n",
      "epoch 0, val_loss 454.6682434082031\n",
      "epoch 1, train_loss 1277.0225830078125\n",
      "epoch 1, val_loss 454.6683044433594\n",
      "epoch 2, train_loss 1277.0225830078125\n",
      "epoch 2, val_loss 454.668212890625\n",
      "epoch 3, train_loss 1277.0224609375\n",
      "epoch 3, val_loss 454.66815185546875\n",
      "epoch 4, train_loss 1277.0224609375\n",
      "epoch 4, val_loss 454.66815185546875\n",
      "epoch 5, train_loss 1277.0224609375\n",
      "epoch 5, val_loss 454.66815185546875\n",
      "epoch 6, train_loss 1277.0224609375\n",
      "epoch 6, val_loss 454.6681213378906\n",
      "epoch 7, train_loss 1277.0224609375\n",
      "epoch 7, val_loss 454.66802978515625\n",
      "epoch 8, train_loss 1277.0224609375\n",
      "epoch 8, val_loss 454.66802978515625\n",
      "epoch 9, train_loss 1277.0224609375\n",
      "epoch 9, val_loss 454.6679992675781\n",
      "epoch 10, train_loss 1277.0224609375\n",
      "epoch 10, val_loss 454.6679992675781\n",
      "epoch 11, train_loss 1277.0223388671875\n",
      "epoch 11, val_loss 454.6679992675781\n",
      "epoch 12, train_loss 1277.0223388671875\n",
      "epoch 12, val_loss 454.66796875\n",
      "epoch 13, train_loss 1277.0223388671875\n",
      "epoch 13, val_loss 454.66790771484375\n",
      "epoch 14, train_loss 1277.0223388671875\n",
      "epoch 14, val_loss 454.6678771972656\n",
      "epoch 15, train_loss 1277.0223388671875\n",
      "epoch 15, val_loss 454.6678771972656\n",
      "epoch 16, train_loss 1277.022216796875\n",
      "epoch 16, val_loss 454.6678771972656\n",
      "epoch 17, train_loss 1277.022216796875\n",
      "epoch 17, val_loss 454.6678466796875\n",
      "epoch 18, train_loss 1277.022216796875\n",
      "epoch 18, val_loss 454.6677551269531\n",
      "epoch 19, train_loss 1277.022216796875\n",
      "epoch 19, val_loss 454.66766357421875\n",
      "epoch 20, train_loss 1277.022216796875\n",
      "epoch 20, val_loss 454.6676940917969\n",
      "epoch 21, train_loss 1277.02197265625\n",
      "epoch 21, val_loss 454.66766357421875\n",
      "epoch 22, train_loss 1277.02197265625\n",
      "epoch 22, val_loss 454.6676330566406\n",
      "epoch 23, train_loss 1277.02197265625\n",
      "epoch 23, val_loss 454.6675720214844\n",
      "epoch 24, train_loss 1277.0220947265625\n",
      "epoch 24, val_loss 454.66754150390625\n",
      "epoch 25, train_loss 1277.02197265625\n",
      "epoch 25, val_loss 454.66754150390625\n",
      "epoch 26, train_loss 1277.02197265625\n",
      "epoch 26, val_loss 454.6675109863281\n",
      "epoch 27, train_loss 1277.02197265625\n",
      "epoch 27, val_loss 454.66754150390625\n",
      "epoch 28, train_loss 1277.02197265625\n",
      "epoch 28, val_loss 454.6674499511719\n",
      "epoch 29, train_loss 1277.0218505859375\n",
      "epoch 29, val_loss 454.6674499511719\n",
      "epoch 30, train_loss 1277.0218505859375\n",
      "epoch 30, val_loss 454.66741943359375\n",
      "epoch 31, train_loss 1277.0218505859375\n",
      "epoch 31, val_loss 454.66741943359375\n",
      "epoch 32, train_loss 1277.02197265625\n",
      "epoch 32, val_loss 454.66741943359375\n",
      "epoch 33, train_loss 1277.0218505859375\n",
      "epoch 33, val_loss 454.6673278808594\n",
      "epoch 34, train_loss 1277.021728515625\n",
      "epoch 34, val_loss 454.66729736328125\n",
      "epoch 35, train_loss 1277.0218505859375\n",
      "epoch 35, val_loss 454.66729736328125\n",
      "epoch 36, train_loss 1277.021728515625\n",
      "epoch 36, val_loss 454.667236328125\n",
      "epoch 37, train_loss 1277.021728515625\n",
      "epoch 37, val_loss 454.6672058105469\n",
      "epoch 38, train_loss 1277.0216064453125\n",
      "epoch 38, val_loss 454.66717529296875\n",
      "epoch 39, train_loss 1277.021728515625\n",
      "epoch 39, val_loss 454.6670837402344\n",
      "epoch 40, train_loss 1277.021728515625\n",
      "epoch 40, val_loss 454.6670837402344\n",
      "epoch 41, train_loss 1277.0216064453125\n",
      "epoch 41, val_loss 454.6671142578125\n",
      "epoch 42, train_loss 1277.0216064453125\n",
      "epoch 42, val_loss 454.6670837402344\n",
      "epoch 43, train_loss 1277.0216064453125\n",
      "epoch 43, val_loss 454.66705322265625\n",
      "epoch 44, train_loss 1277.0216064453125\n",
      "epoch 44, val_loss 454.6669921875\n",
      "epoch 45, train_loss 1277.0216064453125\n",
      "epoch 45, val_loss 454.6669616699219\n",
      "epoch 46, train_loss 1277.0216064453125\n",
      "epoch 46, val_loss 454.66693115234375\n",
      "epoch 47, train_loss 1277.021484375\n",
      "epoch 47, val_loss 454.66693115234375\n",
      "epoch 48, train_loss 1277.0216064453125\n",
      "epoch 48, val_loss 454.66693115234375\n",
      "epoch 49, train_loss 1277.021484375\n",
      "epoch 49, val_loss 454.6667785644531\n",
      "epoch 50, train_loss 1277.0213623046875\n",
      "epoch 50, val_loss 454.6668395996094\n",
      "epoch 51, train_loss 1277.021484375\n",
      "epoch 51, val_loss 454.6667785644531\n",
      "epoch 52, train_loss 1277.021484375\n",
      "epoch 52, val_loss 454.6667785644531\n",
      "epoch 53, train_loss 1277.021484375\n",
      "epoch 53, val_loss 454.6667785644531\n",
      "epoch 54, train_loss 1277.021484375\n",
      "epoch 54, val_loss 454.6667175292969\n",
      "epoch 55, train_loss 1277.0213623046875\n",
      "epoch 55, val_loss 454.6667175292969\n",
      "epoch 56, train_loss 1277.0213623046875\n",
      "epoch 56, val_loss 454.6666564941406\n",
      "epoch 57, train_loss 1277.0213623046875\n",
      "epoch 57, val_loss 454.6666564941406\n",
      "epoch 58, train_loss 1277.0213623046875\n",
      "epoch 58, val_loss 454.6666564941406\n",
      "epoch 59, train_loss 1277.021240234375\n",
      "epoch 59, val_loss 454.6666259765625\n",
      "epoch 60, train_loss 1277.0211181640625\n",
      "epoch 60, val_loss 454.6665954589844\n",
      "epoch 61, train_loss 1277.021240234375\n",
      "epoch 61, val_loss 454.66650390625\n",
      "epoch 62, train_loss 1277.0211181640625\n",
      "epoch 62, val_loss 454.66650390625\n",
      "epoch 63, train_loss 1277.0211181640625\n",
      "epoch 63, val_loss 454.6664733886719\n",
      "epoch 64, train_loss 1277.0211181640625\n",
      "epoch 64, val_loss 454.6664123535156\n",
      "epoch 65, train_loss 1277.0211181640625\n",
      "epoch 65, val_loss 454.6663818359375\n",
      "epoch 66, train_loss 1277.0211181640625\n",
      "epoch 66, val_loss 454.6663818359375\n",
      "epoch 67, train_loss 1277.0211181640625\n",
      "epoch 67, val_loss 454.66632080078125\n",
      "epoch 68, train_loss 1277.0211181640625\n",
      "epoch 68, val_loss 454.66632080078125\n",
      "epoch 69, train_loss 1277.0211181640625\n",
      "epoch 69, val_loss 454.66632080078125\n",
      "epoch 70, train_loss 1277.02099609375\n",
      "epoch 70, val_loss 454.666259765625\n",
      "epoch 71, train_loss 1277.0211181640625\n",
      "epoch 71, val_loss 454.666259765625\n",
      "epoch 72, train_loss 1277.02099609375\n",
      "epoch 72, val_loss 454.666259765625\n",
      "epoch 73, train_loss 1277.02099609375\n",
      "epoch 73, val_loss 454.66619873046875\n",
      "epoch 74, train_loss 1277.0208740234375\n",
      "epoch 74, val_loss 454.66619873046875\n",
      "epoch 75, train_loss 1277.0208740234375\n",
      "epoch 75, val_loss 454.6661376953125\n",
      "epoch 76, train_loss 1277.0208740234375\n",
      "epoch 76, val_loss 454.6661376953125\n",
      "epoch 77, train_loss 1277.0208740234375\n",
      "epoch 77, val_loss 454.66607666015625\n",
      "epoch 78, train_loss 1277.0208740234375\n",
      "epoch 78, val_loss 454.6660461425781\n",
      "epoch 79, train_loss 1277.0206298828125\n",
      "epoch 79, val_loss 454.6659851074219\n",
      "epoch 80, train_loss 1277.0206298828125\n",
      "epoch 80, val_loss 454.6659851074219\n",
      "epoch 81, train_loss 1277.0206298828125\n",
      "epoch 81, val_loss 454.66595458984375\n",
      "epoch 82, train_loss 1277.0206298828125\n",
      "epoch 82, val_loss 454.6659240722656\n",
      "epoch 83, train_loss 1277.0206298828125\n",
      "epoch 83, val_loss 454.6658630371094\n",
      "epoch 84, train_loss 1277.0206298828125\n",
      "epoch 84, val_loss 454.6659240722656\n",
      "epoch 85, train_loss 1277.0206298828125\n",
      "epoch 85, val_loss 454.66583251953125\n",
      "epoch 86, train_loss 1277.0208740234375\n",
      "epoch 86, val_loss 454.6658020019531\n",
      "epoch 87, train_loss 1277.0206298828125\n",
      "epoch 87, val_loss 454.6657409667969\n",
      "epoch 88, train_loss 1277.0206298828125\n",
      "epoch 88, val_loss 454.6657409667969\n",
      "epoch 89, train_loss 1277.0205078125\n",
      "epoch 89, val_loss 454.66571044921875\n",
      "epoch 90, train_loss 1277.0206298828125\n",
      "epoch 90, val_loss 454.66571044921875\n",
      "epoch 91, train_loss 1277.0206298828125\n",
      "epoch 91, val_loss 454.6656799316406\n",
      "epoch 92, train_loss 1277.0206298828125\n",
      "epoch 92, val_loss 454.6656799316406\n",
      "epoch 93, train_loss 1277.0206298828125\n",
      "epoch 93, val_loss 454.6656188964844\n",
      "epoch 94, train_loss 1277.0205078125\n",
      "epoch 94, val_loss 454.66558837890625\n",
      "epoch 95, train_loss 1277.0205078125\n",
      "epoch 95, val_loss 454.66558837890625\n",
      "epoch 96, train_loss 1277.0206298828125\n",
      "epoch 96, val_loss 454.66552734375\n",
      "epoch 97, train_loss 1277.0205078125\n",
      "epoch 97, val_loss 454.66552734375\n",
      "epoch 98, train_loss 1277.0203857421875\n",
      "epoch 98, val_loss 454.6654968261719\n",
      "epoch 99, train_loss 1277.0203857421875\n",
      "epoch 99, val_loss 454.6654052734375\n",
      "Parameter containing:\n",
      "tensor([1.3205e-31], requires_grad=True)\n",
      "iter 175, train_loss_regularization 0.6859578490257263\n",
      "iter 175, val_loss_regularization 0.6859578490257263\n",
      "epoch 0, train_loss 1277.020263671875\n",
      "epoch 0, val_loss 454.6654052734375\n",
      "epoch 1, train_loss 1277.020263671875\n",
      "epoch 1, val_loss 454.6653747558594\n",
      "epoch 2, train_loss 1277.0203857421875\n",
      "epoch 2, val_loss 454.665283203125\n",
      "epoch 3, train_loss 1277.0201416015625\n",
      "epoch 3, val_loss 454.66534423828125\n",
      "epoch 4, train_loss 1277.0201416015625\n",
      "epoch 4, val_loss 454.66522216796875\n",
      "epoch 5, train_loss 1277.0201416015625\n",
      "epoch 5, val_loss 454.6652526855469\n",
      "epoch 6, train_loss 1277.0201416015625\n",
      "epoch 6, val_loss 454.6652526855469\n",
      "epoch 7, train_loss 1277.0201416015625\n",
      "epoch 7, val_loss 454.66522216796875\n",
      "epoch 8, train_loss 1277.0201416015625\n",
      "epoch 8, val_loss 454.6651611328125\n",
      "epoch 9, train_loss 1277.0201416015625\n",
      "epoch 9, val_loss 454.6651306152344\n",
      "epoch 10, train_loss 1277.020263671875\n",
      "epoch 10, val_loss 454.6651611328125\n",
      "epoch 11, train_loss 1277.0201416015625\n",
      "epoch 11, val_loss 454.6651306152344\n",
      "epoch 12, train_loss 1277.02001953125\n",
      "epoch 12, val_loss 454.6651306152344\n",
      "epoch 13, train_loss 1277.02001953125\n",
      "epoch 13, val_loss 454.6650085449219\n",
      "epoch 14, train_loss 1277.02001953125\n",
      "epoch 14, val_loss 454.6650390625\n",
      "epoch 15, train_loss 1277.02001953125\n",
      "epoch 15, val_loss 454.6649475097656\n",
      "epoch 16, train_loss 1277.0201416015625\n",
      "epoch 16, val_loss 454.6650085449219\n",
      "epoch 17, train_loss 1277.0201416015625\n",
      "epoch 17, val_loss 454.6649169921875\n",
      "epoch 18, train_loss 1277.0198974609375\n",
      "epoch 18, val_loss 454.6649169921875\n",
      "epoch 19, train_loss 1277.019775390625\n",
      "epoch 19, val_loss 454.6648254394531\n",
      "epoch 20, train_loss 1277.019775390625\n",
      "epoch 20, val_loss 454.6648254394531\n",
      "epoch 21, train_loss 1277.019775390625\n",
      "epoch 21, val_loss 454.6648254394531\n",
      "epoch 22, train_loss 1277.019775390625\n",
      "epoch 22, val_loss 454.664794921875\n",
      "epoch 23, train_loss 1277.019775390625\n",
      "epoch 23, val_loss 454.6647644042969\n",
      "epoch 24, train_loss 1277.019775390625\n",
      "epoch 24, val_loss 454.6647644042969\n",
      "epoch 25, train_loss 1277.019775390625\n",
      "epoch 25, val_loss 454.6647033691406\n",
      "epoch 26, train_loss 1277.0196533203125\n",
      "epoch 26, val_loss 454.6647033691406\n",
      "epoch 27, train_loss 1277.0196533203125\n",
      "epoch 27, val_loss 454.6646728515625\n",
      "epoch 28, train_loss 1277.019775390625\n",
      "epoch 28, val_loss 454.6645812988281\n",
      "epoch 29, train_loss 1277.019775390625\n",
      "epoch 29, val_loss 454.6645812988281\n",
      "epoch 30, train_loss 1277.019775390625\n",
      "epoch 30, val_loss 454.66448974609375\n",
      "epoch 31, train_loss 1277.019775390625\n",
      "epoch 31, val_loss 454.66455078125\n",
      "epoch 32, train_loss 1277.019775390625\n",
      "epoch 32, val_loss 454.66455078125\n",
      "epoch 33, train_loss 1277.019775390625\n",
      "epoch 33, val_loss 454.6644592285156\n",
      "epoch 34, train_loss 1277.01953125\n",
      "epoch 34, val_loss 454.6644592285156\n",
      "epoch 35, train_loss 1277.01953125\n",
      "epoch 35, val_loss 454.6644592285156\n",
      "epoch 36, train_loss 1277.01953125\n",
      "epoch 36, val_loss 454.6644287109375\n",
      "epoch 37, train_loss 1277.0196533203125\n",
      "epoch 37, val_loss 454.6644287109375\n",
      "epoch 38, train_loss 1277.01953125\n",
      "epoch 38, val_loss 454.6644287109375\n",
      "epoch 39, train_loss 1277.01953125\n",
      "epoch 39, val_loss 454.66424560546875\n",
      "epoch 40, train_loss 1277.01953125\n",
      "epoch 40, val_loss 454.66424560546875\n",
      "epoch 41, train_loss 1277.0194091796875\n",
      "epoch 41, val_loss 454.6642150878906\n",
      "epoch 42, train_loss 1277.01953125\n",
      "epoch 42, val_loss 454.6642150878906\n",
      "epoch 43, train_loss 1277.0194091796875\n",
      "epoch 43, val_loss 454.6641540527344\n",
      "epoch 44, train_loss 1277.019287109375\n",
      "epoch 44, val_loss 454.6640930175781\n",
      "epoch 45, train_loss 1277.0194091796875\n",
      "epoch 45, val_loss 454.6640930175781\n",
      "epoch 46, train_loss 1277.019287109375\n",
      "epoch 46, val_loss 454.6640930175781\n",
      "epoch 47, train_loss 1277.019287109375\n",
      "epoch 47, val_loss 454.6640319824219\n",
      "epoch 48, train_loss 1277.019287109375\n",
      "epoch 48, val_loss 454.6640930175781\n",
      "epoch 49, train_loss 1277.019287109375\n",
      "epoch 49, val_loss 454.66400146484375\n",
      "epoch 50, train_loss 1277.019287109375\n",
      "epoch 50, val_loss 454.6639709472656\n",
      "epoch 51, train_loss 1277.019287109375\n",
      "epoch 51, val_loss 454.66400146484375\n",
      "epoch 52, train_loss 1277.019287109375\n",
      "epoch 52, val_loss 454.66400146484375\n",
      "epoch 53, train_loss 1277.019287109375\n",
      "epoch 53, val_loss 454.6639099121094\n",
      "epoch 54, train_loss 1277.019287109375\n",
      "epoch 54, val_loss 454.66387939453125\n",
      "epoch 55, train_loss 1277.019287109375\n",
      "epoch 55, val_loss 454.663818359375\n",
      "epoch 56, train_loss 1277.0191650390625\n",
      "epoch 56, val_loss 454.6637878417969\n",
      "epoch 57, train_loss 1277.01904296875\n",
      "epoch 57, val_loss 454.663818359375\n",
      "epoch 58, train_loss 1277.01904296875\n",
      "epoch 58, val_loss 454.66375732421875\n",
      "epoch 59, train_loss 1277.0189208984375\n",
      "epoch 59, val_loss 454.6636962890625\n",
      "epoch 60, train_loss 1277.0189208984375\n",
      "epoch 60, val_loss 454.6636657714844\n",
      "epoch 61, train_loss 1277.01904296875\n",
      "epoch 61, val_loss 454.66363525390625\n",
      "epoch 62, train_loss 1277.01904296875\n",
      "epoch 62, val_loss 454.66363525390625\n",
      "epoch 63, train_loss 1277.01904296875\n",
      "epoch 63, val_loss 454.66363525390625\n",
      "epoch 64, train_loss 1277.01904296875\n",
      "epoch 64, val_loss 454.66363525390625\n",
      "epoch 65, train_loss 1277.018798828125\n",
      "epoch 65, val_loss 454.6635437011719\n",
      "epoch 66, train_loss 1277.018798828125\n",
      "epoch 66, val_loss 454.6635437011719\n",
      "epoch 67, train_loss 1277.018798828125\n",
      "epoch 67, val_loss 454.6635437011719\n",
      "epoch 68, train_loss 1277.018798828125\n",
      "epoch 68, val_loss 454.66351318359375\n",
      "epoch 69, train_loss 1277.018798828125\n",
      "epoch 69, val_loss 454.6634216308594\n",
      "epoch 70, train_loss 1277.018798828125\n",
      "epoch 70, val_loss 454.6633605957031\n",
      "epoch 71, train_loss 1277.018798828125\n",
      "epoch 71, val_loss 454.6633605957031\n",
      "epoch 72, train_loss 1277.018798828125\n",
      "epoch 72, val_loss 454.6633605957031\n",
      "epoch 73, train_loss 1277.018798828125\n",
      "epoch 73, val_loss 454.663330078125\n",
      "epoch 74, train_loss 1277.018798828125\n",
      "epoch 74, val_loss 454.6632995605469\n",
      "epoch 75, train_loss 1277.018798828125\n",
      "epoch 75, val_loss 454.6632080078125\n",
      "epoch 76, train_loss 1277.018798828125\n",
      "epoch 76, val_loss 454.6632385253906\n",
      "epoch 77, train_loss 1277.0186767578125\n",
      "epoch 77, val_loss 454.6632385253906\n",
      "epoch 78, train_loss 1277.0186767578125\n",
      "epoch 78, val_loss 454.6632385253906\n",
      "epoch 79, train_loss 1277.0185546875\n",
      "epoch 79, val_loss 454.6631774902344\n",
      "epoch 80, train_loss 1277.0185546875\n",
      "epoch 80, val_loss 454.6631774902344\n",
      "epoch 81, train_loss 1277.0185546875\n",
      "epoch 81, val_loss 454.6630859375\n",
      "epoch 82, train_loss 1277.0185546875\n",
      "epoch 82, val_loss 454.6630859375\n",
      "epoch 83, train_loss 1277.0185546875\n",
      "epoch 83, val_loss 454.6630554199219\n",
      "epoch 84, train_loss 1277.0185546875\n",
      "epoch 84, val_loss 454.6629943847656\n",
      "epoch 85, train_loss 1277.0186767578125\n",
      "epoch 85, val_loss 454.6629943847656\n",
      "epoch 86, train_loss 1277.0185546875\n",
      "epoch 86, val_loss 454.66290283203125\n",
      "epoch 87, train_loss 1277.0185546875\n",
      "epoch 87, val_loss 454.66290283203125\n",
      "epoch 88, train_loss 1277.0184326171875\n",
      "epoch 88, val_loss 454.66290283203125\n",
      "epoch 89, train_loss 1277.018310546875\n",
      "epoch 89, val_loss 454.6628723144531\n",
      "epoch 90, train_loss 1277.018310546875\n",
      "epoch 90, val_loss 454.662841796875\n",
      "epoch 91, train_loss 1277.018310546875\n",
      "epoch 91, val_loss 454.66278076171875\n",
      "epoch 92, train_loss 1277.018310546875\n",
      "epoch 92, val_loss 454.6627502441406\n",
      "epoch 93, train_loss 1277.018310546875\n",
      "epoch 93, val_loss 454.6627502441406\n",
      "epoch 94, train_loss 1277.0184326171875\n",
      "epoch 94, val_loss 454.6627502441406\n",
      "epoch 95, train_loss 1277.0184326171875\n",
      "epoch 95, val_loss 454.6627502441406\n",
      "epoch 96, train_loss 1277.018310546875\n",
      "epoch 96, val_loss 454.66265869140625\n",
      "epoch 97, train_loss 1277.0181884765625\n",
      "epoch 97, val_loss 454.66265869140625\n",
      "epoch 98, train_loss 1277.01806640625\n",
      "epoch 98, val_loss 454.66259765625\n",
      "epoch 99, train_loss 1277.0181884765625\n",
      "epoch 99, val_loss 454.6626281738281\n",
      "Parameter containing:\n",
      "tensor([9.2369e-32], requires_grad=True)\n",
      "iter 176, train_loss_regularization 0.6857372522354126\n",
      "iter 176, val_loss_regularization 0.6857372522354126\n",
      "epoch 0, train_loss 1277.01806640625\n",
      "epoch 0, val_loss 454.66259765625\n",
      "epoch 1, train_loss 1277.01806640625\n",
      "epoch 1, val_loss 454.6625061035156\n",
      "epoch 2, train_loss 1277.01806640625\n",
      "epoch 2, val_loss 454.6624450683594\n",
      "epoch 3, train_loss 1277.01806640625\n",
      "epoch 3, val_loss 454.6624450683594\n",
      "epoch 4, train_loss 1277.0179443359375\n",
      "epoch 4, val_loss 454.6624450683594\n",
      "epoch 5, train_loss 1277.0179443359375\n",
      "epoch 5, val_loss 454.66241455078125\n",
      "epoch 6, train_loss 1277.0179443359375\n",
      "epoch 6, val_loss 454.66241455078125\n",
      "epoch 7, train_loss 1277.0179443359375\n",
      "epoch 7, val_loss 454.6623840332031\n",
      "epoch 8, train_loss 1277.0179443359375\n",
      "epoch 8, val_loss 454.6623229980469\n",
      "epoch 9, train_loss 1277.0179443359375\n",
      "epoch 9, val_loss 454.66229248046875\n",
      "epoch 10, train_loss 1277.0179443359375\n",
      "epoch 10, val_loss 454.66229248046875\n",
      "epoch 11, train_loss 1277.017822265625\n",
      "epoch 11, val_loss 454.66229248046875\n",
      "epoch 12, train_loss 1277.017822265625\n",
      "epoch 12, val_loss 454.6622009277344\n",
      "epoch 13, train_loss 1277.017822265625\n",
      "epoch 13, val_loss 454.66217041015625\n",
      "epoch 14, train_loss 1277.017822265625\n",
      "epoch 14, val_loss 454.66217041015625\n",
      "epoch 15, train_loss 1277.0179443359375\n",
      "epoch 15, val_loss 454.6621398925781\n",
      "epoch 16, train_loss 1277.017822265625\n",
      "epoch 16, val_loss 454.6621398925781\n",
      "epoch 17, train_loss 1277.017822265625\n",
      "epoch 17, val_loss 454.6621398925781\n",
      "epoch 18, train_loss 1277.0177001953125\n",
      "epoch 18, val_loss 454.6620788574219\n",
      "epoch 19, train_loss 1277.017822265625\n",
      "epoch 19, val_loss 454.66204833984375\n",
      "epoch 20, train_loss 1277.0177001953125\n",
      "epoch 20, val_loss 454.6619873046875\n",
      "epoch 21, train_loss 1277.0177001953125\n",
      "epoch 21, val_loss 454.66192626953125\n",
      "epoch 22, train_loss 1277.017578125\n",
      "epoch 22, val_loss 454.6619567871094\n",
      "epoch 23, train_loss 1277.0177001953125\n",
      "epoch 23, val_loss 454.661865234375\n",
      "epoch 24, train_loss 1277.0177001953125\n",
      "epoch 24, val_loss 454.661865234375\n",
      "epoch 25, train_loss 1277.0177001953125\n",
      "epoch 25, val_loss 454.6618347167969\n",
      "epoch 26, train_loss 1277.0174560546875\n",
      "epoch 26, val_loss 454.6618347167969\n",
      "epoch 27, train_loss 1277.0177001953125\n",
      "epoch 27, val_loss 454.66180419921875\n",
      "epoch 28, train_loss 1277.017578125\n",
      "epoch 28, val_loss 454.6617431640625\n",
      "epoch 29, train_loss 1277.0174560546875\n",
      "epoch 29, val_loss 454.6617431640625\n",
      "epoch 30, train_loss 1277.0174560546875\n",
      "epoch 30, val_loss 454.6617126464844\n",
      "epoch 31, train_loss 1277.0174560546875\n",
      "epoch 31, val_loss 454.6617126464844\n",
      "epoch 32, train_loss 1277.0174560546875\n",
      "epoch 32, val_loss 454.6617126464844\n",
      "epoch 33, train_loss 1277.0174560546875\n",
      "epoch 33, val_loss 454.6616516113281\n",
      "epoch 34, train_loss 1277.0174560546875\n",
      "epoch 34, val_loss 454.6615905761719\n",
      "epoch 35, train_loss 1277.0174560546875\n",
      "epoch 35, val_loss 454.6615905761719\n",
      "epoch 36, train_loss 1277.0174560546875\n",
      "epoch 36, val_loss 454.6615295410156\n",
      "epoch 37, train_loss 1277.017333984375\n",
      "epoch 37, val_loss 454.6614990234375\n",
      "epoch 38, train_loss 1277.017333984375\n",
      "epoch 38, val_loss 454.6614990234375\n",
      "epoch 39, train_loss 1277.017333984375\n",
      "epoch 39, val_loss 454.6614685058594\n",
      "epoch 40, train_loss 1277.017333984375\n",
      "epoch 40, val_loss 454.6614074707031\n",
      "epoch 41, train_loss 1277.0172119140625\n",
      "epoch 41, val_loss 454.661376953125\n",
      "epoch 42, train_loss 1277.01708984375\n",
      "epoch 42, val_loss 454.661376953125\n",
      "epoch 43, train_loss 1277.01708984375\n",
      "epoch 43, val_loss 454.661376953125\n",
      "epoch 44, train_loss 1277.0172119140625\n",
      "epoch 44, val_loss 454.6613464355469\n",
      "epoch 45, train_loss 1277.0172119140625\n",
      "epoch 45, val_loss 454.6612548828125\n",
      "epoch 46, train_loss 1277.0172119140625\n",
      "epoch 46, val_loss 454.6612854003906\n",
      "epoch 47, train_loss 1277.0172119140625\n",
      "epoch 47, val_loss 454.66119384765625\n",
      "epoch 48, train_loss 1277.0172119140625\n",
      "epoch 48, val_loss 454.66119384765625\n",
      "epoch 49, train_loss 1277.01708984375\n",
      "epoch 49, val_loss 454.6611633300781\n",
      "epoch 50, train_loss 1277.01708984375\n",
      "epoch 50, val_loss 454.6611328125\n",
      "epoch 51, train_loss 1277.0169677734375\n",
      "epoch 51, val_loss 454.66107177734375\n",
      "epoch 52, train_loss 1277.0169677734375\n",
      "epoch 52, val_loss 454.66107177734375\n",
      "epoch 53, train_loss 1277.0169677734375\n",
      "epoch 53, val_loss 454.66107177734375\n",
      "epoch 54, train_loss 1277.0169677734375\n",
      "epoch 54, val_loss 454.66107177734375\n",
      "epoch 55, train_loss 1277.0169677734375\n",
      "epoch 55, val_loss 454.6610107421875\n",
      "epoch 56, train_loss 1277.0169677734375\n",
      "epoch 56, val_loss 454.6610107421875\n",
      "epoch 57, train_loss 1277.0169677734375\n",
      "epoch 57, val_loss 454.66094970703125\n",
      "epoch 58, train_loss 1277.016845703125\n",
      "epoch 58, val_loss 454.66094970703125\n",
      "epoch 59, train_loss 1277.0167236328125\n",
      "epoch 59, val_loss 454.6609191894531\n",
      "epoch 60, train_loss 1277.016845703125\n",
      "epoch 60, val_loss 454.6609191894531\n",
      "epoch 61, train_loss 1277.016845703125\n",
      "epoch 61, val_loss 454.66082763671875\n",
      "epoch 62, train_loss 1277.0167236328125\n",
      "epoch 62, val_loss 454.66082763671875\n",
      "epoch 63, train_loss 1277.0167236328125\n",
      "epoch 63, val_loss 454.6607360839844\n",
      "epoch 64, train_loss 1277.0166015625\n",
      "epoch 64, val_loss 454.6607360839844\n",
      "epoch 65, train_loss 1277.0166015625\n",
      "epoch 65, val_loss 454.66070556640625\n",
      "epoch 66, train_loss 1277.0167236328125\n",
      "epoch 66, val_loss 454.6606750488281\n",
      "epoch 67, train_loss 1277.0167236328125\n",
      "epoch 67, val_loss 454.6606140136719\n",
      "epoch 68, train_loss 1277.0167236328125\n",
      "epoch 68, val_loss 454.6606140136719\n",
      "epoch 69, train_loss 1277.0167236328125\n",
      "epoch 69, val_loss 454.6606140136719\n",
      "epoch 70, train_loss 1277.016845703125\n",
      "epoch 70, val_loss 454.6606140136719\n",
      "epoch 71, train_loss 1277.0167236328125\n",
      "epoch 71, val_loss 454.6605529785156\n",
      "epoch 72, train_loss 1277.0167236328125\n",
      "epoch 72, val_loss 454.66058349609375\n",
      "epoch 73, train_loss 1277.0167236328125\n",
      "epoch 73, val_loss 454.6605529785156\n",
      "epoch 74, train_loss 1277.0164794921875\n",
      "epoch 74, val_loss 454.6604919433594\n",
      "epoch 75, train_loss 1277.0166015625\n",
      "epoch 75, val_loss 454.6604919433594\n",
      "epoch 76, train_loss 1277.0166015625\n",
      "epoch 76, val_loss 454.6604309082031\n",
      "epoch 77, train_loss 1277.0166015625\n",
      "epoch 77, val_loss 454.6603698730469\n",
      "epoch 78, train_loss 1277.016357421875\n",
      "epoch 78, val_loss 454.6603698730469\n",
      "epoch 79, train_loss 1277.0166015625\n",
      "epoch 79, val_loss 454.66033935546875\n",
      "epoch 80, train_loss 1277.0164794921875\n",
      "epoch 80, val_loss 454.66033935546875\n",
      "epoch 81, train_loss 1277.0164794921875\n",
      "epoch 81, val_loss 454.6602478027344\n",
      "epoch 82, train_loss 1277.0162353515625\n",
      "epoch 82, val_loss 454.6602478027344\n",
      "epoch 83, train_loss 1277.0162353515625\n",
      "epoch 83, val_loss 454.66021728515625\n",
      "epoch 84, train_loss 1277.016357421875\n",
      "epoch 84, val_loss 454.66021728515625\n",
      "epoch 85, train_loss 1277.0162353515625\n",
      "epoch 85, val_loss 454.66015625\n",
      "epoch 86, train_loss 1277.01611328125\n",
      "epoch 86, val_loss 454.66015625\n",
      "epoch 87, train_loss 1277.016357421875\n",
      "epoch 87, val_loss 454.66009521484375\n",
      "epoch 88, train_loss 1277.0162353515625\n",
      "epoch 88, val_loss 454.6601257324219\n",
      "epoch 89, train_loss 1277.01611328125\n",
      "epoch 89, val_loss 454.6600341796875\n",
      "epoch 90, train_loss 1277.01611328125\n",
      "epoch 90, val_loss 454.6600341796875\n",
      "epoch 91, train_loss 1277.0162353515625\n",
      "epoch 91, val_loss 454.6600341796875\n",
      "epoch 92, train_loss 1277.01611328125\n",
      "epoch 92, val_loss 454.6600036621094\n",
      "epoch 93, train_loss 1277.01611328125\n",
      "epoch 93, val_loss 454.659912109375\n",
      "epoch 94, train_loss 1277.01611328125\n",
      "epoch 94, val_loss 454.659912109375\n",
      "epoch 95, train_loss 1277.01611328125\n",
      "epoch 95, val_loss 454.659912109375\n",
      "epoch 96, train_loss 1277.01611328125\n",
      "epoch 96, val_loss 454.6598815917969\n",
      "epoch 97, train_loss 1277.0159912109375\n",
      "epoch 97, val_loss 454.6598205566406\n",
      "epoch 98, train_loss 1277.015869140625\n",
      "epoch 98, val_loss 454.6597900390625\n",
      "epoch 99, train_loss 1277.0159912109375\n",
      "epoch 99, val_loss 454.6598205566406\n",
      "Parameter containing:\n",
      "tensor([6.4619e-32], requires_grad=True)\n",
      "iter 177, train_loss_regularization 0.6855294108390808\n",
      "iter 177, val_loss_regularization 0.6855294108390808\n",
      "epoch 0, train_loss 1277.0159912109375\n",
      "epoch 0, val_loss 454.6597595214844\n",
      "epoch 1, train_loss 1277.0159912109375\n",
      "epoch 1, val_loss 454.6597595214844\n",
      "epoch 2, train_loss 1277.0159912109375\n",
      "epoch 2, val_loss 454.6597595214844\n",
      "epoch 3, train_loss 1277.0159912109375\n",
      "epoch 3, val_loss 454.65966796875\n",
      "epoch 4, train_loss 1277.015869140625\n",
      "epoch 4, val_loss 454.6596374511719\n",
      "epoch 5, train_loss 1277.015869140625\n",
      "epoch 5, val_loss 454.6595764160156\n",
      "epoch 6, train_loss 1277.015869140625\n",
      "epoch 6, val_loss 454.6595764160156\n",
      "epoch 7, train_loss 1277.0157470703125\n",
      "epoch 7, val_loss 454.6595458984375\n",
      "epoch 8, train_loss 1277.0157470703125\n",
      "epoch 8, val_loss 454.65948486328125\n",
      "epoch 9, train_loss 1277.0157470703125\n",
      "epoch 9, val_loss 454.6594543457031\n",
      "epoch 10, train_loss 1277.0157470703125\n",
      "epoch 10, val_loss 454.659423828125\n",
      "epoch 11, train_loss 1277.015869140625\n",
      "epoch 11, val_loss 454.659423828125\n",
      "epoch 12, train_loss 1277.015625\n",
      "epoch 12, val_loss 454.6594543457031\n",
      "epoch 13, train_loss 1277.015869140625\n",
      "epoch 13, val_loss 454.659423828125\n",
      "epoch 14, train_loss 1277.015869140625\n",
      "epoch 14, val_loss 454.65936279296875\n",
      "epoch 15, train_loss 1277.0157470703125\n",
      "epoch 15, val_loss 454.65936279296875\n",
      "epoch 16, train_loss 1277.0157470703125\n",
      "epoch 16, val_loss 454.6593322753906\n",
      "epoch 17, train_loss 1277.015625\n",
      "epoch 17, val_loss 454.6593017578125\n",
      "epoch 18, train_loss 1277.015625\n",
      "epoch 18, val_loss 454.6593017578125\n",
      "epoch 19, train_loss 1277.015625\n",
      "epoch 19, val_loss 454.6592102050781\n",
      "epoch 20, train_loss 1277.015625\n",
      "epoch 20, val_loss 454.6591796875\n",
      "epoch 21, train_loss 1277.0155029296875\n",
      "epoch 21, val_loss 454.6591796875\n",
      "epoch 22, train_loss 1277.015625\n",
      "epoch 22, val_loss 454.65911865234375\n",
      "epoch 23, train_loss 1277.015625\n",
      "epoch 23, val_loss 454.6590881347656\n",
      "epoch 24, train_loss 1277.0155029296875\n",
      "epoch 24, val_loss 454.6590881347656\n",
      "epoch 25, train_loss 1277.0155029296875\n",
      "epoch 25, val_loss 454.6590270996094\n",
      "epoch 26, train_loss 1277.0155029296875\n",
      "epoch 26, val_loss 454.65899658203125\n",
      "epoch 27, train_loss 1277.0155029296875\n",
      "epoch 27, val_loss 454.65899658203125\n",
      "epoch 28, train_loss 1277.0152587890625\n",
      "epoch 28, val_loss 454.6590270996094\n",
      "epoch 29, train_loss 1277.0152587890625\n",
      "epoch 29, val_loss 454.6589660644531\n",
      "epoch 30, train_loss 1277.015380859375\n",
      "epoch 30, val_loss 454.6589050292969\n",
      "epoch 31, train_loss 1277.015380859375\n",
      "epoch 31, val_loss 454.65887451171875\n",
      "epoch 32, train_loss 1277.015380859375\n",
      "epoch 32, val_loss 454.65887451171875\n",
      "epoch 33, train_loss 1277.015380859375\n",
      "epoch 33, val_loss 454.6588439941406\n",
      "epoch 34, train_loss 1277.015380859375\n",
      "epoch 34, val_loss 454.6588439941406\n",
      "epoch 35, train_loss 1277.015380859375\n",
      "epoch 35, val_loss 454.6587829589844\n",
      "epoch 36, train_loss 1277.0152587890625\n",
      "epoch 36, val_loss 454.6587219238281\n",
      "epoch 37, train_loss 1277.01513671875\n",
      "epoch 37, val_loss 454.65875244140625\n",
      "epoch 38, train_loss 1277.01513671875\n",
      "epoch 38, val_loss 454.6586608886719\n",
      "epoch 39, train_loss 1277.0150146484375\n",
      "epoch 39, val_loss 454.6587219238281\n",
      "epoch 40, train_loss 1277.01513671875\n",
      "epoch 40, val_loss 454.6587219238281\n",
      "epoch 41, train_loss 1277.01513671875\n",
      "epoch 41, val_loss 454.6585693359375\n",
      "epoch 42, train_loss 1277.01513671875\n",
      "epoch 42, val_loss 454.6585693359375\n",
      "epoch 43, train_loss 1277.0150146484375\n",
      "epoch 43, val_loss 454.65850830078125\n",
      "epoch 44, train_loss 1277.01513671875\n",
      "epoch 44, val_loss 454.65850830078125\n",
      "epoch 45, train_loss 1277.01513671875\n",
      "epoch 45, val_loss 454.65850830078125\n",
      "epoch 46, train_loss 1277.0150146484375\n",
      "epoch 46, val_loss 454.658447265625\n",
      "epoch 47, train_loss 1277.014892578125\n",
      "epoch 47, val_loss 454.6584167480469\n",
      "epoch 48, train_loss 1277.014892578125\n",
      "epoch 48, val_loss 454.65838623046875\n",
      "epoch 49, train_loss 1277.014892578125\n",
      "epoch 49, val_loss 454.65838623046875\n",
      "epoch 50, train_loss 1277.014892578125\n",
      "epoch 50, val_loss 454.65838623046875\n",
      "epoch 51, train_loss 1277.014892578125\n",
      "epoch 51, val_loss 454.6582946777344\n",
      "epoch 52, train_loss 1277.014892578125\n",
      "epoch 52, val_loss 454.6582946777344\n",
      "epoch 53, train_loss 1277.014892578125\n",
      "epoch 53, val_loss 454.6582946777344\n",
      "epoch 54, train_loss 1277.014892578125\n",
      "epoch 54, val_loss 454.65826416015625\n",
      "epoch 55, train_loss 1277.0147705078125\n",
      "epoch 55, val_loss 454.65826416015625\n",
      "epoch 56, train_loss 1277.014892578125\n",
      "epoch 56, val_loss 454.65826416015625\n",
      "epoch 57, train_loss 1277.0145263671875\n",
      "epoch 57, val_loss 454.6581726074219\n",
      "epoch 58, train_loss 1277.0147705078125\n",
      "epoch 58, val_loss 454.6581115722656\n",
      "epoch 59, train_loss 1277.0147705078125\n",
      "epoch 59, val_loss 454.6581115722656\n",
      "epoch 60, train_loss 1277.0146484375\n",
      "epoch 60, val_loss 454.6580810546875\n",
      "epoch 61, train_loss 1277.0145263671875\n",
      "epoch 61, val_loss 454.6580810546875\n",
      "epoch 62, train_loss 1277.0146484375\n",
      "epoch 62, val_loss 454.6579895019531\n",
      "epoch 63, train_loss 1277.0147705078125\n",
      "epoch 63, val_loss 454.6579895019531\n",
      "epoch 64, train_loss 1277.0147705078125\n",
      "epoch 64, val_loss 454.6579284667969\n",
      "epoch 65, train_loss 1277.0147705078125\n",
      "epoch 65, val_loss 454.6579284667969\n",
      "epoch 66, train_loss 1277.0146484375\n",
      "epoch 66, val_loss 454.6579284667969\n",
      "epoch 67, train_loss 1277.014892578125\n",
      "epoch 67, val_loss 454.6578674316406\n",
      "epoch 68, train_loss 1277.0145263671875\n",
      "epoch 68, val_loss 454.6578674316406\n",
      "epoch 69, train_loss 1277.014404296875\n",
      "epoch 69, val_loss 454.6578369140625\n",
      "epoch 70, train_loss 1277.014404296875\n",
      "epoch 70, val_loss 454.6578063964844\n",
      "epoch 71, train_loss 1277.0145263671875\n",
      "epoch 71, val_loss 454.6578063964844\n",
      "epoch 72, train_loss 1277.0145263671875\n",
      "epoch 72, val_loss 454.6578063964844\n",
      "epoch 73, train_loss 1277.014404296875\n",
      "epoch 73, val_loss 454.65771484375\n",
      "epoch 74, train_loss 1277.0145263671875\n",
      "epoch 74, val_loss 454.6576232910156\n",
      "epoch 75, train_loss 1277.014404296875\n",
      "epoch 75, val_loss 454.65765380859375\n",
      "epoch 76, train_loss 1277.014404296875\n",
      "epoch 76, val_loss 454.65765380859375\n",
      "epoch 77, train_loss 1277.0142822265625\n",
      "epoch 77, val_loss 454.6576232910156\n",
      "epoch 78, train_loss 1277.0142822265625\n",
      "epoch 78, val_loss 454.6575927734375\n",
      "epoch 79, train_loss 1277.0142822265625\n",
      "epoch 79, val_loss 454.65753173828125\n",
      "epoch 80, train_loss 1277.0142822265625\n",
      "epoch 80, val_loss 454.65753173828125\n",
      "epoch 81, train_loss 1277.0142822265625\n",
      "epoch 81, val_loss 454.6575012207031\n",
      "epoch 82, train_loss 1277.0142822265625\n",
      "epoch 82, val_loss 454.6575012207031\n",
      "epoch 83, train_loss 1277.0140380859375\n",
      "epoch 83, val_loss 454.657470703125\n",
      "epoch 84, train_loss 1277.0140380859375\n",
      "epoch 84, val_loss 454.6573181152344\n",
      "epoch 85, train_loss 1277.01416015625\n",
      "epoch 85, val_loss 454.6573181152344\n",
      "epoch 86, train_loss 1277.0142822265625\n",
      "epoch 86, val_loss 454.65728759765625\n",
      "epoch 87, train_loss 1277.01416015625\n",
      "epoch 87, val_loss 454.6573181152344\n",
      "epoch 88, train_loss 1277.0140380859375\n",
      "epoch 88, val_loss 454.65728759765625\n",
      "epoch 89, train_loss 1277.0140380859375\n",
      "epoch 89, val_loss 454.6572570800781\n",
      "epoch 90, train_loss 1277.0140380859375\n",
      "epoch 90, val_loss 454.6571960449219\n",
      "epoch 91, train_loss 1277.0140380859375\n",
      "epoch 91, val_loss 454.6571960449219\n",
      "epoch 92, train_loss 1277.0140380859375\n",
      "epoch 92, val_loss 454.6571960449219\n",
      "epoch 93, train_loss 1277.0140380859375\n",
      "epoch 93, val_loss 454.6571960449219\n",
      "epoch 94, train_loss 1277.013916015625\n",
      "epoch 94, val_loss 454.65716552734375\n",
      "epoch 95, train_loss 1277.013916015625\n",
      "epoch 95, val_loss 454.6570739746094\n",
      "epoch 96, train_loss 1277.013916015625\n",
      "epoch 96, val_loss 454.6570739746094\n",
      "epoch 97, train_loss 1277.013916015625\n",
      "epoch 97, val_loss 454.6571350097656\n",
      "epoch 98, train_loss 1277.0137939453125\n",
      "epoch 98, val_loss 454.6570129394531\n",
      "epoch 99, train_loss 1277.0137939453125\n",
      "epoch 99, val_loss 454.6569519042969\n",
      "Parameter containing:\n",
      "tensor([4.5209e-32], requires_grad=True)\n",
      "iter 178, train_loss_regularization 0.6853340864181519\n",
      "iter 178, val_loss_regularization 0.6853340864181519\n",
      "epoch 0, train_loss 1277.0137939453125\n",
      "epoch 0, val_loss 454.6569519042969\n",
      "epoch 1, train_loss 1277.0137939453125\n",
      "epoch 1, val_loss 454.65692138671875\n",
      "epoch 2, train_loss 1277.0137939453125\n",
      "epoch 2, val_loss 454.6569519042969\n",
      "epoch 3, train_loss 1277.0137939453125\n",
      "epoch 3, val_loss 454.65692138671875\n",
      "epoch 4, train_loss 1277.0137939453125\n",
      "epoch 4, val_loss 454.65679931640625\n",
      "epoch 5, train_loss 1277.0137939453125\n",
      "epoch 5, val_loss 454.65679931640625\n",
      "epoch 6, train_loss 1277.0137939453125\n",
      "epoch 6, val_loss 454.65673828125\n",
      "epoch 7, train_loss 1277.0137939453125\n",
      "epoch 7, val_loss 454.65673828125\n",
      "epoch 8, train_loss 1277.0137939453125\n",
      "epoch 8, val_loss 454.65673828125\n",
      "epoch 9, train_loss 1277.013671875\n",
      "epoch 9, val_loss 454.65673828125\n",
      "epoch 10, train_loss 1277.0137939453125\n",
      "epoch 10, val_loss 454.65667724609375\n",
      "epoch 11, train_loss 1277.0135498046875\n",
      "epoch 11, val_loss 454.6566162109375\n",
      "epoch 12, train_loss 1277.0135498046875\n",
      "epoch 12, val_loss 454.6566162109375\n",
      "epoch 13, train_loss 1277.0135498046875\n",
      "epoch 13, val_loss 454.6566162109375\n",
      "epoch 14, train_loss 1277.0135498046875\n",
      "epoch 14, val_loss 454.6565856933594\n",
      "epoch 15, train_loss 1277.0135498046875\n",
      "epoch 15, val_loss 454.6565856933594\n",
      "epoch 16, train_loss 1277.0135498046875\n",
      "epoch 16, val_loss 454.656494140625\n",
      "epoch 17, train_loss 1277.013427734375\n",
      "epoch 17, val_loss 454.6564636230469\n",
      "epoch 18, train_loss 1277.013427734375\n",
      "epoch 18, val_loss 454.6564636230469\n",
      "epoch 19, train_loss 1277.013427734375\n",
      "epoch 19, val_loss 454.6564636230469\n",
      "epoch 20, train_loss 1277.0135498046875\n",
      "epoch 20, val_loss 454.6564025878906\n",
      "epoch 21, train_loss 1277.0135498046875\n",
      "epoch 21, val_loss 454.6563720703125\n",
      "epoch 22, train_loss 1277.0133056640625\n",
      "epoch 22, val_loss 454.6563720703125\n",
      "epoch 23, train_loss 1277.0133056640625\n",
      "epoch 23, val_loss 454.6564025878906\n",
      "epoch 24, train_loss 1277.0133056640625\n",
      "epoch 24, val_loss 454.6563415527344\n",
      "epoch 25, train_loss 1277.013427734375\n",
      "epoch 25, val_loss 454.6563415527344\n",
      "epoch 26, train_loss 1277.0133056640625\n",
      "epoch 26, val_loss 454.6562194824219\n",
      "epoch 27, train_loss 1277.0133056640625\n",
      "epoch 27, val_loss 454.6561584472656\n",
      "epoch 28, train_loss 1277.0133056640625\n",
      "epoch 28, val_loss 454.6561584472656\n",
      "epoch 29, train_loss 1277.0133056640625\n",
      "epoch 29, val_loss 454.6561584472656\n",
      "epoch 30, train_loss 1277.01318359375\n",
      "epoch 30, val_loss 454.6561279296875\n",
      "epoch 31, train_loss 1277.0130615234375\n",
      "epoch 31, val_loss 454.6561279296875\n",
      "epoch 32, train_loss 1277.012939453125\n",
      "epoch 32, val_loss 454.6560363769531\n",
      "epoch 33, train_loss 1277.0130615234375\n",
      "epoch 33, val_loss 454.656005859375\n",
      "epoch 34, train_loss 1277.0130615234375\n",
      "epoch 34, val_loss 454.6560363769531\n",
      "epoch 35, train_loss 1277.01318359375\n",
      "epoch 35, val_loss 454.6560363769531\n",
      "epoch 36, train_loss 1277.0130615234375\n",
      "epoch 36, val_loss 454.656005859375\n",
      "epoch 37, train_loss 1277.012939453125\n",
      "epoch 37, val_loss 454.65594482421875\n",
      "epoch 38, train_loss 1277.012939453125\n",
      "epoch 38, val_loss 454.6559143066406\n",
      "epoch 39, train_loss 1277.012939453125\n",
      "epoch 39, val_loss 454.6559143066406\n",
      "epoch 40, train_loss 1277.0128173828125\n",
      "epoch 40, val_loss 454.6559143066406\n",
      "epoch 41, train_loss 1277.012939453125\n",
      "epoch 41, val_loss 454.6558837890625\n",
      "epoch 42, train_loss 1277.0128173828125\n",
      "epoch 42, val_loss 454.6557922363281\n",
      "epoch 43, train_loss 1277.0128173828125\n",
      "epoch 43, val_loss 454.65576171875\n",
      "epoch 44, train_loss 1277.012939453125\n",
      "epoch 44, val_loss 454.65576171875\n",
      "epoch 45, train_loss 1277.012939453125\n",
      "epoch 45, val_loss 454.6556701660156\n",
      "epoch 46, train_loss 1277.0128173828125\n",
      "epoch 46, val_loss 454.6556701660156\n",
      "epoch 47, train_loss 1277.012939453125\n",
      "epoch 47, val_loss 454.6556701660156\n",
      "epoch 48, train_loss 1277.012939453125\n",
      "epoch 48, val_loss 454.6556396484375\n",
      "epoch 49, train_loss 1277.0126953125\n",
      "epoch 49, val_loss 454.65557861328125\n",
      "epoch 50, train_loss 1277.0126953125\n",
      "epoch 50, val_loss 454.65557861328125\n",
      "epoch 51, train_loss 1277.0126953125\n",
      "epoch 51, val_loss 454.65557861328125\n",
      "epoch 52, train_loss 1277.0128173828125\n",
      "epoch 52, val_loss 454.6555480957031\n",
      "epoch 53, train_loss 1277.0126953125\n",
      "epoch 53, val_loss 454.6555480957031\n",
      "epoch 54, train_loss 1277.0126953125\n",
      "epoch 54, val_loss 454.65545654296875\n",
      "epoch 55, train_loss 1277.0125732421875\n",
      "epoch 55, val_loss 454.65545654296875\n",
      "epoch 56, train_loss 1277.0126953125\n",
      "epoch 56, val_loss 454.6554260253906\n",
      "epoch 57, train_loss 1277.0126953125\n",
      "epoch 57, val_loss 454.6553649902344\n",
      "epoch 58, train_loss 1277.0125732421875\n",
      "epoch 58, val_loss 454.6553649902344\n",
      "epoch 59, train_loss 1277.012451171875\n",
      "epoch 59, val_loss 454.65533447265625\n",
      "epoch 60, train_loss 1277.012451171875\n",
      "epoch 60, val_loss 454.65533447265625\n",
      "epoch 61, train_loss 1277.0125732421875\n",
      "epoch 61, val_loss 454.6553039550781\n",
      "epoch 62, train_loss 1277.012451171875\n",
      "epoch 62, val_loss 454.6553039550781\n",
      "epoch 63, train_loss 1277.012451171875\n",
      "epoch 63, val_loss 454.6552429199219\n",
      "epoch 64, train_loss 1277.012451171875\n",
      "epoch 64, val_loss 454.65521240234375\n",
      "epoch 65, train_loss 1277.012451171875\n",
      "epoch 65, val_loss 454.6551818847656\n",
      "epoch 66, train_loss 1277.012451171875\n",
      "epoch 66, val_loss 454.6551818847656\n",
      "epoch 67, train_loss 1277.012451171875\n",
      "epoch 67, val_loss 454.65509033203125\n",
      "epoch 68, train_loss 1277.0125732421875\n",
      "epoch 68, val_loss 454.65509033203125\n",
      "epoch 69, train_loss 1277.012451171875\n",
      "epoch 69, val_loss 454.655029296875\n",
      "epoch 70, train_loss 1277.012451171875\n",
      "epoch 70, val_loss 454.6549987792969\n",
      "epoch 71, train_loss 1277.0123291015625\n",
      "epoch 71, val_loss 454.6549987792969\n",
      "epoch 72, train_loss 1277.0123291015625\n",
      "epoch 72, val_loss 454.65496826171875\n",
      "epoch 73, train_loss 1277.0123291015625\n",
      "epoch 73, val_loss 454.65496826171875\n",
      "epoch 74, train_loss 1277.01220703125\n",
      "epoch 74, val_loss 454.6549072265625\n",
      "epoch 75, train_loss 1277.01220703125\n",
      "epoch 75, val_loss 454.6549072265625\n",
      "epoch 76, train_loss 1277.01220703125\n",
      "epoch 76, val_loss 454.6548767089844\n",
      "epoch 77, train_loss 1277.01220703125\n",
      "epoch 77, val_loss 454.6548767089844\n",
      "epoch 78, train_loss 1277.01220703125\n",
      "epoch 78, val_loss 454.65478515625\n",
      "epoch 79, train_loss 1277.011962890625\n",
      "epoch 79, val_loss 454.65484619140625\n",
      "epoch 80, train_loss 1277.01220703125\n",
      "epoch 80, val_loss 454.65478515625\n",
      "epoch 81, train_loss 1277.0120849609375\n",
      "epoch 81, val_loss 454.6546630859375\n",
      "epoch 82, train_loss 1277.0120849609375\n",
      "epoch 82, val_loss 454.6546630859375\n",
      "epoch 83, train_loss 1277.0120849609375\n",
      "epoch 83, val_loss 454.6546630859375\n",
      "epoch 84, train_loss 1277.01220703125\n",
      "epoch 84, val_loss 454.6546630859375\n",
      "epoch 85, train_loss 1277.01220703125\n",
      "epoch 85, val_loss 454.6545715332031\n",
      "epoch 86, train_loss 1277.011962890625\n",
      "epoch 86, val_loss 454.6545715332031\n",
      "epoch 87, train_loss 1277.011962890625\n",
      "epoch 87, val_loss 454.654541015625\n",
      "epoch 88, train_loss 1277.011962890625\n",
      "epoch 88, val_loss 454.6545104980469\n",
      "epoch 89, train_loss 1277.011962890625\n",
      "epoch 89, val_loss 454.6545104980469\n",
      "epoch 90, train_loss 1277.011962890625\n",
      "epoch 90, val_loss 454.6545104980469\n",
      "epoch 91, train_loss 1277.011962890625\n",
      "epoch 91, val_loss 454.6544494628906\n",
      "epoch 92, train_loss 1277.0118408203125\n",
      "epoch 92, val_loss 454.6544189453125\n",
      "epoch 93, train_loss 1277.011962890625\n",
      "epoch 93, val_loss 454.6543884277344\n",
      "epoch 94, train_loss 1277.011962890625\n",
      "epoch 94, val_loss 454.6543884277344\n",
      "epoch 95, train_loss 1277.0118408203125\n",
      "epoch 95, val_loss 454.6543884277344\n",
      "epoch 96, train_loss 1277.01171875\n",
      "epoch 96, val_loss 454.654296875\n",
      "epoch 97, train_loss 1277.01171875\n",
      "epoch 97, val_loss 454.65423583984375\n",
      "epoch 98, train_loss 1277.01171875\n",
      "epoch 98, val_loss 454.65423583984375\n",
      "epoch 99, train_loss 1277.01171875\n",
      "epoch 99, val_loss 454.6542053222656\n",
      "Parameter containing:\n",
      "tensor([3.1632e-32], requires_grad=True)\n",
      "iter 179, train_loss_regularization 0.6851511597633362\n",
      "iter 179, val_loss_regularization 0.6851511597633362\n",
      "epoch 0, train_loss 1277.01171875\n",
      "epoch 0, val_loss 454.6541748046875\n",
      "epoch 1, train_loss 1277.01171875\n",
      "epoch 1, val_loss 454.6541748046875\n",
      "epoch 2, train_loss 1277.0115966796875\n",
      "epoch 2, val_loss 454.6541748046875\n",
      "epoch 3, train_loss 1277.01171875\n",
      "epoch 3, val_loss 454.6541748046875\n",
      "epoch 4, train_loss 1277.01171875\n",
      "epoch 4, val_loss 454.65411376953125\n",
      "epoch 5, train_loss 1277.0115966796875\n",
      "epoch 5, val_loss 454.6540832519531\n",
      "epoch 6, train_loss 1277.01171875\n",
      "epoch 6, val_loss 454.6540832519531\n",
      "epoch 7, train_loss 1277.01171875\n",
      "epoch 7, val_loss 454.65399169921875\n",
      "epoch 8, train_loss 1277.01171875\n",
      "epoch 8, val_loss 454.6539306640625\n",
      "epoch 9, train_loss 1277.01171875\n",
      "epoch 9, val_loss 454.6539306640625\n",
      "epoch 10, train_loss 1277.011474609375\n",
      "epoch 10, val_loss 454.6539306640625\n",
      "epoch 11, train_loss 1277.011474609375\n",
      "epoch 11, val_loss 454.6539306640625\n",
      "epoch 12, train_loss 1277.011474609375\n",
      "epoch 12, val_loss 454.65386962890625\n",
      "epoch 13, train_loss 1277.011474609375\n",
      "epoch 13, val_loss 454.6538391113281\n",
      "epoch 14, train_loss 1277.0115966796875\n",
      "epoch 14, val_loss 454.6538391113281\n",
      "epoch 15, train_loss 1277.011474609375\n",
      "epoch 15, val_loss 454.6537780761719\n",
      "epoch 16, train_loss 1277.011474609375\n",
      "epoch 16, val_loss 454.65374755859375\n",
      "epoch 17, train_loss 1277.011474609375\n",
      "epoch 17, val_loss 454.65374755859375\n",
      "epoch 18, train_loss 1277.0113525390625\n",
      "epoch 18, val_loss 454.65374755859375\n",
      "epoch 19, train_loss 1277.01123046875\n",
      "epoch 19, val_loss 454.6536560058594\n",
      "epoch 20, train_loss 1277.01123046875\n",
      "epoch 20, val_loss 454.6536560058594\n",
      "epoch 21, train_loss 1277.01123046875\n",
      "epoch 21, val_loss 454.6536560058594\n",
      "epoch 22, train_loss 1277.0111083984375\n",
      "epoch 22, val_loss 454.65362548828125\n",
      "epoch 23, train_loss 1277.01123046875\n",
      "epoch 23, val_loss 454.6535949707031\n",
      "epoch 24, train_loss 1277.01123046875\n",
      "epoch 24, val_loss 454.6535949707031\n",
      "epoch 25, train_loss 1277.01123046875\n",
      "epoch 25, val_loss 454.65350341796875\n",
      "epoch 26, train_loss 1277.0111083984375\n",
      "epoch 26, val_loss 454.6534729003906\n",
      "epoch 27, train_loss 1277.0111083984375\n",
      "epoch 27, val_loss 454.6534729003906\n",
      "epoch 28, train_loss 1277.0111083984375\n",
      "epoch 28, val_loss 454.6534118652344\n",
      "epoch 29, train_loss 1277.0111083984375\n",
      "epoch 29, val_loss 454.6534729003906\n",
      "epoch 30, train_loss 1277.0111083984375\n",
      "epoch 30, val_loss 454.6533203125\n",
      "epoch 31, train_loss 1277.010986328125\n",
      "epoch 31, val_loss 454.6533203125\n",
      "epoch 32, train_loss 1277.010986328125\n",
      "epoch 32, val_loss 454.6533203125\n",
      "epoch 33, train_loss 1277.0111083984375\n",
      "epoch 33, val_loss 454.6532897949219\n",
      "epoch 34, train_loss 1277.0111083984375\n",
      "epoch 34, val_loss 454.6532897949219\n",
      "epoch 35, train_loss 1277.010986328125\n",
      "epoch 35, val_loss 454.6531982421875\n",
      "epoch 36, train_loss 1277.0111083984375\n",
      "epoch 36, val_loss 454.6531982421875\n",
      "epoch 37, train_loss 1277.0108642578125\n",
      "epoch 37, val_loss 454.6531982421875\n",
      "epoch 38, train_loss 1277.010986328125\n",
      "epoch 38, val_loss 454.65313720703125\n",
      "epoch 39, train_loss 1277.0108642578125\n",
      "epoch 39, val_loss 454.65313720703125\n",
      "epoch 40, train_loss 1277.0107421875\n",
      "epoch 40, val_loss 454.6531677246094\n",
      "epoch 41, train_loss 1277.0107421875\n",
      "epoch 41, val_loss 454.653076171875\n",
      "epoch 42, train_loss 1277.0108642578125\n",
      "epoch 42, val_loss 454.6530456542969\n",
      "epoch 43, train_loss 1277.0108642578125\n",
      "epoch 43, val_loss 454.6530456542969\n",
      "epoch 44, train_loss 1277.0107421875\n",
      "epoch 44, val_loss 454.65301513671875\n",
      "epoch 45, train_loss 1277.0108642578125\n",
      "epoch 45, val_loss 454.65301513671875\n",
      "epoch 46, train_loss 1277.0108642578125\n",
      "epoch 46, val_loss 454.6529235839844\n",
      "epoch 47, train_loss 1277.0108642578125\n",
      "epoch 47, val_loss 454.6529235839844\n",
      "epoch 48, train_loss 1277.0108642578125\n",
      "epoch 48, val_loss 454.6529235839844\n",
      "epoch 49, train_loss 1277.0108642578125\n",
      "epoch 49, val_loss 454.6528625488281\n",
      "epoch 50, train_loss 1277.0106201171875\n",
      "epoch 50, val_loss 454.6529235839844\n",
      "epoch 51, train_loss 1277.0106201171875\n",
      "epoch 51, val_loss 454.65283203125\n",
      "epoch 52, train_loss 1277.0106201171875\n",
      "epoch 52, val_loss 454.6527404785156\n",
      "epoch 53, train_loss 1277.0106201171875\n",
      "epoch 53, val_loss 454.6527404785156\n",
      "epoch 54, train_loss 1277.0106201171875\n",
      "epoch 54, val_loss 454.6527099609375\n",
      "epoch 55, train_loss 1277.0106201171875\n",
      "epoch 55, val_loss 454.6527099609375\n",
      "epoch 56, train_loss 1277.0106201171875\n",
      "epoch 56, val_loss 454.6526184082031\n",
      "epoch 57, train_loss 1277.0107421875\n",
      "epoch 57, val_loss 454.6526794433594\n",
      "epoch 58, train_loss 1277.0106201171875\n",
      "epoch 58, val_loss 454.652587890625\n",
      "epoch 59, train_loss 1277.010498046875\n",
      "epoch 59, val_loss 454.652587890625\n",
      "epoch 60, train_loss 1277.0103759765625\n",
      "epoch 60, val_loss 454.652587890625\n",
      "epoch 61, train_loss 1277.0103759765625\n",
      "epoch 61, val_loss 454.652587890625\n",
      "epoch 62, train_loss 1277.0103759765625\n",
      "epoch 62, val_loss 454.65252685546875\n",
      "epoch 63, train_loss 1277.0103759765625\n",
      "epoch 63, val_loss 454.6524658203125\n",
      "epoch 64, train_loss 1277.0103759765625\n",
      "epoch 64, val_loss 454.6524658203125\n",
      "epoch 65, train_loss 1277.0103759765625\n",
      "epoch 65, val_loss 454.65240478515625\n",
      "epoch 66, train_loss 1277.010498046875\n",
      "epoch 66, val_loss 454.65240478515625\n",
      "epoch 67, train_loss 1277.0103759765625\n",
      "epoch 67, val_loss 454.65240478515625\n",
      "epoch 68, train_loss 1277.0103759765625\n",
      "epoch 68, val_loss 454.65234375\n",
      "epoch 69, train_loss 1277.0103759765625\n",
      "epoch 69, val_loss 454.65234375\n",
      "epoch 70, train_loss 1277.0103759765625\n",
      "epoch 70, val_loss 454.6522521972656\n",
      "epoch 71, train_loss 1277.0103759765625\n",
      "epoch 71, val_loss 454.6522521972656\n",
      "epoch 72, train_loss 1277.01025390625\n",
      "epoch 72, val_loss 454.6522521972656\n",
      "epoch 73, train_loss 1277.0101318359375\n",
      "epoch 73, val_loss 454.6522521972656\n",
      "epoch 74, train_loss 1277.0101318359375\n",
      "epoch 74, val_loss 454.65216064453125\n",
      "epoch 75, train_loss 1277.0101318359375\n",
      "epoch 75, val_loss 454.65216064453125\n",
      "epoch 76, train_loss 1277.0101318359375\n",
      "epoch 76, val_loss 454.6521301269531\n",
      "epoch 77, train_loss 1277.0101318359375\n",
      "epoch 77, val_loss 454.6521301269531\n",
      "epoch 78, train_loss 1277.0101318359375\n",
      "epoch 78, val_loss 454.6521301269531\n",
      "epoch 79, train_loss 1277.0101318359375\n",
      "epoch 79, val_loss 454.6520080566406\n",
      "epoch 80, train_loss 1277.0101318359375\n",
      "epoch 80, val_loss 454.6520080566406\n",
      "epoch 81, train_loss 1277.0101318359375\n",
      "epoch 81, val_loss 454.6520080566406\n",
      "epoch 82, train_loss 1277.0098876953125\n",
      "epoch 82, val_loss 454.6519470214844\n",
      "epoch 83, train_loss 1277.0098876953125\n",
      "epoch 83, val_loss 454.6519470214844\n",
      "epoch 84, train_loss 1277.009765625\n",
      "epoch 84, val_loss 454.6518859863281\n",
      "epoch 85, train_loss 1277.0098876953125\n",
      "epoch 85, val_loss 454.6518859863281\n",
      "epoch 86, train_loss 1277.0098876953125\n",
      "epoch 86, val_loss 454.6518859863281\n",
      "epoch 87, train_loss 1277.0098876953125\n",
      "epoch 87, val_loss 454.6518249511719\n",
      "epoch 88, train_loss 1277.0098876953125\n",
      "epoch 88, val_loss 454.6518249511719\n",
      "epoch 89, train_loss 1277.0098876953125\n",
      "epoch 89, val_loss 454.6518249511719\n",
      "epoch 90, train_loss 1277.0101318359375\n",
      "epoch 90, val_loss 454.6517639160156\n",
      "epoch 91, train_loss 1277.010009765625\n",
      "epoch 91, val_loss 454.6517639160156\n",
      "epoch 92, train_loss 1277.010009765625\n",
      "epoch 92, val_loss 454.65167236328125\n",
      "epoch 93, train_loss 1277.009765625\n",
      "epoch 93, val_loss 454.65167236328125\n",
      "epoch 94, train_loss 1277.0098876953125\n",
      "epoch 94, val_loss 454.651611328125\n",
      "epoch 95, train_loss 1277.009765625\n",
      "epoch 95, val_loss 454.651611328125\n",
      "epoch 96, train_loss 1277.009765625\n",
      "epoch 96, val_loss 454.6515808105469\n",
      "epoch 97, train_loss 1277.009765625\n",
      "epoch 97, val_loss 454.65155029296875\n",
      "epoch 98, train_loss 1277.0096435546875\n",
      "epoch 98, val_loss 454.6514892578125\n",
      "epoch 99, train_loss 1277.0096435546875\n",
      "epoch 99, val_loss 454.6514892578125\n",
      "Parameter containing:\n",
      "tensor([2.2134e-32], requires_grad=True)\n",
      "iter 180, train_loss_regularization 0.6849803924560547\n",
      "iter 180, val_loss_regularization 0.6849803924560547\n",
      "epoch 0, train_loss 1277.0096435546875\n",
      "epoch 0, val_loss 454.6514892578125\n",
      "epoch 1, train_loss 1277.0096435546875\n",
      "epoch 1, val_loss 454.65142822265625\n",
      "epoch 2, train_loss 1277.0096435546875\n",
      "epoch 2, val_loss 454.65142822265625\n",
      "epoch 3, train_loss 1277.0096435546875\n",
      "epoch 3, val_loss 454.65142822265625\n",
      "epoch 4, train_loss 1277.0096435546875\n",
      "epoch 4, val_loss 454.6513671875\n",
      "epoch 5, train_loss 1277.0093994140625\n",
      "epoch 5, val_loss 454.6513671875\n",
      "epoch 6, train_loss 1277.00927734375\n",
      "epoch 6, val_loss 454.6513671875\n",
      "epoch 7, train_loss 1277.0093994140625\n",
      "epoch 7, val_loss 454.65130615234375\n",
      "epoch 8, train_loss 1277.0093994140625\n",
      "epoch 8, val_loss 454.6512451171875\n",
      "epoch 9, train_loss 1277.009521484375\n",
      "epoch 9, val_loss 454.6512451171875\n",
      "epoch 10, train_loss 1277.009521484375\n",
      "epoch 10, val_loss 454.6512451171875\n",
      "epoch 11, train_loss 1277.009521484375\n",
      "epoch 11, val_loss 454.6511535644531\n",
      "epoch 12, train_loss 1277.009521484375\n",
      "epoch 12, val_loss 454.6510925292969\n",
      "epoch 13, train_loss 1277.0093994140625\n",
      "epoch 13, val_loss 454.6510925292969\n",
      "epoch 14, train_loss 1277.00927734375\n",
      "epoch 14, val_loss 454.6510925292969\n",
      "epoch 15, train_loss 1277.00927734375\n",
      "epoch 15, val_loss 454.6510314941406\n",
      "epoch 16, train_loss 1277.00927734375\n",
      "epoch 16, val_loss 454.6510314941406\n",
      "epoch 17, train_loss 1277.00927734375\n",
      "epoch 17, val_loss 454.6510314941406\n",
      "epoch 18, train_loss 1277.00927734375\n",
      "epoch 18, val_loss 454.6510009765625\n",
      "epoch 19, train_loss 1277.00927734375\n",
      "epoch 19, val_loss 454.6509704589844\n",
      "epoch 20, train_loss 1277.00927734375\n",
      "epoch 20, val_loss 454.6509704589844\n",
      "epoch 21, train_loss 1277.00927734375\n",
      "epoch 21, val_loss 454.6509704589844\n",
      "epoch 22, train_loss 1277.00927734375\n",
      "epoch 22, val_loss 454.6509094238281\n",
      "epoch 23, train_loss 1277.009033203125\n",
      "epoch 23, val_loss 454.6508483886719\n",
      "epoch 24, train_loss 1277.0091552734375\n",
      "epoch 24, val_loss 454.6508483886719\n",
      "epoch 25, train_loss 1277.009033203125\n",
      "epoch 25, val_loss 454.6507873535156\n",
      "epoch 26, train_loss 1277.009033203125\n",
      "epoch 26, val_loss 454.6507873535156\n",
      "epoch 27, train_loss 1277.009033203125\n",
      "epoch 27, val_loss 454.6507568359375\n",
      "epoch 28, train_loss 1277.009033203125\n",
      "epoch 28, val_loss 454.6507568359375\n",
      "epoch 29, train_loss 1277.009033203125\n",
      "epoch 29, val_loss 454.6506652832031\n",
      "epoch 30, train_loss 1277.009033203125\n",
      "epoch 30, val_loss 454.6506652832031\n",
      "epoch 31, train_loss 1277.0089111328125\n",
      "epoch 31, val_loss 454.650634765625\n",
      "epoch 32, train_loss 1277.009033203125\n",
      "epoch 32, val_loss 454.6506652832031\n",
      "epoch 33, train_loss 1277.0089111328125\n",
      "epoch 33, val_loss 454.650634765625\n",
      "epoch 34, train_loss 1277.0089111328125\n",
      "epoch 34, val_loss 454.6505126953125\n",
      "epoch 35, train_loss 1277.009033203125\n",
      "epoch 35, val_loss 454.65045166015625\n",
      "epoch 36, train_loss 1277.009033203125\n",
      "epoch 36, val_loss 454.6505126953125\n",
      "epoch 37, train_loss 1277.009033203125\n",
      "epoch 37, val_loss 454.65045166015625\n",
      "epoch 38, train_loss 1277.0089111328125\n",
      "epoch 38, val_loss 454.65045166015625\n",
      "epoch 39, train_loss 1277.0089111328125\n",
      "epoch 39, val_loss 454.65045166015625\n",
      "epoch 40, train_loss 1277.0087890625\n",
      "epoch 40, val_loss 454.6503601074219\n",
      "epoch 41, train_loss 1277.0087890625\n",
      "epoch 41, val_loss 454.65032958984375\n",
      "epoch 42, train_loss 1277.0087890625\n",
      "epoch 42, val_loss 454.65032958984375\n",
      "epoch 43, train_loss 1277.0087890625\n",
      "epoch 43, val_loss 454.65032958984375\n",
      "epoch 44, train_loss 1277.0087890625\n",
      "epoch 44, val_loss 454.6502990722656\n",
      "epoch 45, train_loss 1277.0087890625\n",
      "epoch 45, val_loss 454.6502380371094\n",
      "epoch 46, train_loss 1277.0087890625\n",
      "epoch 46, val_loss 454.6502380371094\n",
      "epoch 47, train_loss 1277.0087890625\n",
      "epoch 47, val_loss 454.65020751953125\n",
      "epoch 48, train_loss 1277.0087890625\n",
      "epoch 48, val_loss 454.6501770019531\n",
      "epoch 49, train_loss 1277.008544921875\n",
      "epoch 49, val_loss 454.6501770019531\n",
      "epoch 50, train_loss 1277.008544921875\n",
      "epoch 50, val_loss 454.6501159667969\n",
      "epoch 51, train_loss 1277.008544921875\n",
      "epoch 51, val_loss 454.65008544921875\n",
      "epoch 52, train_loss 1277.008544921875\n",
      "epoch 52, val_loss 454.65008544921875\n",
      "epoch 53, train_loss 1277.008544921875\n",
      "epoch 53, val_loss 454.65008544921875\n",
      "epoch 54, train_loss 1277.008544921875\n",
      "epoch 54, val_loss 454.6499938964844\n",
      "epoch 55, train_loss 1277.0084228515625\n",
      "epoch 55, val_loss 454.64996337890625\n",
      "epoch 56, train_loss 1277.008544921875\n",
      "epoch 56, val_loss 454.64990234375\n",
      "epoch 57, train_loss 1277.008544921875\n",
      "epoch 57, val_loss 454.6498718261719\n",
      "epoch 58, train_loss 1277.0084228515625\n",
      "epoch 58, val_loss 454.6498718261719\n",
      "epoch 59, train_loss 1277.008544921875\n",
      "epoch 59, val_loss 454.6498718261719\n",
      "epoch 60, train_loss 1277.00830078125\n",
      "epoch 60, val_loss 454.6498718261719\n",
      "epoch 61, train_loss 1277.008544921875\n",
      "epoch 61, val_loss 454.64984130859375\n",
      "epoch 62, train_loss 1277.0084228515625\n",
      "epoch 62, val_loss 454.64971923828125\n",
      "epoch 63, train_loss 1277.00830078125\n",
      "epoch 63, val_loss 454.64971923828125\n",
      "epoch 64, train_loss 1277.00830078125\n",
      "epoch 64, val_loss 454.6497497558594\n",
      "epoch 65, train_loss 1277.00830078125\n",
      "epoch 65, val_loss 454.6497497558594\n",
      "epoch 66, train_loss 1277.00830078125\n",
      "epoch 66, val_loss 454.64971923828125\n",
      "epoch 67, train_loss 1277.00830078125\n",
      "epoch 67, val_loss 454.6496276855469\n",
      "epoch 68, train_loss 1277.00830078125\n",
      "epoch 68, val_loss 454.6496276855469\n",
      "epoch 69, train_loss 1277.00830078125\n",
      "epoch 69, val_loss 454.6496276855469\n",
      "epoch 70, train_loss 1277.0081787109375\n",
      "epoch 70, val_loss 454.64959716796875\n",
      "epoch 71, train_loss 1277.0081787109375\n",
      "epoch 71, val_loss 454.64959716796875\n",
      "epoch 72, train_loss 1277.0081787109375\n",
      "epoch 72, val_loss 454.6495361328125\n",
      "epoch 73, train_loss 1277.008056640625\n",
      "epoch 73, val_loss 454.6495056152344\n",
      "epoch 74, train_loss 1277.0081787109375\n",
      "epoch 74, val_loss 454.6494445800781\n",
      "epoch 75, train_loss 1277.008056640625\n",
      "epoch 75, val_loss 454.6495056152344\n",
      "epoch 76, train_loss 1277.008056640625\n",
      "epoch 76, val_loss 454.6495056152344\n",
      "epoch 77, train_loss 1277.008056640625\n",
      "epoch 77, val_loss 454.6494140625\n",
      "epoch 78, train_loss 1277.008056640625\n",
      "epoch 78, val_loss 454.6492919921875\n",
      "epoch 79, train_loss 1277.0079345703125\n",
      "epoch 79, val_loss 454.6492919921875\n",
      "epoch 80, train_loss 1277.0079345703125\n",
      "epoch 80, val_loss 454.6492919921875\n",
      "epoch 81, train_loss 1277.008056640625\n",
      "epoch 81, val_loss 454.6492919921875\n",
      "epoch 82, train_loss 1277.008056640625\n",
      "epoch 82, val_loss 454.6492614746094\n",
      "epoch 83, train_loss 1277.008056640625\n",
      "epoch 83, val_loss 454.6492614746094\n",
      "epoch 84, train_loss 1277.0079345703125\n",
      "epoch 84, val_loss 454.6492004394531\n",
      "epoch 85, train_loss 1277.008056640625\n",
      "epoch 85, val_loss 454.649169921875\n",
      "epoch 86, train_loss 1277.0079345703125\n",
      "epoch 86, val_loss 454.649169921875\n",
      "epoch 87, train_loss 1277.008056640625\n",
      "epoch 87, val_loss 454.649169921875\n",
      "epoch 88, train_loss 1277.0079345703125\n",
      "epoch 88, val_loss 454.6491394042969\n",
      "epoch 89, train_loss 1277.0079345703125\n",
      "epoch 89, val_loss 454.6490478515625\n",
      "epoch 90, train_loss 1277.0078125\n",
      "epoch 90, val_loss 454.6490783691406\n",
      "epoch 91, train_loss 1277.0078125\n",
      "epoch 91, val_loss 454.64898681640625\n",
      "epoch 92, train_loss 1277.0079345703125\n",
      "epoch 92, val_loss 454.64898681640625\n",
      "epoch 93, train_loss 1277.0078125\n",
      "epoch 93, val_loss 454.64898681640625\n",
      "epoch 94, train_loss 1277.0078125\n",
      "epoch 94, val_loss 454.64892578125\n",
      "epoch 95, train_loss 1277.0076904296875\n",
      "epoch 95, val_loss 454.64892578125\n",
      "epoch 96, train_loss 1277.007568359375\n",
      "epoch 96, val_loss 454.64886474609375\n",
      "epoch 97, train_loss 1277.0074462890625\n",
      "epoch 97, val_loss 454.64886474609375\n",
      "epoch 98, train_loss 1277.0076904296875\n",
      "epoch 98, val_loss 454.6488037109375\n",
      "epoch 99, train_loss 1277.0076904296875\n",
      "epoch 99, val_loss 454.6488037109375\n",
      "Parameter containing:\n",
      "tensor([1.5489e-32], requires_grad=True)\n",
      "iter 181, train_loss_regularization 0.684821605682373\n",
      "iter 181, val_loss_regularization 0.684821605682373\n",
      "epoch 0, train_loss 1277.0076904296875\n",
      "epoch 0, val_loss 454.64874267578125\n",
      "epoch 1, train_loss 1277.007568359375\n",
      "epoch 1, val_loss 454.64874267578125\n",
      "epoch 2, train_loss 1277.0076904296875\n",
      "epoch 2, val_loss 454.6487121582031\n",
      "epoch 3, train_loss 1277.0076904296875\n",
      "epoch 3, val_loss 454.64874267578125\n",
      "epoch 4, train_loss 1277.0074462890625\n",
      "epoch 4, val_loss 454.6487121582031\n",
      "epoch 5, train_loss 1277.0074462890625\n",
      "epoch 5, val_loss 454.64862060546875\n",
      "epoch 6, train_loss 1277.0074462890625\n",
      "epoch 6, val_loss 454.6485900878906\n",
      "epoch 7, train_loss 1277.00732421875\n",
      "epoch 7, val_loss 454.6485900878906\n",
      "epoch 8, train_loss 1277.0074462890625\n",
      "epoch 8, val_loss 454.6485290527344\n",
      "epoch 9, train_loss 1277.0074462890625\n",
      "epoch 9, val_loss 454.6485290527344\n",
      "epoch 10, train_loss 1277.0074462890625\n",
      "epoch 10, val_loss 454.64849853515625\n",
      "epoch 11, train_loss 1277.0074462890625\n",
      "epoch 11, val_loss 454.6484069824219\n",
      "epoch 12, train_loss 1277.0074462890625\n",
      "epoch 12, val_loss 454.6484680175781\n",
      "epoch 13, train_loss 1277.00732421875\n",
      "epoch 13, val_loss 454.6484069824219\n",
      "epoch 14, train_loss 1277.00732421875\n",
      "epoch 14, val_loss 454.6484069824219\n",
      "epoch 15, train_loss 1277.0072021484375\n",
      "epoch 15, val_loss 454.64837646484375\n",
      "epoch 16, train_loss 1277.0072021484375\n",
      "epoch 16, val_loss 454.6483459472656\n",
      "epoch 17, train_loss 1277.007080078125\n",
      "epoch 17, val_loss 454.6483459472656\n",
      "epoch 18, train_loss 1277.0072021484375\n",
      "epoch 18, val_loss 454.64825439453125\n",
      "epoch 19, train_loss 1277.0072021484375\n",
      "epoch 19, val_loss 454.64825439453125\n",
      "epoch 20, train_loss 1277.0072021484375\n",
      "epoch 20, val_loss 454.648193359375\n",
      "epoch 21, train_loss 1277.007080078125\n",
      "epoch 21, val_loss 454.6481628417969\n",
      "epoch 22, train_loss 1277.007080078125\n",
      "epoch 22, val_loss 454.6481628417969\n",
      "epoch 23, train_loss 1277.0072021484375\n",
      "epoch 23, val_loss 454.64813232421875\n",
      "epoch 24, train_loss 1277.0072021484375\n",
      "epoch 24, val_loss 454.64813232421875\n",
      "epoch 25, train_loss 1277.0072021484375\n",
      "epoch 25, val_loss 454.6480712890625\n",
      "epoch 26, train_loss 1277.0072021484375\n",
      "epoch 26, val_loss 454.6480712890625\n",
      "epoch 27, train_loss 1277.007080078125\n",
      "epoch 27, val_loss 454.6480407714844\n",
      "epoch 28, train_loss 1277.0072021484375\n",
      "epoch 28, val_loss 454.6480407714844\n",
      "epoch 29, train_loss 1277.0069580078125\n",
      "epoch 29, val_loss 454.64801025390625\n",
      "epoch 30, train_loss 1277.007080078125\n",
      "epoch 30, val_loss 454.64801025390625\n",
      "epoch 31, train_loss 1277.0069580078125\n",
      "epoch 31, val_loss 454.64794921875\n",
      "epoch 32, train_loss 1277.0069580078125\n",
      "epoch 32, val_loss 454.6479187011719\n",
      "epoch 33, train_loss 1277.0069580078125\n",
      "epoch 33, val_loss 454.6479187011719\n",
      "epoch 34, train_loss 1277.0069580078125\n",
      "epoch 34, val_loss 454.64788818359375\n",
      "epoch 35, train_loss 1277.0069580078125\n",
      "epoch 35, val_loss 454.6478271484375\n",
      "epoch 36, train_loss 1277.0069580078125\n",
      "epoch 36, val_loss 454.6477966308594\n",
      "epoch 37, train_loss 1277.0068359375\n",
      "epoch 37, val_loss 454.6477355957031\n",
      "epoch 38, train_loss 1277.0068359375\n",
      "epoch 38, val_loss 454.6477355957031\n",
      "epoch 39, train_loss 1277.0067138671875\n",
      "epoch 39, val_loss 454.647705078125\n",
      "epoch 40, train_loss 1277.0067138671875\n",
      "epoch 40, val_loss 454.647705078125\n",
      "epoch 41, train_loss 1277.0067138671875\n",
      "epoch 41, val_loss 454.647705078125\n",
      "epoch 42, train_loss 1277.0067138671875\n",
      "epoch 42, val_loss 454.6476135253906\n",
      "epoch 43, train_loss 1277.0067138671875\n",
      "epoch 43, val_loss 454.6476135253906\n",
      "epoch 44, train_loss 1277.0067138671875\n",
      "epoch 44, val_loss 454.6475830078125\n",
      "epoch 45, train_loss 1277.0067138671875\n",
      "epoch 45, val_loss 454.6475830078125\n",
      "epoch 46, train_loss 1277.0067138671875\n",
      "epoch 46, val_loss 454.6475524902344\n",
      "epoch 47, train_loss 1277.0067138671875\n",
      "epoch 47, val_loss 454.6474609375\n",
      "epoch 48, train_loss 1277.0067138671875\n",
      "epoch 48, val_loss 454.6474914550781\n",
      "epoch 49, train_loss 1277.006591796875\n",
      "epoch 49, val_loss 454.6474609375\n",
      "epoch 50, train_loss 1277.0067138671875\n",
      "epoch 50, val_loss 454.6474304199219\n",
      "epoch 51, train_loss 1277.0067138671875\n",
      "epoch 51, val_loss 454.6473693847656\n",
      "epoch 52, train_loss 1277.0067138671875\n",
      "epoch 52, val_loss 454.6473693847656\n",
      "epoch 53, train_loss 1277.006591796875\n",
      "epoch 53, val_loss 454.6473388671875\n",
      "epoch 54, train_loss 1277.0064697265625\n",
      "epoch 54, val_loss 454.6473388671875\n",
      "epoch 55, train_loss 1277.0064697265625\n",
      "epoch 55, val_loss 454.6473388671875\n",
      "epoch 56, train_loss 1277.0064697265625\n",
      "epoch 56, val_loss 454.6472473144531\n",
      "epoch 57, train_loss 1277.0064697265625\n",
      "epoch 57, val_loss 454.6472473144531\n",
      "epoch 58, train_loss 1277.0064697265625\n",
      "epoch 58, val_loss 454.647216796875\n",
      "epoch 59, train_loss 1277.0064697265625\n",
      "epoch 59, val_loss 454.64715576171875\n",
      "epoch 60, train_loss 1277.00634765625\n",
      "epoch 60, val_loss 454.647216796875\n",
      "epoch 61, train_loss 1277.00634765625\n",
      "epoch 61, val_loss 454.6470947265625\n",
      "epoch 62, train_loss 1277.00634765625\n",
      "epoch 62, val_loss 454.6470947265625\n",
      "epoch 63, train_loss 1277.0062255859375\n",
      "epoch 63, val_loss 454.64703369140625\n",
      "epoch 64, train_loss 1277.0062255859375\n",
      "epoch 64, val_loss 454.64703369140625\n",
      "epoch 65, train_loss 1277.0062255859375\n",
      "epoch 65, val_loss 454.6470031738281\n",
      "epoch 66, train_loss 1277.00634765625\n",
      "epoch 66, val_loss 454.6470031738281\n",
      "epoch 67, train_loss 1277.00634765625\n",
      "epoch 67, val_loss 454.6470031738281\n",
      "epoch 68, train_loss 1277.0062255859375\n",
      "epoch 68, val_loss 454.64697265625\n",
      "epoch 69, train_loss 1277.0062255859375\n",
      "epoch 69, val_loss 454.6468811035156\n",
      "epoch 70, train_loss 1277.006103515625\n",
      "epoch 70, val_loss 454.6468811035156\n",
      "epoch 71, train_loss 1277.0062255859375\n",
      "epoch 71, val_loss 454.6468811035156\n",
      "epoch 72, train_loss 1277.0062255859375\n",
      "epoch 72, val_loss 454.6468200683594\n",
      "epoch 73, train_loss 1277.0062255859375\n",
      "epoch 73, val_loss 454.6468200683594\n",
      "epoch 74, train_loss 1277.0062255859375\n",
      "epoch 74, val_loss 454.64678955078125\n",
      "epoch 75, train_loss 1277.0062255859375\n",
      "epoch 75, val_loss 454.64678955078125\n",
      "epoch 76, train_loss 1277.0062255859375\n",
      "epoch 76, val_loss 454.64678955078125\n",
      "epoch 77, train_loss 1277.0062255859375\n",
      "epoch 77, val_loss 454.6466979980469\n",
      "epoch 78, train_loss 1277.0062255859375\n",
      "epoch 78, val_loss 454.64666748046875\n",
      "epoch 79, train_loss 1277.006103515625\n",
      "epoch 79, val_loss 454.64666748046875\n",
      "epoch 80, train_loss 1277.0059814453125\n",
      "epoch 80, val_loss 454.6465759277344\n",
      "epoch 81, train_loss 1277.0059814453125\n",
      "epoch 81, val_loss 454.6465759277344\n",
      "epoch 82, train_loss 1277.005859375\n",
      "epoch 82, val_loss 454.6465759277344\n",
      "epoch 83, train_loss 1277.005859375\n",
      "epoch 83, val_loss 454.64654541015625\n",
      "epoch 84, train_loss 1277.0059814453125\n",
      "epoch 84, val_loss 454.6465148925781\n",
      "epoch 85, train_loss 1277.005859375\n",
      "epoch 85, val_loss 454.6464538574219\n",
      "epoch 86, train_loss 1277.005859375\n",
      "epoch 86, val_loss 454.6464538574219\n",
      "epoch 87, train_loss 1277.005859375\n",
      "epoch 87, val_loss 454.6464538574219\n",
      "epoch 88, train_loss 1277.005859375\n",
      "epoch 88, val_loss 454.6464538574219\n",
      "epoch 89, train_loss 1277.005859375\n",
      "epoch 89, val_loss 454.6463317871094\n",
      "epoch 90, train_loss 1277.005859375\n",
      "epoch 90, val_loss 454.6463317871094\n",
      "epoch 91, train_loss 1277.005859375\n",
      "epoch 91, val_loss 454.6463317871094\n",
      "epoch 92, train_loss 1277.005859375\n",
      "epoch 92, val_loss 454.64630126953125\n",
      "epoch 93, train_loss 1277.005859375\n",
      "epoch 93, val_loss 454.64630126953125\n",
      "epoch 94, train_loss 1277.0057373046875\n",
      "epoch 94, val_loss 454.646240234375\n",
      "epoch 95, train_loss 1277.0057373046875\n",
      "epoch 95, val_loss 454.6462097167969\n",
      "epoch 96, train_loss 1277.0057373046875\n",
      "epoch 96, val_loss 454.6462097167969\n",
      "epoch 97, train_loss 1277.0057373046875\n",
      "epoch 97, val_loss 454.6462097167969\n",
      "epoch 98, train_loss 1277.005615234375\n",
      "epoch 98, val_loss 454.64617919921875\n",
      "epoch 99, train_loss 1277.005615234375\n",
      "epoch 99, val_loss 454.6461181640625\n",
      "Parameter containing:\n",
      "tensor([1.0840e-32], requires_grad=True)\n",
      "iter 182, train_loss_regularization 0.684674859046936\n",
      "iter 182, val_loss_regularization 0.684674859046936\n",
      "epoch 0, train_loss 1277.005615234375\n",
      "epoch 0, val_loss 454.6460876464844\n",
      "epoch 1, train_loss 1277.005615234375\n",
      "epoch 1, val_loss 454.6460876464844\n",
      "epoch 2, train_loss 1277.0054931640625\n",
      "epoch 2, val_loss 454.64599609375\n",
      "epoch 3, train_loss 1277.00537109375\n",
      "epoch 3, val_loss 454.6460266113281\n",
      "epoch 4, train_loss 1277.00537109375\n",
      "epoch 4, val_loss 454.64599609375\n",
      "epoch 5, train_loss 1277.00537109375\n",
      "epoch 5, val_loss 454.6459655761719\n",
      "epoch 6, train_loss 1277.0054931640625\n",
      "epoch 6, val_loss 454.6458740234375\n",
      "epoch 7, train_loss 1277.0054931640625\n",
      "epoch 7, val_loss 454.6458740234375\n",
      "epoch 8, train_loss 1277.00537109375\n",
      "epoch 8, val_loss 454.6458740234375\n",
      "epoch 9, train_loss 1277.00537109375\n",
      "epoch 9, val_loss 454.6458740234375\n",
      "epoch 10, train_loss 1277.0054931640625\n",
      "epoch 10, val_loss 454.6458740234375\n",
      "epoch 11, train_loss 1277.00537109375\n",
      "epoch 11, val_loss 454.645751953125\n",
      "epoch 12, train_loss 1277.00537109375\n",
      "epoch 12, val_loss 454.645751953125\n",
      "epoch 13, train_loss 1277.00537109375\n",
      "epoch 13, val_loss 454.645751953125\n",
      "epoch 14, train_loss 1277.00537109375\n",
      "epoch 14, val_loss 454.645751953125\n",
      "epoch 15, train_loss 1277.00537109375\n",
      "epoch 15, val_loss 454.6457214355469\n",
      "epoch 16, train_loss 1277.0052490234375\n",
      "epoch 16, val_loss 454.6457214355469\n",
      "epoch 17, train_loss 1277.00537109375\n",
      "epoch 17, val_loss 454.6456604003906\n",
      "epoch 18, train_loss 1277.005126953125\n",
      "epoch 18, val_loss 454.64556884765625\n",
      "epoch 19, train_loss 1277.0052490234375\n",
      "epoch 19, val_loss 454.6455383300781\n",
      "epoch 20, train_loss 1277.005126953125\n",
      "epoch 20, val_loss 454.6455383300781\n",
      "epoch 21, train_loss 1277.005126953125\n",
      "epoch 21, val_loss 454.64556884765625\n",
      "epoch 22, train_loss 1277.005126953125\n",
      "epoch 22, val_loss 454.6455383300781\n",
      "epoch 23, train_loss 1277.005126953125\n",
      "epoch 23, val_loss 454.6454162597656\n",
      "epoch 24, train_loss 1277.005126953125\n",
      "epoch 24, val_loss 454.6454162597656\n",
      "epoch 25, train_loss 1277.005126953125\n",
      "epoch 25, val_loss 454.6454162597656\n",
      "epoch 26, train_loss 1277.005126953125\n",
      "epoch 26, val_loss 454.64544677734375\n",
      "epoch 27, train_loss 1277.005126953125\n",
      "epoch 27, val_loss 454.6453857421875\n",
      "epoch 28, train_loss 1277.0050048828125\n",
      "epoch 28, val_loss 454.64532470703125\n",
      "epoch 29, train_loss 1277.0050048828125\n",
      "epoch 29, val_loss 454.6452941894531\n",
      "epoch 30, train_loss 1277.0048828125\n",
      "epoch 30, val_loss 454.6452941894531\n",
      "epoch 31, train_loss 1277.0050048828125\n",
      "epoch 31, val_loss 454.6452941894531\n",
      "epoch 32, train_loss 1277.0050048828125\n",
      "epoch 32, val_loss 454.6452941894531\n",
      "epoch 33, train_loss 1277.0048828125\n",
      "epoch 33, val_loss 454.64520263671875\n",
      "epoch 34, train_loss 1277.0050048828125\n",
      "epoch 34, val_loss 454.6451110839844\n",
      "epoch 35, train_loss 1277.0050048828125\n",
      "epoch 35, val_loss 454.6451110839844\n",
      "epoch 36, train_loss 1277.0048828125\n",
      "epoch 36, val_loss 454.6451721191406\n",
      "epoch 37, train_loss 1277.0048828125\n",
      "epoch 37, val_loss 454.6451110839844\n",
      "epoch 38, train_loss 1277.0048828125\n",
      "epoch 38, val_loss 454.64508056640625\n",
      "epoch 39, train_loss 1277.0048828125\n",
      "epoch 39, val_loss 454.6450500488281\n",
      "epoch 40, train_loss 1277.0048828125\n",
      "epoch 40, val_loss 454.6449890136719\n",
      "epoch 41, train_loss 1277.0048828125\n",
      "epoch 41, val_loss 454.6449890136719\n",
      "epoch 42, train_loss 1277.0048828125\n",
      "epoch 42, val_loss 454.6449890136719\n",
      "epoch 43, train_loss 1277.0047607421875\n",
      "epoch 43, val_loss 454.6449890136719\n",
      "epoch 44, train_loss 1277.004638671875\n",
      "epoch 44, val_loss 454.6449279785156\n",
      "epoch 45, train_loss 1277.004638671875\n",
      "epoch 45, val_loss 454.6448669433594\n",
      "epoch 46, train_loss 1277.004638671875\n",
      "epoch 46, val_loss 454.64483642578125\n",
      "epoch 47, train_loss 1277.004638671875\n",
      "epoch 47, val_loss 454.64483642578125\n",
      "epoch 48, train_loss 1277.004638671875\n",
      "epoch 48, val_loss 454.6448059082031\n",
      "epoch 49, train_loss 1277.004638671875\n",
      "epoch 49, val_loss 454.6448059082031\n",
      "epoch 50, train_loss 1277.004638671875\n",
      "epoch 50, val_loss 454.64471435546875\n",
      "epoch 51, train_loss 1277.0045166015625\n",
      "epoch 51, val_loss 454.64471435546875\n",
      "epoch 52, train_loss 1277.0045166015625\n",
      "epoch 52, val_loss 454.64471435546875\n",
      "epoch 53, train_loss 1277.0045166015625\n",
      "epoch 53, val_loss 454.64471435546875\n",
      "epoch 54, train_loss 1277.00439453125\n",
      "epoch 54, val_loss 454.6446533203125\n",
      "epoch 55, train_loss 1277.00439453125\n",
      "epoch 55, val_loss 454.6446533203125\n",
      "epoch 56, train_loss 1277.00439453125\n",
      "epoch 56, val_loss 454.6446228027344\n",
      "epoch 57, train_loss 1277.0045166015625\n",
      "epoch 57, val_loss 454.64459228515625\n",
      "epoch 58, train_loss 1277.0045166015625\n",
      "epoch 58, val_loss 454.64453125\n",
      "epoch 59, train_loss 1277.0045166015625\n",
      "epoch 59, val_loss 454.64459228515625\n",
      "epoch 60, train_loss 1277.0045166015625\n",
      "epoch 60, val_loss 454.6445007324219\n",
      "epoch 61, train_loss 1277.0045166015625\n",
      "epoch 61, val_loss 454.6445007324219\n",
      "epoch 62, train_loss 1277.0042724609375\n",
      "epoch 62, val_loss 454.6444091796875\n",
      "epoch 63, train_loss 1277.0042724609375\n",
      "epoch 63, val_loss 454.6444091796875\n",
      "epoch 64, train_loss 1277.0042724609375\n",
      "epoch 64, val_loss 454.6444091796875\n",
      "epoch 65, train_loss 1277.004150390625\n",
      "epoch 65, val_loss 454.6443786621094\n",
      "epoch 66, train_loss 1277.004150390625\n",
      "epoch 66, val_loss 454.64434814453125\n",
      "epoch 67, train_loss 1277.004150390625\n",
      "epoch 67, val_loss 454.64434814453125\n",
      "epoch 68, train_loss 1277.004150390625\n",
      "epoch 68, val_loss 454.644287109375\n",
      "epoch 69, train_loss 1277.004150390625\n",
      "epoch 69, val_loss 454.6442565917969\n",
      "epoch 70, train_loss 1277.004150390625\n",
      "epoch 70, val_loss 454.6441955566406\n",
      "epoch 71, train_loss 1277.0040283203125\n",
      "epoch 71, val_loss 454.6441955566406\n",
      "epoch 72, train_loss 1277.004150390625\n",
      "epoch 72, val_loss 454.6441955566406\n",
      "epoch 73, train_loss 1277.0042724609375\n",
      "epoch 73, val_loss 454.6441345214844\n",
      "epoch 74, train_loss 1277.004150390625\n",
      "epoch 74, val_loss 454.6441345214844\n",
      "epoch 75, train_loss 1277.004150390625\n",
      "epoch 75, val_loss 454.6440734863281\n",
      "epoch 76, train_loss 1277.004150390625\n",
      "epoch 76, val_loss 454.64404296875\n",
      "epoch 77, train_loss 1277.0040283203125\n",
      "epoch 77, val_loss 454.6440124511719\n",
      "epoch 78, train_loss 1277.0040283203125\n",
      "epoch 78, val_loss 454.6440124511719\n",
      "epoch 79, train_loss 1277.0040283203125\n",
      "epoch 79, val_loss 454.6440124511719\n",
      "epoch 80, train_loss 1277.0040283203125\n",
      "epoch 80, val_loss 454.6439514160156\n",
      "epoch 81, train_loss 1277.0040283203125\n",
      "epoch 81, val_loss 454.6439514160156\n",
      "epoch 82, train_loss 1277.0040283203125\n",
      "epoch 82, val_loss 454.6439208984375\n",
      "epoch 83, train_loss 1277.00390625\n",
      "epoch 83, val_loss 454.6439208984375\n",
      "epoch 84, train_loss 1277.0040283203125\n",
      "epoch 84, val_loss 454.64385986328125\n",
      "epoch 85, train_loss 1277.0037841796875\n",
      "epoch 85, val_loss 454.6439208984375\n",
      "epoch 86, train_loss 1277.0037841796875\n",
      "epoch 86, val_loss 454.6438293457031\n",
      "epoch 87, train_loss 1277.00390625\n",
      "epoch 87, val_loss 454.64373779296875\n",
      "epoch 88, train_loss 1277.0037841796875\n",
      "epoch 88, val_loss 454.6438293457031\n",
      "epoch 89, train_loss 1277.0037841796875\n",
      "epoch 89, val_loss 454.64373779296875\n",
      "epoch 90, train_loss 1277.0037841796875\n",
      "epoch 90, val_loss 454.6436767578125\n",
      "epoch 91, train_loss 1277.0037841796875\n",
      "epoch 91, val_loss 454.6436767578125\n",
      "epoch 92, train_loss 1277.0037841796875\n",
      "epoch 92, val_loss 454.6435852050781\n",
      "epoch 93, train_loss 1277.003662109375\n",
      "epoch 93, val_loss 454.64361572265625\n",
      "epoch 94, train_loss 1277.003662109375\n",
      "epoch 94, val_loss 454.6435852050781\n",
      "epoch 95, train_loss 1277.0035400390625\n",
      "epoch 95, val_loss 454.6435852050781\n",
      "epoch 96, train_loss 1277.003662109375\n",
      "epoch 96, val_loss 454.6435546875\n",
      "epoch 97, train_loss 1277.003662109375\n",
      "epoch 97, val_loss 454.6435546875\n",
      "epoch 98, train_loss 1277.0035400390625\n",
      "epoch 98, val_loss 454.64349365234375\n",
      "epoch 99, train_loss 1277.0035400390625\n",
      "epoch 99, val_loss 454.64349365234375\n",
      "Parameter containing:\n",
      "tensor([7.5868e-33], requires_grad=True)\n",
      "iter 183, train_loss_regularization 0.6845398545265198\n",
      "iter 183, val_loss_regularization 0.6845398545265198\n",
      "epoch 0, train_loss 1277.003662109375\n",
      "epoch 0, val_loss 454.6434631347656\n",
      "epoch 1, train_loss 1277.003662109375\n",
      "epoch 1, val_loss 454.6434020996094\n",
      "epoch 2, train_loss 1277.0035400390625\n",
      "epoch 2, val_loss 454.6434020996094\n",
      "epoch 3, train_loss 1277.0035400390625\n",
      "epoch 3, val_loss 454.64337158203125\n",
      "epoch 4, train_loss 1277.0035400390625\n",
      "epoch 4, val_loss 454.6433410644531\n",
      "epoch 5, train_loss 1277.0035400390625\n",
      "epoch 5, val_loss 454.6432800292969\n",
      "epoch 6, train_loss 1277.00341796875\n",
      "epoch 6, val_loss 454.6432800292969\n",
      "epoch 7, train_loss 1277.00341796875\n",
      "epoch 7, val_loss 454.64324951171875\n",
      "epoch 8, train_loss 1277.0035400390625\n",
      "epoch 8, val_loss 454.64324951171875\n",
      "epoch 9, train_loss 1277.0035400390625\n",
      "epoch 9, val_loss 454.6431579589844\n",
      "epoch 10, train_loss 1277.0035400390625\n",
      "epoch 10, val_loss 454.6431579589844\n",
      "epoch 11, train_loss 1277.0032958984375\n",
      "epoch 11, val_loss 454.64312744140625\n",
      "epoch 12, train_loss 1277.0032958984375\n",
      "epoch 12, val_loss 454.6430969238281\n",
      "epoch 13, train_loss 1277.00341796875\n",
      "epoch 13, val_loss 454.6430969238281\n",
      "epoch 14, train_loss 1277.00341796875\n",
      "epoch 14, val_loss 454.6430358886719\n",
      "epoch 15, train_loss 1277.0032958984375\n",
      "epoch 15, val_loss 454.6430358886719\n",
      "epoch 16, train_loss 1277.0032958984375\n",
      "epoch 16, val_loss 454.64300537109375\n",
      "epoch 17, train_loss 1277.0032958984375\n",
      "epoch 17, val_loss 454.6430358886719\n",
      "epoch 18, train_loss 1277.0032958984375\n",
      "epoch 18, val_loss 454.6429443359375\n",
      "epoch 19, train_loss 1277.0032958984375\n",
      "epoch 19, val_loss 454.6429138183594\n",
      "epoch 20, train_loss 1277.0032958984375\n",
      "epoch 20, val_loss 454.6429138183594\n",
      "epoch 21, train_loss 1277.0032958984375\n",
      "epoch 21, val_loss 454.64288330078125\n",
      "epoch 22, train_loss 1277.003173828125\n",
      "epoch 22, val_loss 454.64288330078125\n",
      "epoch 23, train_loss 1277.003173828125\n",
      "epoch 23, val_loss 454.64288330078125\n",
      "epoch 24, train_loss 1277.003173828125\n",
      "epoch 24, val_loss 454.6427917480469\n",
      "epoch 25, train_loss 1277.003173828125\n",
      "epoch 25, val_loss 454.6427917480469\n",
      "epoch 26, train_loss 1277.003173828125\n",
      "epoch 26, val_loss 454.64276123046875\n",
      "epoch 27, train_loss 1277.003173828125\n",
      "epoch 27, val_loss 454.64276123046875\n",
      "epoch 28, train_loss 1277.0030517578125\n",
      "epoch 28, val_loss 454.64276123046875\n",
      "epoch 29, train_loss 1277.0030517578125\n",
      "epoch 29, val_loss 454.6427001953125\n",
      "epoch 30, train_loss 1277.0030517578125\n",
      "epoch 30, val_loss 454.6426696777344\n",
      "epoch 31, train_loss 1277.0030517578125\n",
      "epoch 31, val_loss 454.6426696777344\n",
      "epoch 32, train_loss 1277.0030517578125\n",
      "epoch 32, val_loss 454.6424865722656\n",
      "epoch 33, train_loss 1277.0030517578125\n",
      "epoch 33, val_loss 454.6425476074219\n",
      "epoch 34, train_loss 1277.0028076171875\n",
      "epoch 34, val_loss 454.6425476074219\n",
      "epoch 35, train_loss 1277.0028076171875\n",
      "epoch 35, val_loss 454.6424865722656\n",
      "epoch 36, train_loss 1277.0028076171875\n",
      "epoch 36, val_loss 454.6424560546875\n",
      "epoch 37, train_loss 1277.0029296875\n",
      "epoch 37, val_loss 454.6424560546875\n",
      "epoch 38, train_loss 1277.0029296875\n",
      "epoch 38, val_loss 454.6424560546875\n",
      "epoch 39, train_loss 1277.0028076171875\n",
      "epoch 39, val_loss 454.6424255371094\n",
      "epoch 40, train_loss 1277.0028076171875\n",
      "epoch 40, val_loss 454.6423645019531\n",
      "epoch 41, train_loss 1277.0028076171875\n",
      "epoch 41, val_loss 454.642333984375\n",
      "epoch 42, train_loss 1277.0028076171875\n",
      "epoch 42, val_loss 454.642333984375\n",
      "epoch 43, train_loss 1277.0028076171875\n",
      "epoch 43, val_loss 454.642333984375\n",
      "epoch 44, train_loss 1277.0028076171875\n",
      "epoch 44, val_loss 454.6423034667969\n",
      "epoch 45, train_loss 1277.0028076171875\n",
      "epoch 45, val_loss 454.6423034667969\n",
      "epoch 46, train_loss 1277.002685546875\n",
      "epoch 46, val_loss 454.6422424316406\n",
      "epoch 47, train_loss 1277.002685546875\n",
      "epoch 47, val_loss 454.6422119140625\n",
      "epoch 48, train_loss 1277.0028076171875\n",
      "epoch 48, val_loss 454.6421813964844\n",
      "epoch 49, train_loss 1277.002685546875\n",
      "epoch 49, val_loss 454.6421203613281\n",
      "epoch 50, train_loss 1277.002685546875\n",
      "epoch 50, val_loss 454.6421813964844\n",
      "epoch 51, train_loss 1277.0025634765625\n",
      "epoch 51, val_loss 454.6421203613281\n",
      "epoch 52, train_loss 1277.00244140625\n",
      "epoch 52, val_loss 454.64202880859375\n",
      "epoch 53, train_loss 1277.00244140625\n",
      "epoch 53, val_loss 454.6419982910156\n",
      "epoch 54, train_loss 1277.00244140625\n",
      "epoch 54, val_loss 454.6419982910156\n",
      "epoch 55, train_loss 1277.002685546875\n",
      "epoch 55, val_loss 454.6419677734375\n",
      "epoch 56, train_loss 1277.002685546875\n",
      "epoch 56, val_loss 454.6419677734375\n",
      "epoch 57, train_loss 1277.002685546875\n",
      "epoch 57, val_loss 454.64190673828125\n",
      "epoch 58, train_loss 1277.00244140625\n",
      "epoch 58, val_loss 454.64190673828125\n",
      "epoch 59, train_loss 1277.00244140625\n",
      "epoch 59, val_loss 454.6418762207031\n",
      "epoch 60, train_loss 1277.00244140625\n",
      "epoch 60, val_loss 454.6418762207031\n",
      "epoch 61, train_loss 1277.0023193359375\n",
      "epoch 61, val_loss 454.6418762207031\n",
      "epoch 62, train_loss 1277.0023193359375\n",
      "epoch 62, val_loss 454.64178466796875\n",
      "epoch 63, train_loss 1277.0023193359375\n",
      "epoch 63, val_loss 454.6417541503906\n",
      "epoch 64, train_loss 1277.0023193359375\n",
      "epoch 64, val_loss 454.6417541503906\n",
      "epoch 65, train_loss 1277.0023193359375\n",
      "epoch 65, val_loss 454.6416931152344\n",
      "epoch 66, train_loss 1277.00244140625\n",
      "epoch 66, val_loss 454.6416931152344\n",
      "epoch 67, train_loss 1277.0023193359375\n",
      "epoch 67, val_loss 454.6416931152344\n",
      "epoch 68, train_loss 1277.0023193359375\n",
      "epoch 68, val_loss 454.6416931152344\n",
      "epoch 69, train_loss 1277.002197265625\n",
      "epoch 69, val_loss 454.6416320800781\n",
      "epoch 70, train_loss 1277.002197265625\n",
      "epoch 70, val_loss 454.6416320800781\n",
      "epoch 71, train_loss 1277.002197265625\n",
      "epoch 71, val_loss 454.6415710449219\n",
      "epoch 72, train_loss 1277.002197265625\n",
      "epoch 72, val_loss 454.6415710449219\n",
      "epoch 73, train_loss 1277.0020751953125\n",
      "epoch 73, val_loss 454.6415710449219\n",
      "epoch 74, train_loss 1277.002197265625\n",
      "epoch 74, val_loss 454.6415100097656\n",
      "epoch 75, train_loss 1277.002197265625\n",
      "epoch 75, val_loss 454.64141845703125\n",
      "epoch 76, train_loss 1277.002197265625\n",
      "epoch 76, val_loss 454.6413879394531\n",
      "epoch 77, train_loss 1277.002197265625\n",
      "epoch 77, val_loss 454.64141845703125\n",
      "epoch 78, train_loss 1277.002197265625\n",
      "epoch 78, val_loss 454.64141845703125\n",
      "epoch 79, train_loss 1277.002197265625\n",
      "epoch 79, val_loss 454.6413269042969\n",
      "epoch 80, train_loss 1277.001953125\n",
      "epoch 80, val_loss 454.64129638671875\n",
      "epoch 81, train_loss 1277.0018310546875\n",
      "epoch 81, val_loss 454.6413269042969\n",
      "epoch 82, train_loss 1277.001953125\n",
      "epoch 82, val_loss 454.64129638671875\n",
      "epoch 83, train_loss 1277.001953125\n",
      "epoch 83, val_loss 454.64129638671875\n",
      "epoch 84, train_loss 1277.001953125\n",
      "epoch 84, val_loss 454.6412353515625\n",
      "epoch 85, train_loss 1277.001953125\n",
      "epoch 85, val_loss 454.6412353515625\n",
      "epoch 86, train_loss 1277.001953125\n",
      "epoch 86, val_loss 454.64117431640625\n",
      "epoch 87, train_loss 1277.0018310546875\n",
      "epoch 87, val_loss 454.64117431640625\n",
      "epoch 88, train_loss 1277.0018310546875\n",
      "epoch 88, val_loss 454.64111328125\n",
      "epoch 89, train_loss 1277.0018310546875\n",
      "epoch 89, val_loss 454.64117431640625\n",
      "epoch 90, train_loss 1277.0018310546875\n",
      "epoch 90, val_loss 454.6410827636719\n",
      "epoch 91, train_loss 1277.001953125\n",
      "epoch 91, val_loss 454.64105224609375\n",
      "epoch 92, train_loss 1277.001708984375\n",
      "epoch 92, val_loss 454.6409912109375\n",
      "epoch 93, train_loss 1277.001708984375\n",
      "epoch 93, val_loss 454.6409912109375\n",
      "epoch 94, train_loss 1277.001708984375\n",
      "epoch 94, val_loss 454.6409606933594\n",
      "epoch 95, train_loss 1277.001708984375\n",
      "epoch 95, val_loss 454.64093017578125\n",
      "epoch 96, train_loss 1277.001708984375\n",
      "epoch 96, val_loss 454.64093017578125\n",
      "epoch 97, train_loss 1277.001708984375\n",
      "epoch 97, val_loss 454.64093017578125\n",
      "epoch 98, train_loss 1277.001708984375\n",
      "epoch 98, val_loss 454.640869140625\n",
      "epoch 99, train_loss 1277.001708984375\n",
      "epoch 99, val_loss 454.6408386230469\n",
      "Parameter containing:\n",
      "tensor([5.3102e-33], requires_grad=True)\n",
      "iter 184, train_loss_regularization 0.6844163537025452\n",
      "iter 184, val_loss_regularization 0.6844163537025452\n",
      "epoch 0, train_loss 1277.001708984375\n",
      "epoch 0, val_loss 454.6408386230469\n",
      "epoch 1, train_loss 1277.0018310546875\n",
      "epoch 1, val_loss 454.6408386230469\n",
      "epoch 2, train_loss 1277.0015869140625\n",
      "epoch 2, val_loss 454.6407775878906\n",
      "epoch 3, train_loss 1277.00146484375\n",
      "epoch 3, val_loss 454.6407165527344\n",
      "epoch 4, train_loss 1277.00146484375\n",
      "epoch 4, val_loss 454.6407165527344\n",
      "epoch 5, train_loss 1277.0015869140625\n",
      "epoch 5, val_loss 454.6407165527344\n",
      "epoch 6, train_loss 1277.00146484375\n",
      "epoch 6, val_loss 454.640625\n",
      "epoch 7, train_loss 1277.00146484375\n",
      "epoch 7, val_loss 454.640625\n",
      "epoch 8, train_loss 1277.00146484375\n",
      "epoch 8, val_loss 454.640625\n",
      "epoch 9, train_loss 1277.00146484375\n",
      "epoch 9, val_loss 454.6405944824219\n",
      "epoch 10, train_loss 1277.00146484375\n",
      "epoch 10, val_loss 454.6405334472656\n",
      "epoch 11, train_loss 1277.00146484375\n",
      "epoch 11, val_loss 454.6405334472656\n",
      "epoch 12, train_loss 1277.00146484375\n",
      "epoch 12, val_loss 454.6405029296875\n",
      "epoch 13, train_loss 1277.00146484375\n",
      "epoch 13, val_loss 454.6405029296875\n",
      "epoch 14, train_loss 1277.00146484375\n",
      "epoch 14, val_loss 454.6404724121094\n",
      "epoch 15, train_loss 1277.0013427734375\n",
      "epoch 15, val_loss 454.6404113769531\n",
      "epoch 16, train_loss 1277.0013427734375\n",
      "epoch 16, val_loss 454.6404113769531\n",
      "epoch 17, train_loss 1277.0013427734375\n",
      "epoch 17, val_loss 454.6404113769531\n",
      "epoch 18, train_loss 1277.0013427734375\n",
      "epoch 18, val_loss 454.64031982421875\n",
      "epoch 19, train_loss 1277.001220703125\n",
      "epoch 19, val_loss 454.64031982421875\n",
      "epoch 20, train_loss 1277.0013427734375\n",
      "epoch 20, val_loss 454.6402587890625\n",
      "epoch 21, train_loss 1277.0013427734375\n",
      "epoch 21, val_loss 454.6402587890625\n",
      "epoch 22, train_loss 1277.0013427734375\n",
      "epoch 22, val_loss 454.64019775390625\n",
      "epoch 23, train_loss 1277.001220703125\n",
      "epoch 23, val_loss 454.64019775390625\n",
      "epoch 24, train_loss 1277.001220703125\n",
      "epoch 24, val_loss 454.6401672363281\n",
      "epoch 25, train_loss 1277.001220703125\n",
      "epoch 25, val_loss 454.64013671875\n",
      "epoch 26, train_loss 1277.001220703125\n",
      "epoch 26, val_loss 454.64013671875\n",
      "epoch 27, train_loss 1277.001220703125\n",
      "epoch 27, val_loss 454.64013671875\n",
      "epoch 28, train_loss 1277.0009765625\n",
      "epoch 28, val_loss 454.64007568359375\n",
      "epoch 29, train_loss 1277.0009765625\n",
      "epoch 29, val_loss 454.64007568359375\n",
      "epoch 30, train_loss 1277.0009765625\n",
      "epoch 30, val_loss 454.64007568359375\n",
      "epoch 31, train_loss 1277.0009765625\n",
      "epoch 31, val_loss 454.6400146484375\n",
      "epoch 32, train_loss 1277.0009765625\n",
      "epoch 32, val_loss 454.6399230957031\n",
      "epoch 33, train_loss 1277.0009765625\n",
      "epoch 33, val_loss 454.63995361328125\n",
      "epoch 34, train_loss 1277.0009765625\n",
      "epoch 34, val_loss 454.6399230957031\n",
      "epoch 35, train_loss 1277.0009765625\n",
      "epoch 35, val_loss 454.6399230957031\n",
      "epoch 36, train_loss 1277.0009765625\n",
      "epoch 36, val_loss 454.6399230957031\n",
      "epoch 37, train_loss 1277.0008544921875\n",
      "epoch 37, val_loss 454.63983154296875\n",
      "epoch 38, train_loss 1277.000732421875\n",
      "epoch 38, val_loss 454.6398010253906\n",
      "epoch 39, train_loss 1277.0009765625\n",
      "epoch 39, val_loss 454.6398010253906\n",
      "epoch 40, train_loss 1277.0008544921875\n",
      "epoch 40, val_loss 454.63983154296875\n",
      "epoch 41, train_loss 1277.0008544921875\n",
      "epoch 41, val_loss 454.6397399902344\n",
      "epoch 42, train_loss 1277.0008544921875\n",
      "epoch 42, val_loss 454.63970947265625\n",
      "epoch 43, train_loss 1277.0008544921875\n",
      "epoch 43, val_loss 454.6396789550781\n",
      "epoch 44, train_loss 1277.0008544921875\n",
      "epoch 44, val_loss 454.6396789550781\n",
      "epoch 45, train_loss 1277.0008544921875\n",
      "epoch 45, val_loss 454.6396179199219\n",
      "epoch 46, train_loss 1277.000732421875\n",
      "epoch 46, val_loss 454.63958740234375\n",
      "epoch 47, train_loss 1277.000732421875\n",
      "epoch 47, val_loss 454.6396179199219\n",
      "epoch 48, train_loss 1277.0006103515625\n",
      "epoch 48, val_loss 454.6395568847656\n",
      "epoch 49, train_loss 1277.000732421875\n",
      "epoch 49, val_loss 454.6395568847656\n",
      "epoch 50, train_loss 1277.000732421875\n",
      "epoch 50, val_loss 454.63946533203125\n",
      "epoch 51, train_loss 1277.000732421875\n",
      "epoch 51, val_loss 454.639404296875\n",
      "epoch 52, train_loss 1277.00048828125\n",
      "epoch 52, val_loss 454.63946533203125\n",
      "epoch 53, train_loss 1277.00048828125\n",
      "epoch 53, val_loss 454.63946533203125\n",
      "epoch 54, train_loss 1277.0006103515625\n",
      "epoch 54, val_loss 454.639404296875\n",
      "epoch 55, train_loss 1277.0006103515625\n",
      "epoch 55, val_loss 454.6393737792969\n",
      "epoch 56, train_loss 1277.00048828125\n",
      "epoch 56, val_loss 454.6393737792969\n",
      "epoch 57, train_loss 1277.0006103515625\n",
      "epoch 57, val_loss 454.6393737792969\n",
      "epoch 58, train_loss 1277.00048828125\n",
      "epoch 58, val_loss 454.63934326171875\n",
      "epoch 59, train_loss 1277.00048828125\n",
      "epoch 59, val_loss 454.63934326171875\n",
      "epoch 60, train_loss 1277.0006103515625\n",
      "epoch 60, val_loss 454.63934326171875\n",
      "epoch 61, train_loss 1277.00048828125\n",
      "epoch 61, val_loss 454.6392517089844\n",
      "epoch 62, train_loss 1277.00048828125\n",
      "epoch 62, val_loss 454.63916015625\n",
      "epoch 63, train_loss 1277.00048828125\n",
      "epoch 63, val_loss 454.6391296386719\n",
      "epoch 64, train_loss 1277.00048828125\n",
      "epoch 64, val_loss 454.6391296386719\n",
      "epoch 65, train_loss 1277.0003662109375\n",
      "epoch 65, val_loss 454.6390686035156\n",
      "epoch 66, train_loss 1277.0003662109375\n",
      "epoch 66, val_loss 454.6390686035156\n",
      "epoch 67, train_loss 1277.0003662109375\n",
      "epoch 67, val_loss 454.6390380859375\n",
      "epoch 68, train_loss 1277.00048828125\n",
      "epoch 68, val_loss 454.6390380859375\n",
      "epoch 69, train_loss 1277.0003662109375\n",
      "epoch 69, val_loss 454.6390075683594\n",
      "epoch 70, train_loss 1277.0003662109375\n",
      "epoch 70, val_loss 454.6390075683594\n",
      "epoch 71, train_loss 1277.0003662109375\n",
      "epoch 71, val_loss 454.6389465332031\n",
      "epoch 72, train_loss 1277.0003662109375\n",
      "epoch 72, val_loss 454.6389465332031\n",
      "epoch 73, train_loss 1277.0001220703125\n",
      "epoch 73, val_loss 454.6389465332031\n",
      "epoch 74, train_loss 1277.000244140625\n",
      "epoch 74, val_loss 454.638916015625\n",
      "epoch 75, train_loss 1277.0001220703125\n",
      "epoch 75, val_loss 454.6388854980469\n",
      "epoch 76, train_loss 1277.0001220703125\n",
      "epoch 76, val_loss 454.638916015625\n",
      "epoch 77, train_loss 1277.0001220703125\n",
      "epoch 77, val_loss 454.6388244628906\n",
      "epoch 78, train_loss 1277.0001220703125\n",
      "epoch 78, val_loss 454.6387634277344\n",
      "epoch 79, train_loss 1277.0001220703125\n",
      "epoch 79, val_loss 454.6387634277344\n",
      "epoch 80, train_loss 1277.0\n",
      "epoch 80, val_loss 454.6387634277344\n",
      "epoch 81, train_loss 1277.0\n",
      "epoch 81, val_loss 454.6387023925781\n",
      "epoch 82, train_loss 1277.0001220703125\n",
      "epoch 82, val_loss 454.6387023925781\n",
      "epoch 83, train_loss 1277.0001220703125\n",
      "epoch 83, val_loss 454.638671875\n",
      "epoch 84, train_loss 1277.0001220703125\n",
      "epoch 84, val_loss 454.638671875\n",
      "epoch 85, train_loss 1277.0001220703125\n",
      "epoch 85, val_loss 454.6385498046875\n",
      "epoch 86, train_loss 1277.0\n",
      "epoch 86, val_loss 454.6385803222656\n",
      "epoch 87, train_loss 1276.9998779296875\n",
      "epoch 87, val_loss 454.6385803222656\n",
      "epoch 88, train_loss 1276.9998779296875\n",
      "epoch 88, val_loss 454.6385498046875\n",
      "epoch 89, train_loss 1276.9998779296875\n",
      "epoch 89, val_loss 454.6385498046875\n",
      "epoch 90, train_loss 1276.9998779296875\n",
      "epoch 90, val_loss 454.6384582519531\n",
      "epoch 91, train_loss 1276.9998779296875\n",
      "epoch 91, val_loss 454.638427734375\n",
      "epoch 92, train_loss 1276.9998779296875\n",
      "epoch 92, val_loss 454.63836669921875\n",
      "epoch 93, train_loss 1276.9998779296875\n",
      "epoch 93, val_loss 454.638427734375\n",
      "epoch 94, train_loss 1276.9998779296875\n",
      "epoch 94, val_loss 454.6383361816406\n",
      "epoch 95, train_loss 1276.9998779296875\n",
      "epoch 95, val_loss 454.6383056640625\n",
      "epoch 96, train_loss 1276.999755859375\n",
      "epoch 96, val_loss 454.6383056640625\n",
      "epoch 97, train_loss 1276.999755859375\n",
      "epoch 97, val_loss 454.6383056640625\n",
      "epoch 98, train_loss 1276.9996337890625\n",
      "epoch 98, val_loss 454.6383056640625\n",
      "epoch 99, train_loss 1276.9996337890625\n",
      "epoch 99, val_loss 454.6382141113281\n",
      "Parameter containing:\n",
      "tensor([3.7170e-33], requires_grad=True)\n",
      "iter 185, train_loss_regularization 0.6843043565750122\n",
      "iter 185, val_loss_regularization 0.6843043565750122\n",
      "epoch 0, train_loss 1276.9996337890625\n",
      "epoch 0, val_loss 454.6382141113281\n",
      "epoch 1, train_loss 1276.9996337890625\n",
      "epoch 1, val_loss 454.6381530761719\n",
      "epoch 2, train_loss 1276.9996337890625\n",
      "epoch 2, val_loss 454.6382141113281\n",
      "epoch 3, train_loss 1276.999755859375\n",
      "epoch 3, val_loss 454.6381530761719\n",
      "epoch 4, train_loss 1276.999755859375\n",
      "epoch 4, val_loss 454.6382141113281\n",
      "epoch 5, train_loss 1276.999755859375\n",
      "epoch 5, val_loss 454.63800048828125\n",
      "epoch 6, train_loss 1276.999755859375\n",
      "epoch 6, val_loss 454.63800048828125\n",
      "epoch 7, train_loss 1276.999755859375\n",
      "epoch 7, val_loss 454.63800048828125\n",
      "epoch 8, train_loss 1276.999755859375\n",
      "epoch 8, val_loss 454.63800048828125\n",
      "epoch 9, train_loss 1276.9996337890625\n",
      "epoch 9, val_loss 454.6379089355469\n",
      "epoch 10, train_loss 1276.99951171875\n",
      "epoch 10, val_loss 454.6379699707031\n",
      "epoch 11, train_loss 1276.99951171875\n",
      "epoch 11, val_loss 454.63787841796875\n",
      "epoch 12, train_loss 1276.99951171875\n",
      "epoch 12, val_loss 454.63787841796875\n",
      "epoch 13, train_loss 1276.99951171875\n",
      "epoch 13, val_loss 454.63787841796875\n",
      "epoch 14, train_loss 1276.9993896484375\n",
      "epoch 14, val_loss 454.63787841796875\n",
      "epoch 15, train_loss 1276.9993896484375\n",
      "epoch 15, val_loss 454.6378479003906\n",
      "epoch 16, train_loss 1276.9993896484375\n",
      "epoch 16, val_loss 454.63787841796875\n",
      "epoch 17, train_loss 1276.9993896484375\n",
      "epoch 17, val_loss 454.6377868652344\n",
      "epoch 18, train_loss 1276.9993896484375\n",
      "epoch 18, val_loss 454.6377868652344\n",
      "epoch 19, train_loss 1276.9993896484375\n",
      "epoch 19, val_loss 454.63775634765625\n",
      "epoch 20, train_loss 1276.9991455078125\n",
      "epoch 20, val_loss 454.6376953125\n",
      "epoch 21, train_loss 1276.9991455078125\n",
      "epoch 21, val_loss 454.63775634765625\n",
      "epoch 22, train_loss 1276.9991455078125\n",
      "epoch 22, val_loss 454.6376647949219\n",
      "epoch 23, train_loss 1276.999267578125\n",
      "epoch 23, val_loss 454.6375732421875\n",
      "epoch 24, train_loss 1276.999267578125\n",
      "epoch 24, val_loss 454.6375732421875\n",
      "epoch 25, train_loss 1276.999267578125\n",
      "epoch 25, val_loss 454.6375732421875\n",
      "epoch 26, train_loss 1276.999267578125\n",
      "epoch 26, val_loss 454.6375732421875\n",
      "epoch 27, train_loss 1276.999267578125\n",
      "epoch 27, val_loss 454.6375427246094\n",
      "epoch 28, train_loss 1276.999267578125\n",
      "epoch 28, val_loss 454.63751220703125\n",
      "epoch 29, train_loss 1276.999267578125\n",
      "epoch 29, val_loss 454.63751220703125\n",
      "epoch 30, train_loss 1276.9991455078125\n",
      "epoch 30, val_loss 454.637451171875\n",
      "epoch 31, train_loss 1276.9991455078125\n",
      "epoch 31, val_loss 454.6374206542969\n",
      "epoch 32, train_loss 1276.9990234375\n",
      "epoch 32, val_loss 454.6374206542969\n",
      "epoch 33, train_loss 1276.9990234375\n",
      "epoch 33, val_loss 454.63739013671875\n",
      "epoch 34, train_loss 1276.9990234375\n",
      "epoch 34, val_loss 454.6373291015625\n",
      "epoch 35, train_loss 1276.9990234375\n",
      "epoch 35, val_loss 454.6372985839844\n",
      "epoch 36, train_loss 1276.9990234375\n",
      "epoch 36, val_loss 454.6372375488281\n",
      "epoch 37, train_loss 1276.9990234375\n",
      "epoch 37, val_loss 454.6372375488281\n",
      "epoch 38, train_loss 1276.9990234375\n",
      "epoch 38, val_loss 454.63720703125\n",
      "epoch 39, train_loss 1276.9990234375\n",
      "epoch 39, val_loss 454.63720703125\n",
      "epoch 40, train_loss 1276.9990234375\n",
      "epoch 40, val_loss 454.6371765136719\n",
      "epoch 41, train_loss 1276.9989013671875\n",
      "epoch 41, val_loss 454.6371154785156\n",
      "epoch 42, train_loss 1276.9989013671875\n",
      "epoch 42, val_loss 454.6371154785156\n",
      "epoch 43, train_loss 1276.998779296875\n",
      "epoch 43, val_loss 454.6371154785156\n",
      "epoch 44, train_loss 1276.998779296875\n",
      "epoch 44, val_loss 454.6371154785156\n",
      "epoch 45, train_loss 1276.998779296875\n",
      "epoch 45, val_loss 454.6370849609375\n",
      "epoch 46, train_loss 1276.998779296875\n",
      "epoch 46, val_loss 454.6370544433594\n",
      "epoch 47, train_loss 1276.998779296875\n",
      "epoch 47, val_loss 454.6370544433594\n",
      "epoch 48, train_loss 1276.998779296875\n",
      "epoch 48, val_loss 454.6370544433594\n",
      "epoch 49, train_loss 1276.998779296875\n",
      "epoch 49, val_loss 454.636962890625\n",
      "epoch 50, train_loss 1276.998779296875\n",
      "epoch 50, val_loss 454.636962890625\n",
      "epoch 51, train_loss 1276.9989013671875\n",
      "epoch 51, val_loss 454.6368713378906\n",
      "epoch 52, train_loss 1276.998779296875\n",
      "epoch 52, val_loss 454.6368408203125\n",
      "epoch 53, train_loss 1276.998779296875\n",
      "epoch 53, val_loss 454.63677978515625\n",
      "epoch 54, train_loss 1276.998779296875\n",
      "epoch 54, val_loss 454.6368408203125\n",
      "epoch 55, train_loss 1276.9986572265625\n",
      "epoch 55, val_loss 454.63677978515625\n",
      "epoch 56, train_loss 1276.99853515625\n",
      "epoch 56, val_loss 454.63677978515625\n",
      "epoch 57, train_loss 1276.99853515625\n",
      "epoch 57, val_loss 454.63671875\n",
      "epoch 58, train_loss 1276.99853515625\n",
      "epoch 58, val_loss 454.63671875\n",
      "epoch 59, train_loss 1276.99853515625\n",
      "epoch 59, val_loss 454.63671875\n",
      "epoch 60, train_loss 1276.99853515625\n",
      "epoch 60, val_loss 454.63665771484375\n",
      "epoch 61, train_loss 1276.9986572265625\n",
      "epoch 61, val_loss 454.63665771484375\n",
      "epoch 62, train_loss 1276.9986572265625\n",
      "epoch 62, val_loss 454.6366271972656\n",
      "epoch 63, train_loss 1276.9986572265625\n",
      "epoch 63, val_loss 454.6366271972656\n",
      "epoch 64, train_loss 1276.9984130859375\n",
      "epoch 64, val_loss 454.6365966796875\n",
      "epoch 65, train_loss 1276.99853515625\n",
      "epoch 65, val_loss 454.6365966796875\n",
      "epoch 66, train_loss 1276.99853515625\n",
      "epoch 66, val_loss 454.6365051269531\n",
      "epoch 67, train_loss 1276.998291015625\n",
      "epoch 67, val_loss 454.6365051269531\n",
      "epoch 68, train_loss 1276.9984130859375\n",
      "epoch 68, val_loss 454.6364440917969\n",
      "epoch 69, train_loss 1276.9984130859375\n",
      "epoch 69, val_loss 454.63641357421875\n",
      "epoch 70, train_loss 1276.9984130859375\n",
      "epoch 70, val_loss 454.63641357421875\n",
      "epoch 71, train_loss 1276.9984130859375\n",
      "epoch 71, val_loss 454.63641357421875\n",
      "epoch 72, train_loss 1276.9984130859375\n",
      "epoch 72, val_loss 454.6363220214844\n",
      "epoch 73, train_loss 1276.9984130859375\n",
      "epoch 73, val_loss 454.63629150390625\n",
      "epoch 74, train_loss 1276.998291015625\n",
      "epoch 74, val_loss 454.6362609863281\n",
      "epoch 75, train_loss 1276.998291015625\n",
      "epoch 75, val_loss 454.63629150390625\n",
      "epoch 76, train_loss 1276.998291015625\n",
      "epoch 76, val_loss 454.6362609863281\n",
      "epoch 77, train_loss 1276.998291015625\n",
      "epoch 77, val_loss 454.6361999511719\n",
      "epoch 78, train_loss 1276.9981689453125\n",
      "epoch 78, val_loss 454.63616943359375\n",
      "epoch 79, train_loss 1276.9981689453125\n",
      "epoch 79, val_loss 454.6361999511719\n",
      "epoch 80, train_loss 1276.9981689453125\n",
      "epoch 80, val_loss 454.6361389160156\n",
      "epoch 81, train_loss 1276.9981689453125\n",
      "epoch 81, val_loss 454.6360778808594\n",
      "epoch 82, train_loss 1276.9981689453125\n",
      "epoch 82, val_loss 454.6360778808594\n",
      "epoch 83, train_loss 1276.998046875\n",
      "epoch 83, val_loss 454.6360778808594\n",
      "epoch 84, train_loss 1276.998046875\n",
      "epoch 84, val_loss 454.6360778808594\n",
      "epoch 85, train_loss 1276.998291015625\n",
      "epoch 85, val_loss 454.63604736328125\n",
      "epoch 86, train_loss 1276.998046875\n",
      "epoch 86, val_loss 454.635986328125\n",
      "epoch 87, train_loss 1276.9981689453125\n",
      "epoch 87, val_loss 454.635986328125\n",
      "epoch 88, train_loss 1276.998046875\n",
      "epoch 88, val_loss 454.6359558105469\n",
      "epoch 89, train_loss 1276.997802734375\n",
      "epoch 89, val_loss 454.63592529296875\n",
      "epoch 90, train_loss 1276.997802734375\n",
      "epoch 90, val_loss 454.63592529296875\n",
      "epoch 91, train_loss 1276.9979248046875\n",
      "epoch 91, val_loss 454.6358642578125\n",
      "epoch 92, train_loss 1276.997802734375\n",
      "epoch 92, val_loss 454.6358337402344\n",
      "epoch 93, train_loss 1276.997802734375\n",
      "epoch 93, val_loss 454.6358337402344\n",
      "epoch 94, train_loss 1276.9979248046875\n",
      "epoch 94, val_loss 454.63580322265625\n",
      "epoch 95, train_loss 1276.9979248046875\n",
      "epoch 95, val_loss 454.6358337402344\n",
      "epoch 96, train_loss 1276.9979248046875\n",
      "epoch 96, val_loss 454.6357421875\n",
      "epoch 97, train_loss 1276.997802734375\n",
      "epoch 97, val_loss 454.6357421875\n",
      "epoch 98, train_loss 1276.997802734375\n",
      "epoch 98, val_loss 454.63568115234375\n",
      "epoch 99, train_loss 1276.997802734375\n",
      "epoch 99, val_loss 454.6356201171875\n",
      "Parameter containing:\n",
      "tensor([2.6019e-33], requires_grad=True)\n",
      "iter 186, train_loss_regularization 0.6842037439346313\n",
      "iter 186, val_loss_regularization 0.6842037439346313\n",
      "epoch 0, train_loss 1276.9979248046875\n",
      "epoch 0, val_loss 454.6356201171875\n",
      "epoch 1, train_loss 1276.9979248046875\n",
      "epoch 1, val_loss 454.6356201171875\n",
      "epoch 2, train_loss 1276.997802734375\n",
      "epoch 2, val_loss 454.6356201171875\n",
      "epoch 3, train_loss 1276.997802734375\n",
      "epoch 3, val_loss 454.635498046875\n",
      "epoch 4, train_loss 1276.997802734375\n",
      "epoch 4, val_loss 454.6355285644531\n",
      "epoch 5, train_loss 1276.997802734375\n",
      "epoch 5, val_loss 454.635498046875\n",
      "epoch 6, train_loss 1276.997802734375\n",
      "epoch 6, val_loss 454.635498046875\n",
      "epoch 7, train_loss 1276.997802734375\n",
      "epoch 7, val_loss 454.635498046875\n",
      "epoch 8, train_loss 1276.9976806640625\n",
      "epoch 8, val_loss 454.6354675292969\n",
      "epoch 9, train_loss 1276.9976806640625\n",
      "epoch 9, val_loss 454.6354064941406\n",
      "epoch 10, train_loss 1276.997802734375\n",
      "epoch 10, val_loss 454.6353454589844\n",
      "epoch 11, train_loss 1276.9976806640625\n",
      "epoch 11, val_loss 454.6353454589844\n",
      "epoch 12, train_loss 1276.9976806640625\n",
      "epoch 12, val_loss 454.6353454589844\n",
      "epoch 13, train_loss 1276.99755859375\n",
      "epoch 13, val_loss 454.6353454589844\n",
      "epoch 14, train_loss 1276.99755859375\n",
      "epoch 14, val_loss 454.63525390625\n",
      "epoch 15, train_loss 1276.99755859375\n",
      "epoch 15, val_loss 454.63525390625\n",
      "epoch 16, train_loss 1276.99755859375\n",
      "epoch 16, val_loss 454.63525390625\n",
      "epoch 17, train_loss 1276.99755859375\n",
      "epoch 17, val_loss 454.6351623535156\n",
      "epoch 18, train_loss 1276.99755859375\n",
      "epoch 18, val_loss 454.6351623535156\n",
      "epoch 19, train_loss 1276.997314453125\n",
      "epoch 19, val_loss 454.6351623535156\n",
      "epoch 20, train_loss 1276.997314453125\n",
      "epoch 20, val_loss 454.6351318359375\n",
      "epoch 21, train_loss 1276.9974365234375\n",
      "epoch 21, val_loss 454.6351318359375\n",
      "epoch 22, train_loss 1276.9974365234375\n",
      "epoch 22, val_loss 454.6350402832031\n",
      "epoch 23, train_loss 1276.99755859375\n",
      "epoch 23, val_loss 454.6350402832031\n",
      "epoch 24, train_loss 1276.997314453125\n",
      "epoch 24, val_loss 454.635009765625\n",
      "epoch 25, train_loss 1276.997314453125\n",
      "epoch 25, val_loss 454.635009765625\n",
      "epoch 26, train_loss 1276.9971923828125\n",
      "epoch 26, val_loss 454.6349182128906\n",
      "epoch 27, train_loss 1276.9971923828125\n",
      "epoch 27, val_loss 454.6348876953125\n",
      "epoch 28, train_loss 1276.9971923828125\n",
      "epoch 28, val_loss 454.6349182128906\n",
      "epoch 29, train_loss 1276.9971923828125\n",
      "epoch 29, val_loss 454.6349182128906\n",
      "epoch 30, train_loss 1276.9971923828125\n",
      "epoch 30, val_loss 454.6348876953125\n",
      "epoch 31, train_loss 1276.9971923828125\n",
      "epoch 31, val_loss 454.6348876953125\n",
      "epoch 32, train_loss 1276.997314453125\n",
      "epoch 32, val_loss 454.63482666015625\n",
      "epoch 33, train_loss 1276.9971923828125\n",
      "epoch 33, val_loss 454.6347961425781\n",
      "epoch 34, train_loss 1276.9971923828125\n",
      "epoch 34, val_loss 454.6347961425781\n",
      "epoch 35, train_loss 1276.997314453125\n",
      "epoch 35, val_loss 454.6347961425781\n",
      "epoch 36, train_loss 1276.9971923828125\n",
      "epoch 36, val_loss 454.6347351074219\n",
      "epoch 37, train_loss 1276.9969482421875\n",
      "epoch 37, val_loss 454.6347351074219\n",
      "epoch 38, train_loss 1276.9969482421875\n",
      "epoch 38, val_loss 454.6346130371094\n",
      "epoch 39, train_loss 1276.9971923828125\n",
      "epoch 39, val_loss 454.6346740722656\n",
      "epoch 40, train_loss 1276.9969482421875\n",
      "epoch 40, val_loss 454.63458251953125\n",
      "epoch 41, train_loss 1276.9969482421875\n",
      "epoch 41, val_loss 454.63458251953125\n",
      "epoch 42, train_loss 1276.9969482421875\n",
      "epoch 42, val_loss 454.6345520019531\n",
      "epoch 43, train_loss 1276.9969482421875\n",
      "epoch 43, val_loss 454.6344909667969\n",
      "epoch 44, train_loss 1276.9969482421875\n",
      "epoch 44, val_loss 454.63446044921875\n",
      "epoch 45, train_loss 1276.9969482421875\n",
      "epoch 45, val_loss 454.63446044921875\n",
      "epoch 46, train_loss 1276.9969482421875\n",
      "epoch 46, val_loss 454.63446044921875\n",
      "epoch 47, train_loss 1276.9969482421875\n",
      "epoch 47, val_loss 454.63446044921875\n",
      "epoch 48, train_loss 1276.9969482421875\n",
      "epoch 48, val_loss 454.63446044921875\n",
      "epoch 49, train_loss 1276.9969482421875\n",
      "epoch 49, val_loss 454.6344299316406\n",
      "epoch 50, train_loss 1276.9969482421875\n",
      "epoch 50, val_loss 454.6343688964844\n",
      "epoch 51, train_loss 1276.9967041015625\n",
      "epoch 51, val_loss 454.63433837890625\n",
      "epoch 52, train_loss 1276.996826171875\n",
      "epoch 52, val_loss 454.63433837890625\n",
      "epoch 53, train_loss 1276.996826171875\n",
      "epoch 53, val_loss 454.6342468261719\n",
      "epoch 54, train_loss 1276.996826171875\n",
      "epoch 54, val_loss 454.6342468261719\n",
      "epoch 55, train_loss 1276.996826171875\n",
      "epoch 55, val_loss 454.63421630859375\n",
      "epoch 56, train_loss 1276.996826171875\n",
      "epoch 56, val_loss 454.63421630859375\n",
      "epoch 57, train_loss 1276.996826171875\n",
      "epoch 57, val_loss 454.6341552734375\n",
      "epoch 58, train_loss 1276.9967041015625\n",
      "epoch 58, val_loss 454.6341552734375\n",
      "epoch 59, train_loss 1276.9967041015625\n",
      "epoch 59, val_loss 454.6341552734375\n",
      "epoch 60, train_loss 1276.9967041015625\n",
      "epoch 60, val_loss 454.6341247558594\n",
      "epoch 61, train_loss 1276.99658203125\n",
      "epoch 61, val_loss 454.63409423828125\n",
      "epoch 62, train_loss 1276.9967041015625\n",
      "epoch 62, val_loss 454.63409423828125\n",
      "epoch 63, train_loss 1276.99658203125\n",
      "epoch 63, val_loss 454.6340026855469\n",
      "epoch 64, train_loss 1276.99658203125\n",
      "epoch 64, val_loss 454.6340026855469\n",
      "epoch 65, train_loss 1276.9964599609375\n",
      "epoch 65, val_loss 454.63397216796875\n",
      "epoch 66, train_loss 1276.99658203125\n",
      "epoch 66, val_loss 454.6339111328125\n",
      "epoch 67, train_loss 1276.9964599609375\n",
      "epoch 67, val_loss 454.6338806152344\n",
      "epoch 68, train_loss 1276.9964599609375\n",
      "epoch 68, val_loss 454.6338806152344\n",
      "epoch 69, train_loss 1276.9964599609375\n",
      "epoch 69, val_loss 454.6338806152344\n",
      "epoch 70, train_loss 1276.9964599609375\n",
      "epoch 70, val_loss 454.6338806152344\n",
      "epoch 71, train_loss 1276.99658203125\n",
      "epoch 71, val_loss 454.6337890625\n",
      "epoch 72, train_loss 1276.9964599609375\n",
      "epoch 72, val_loss 454.6337890625\n",
      "epoch 73, train_loss 1276.9964599609375\n",
      "epoch 73, val_loss 454.6337585449219\n",
      "epoch 74, train_loss 1276.9964599609375\n",
      "epoch 74, val_loss 454.6337585449219\n",
      "epoch 75, train_loss 1276.9964599609375\n",
      "epoch 75, val_loss 454.6336975097656\n",
      "epoch 76, train_loss 1276.996337890625\n",
      "epoch 76, val_loss 454.6337585449219\n",
      "epoch 77, train_loss 1276.9964599609375\n",
      "epoch 77, val_loss 454.6336975097656\n",
      "epoch 78, train_loss 1276.9964599609375\n",
      "epoch 78, val_loss 454.6336364746094\n",
      "epoch 79, train_loss 1276.996337890625\n",
      "epoch 79, val_loss 454.6336669921875\n",
      "epoch 80, train_loss 1276.9964599609375\n",
      "epoch 80, val_loss 454.6336364746094\n",
      "epoch 81, train_loss 1276.996337890625\n",
      "epoch 81, val_loss 454.6336364746094\n",
      "epoch 82, train_loss 1276.996337890625\n",
      "epoch 82, val_loss 454.633544921875\n",
      "epoch 83, train_loss 1276.9962158203125\n",
      "epoch 83, val_loss 454.6335754394531\n",
      "epoch 84, train_loss 1276.9962158203125\n",
      "epoch 84, val_loss 454.6335144042969\n",
      "epoch 85, train_loss 1276.99609375\n",
      "epoch 85, val_loss 454.6334228515625\n",
      "epoch 86, train_loss 1276.9959716796875\n",
      "epoch 86, val_loss 454.6334228515625\n",
      "epoch 87, train_loss 1276.9962158203125\n",
      "epoch 87, val_loss 454.63336181640625\n",
      "epoch 88, train_loss 1276.9962158203125\n",
      "epoch 88, val_loss 454.6334228515625\n",
      "epoch 89, train_loss 1276.9962158203125\n",
      "epoch 89, val_loss 454.63336181640625\n",
      "epoch 90, train_loss 1276.9962158203125\n",
      "epoch 90, val_loss 454.6333312988281\n",
      "epoch 91, train_loss 1276.9962158203125\n",
      "epoch 91, val_loss 454.63330078125\n",
      "epoch 92, train_loss 1276.9962158203125\n",
      "epoch 92, val_loss 454.63330078125\n",
      "epoch 93, train_loss 1276.9962158203125\n",
      "epoch 93, val_loss 454.63330078125\n",
      "epoch 94, train_loss 1276.9962158203125\n",
      "epoch 94, val_loss 454.63323974609375\n",
      "epoch 95, train_loss 1276.9959716796875\n",
      "epoch 95, val_loss 454.63323974609375\n",
      "epoch 96, train_loss 1276.9959716796875\n",
      "epoch 96, val_loss 454.6331787109375\n",
      "epoch 97, train_loss 1276.9959716796875\n",
      "epoch 97, val_loss 454.6331787109375\n",
      "epoch 98, train_loss 1276.9959716796875\n",
      "epoch 98, val_loss 454.63311767578125\n",
      "epoch 99, train_loss 1276.9959716796875\n",
      "epoch 99, val_loss 454.63311767578125\n",
      "Parameter containing:\n",
      "tensor([1.8215e-33], requires_grad=True)\n",
      "iter 187, train_loss_regularization 0.6841140389442444\n",
      "iter 187, val_loss_regularization 0.6841140389442444\n",
      "epoch 0, train_loss 1276.9959716796875\n",
      "epoch 0, val_loss 454.63311767578125\n",
      "epoch 1, train_loss 1276.9959716796875\n",
      "epoch 1, val_loss 454.6330871582031\n",
      "epoch 2, train_loss 1276.9959716796875\n",
      "epoch 2, val_loss 454.63299560546875\n",
      "epoch 3, train_loss 1276.995849609375\n",
      "epoch 3, val_loss 454.63299560546875\n",
      "epoch 4, train_loss 1276.9959716796875\n",
      "epoch 4, val_loss 454.63299560546875\n",
      "epoch 5, train_loss 1276.995849609375\n",
      "epoch 5, val_loss 454.6329040527344\n",
      "epoch 6, train_loss 1276.995849609375\n",
      "epoch 6, val_loss 454.6329650878906\n",
      "epoch 7, train_loss 1276.9957275390625\n",
      "epoch 7, val_loss 454.6329040527344\n",
      "epoch 8, train_loss 1276.9957275390625\n",
      "epoch 8, val_loss 454.63287353515625\n",
      "epoch 9, train_loss 1276.9957275390625\n",
      "epoch 9, val_loss 454.63287353515625\n",
      "epoch 10, train_loss 1276.99560546875\n",
      "epoch 10, val_loss 454.6328430175781\n",
      "epoch 11, train_loss 1276.9957275390625\n",
      "epoch 11, val_loss 454.6327819824219\n",
      "epoch 12, train_loss 1276.99560546875\n",
      "epoch 12, val_loss 454.63275146484375\n",
      "epoch 13, train_loss 1276.99560546875\n",
      "epoch 13, val_loss 454.6327209472656\n",
      "epoch 14, train_loss 1276.99560546875\n",
      "epoch 14, val_loss 454.6327209472656\n",
      "epoch 15, train_loss 1276.9957275390625\n",
      "epoch 15, val_loss 454.6326599121094\n",
      "epoch 16, train_loss 1276.9957275390625\n",
      "epoch 16, val_loss 454.6326599121094\n",
      "epoch 17, train_loss 1276.9957275390625\n",
      "epoch 17, val_loss 454.6326599121094\n",
      "epoch 18, train_loss 1276.9957275390625\n",
      "epoch 18, val_loss 454.63262939453125\n",
      "epoch 19, train_loss 1276.99560546875\n",
      "epoch 19, val_loss 454.632568359375\n",
      "epoch 20, train_loss 1276.9954833984375\n",
      "epoch 20, val_loss 454.632568359375\n",
      "epoch 21, train_loss 1276.9954833984375\n",
      "epoch 21, val_loss 454.632568359375\n",
      "epoch 22, train_loss 1276.995361328125\n",
      "epoch 22, val_loss 454.6325378417969\n",
      "epoch 23, train_loss 1276.995361328125\n",
      "epoch 23, val_loss 454.6325378417969\n",
      "epoch 24, train_loss 1276.9954833984375\n",
      "epoch 24, val_loss 454.6325378417969\n",
      "epoch 25, train_loss 1276.99560546875\n",
      "epoch 25, val_loss 454.63250732421875\n",
      "epoch 26, train_loss 1276.99560546875\n",
      "epoch 26, val_loss 454.6324462890625\n",
      "epoch 27, train_loss 1276.9954833984375\n",
      "epoch 27, val_loss 454.6324157714844\n",
      "epoch 28, train_loss 1276.9954833984375\n",
      "epoch 28, val_loss 454.63232421875\n",
      "epoch 29, train_loss 1276.995361328125\n",
      "epoch 29, val_loss 454.63232421875\n",
      "epoch 30, train_loss 1276.995361328125\n",
      "epoch 30, val_loss 454.63232421875\n",
      "epoch 31, train_loss 1276.995361328125\n",
      "epoch 31, val_loss 454.63226318359375\n",
      "epoch 32, train_loss 1276.995361328125\n",
      "epoch 32, val_loss 454.63226318359375\n",
      "epoch 33, train_loss 1276.9951171875\n",
      "epoch 33, val_loss 454.63226318359375\n",
      "epoch 34, train_loss 1276.9952392578125\n",
      "epoch 34, val_loss 454.6322021484375\n",
      "epoch 35, train_loss 1276.9952392578125\n",
      "epoch 35, val_loss 454.6322021484375\n",
      "epoch 36, train_loss 1276.995361328125\n",
      "epoch 36, val_loss 454.6321716308594\n",
      "epoch 37, train_loss 1276.995361328125\n",
      "epoch 37, val_loss 454.6321716308594\n",
      "epoch 38, train_loss 1276.9952392578125\n",
      "epoch 38, val_loss 454.632080078125\n",
      "epoch 39, train_loss 1276.995361328125\n",
      "epoch 39, val_loss 454.6321105957031\n",
      "epoch 40, train_loss 1276.9952392578125\n",
      "epoch 40, val_loss 454.6321105957031\n",
      "epoch 41, train_loss 1276.9952392578125\n",
      "epoch 41, val_loss 454.632080078125\n",
      "epoch 42, train_loss 1276.9952392578125\n",
      "epoch 42, val_loss 454.6320495605469\n",
      "epoch 43, train_loss 1276.9952392578125\n",
      "epoch 43, val_loss 454.6319885253906\n",
      "epoch 44, train_loss 1276.9951171875\n",
      "epoch 44, val_loss 454.6319580078125\n",
      "epoch 45, train_loss 1276.9951171875\n",
      "epoch 45, val_loss 454.6319580078125\n",
      "epoch 46, train_loss 1276.9951171875\n",
      "epoch 46, val_loss 454.6319580078125\n",
      "epoch 47, train_loss 1276.9951171875\n",
      "epoch 47, val_loss 454.6319274902344\n",
      "epoch 48, train_loss 1276.9951171875\n",
      "epoch 48, val_loss 454.6318664550781\n",
      "epoch 49, train_loss 1276.9951171875\n",
      "epoch 49, val_loss 454.6318359375\n",
      "epoch 50, train_loss 1276.9951171875\n",
      "epoch 50, val_loss 454.6318359375\n",
      "epoch 51, train_loss 1276.9949951171875\n",
      "epoch 51, val_loss 454.6318359375\n",
      "epoch 52, train_loss 1276.9949951171875\n",
      "epoch 52, val_loss 454.6318054199219\n",
      "epoch 53, train_loss 1276.9949951171875\n",
      "epoch 53, val_loss 454.6317443847656\n",
      "epoch 54, train_loss 1276.994873046875\n",
      "epoch 54, val_loss 454.6317138671875\n",
      "epoch 55, train_loss 1276.994873046875\n",
      "epoch 55, val_loss 454.6317138671875\n",
      "epoch 56, train_loss 1276.9947509765625\n",
      "epoch 56, val_loss 454.6316223144531\n",
      "epoch 57, train_loss 1276.994873046875\n",
      "epoch 57, val_loss 454.6316223144531\n",
      "epoch 58, train_loss 1276.994873046875\n",
      "epoch 58, val_loss 454.6316223144531\n",
      "epoch 59, train_loss 1276.994873046875\n",
      "epoch 59, val_loss 454.6316223144531\n",
      "epoch 60, train_loss 1276.994873046875\n",
      "epoch 60, val_loss 454.63153076171875\n",
      "epoch 61, train_loss 1276.994873046875\n",
      "epoch 61, val_loss 454.6315002441406\n",
      "epoch 62, train_loss 1276.9947509765625\n",
      "epoch 62, val_loss 454.63153076171875\n",
      "epoch 63, train_loss 1276.9947509765625\n",
      "epoch 63, val_loss 454.6315002441406\n",
      "epoch 64, train_loss 1276.9947509765625\n",
      "epoch 64, val_loss 454.6315002441406\n",
      "epoch 65, train_loss 1276.994873046875\n",
      "epoch 65, val_loss 454.6314697265625\n",
      "epoch 66, train_loss 1276.9947509765625\n",
      "epoch 66, val_loss 454.63140869140625\n",
      "epoch 67, train_loss 1276.99462890625\n",
      "epoch 67, val_loss 454.6313781738281\n",
      "epoch 68, train_loss 1276.99462890625\n",
      "epoch 68, val_loss 454.63140869140625\n",
      "epoch 69, train_loss 1276.99462890625\n",
      "epoch 69, val_loss 454.6313781738281\n",
      "epoch 70, train_loss 1276.99462890625\n",
      "epoch 70, val_loss 454.6313781738281\n",
      "epoch 71, train_loss 1276.99462890625\n",
      "epoch 71, val_loss 454.6313781738281\n",
      "epoch 72, train_loss 1276.9947509765625\n",
      "epoch 72, val_loss 454.6312561035156\n",
      "epoch 73, train_loss 1276.99462890625\n",
      "epoch 73, val_loss 454.6312561035156\n",
      "epoch 74, train_loss 1276.9945068359375\n",
      "epoch 74, val_loss 454.6312561035156\n",
      "epoch 75, train_loss 1276.99462890625\n",
      "epoch 75, val_loss 454.63116455078125\n",
      "epoch 76, train_loss 1276.99462890625\n",
      "epoch 76, val_loss 454.63116455078125\n",
      "epoch 77, train_loss 1276.99462890625\n",
      "epoch 77, val_loss 454.6311340332031\n",
      "epoch 78, train_loss 1276.9945068359375\n",
      "epoch 78, val_loss 454.63104248046875\n",
      "epoch 79, train_loss 1276.9945068359375\n",
      "epoch 79, val_loss 454.6310729980469\n",
      "epoch 80, train_loss 1276.9945068359375\n",
      "epoch 80, val_loss 454.63104248046875\n",
      "epoch 81, train_loss 1276.994384765625\n",
      "epoch 81, val_loss 454.63104248046875\n",
      "epoch 82, train_loss 1276.994384765625\n",
      "epoch 82, val_loss 454.63104248046875\n",
      "epoch 83, train_loss 1276.994384765625\n",
      "epoch 83, val_loss 454.6309509277344\n",
      "epoch 84, train_loss 1276.994384765625\n",
      "epoch 84, val_loss 454.6309509277344\n",
      "epoch 85, train_loss 1276.994384765625\n",
      "epoch 85, val_loss 454.63092041015625\n",
      "epoch 86, train_loss 1276.994384765625\n",
      "epoch 86, val_loss 454.63092041015625\n",
      "epoch 87, train_loss 1276.994384765625\n",
      "epoch 87, val_loss 454.6308898925781\n",
      "epoch 88, train_loss 1276.994384765625\n",
      "epoch 88, val_loss 454.6308898925781\n",
      "epoch 89, train_loss 1276.994384765625\n",
      "epoch 89, val_loss 454.63079833984375\n",
      "epoch 90, train_loss 1276.9942626953125\n",
      "epoch 90, val_loss 454.63079833984375\n",
      "epoch 91, train_loss 1276.994140625\n",
      "epoch 91, val_loss 454.63079833984375\n",
      "epoch 92, train_loss 1276.994140625\n",
      "epoch 92, val_loss 454.63079833984375\n",
      "epoch 93, train_loss 1276.994140625\n",
      "epoch 93, val_loss 454.6307373046875\n",
      "epoch 94, train_loss 1276.9942626953125\n",
      "epoch 94, val_loss 454.6307373046875\n",
      "epoch 95, train_loss 1276.9942626953125\n",
      "epoch 95, val_loss 454.6307373046875\n",
      "epoch 96, train_loss 1276.9942626953125\n",
      "epoch 96, val_loss 454.63067626953125\n",
      "epoch 97, train_loss 1276.994140625\n",
      "epoch 97, val_loss 454.63067626953125\n",
      "epoch 98, train_loss 1276.9940185546875\n",
      "epoch 98, val_loss 454.630615234375\n",
      "epoch 99, train_loss 1276.994140625\n",
      "epoch 99, val_loss 454.6305847167969\n",
      "Parameter containing:\n",
      "tensor([1.2752e-33], requires_grad=True)\n",
      "iter 188, train_loss_regularization 0.6840265393257141\n",
      "iter 188, val_loss_regularization 0.6840265393257141\n",
      "epoch 0, train_loss 1276.994140625\n",
      "epoch 0, val_loss 454.6305847167969\n",
      "epoch 1, train_loss 1276.994140625\n",
      "epoch 1, val_loss 454.63055419921875\n",
      "epoch 2, train_loss 1276.9940185546875\n",
      "epoch 2, val_loss 454.6304626464844\n",
      "epoch 3, train_loss 1276.9940185546875\n",
      "epoch 3, val_loss 454.6304626464844\n",
      "epoch 4, train_loss 1276.9940185546875\n",
      "epoch 4, val_loss 454.6304626464844\n",
      "epoch 5, train_loss 1276.9940185546875\n",
      "epoch 5, val_loss 454.6304016113281\n",
      "epoch 6, train_loss 1276.9940185546875\n",
      "epoch 6, val_loss 454.6304626464844\n",
      "epoch 7, train_loss 1276.993896484375\n",
      "epoch 7, val_loss 454.63037109375\n",
      "epoch 8, train_loss 1276.993896484375\n",
      "epoch 8, val_loss 454.6303405761719\n",
      "epoch 9, train_loss 1276.993896484375\n",
      "epoch 9, val_loss 454.6303405761719\n",
      "epoch 10, train_loss 1276.993896484375\n",
      "epoch 10, val_loss 454.6303405761719\n",
      "epoch 11, train_loss 1276.993896484375\n",
      "epoch 11, val_loss 454.6302795410156\n",
      "epoch 12, train_loss 1276.9940185546875\n",
      "epoch 12, val_loss 454.6302795410156\n",
      "epoch 13, train_loss 1276.9937744140625\n",
      "epoch 13, val_loss 454.6302490234375\n",
      "epoch 14, train_loss 1276.9937744140625\n",
      "epoch 14, val_loss 454.6302185058594\n",
      "epoch 15, train_loss 1276.9937744140625\n",
      "epoch 15, val_loss 454.6302185058594\n",
      "epoch 16, train_loss 1276.9937744140625\n",
      "epoch 16, val_loss 454.6301574707031\n",
      "epoch 17, train_loss 1276.99365234375\n",
      "epoch 17, val_loss 454.6301574707031\n",
      "epoch 18, train_loss 1276.99365234375\n",
      "epoch 18, val_loss 454.630126953125\n",
      "epoch 19, train_loss 1276.99365234375\n",
      "epoch 19, val_loss 454.630126953125\n",
      "epoch 20, train_loss 1276.99365234375\n",
      "epoch 20, val_loss 454.6300964355469\n",
      "epoch 21, train_loss 1276.99365234375\n",
      "epoch 21, val_loss 454.6300048828125\n",
      "epoch 22, train_loss 1276.9937744140625\n",
      "epoch 22, val_loss 454.6300354003906\n",
      "epoch 23, train_loss 1276.9937744140625\n",
      "epoch 23, val_loss 454.6300354003906\n",
      "epoch 24, train_loss 1276.9937744140625\n",
      "epoch 24, val_loss 454.6299133300781\n",
      "epoch 25, train_loss 1276.99365234375\n",
      "epoch 25, val_loss 454.6299133300781\n",
      "epoch 26, train_loss 1276.99365234375\n",
      "epoch 26, val_loss 454.6299133300781\n",
      "epoch 27, train_loss 1276.99365234375\n",
      "epoch 27, val_loss 454.6298828125\n",
      "epoch 28, train_loss 1276.99365234375\n",
      "epoch 28, val_loss 454.6298828125\n",
      "epoch 29, train_loss 1276.9935302734375\n",
      "epoch 29, val_loss 454.62982177734375\n",
      "epoch 30, train_loss 1276.9935302734375\n",
      "epoch 30, val_loss 454.62982177734375\n",
      "epoch 31, train_loss 1276.9935302734375\n",
      "epoch 31, val_loss 454.6297607421875\n",
      "epoch 32, train_loss 1276.9935302734375\n",
      "epoch 32, val_loss 454.6297607421875\n",
      "epoch 33, train_loss 1276.9935302734375\n",
      "epoch 33, val_loss 454.6297607421875\n",
      "epoch 34, train_loss 1276.9935302734375\n",
      "epoch 34, val_loss 454.62969970703125\n",
      "epoch 35, train_loss 1276.9935302734375\n",
      "epoch 35, val_loss 454.6297607421875\n",
      "epoch 36, train_loss 1276.9935302734375\n",
      "epoch 36, val_loss 454.62969970703125\n",
      "epoch 37, train_loss 1276.9935302734375\n",
      "epoch 37, val_loss 454.629638671875\n",
      "epoch 38, train_loss 1276.9935302734375\n",
      "epoch 38, val_loss 454.629638671875\n",
      "epoch 39, train_loss 1276.993408203125\n",
      "epoch 39, val_loss 454.629638671875\n",
      "epoch 40, train_loss 1276.9932861328125\n",
      "epoch 40, val_loss 454.62957763671875\n",
      "epoch 41, train_loss 1276.9932861328125\n",
      "epoch 41, val_loss 454.62957763671875\n",
      "epoch 42, train_loss 1276.9932861328125\n",
      "epoch 42, val_loss 454.6294860839844\n",
      "epoch 43, train_loss 1276.9932861328125\n",
      "epoch 43, val_loss 454.6294860839844\n",
      "epoch 44, train_loss 1276.9932861328125\n",
      "epoch 44, val_loss 454.6294860839844\n",
      "epoch 45, train_loss 1276.993408203125\n",
      "epoch 45, val_loss 454.6293640136719\n",
      "epoch 46, train_loss 1276.9932861328125\n",
      "epoch 46, val_loss 454.6293640136719\n",
      "epoch 47, train_loss 1276.9932861328125\n",
      "epoch 47, val_loss 454.6293640136719\n",
      "epoch 48, train_loss 1276.9932861328125\n",
      "epoch 48, val_loss 454.62933349609375\n",
      "epoch 49, train_loss 1276.9931640625\n",
      "epoch 49, val_loss 454.6293029785156\n",
      "epoch 50, train_loss 1276.9931640625\n",
      "epoch 50, val_loss 454.6293029785156\n",
      "epoch 51, train_loss 1276.9930419921875\n",
      "epoch 51, val_loss 454.6293029785156\n",
      "epoch 52, train_loss 1276.9930419921875\n",
      "epoch 52, val_loss 454.6293029785156\n",
      "epoch 53, train_loss 1276.9930419921875\n",
      "epoch 53, val_loss 454.6292419433594\n",
      "epoch 54, train_loss 1276.9930419921875\n",
      "epoch 54, val_loss 454.62921142578125\n",
      "epoch 55, train_loss 1276.9930419921875\n",
      "epoch 55, val_loss 454.62921142578125\n",
      "epoch 56, train_loss 1276.9930419921875\n",
      "epoch 56, val_loss 454.6291809082031\n",
      "epoch 57, train_loss 1276.9930419921875\n",
      "epoch 57, val_loss 454.6291809082031\n",
      "epoch 58, train_loss 1276.9930419921875\n",
      "epoch 58, val_loss 454.6291198730469\n",
      "epoch 59, train_loss 1276.9930419921875\n",
      "epoch 59, val_loss 454.6291198730469\n",
      "epoch 60, train_loss 1276.992919921875\n",
      "epoch 60, val_loss 454.62908935546875\n",
      "epoch 61, train_loss 1276.992919921875\n",
      "epoch 61, val_loss 454.6290283203125\n",
      "epoch 62, train_loss 1276.9927978515625\n",
      "epoch 62, val_loss 454.6289978027344\n",
      "epoch 63, train_loss 1276.9930419921875\n",
      "epoch 63, val_loss 454.6289978027344\n",
      "epoch 64, train_loss 1276.992919921875\n",
      "epoch 64, val_loss 454.6289978027344\n",
      "epoch 65, train_loss 1276.992919921875\n",
      "epoch 65, val_loss 454.62896728515625\n",
      "epoch 66, train_loss 1276.992919921875\n",
      "epoch 66, val_loss 454.6288757324219\n",
      "epoch 67, train_loss 1276.992919921875\n",
      "epoch 67, val_loss 454.6288757324219\n",
      "epoch 68, train_loss 1276.9927978515625\n",
      "epoch 68, val_loss 454.6288757324219\n",
      "epoch 69, train_loss 1276.9927978515625\n",
      "epoch 69, val_loss 454.62884521484375\n",
      "epoch 70, train_loss 1276.9927978515625\n",
      "epoch 70, val_loss 454.62884521484375\n",
      "epoch 71, train_loss 1276.9930419921875\n",
      "epoch 71, val_loss 454.6287841796875\n",
      "epoch 72, train_loss 1276.9927978515625\n",
      "epoch 72, val_loss 454.6287536621094\n",
      "epoch 73, train_loss 1276.9927978515625\n",
      "epoch 73, val_loss 454.6287536621094\n",
      "epoch 74, train_loss 1276.9927978515625\n",
      "epoch 74, val_loss 454.62872314453125\n",
      "epoch 75, train_loss 1276.9927978515625\n",
      "epoch 75, val_loss 454.62872314453125\n",
      "epoch 76, train_loss 1276.99267578125\n",
      "epoch 76, val_loss 454.628662109375\n",
      "epoch 77, train_loss 1276.9927978515625\n",
      "epoch 77, val_loss 454.6285705566406\n",
      "epoch 78, train_loss 1276.9927978515625\n",
      "epoch 78, val_loss 454.6285705566406\n",
      "epoch 79, train_loss 1276.9927978515625\n",
      "epoch 79, val_loss 454.6285705566406\n",
      "epoch 80, train_loss 1276.99267578125\n",
      "epoch 80, val_loss 454.6285705566406\n",
      "epoch 81, train_loss 1276.99267578125\n",
      "epoch 81, val_loss 454.6285705566406\n",
      "epoch 82, train_loss 1276.99267578125\n",
      "epoch 82, val_loss 454.6285400390625\n",
      "epoch 83, train_loss 1276.99267578125\n",
      "epoch 83, val_loss 454.6285095214844\n",
      "epoch 84, train_loss 1276.99267578125\n",
      "epoch 84, val_loss 454.6284484863281\n",
      "epoch 85, train_loss 1276.9925537109375\n",
      "epoch 85, val_loss 454.6284484863281\n",
      "epoch 86, train_loss 1276.9925537109375\n",
      "epoch 86, val_loss 454.62841796875\n",
      "epoch 87, train_loss 1276.9923095703125\n",
      "epoch 87, val_loss 454.62841796875\n",
      "epoch 88, train_loss 1276.9925537109375\n",
      "epoch 88, val_loss 454.62841796875\n",
      "epoch 89, train_loss 1276.992431640625\n",
      "epoch 89, val_loss 454.6283874511719\n",
      "epoch 90, train_loss 1276.9925537109375\n",
      "epoch 90, val_loss 454.6282958984375\n",
      "epoch 91, train_loss 1276.9925537109375\n",
      "epoch 91, val_loss 454.62823486328125\n",
      "epoch 92, train_loss 1276.9925537109375\n",
      "epoch 92, val_loss 454.62823486328125\n",
      "epoch 93, train_loss 1276.9925537109375\n",
      "epoch 93, val_loss 454.6282043457031\n",
      "epoch 94, train_loss 1276.9925537109375\n",
      "epoch 94, val_loss 454.6282043457031\n",
      "epoch 95, train_loss 1276.9925537109375\n",
      "epoch 95, val_loss 454.62811279296875\n",
      "epoch 96, train_loss 1276.9923095703125\n",
      "epoch 96, val_loss 454.62811279296875\n",
      "epoch 97, train_loss 1276.9923095703125\n",
      "epoch 97, val_loss 454.628173828125\n",
      "epoch 98, train_loss 1276.9923095703125\n",
      "epoch 98, val_loss 454.62811279296875\n",
      "epoch 99, train_loss 1276.9921875\n",
      "epoch 99, val_loss 454.62811279296875\n",
      "Parameter containing:\n",
      "tensor([8.9277e-34], requires_grad=True)\n",
      "iter 189, train_loss_regularization 0.6839498281478882\n",
      "iter 189, val_loss_regularization 0.6839498281478882\n",
      "epoch 0, train_loss 1276.9921875\n",
      "epoch 0, val_loss 454.6280822753906\n",
      "epoch 1, train_loss 1276.9923095703125\n",
      "epoch 1, val_loss 454.6280517578125\n",
      "epoch 2, train_loss 1276.9921875\n",
      "epoch 2, val_loss 454.62799072265625\n",
      "epoch 3, train_loss 1276.9921875\n",
      "epoch 3, val_loss 454.62799072265625\n",
      "epoch 4, train_loss 1276.9921875\n",
      "epoch 4, val_loss 454.6279602050781\n",
      "epoch 5, train_loss 1276.9921875\n",
      "epoch 5, val_loss 454.6279602050781\n",
      "epoch 6, train_loss 1276.9923095703125\n",
      "epoch 6, val_loss 454.6279602050781\n",
      "epoch 7, train_loss 1276.9923095703125\n",
      "epoch 7, val_loss 454.62786865234375\n",
      "epoch 8, train_loss 1276.9921875\n",
      "epoch 8, val_loss 454.6278381347656\n",
      "epoch 9, train_loss 1276.9921875\n",
      "epoch 9, val_loss 454.6278381347656\n",
      "epoch 10, train_loss 1276.9921875\n",
      "epoch 10, val_loss 454.6278381347656\n",
      "epoch 11, train_loss 1276.9921875\n",
      "epoch 11, val_loss 454.6278381347656\n",
      "epoch 12, train_loss 1276.9920654296875\n",
      "epoch 12, val_loss 454.62774658203125\n",
      "epoch 13, train_loss 1276.991943359375\n",
      "epoch 13, val_loss 454.62774658203125\n",
      "epoch 14, train_loss 1276.991943359375\n",
      "epoch 14, val_loss 454.6277160644531\n",
      "epoch 15, train_loss 1276.991943359375\n",
      "epoch 15, val_loss 454.6277160644531\n",
      "epoch 16, train_loss 1276.9921875\n",
      "epoch 16, val_loss 454.6276550292969\n",
      "epoch 17, train_loss 1276.991943359375\n",
      "epoch 17, val_loss 454.6276550292969\n",
      "epoch 18, train_loss 1276.991943359375\n",
      "epoch 18, val_loss 454.62762451171875\n",
      "epoch 19, train_loss 1276.991943359375\n",
      "epoch 19, val_loss 454.6275939941406\n",
      "epoch 20, train_loss 1276.9918212890625\n",
      "epoch 20, val_loss 454.62750244140625\n",
      "epoch 21, train_loss 1276.991943359375\n",
      "epoch 21, val_loss 454.62750244140625\n",
      "epoch 22, train_loss 1276.991943359375\n",
      "epoch 22, val_loss 454.6275329589844\n",
      "epoch 23, train_loss 1276.9918212890625\n",
      "epoch 23, val_loss 454.62750244140625\n",
      "epoch 24, train_loss 1276.99169921875\n",
      "epoch 24, val_loss 454.6274719238281\n",
      "epoch 25, train_loss 1276.99169921875\n",
      "epoch 25, val_loss 454.6274108886719\n",
      "epoch 26, train_loss 1276.99169921875\n",
      "epoch 26, val_loss 454.6274108886719\n",
      "epoch 27, train_loss 1276.9918212890625\n",
      "epoch 27, val_loss 454.6274108886719\n",
      "epoch 28, train_loss 1276.991943359375\n",
      "epoch 28, val_loss 454.6274108886719\n",
      "epoch 29, train_loss 1276.991943359375\n",
      "epoch 29, val_loss 454.62738037109375\n",
      "epoch 30, train_loss 1276.991943359375\n",
      "epoch 30, val_loss 454.62738037109375\n",
      "epoch 31, train_loss 1276.9918212890625\n",
      "epoch 31, val_loss 454.6272888183594\n",
      "epoch 32, train_loss 1276.99169921875\n",
      "epoch 32, val_loss 454.6272888183594\n",
      "epoch 33, train_loss 1276.99169921875\n",
      "epoch 33, val_loss 454.6272888183594\n",
      "epoch 34, train_loss 1276.99169921875\n",
      "epoch 34, val_loss 454.62725830078125\n",
      "epoch 35, train_loss 1276.99169921875\n",
      "epoch 35, val_loss 454.62725830078125\n",
      "epoch 36, train_loss 1276.99169921875\n",
      "epoch 36, val_loss 454.6271667480469\n",
      "epoch 37, train_loss 1276.99169921875\n",
      "epoch 37, val_loss 454.6270751953125\n",
      "epoch 38, train_loss 1276.99169921875\n",
      "epoch 38, val_loss 454.6270751953125\n",
      "epoch 39, train_loss 1276.9915771484375\n",
      "epoch 39, val_loss 454.6270751953125\n",
      "epoch 40, train_loss 1276.991455078125\n",
      "epoch 40, val_loss 454.6270751953125\n",
      "epoch 41, train_loss 1276.991455078125\n",
      "epoch 41, val_loss 454.6270446777344\n",
      "epoch 42, train_loss 1276.991455078125\n",
      "epoch 42, val_loss 454.6270446777344\n",
      "epoch 43, train_loss 1276.991455078125\n",
      "epoch 43, val_loss 454.626953125\n",
      "epoch 44, train_loss 1276.991455078125\n",
      "epoch 44, val_loss 454.626953125\n",
      "epoch 45, train_loss 1276.991455078125\n",
      "epoch 45, val_loss 454.626953125\n",
      "epoch 46, train_loss 1276.991455078125\n",
      "epoch 46, val_loss 454.626953125\n",
      "epoch 47, train_loss 1276.991455078125\n",
      "epoch 47, val_loss 454.626953125\n",
      "epoch 48, train_loss 1276.9912109375\n",
      "epoch 48, val_loss 454.6268615722656\n",
      "epoch 49, train_loss 1276.9912109375\n",
      "epoch 49, val_loss 454.6268615722656\n",
      "epoch 50, train_loss 1276.9912109375\n",
      "epoch 50, val_loss 454.6268310546875\n",
      "epoch 51, train_loss 1276.9913330078125\n",
      "epoch 51, val_loss 454.6268310546875\n",
      "epoch 52, train_loss 1276.9913330078125\n",
      "epoch 52, val_loss 454.6267395019531\n",
      "epoch 53, train_loss 1276.9913330078125\n",
      "epoch 53, val_loss 454.6267395019531\n",
      "epoch 54, train_loss 1276.9913330078125\n",
      "epoch 54, val_loss 454.6268005371094\n",
      "epoch 55, train_loss 1276.9913330078125\n",
      "epoch 55, val_loss 454.626708984375\n",
      "epoch 56, train_loss 1276.9912109375\n",
      "epoch 56, val_loss 454.6266784667969\n",
      "epoch 57, train_loss 1276.9912109375\n",
      "epoch 57, val_loss 454.6266784667969\n",
      "epoch 58, train_loss 1276.9912109375\n",
      "epoch 58, val_loss 454.6266784667969\n",
      "epoch 59, train_loss 1276.9912109375\n",
      "epoch 59, val_loss 454.6265869140625\n",
      "epoch 60, train_loss 1276.9912109375\n",
      "epoch 60, val_loss 454.6265563964844\n",
      "epoch 61, train_loss 1276.9912109375\n",
      "epoch 61, val_loss 454.6265563964844\n",
      "epoch 62, train_loss 1276.9912109375\n",
      "epoch 62, val_loss 454.6265563964844\n",
      "epoch 63, train_loss 1276.9912109375\n",
      "epoch 63, val_loss 454.6265563964844\n",
      "epoch 64, train_loss 1276.9910888671875\n",
      "epoch 64, val_loss 454.62646484375\n",
      "epoch 65, train_loss 1276.9912109375\n",
      "epoch 65, val_loss 454.62646484375\n",
      "epoch 66, train_loss 1276.9910888671875\n",
      "epoch 66, val_loss 454.62640380859375\n",
      "epoch 67, train_loss 1276.9912109375\n",
      "epoch 67, val_loss 454.6263732910156\n",
      "epoch 68, train_loss 1276.9910888671875\n",
      "epoch 68, val_loss 454.6263732910156\n",
      "epoch 69, train_loss 1276.9910888671875\n",
      "epoch 69, val_loss 454.62628173828125\n",
      "epoch 70, train_loss 1276.990966796875\n",
      "epoch 70, val_loss 454.6263427734375\n",
      "epoch 71, train_loss 1276.990966796875\n",
      "epoch 71, val_loss 454.6263427734375\n",
      "epoch 72, train_loss 1276.990966796875\n",
      "epoch 72, val_loss 454.6263427734375\n",
      "epoch 73, train_loss 1276.9910888671875\n",
      "epoch 73, val_loss 454.626220703125\n",
      "epoch 74, train_loss 1276.9910888671875\n",
      "epoch 74, val_loss 454.6262512207031\n",
      "epoch 75, train_loss 1276.990966796875\n",
      "epoch 75, val_loss 454.6262512207031\n",
      "epoch 76, train_loss 1276.990966796875\n",
      "epoch 76, val_loss 454.6262512207031\n",
      "epoch 77, train_loss 1276.990966796875\n",
      "epoch 77, val_loss 454.62615966796875\n",
      "epoch 78, train_loss 1276.990966796875\n",
      "epoch 78, val_loss 454.62615966796875\n",
      "epoch 79, train_loss 1276.9908447265625\n",
      "epoch 79, val_loss 454.62615966796875\n",
      "epoch 80, train_loss 1276.990966796875\n",
      "epoch 80, val_loss 454.6261291503906\n",
      "epoch 81, train_loss 1276.990966796875\n",
      "epoch 81, val_loss 454.6260681152344\n",
      "epoch 82, train_loss 1276.990966796875\n",
      "epoch 82, val_loss 454.6260070800781\n",
      "epoch 83, train_loss 1276.990966796875\n",
      "epoch 83, val_loss 454.6260681152344\n",
      "epoch 84, train_loss 1276.9908447265625\n",
      "epoch 84, val_loss 454.62591552734375\n",
      "epoch 85, train_loss 1276.99072265625\n",
      "epoch 85, val_loss 454.6259460449219\n",
      "epoch 86, train_loss 1276.99072265625\n",
      "epoch 86, val_loss 454.62591552734375\n",
      "epoch 87, train_loss 1276.99072265625\n",
      "epoch 87, val_loss 454.6258850097656\n",
      "epoch 88, train_loss 1276.99072265625\n",
      "epoch 88, val_loss 454.6258850097656\n",
      "epoch 89, train_loss 1276.99072265625\n",
      "epoch 89, val_loss 454.6258850097656\n",
      "epoch 90, train_loss 1276.99072265625\n",
      "epoch 90, val_loss 454.6258239746094\n",
      "epoch 91, train_loss 1276.99072265625\n",
      "epoch 91, val_loss 454.62579345703125\n",
      "epoch 92, train_loss 1276.9906005859375\n",
      "epoch 92, val_loss 454.6258239746094\n",
      "epoch 93, train_loss 1276.9906005859375\n",
      "epoch 93, val_loss 454.62579345703125\n",
      "epoch 94, train_loss 1276.9906005859375\n",
      "epoch 94, val_loss 454.6257629394531\n",
      "epoch 95, train_loss 1276.9906005859375\n",
      "epoch 95, val_loss 454.6257629394531\n",
      "epoch 96, train_loss 1276.9906005859375\n",
      "epoch 96, val_loss 454.6257629394531\n",
      "epoch 97, train_loss 1276.990478515625\n",
      "epoch 97, val_loss 454.6256103515625\n",
      "epoch 98, train_loss 1276.990478515625\n",
      "epoch 98, val_loss 454.6256103515625\n",
      "epoch 99, train_loss 1276.990478515625\n",
      "epoch 99, val_loss 454.6256103515625\n",
      "Parameter containing:\n",
      "tensor([6.2507e-34], requires_grad=True)\n",
      "iter 190, train_loss_regularization 0.6838840246200562\n",
      "iter 190, val_loss_regularization 0.6838840246200562\n",
      "epoch 0, train_loss 1276.9903564453125\n",
      "epoch 0, val_loss 454.6255798339844\n",
      "epoch 1, train_loss 1276.9903564453125\n",
      "epoch 1, val_loss 454.6255798339844\n",
      "epoch 2, train_loss 1276.9903564453125\n",
      "epoch 2, val_loss 454.62554931640625\n",
      "epoch 3, train_loss 1276.990478515625\n",
      "epoch 3, val_loss 454.62548828125\n",
      "epoch 4, train_loss 1276.9903564453125\n",
      "epoch 4, val_loss 454.62548828125\n",
      "epoch 5, train_loss 1276.9903564453125\n",
      "epoch 5, val_loss 454.6254577636719\n",
      "epoch 6, train_loss 1276.9903564453125\n",
      "epoch 6, val_loss 454.6254577636719\n",
      "epoch 7, train_loss 1276.9903564453125\n",
      "epoch 7, val_loss 454.62542724609375\n",
      "epoch 8, train_loss 1276.990234375\n",
      "epoch 8, val_loss 454.62542724609375\n",
      "epoch 9, train_loss 1276.990234375\n",
      "epoch 9, val_loss 454.6253662109375\n",
      "epoch 10, train_loss 1276.9901123046875\n",
      "epoch 10, val_loss 454.6253662109375\n",
      "epoch 11, train_loss 1276.9901123046875\n",
      "epoch 11, val_loss 454.6253356933594\n",
      "epoch 12, train_loss 1276.9901123046875\n",
      "epoch 12, val_loss 454.62530517578125\n",
      "epoch 13, train_loss 1276.9903564453125\n",
      "epoch 13, val_loss 454.625244140625\n",
      "epoch 14, train_loss 1276.9901123046875\n",
      "epoch 14, val_loss 454.625244140625\n",
      "epoch 15, train_loss 1276.9903564453125\n",
      "epoch 15, val_loss 454.6251525878906\n",
      "epoch 16, train_loss 1276.990234375\n",
      "epoch 16, val_loss 454.6251525878906\n",
      "epoch 17, train_loss 1276.990234375\n",
      "epoch 17, val_loss 454.6252136230469\n",
      "epoch 18, train_loss 1276.990234375\n",
      "epoch 18, val_loss 454.6251525878906\n",
      "epoch 19, train_loss 1276.990234375\n",
      "epoch 19, val_loss 454.6251525878906\n",
      "epoch 20, train_loss 1276.989990234375\n",
      "epoch 20, val_loss 454.6251220703125\n",
      "epoch 21, train_loss 1276.989990234375\n",
      "epoch 21, val_loss 454.6250305175781\n",
      "epoch 22, train_loss 1276.9901123046875\n",
      "epoch 22, val_loss 454.6250305175781\n",
      "epoch 23, train_loss 1276.9901123046875\n",
      "epoch 23, val_loss 454.6250305175781\n",
      "epoch 24, train_loss 1276.9901123046875\n",
      "epoch 24, val_loss 454.6250305175781\n",
      "epoch 25, train_loss 1276.9901123046875\n",
      "epoch 25, val_loss 454.6250305175781\n",
      "epoch 26, train_loss 1276.9901123046875\n",
      "epoch 26, val_loss 454.625\n",
      "epoch 27, train_loss 1276.9901123046875\n",
      "epoch 27, val_loss 454.6249084472656\n",
      "epoch 28, train_loss 1276.989990234375\n",
      "epoch 28, val_loss 454.6248474121094\n",
      "epoch 29, train_loss 1276.9898681640625\n",
      "epoch 29, val_loss 454.6248779296875\n",
      "epoch 30, train_loss 1276.9898681640625\n",
      "epoch 30, val_loss 454.6248779296875\n",
      "epoch 31, train_loss 1276.9898681640625\n",
      "epoch 31, val_loss 454.6248474121094\n",
      "epoch 32, train_loss 1276.9898681640625\n",
      "epoch 32, val_loss 454.624755859375\n",
      "epoch 33, train_loss 1276.9898681640625\n",
      "epoch 33, val_loss 454.624755859375\n",
      "epoch 34, train_loss 1276.9898681640625\n",
      "epoch 34, val_loss 454.624755859375\n",
      "epoch 35, train_loss 1276.9898681640625\n",
      "epoch 35, val_loss 454.62469482421875\n",
      "epoch 36, train_loss 1276.989990234375\n",
      "epoch 36, val_loss 454.62469482421875\n",
      "epoch 37, train_loss 1276.989990234375\n",
      "epoch 37, val_loss 454.6246643066406\n",
      "epoch 38, train_loss 1276.9898681640625\n",
      "epoch 38, val_loss 454.62469482421875\n",
      "epoch 39, train_loss 1276.9898681640625\n",
      "epoch 39, val_loss 454.6246337890625\n",
      "epoch 40, train_loss 1276.98974609375\n",
      "epoch 40, val_loss 454.6246337890625\n",
      "epoch 41, train_loss 1276.98974609375\n",
      "epoch 41, val_loss 454.6246337890625\n",
      "epoch 42, train_loss 1276.9896240234375\n",
      "epoch 42, val_loss 454.62457275390625\n",
      "epoch 43, train_loss 1276.9896240234375\n",
      "epoch 43, val_loss 454.6245422363281\n",
      "epoch 44, train_loss 1276.98974609375\n",
      "epoch 44, val_loss 454.62445068359375\n",
      "epoch 45, train_loss 1276.98974609375\n",
      "epoch 45, val_loss 454.62445068359375\n",
      "epoch 46, train_loss 1276.989501953125\n",
      "epoch 46, val_loss 454.62445068359375\n",
      "epoch 47, train_loss 1276.9896240234375\n",
      "epoch 47, val_loss 454.62445068359375\n",
      "epoch 48, train_loss 1276.9896240234375\n",
      "epoch 48, val_loss 454.62445068359375\n",
      "epoch 49, train_loss 1276.9896240234375\n",
      "epoch 49, val_loss 454.6244201660156\n",
      "epoch 50, train_loss 1276.9896240234375\n",
      "epoch 50, val_loss 454.6243896484375\n",
      "epoch 51, train_loss 1276.9896240234375\n",
      "epoch 51, val_loss 454.62432861328125\n",
      "epoch 52, train_loss 1276.9896240234375\n",
      "epoch 52, val_loss 454.62432861328125\n",
      "epoch 53, train_loss 1276.989501953125\n",
      "epoch 53, val_loss 454.6242370605469\n",
      "epoch 54, train_loss 1276.989501953125\n",
      "epoch 54, val_loss 454.6242370605469\n",
      "epoch 55, train_loss 1276.989501953125\n",
      "epoch 55, val_loss 454.6242980957031\n",
      "epoch 56, train_loss 1276.9893798828125\n",
      "epoch 56, val_loss 454.6241760253906\n",
      "epoch 57, train_loss 1276.9893798828125\n",
      "epoch 57, val_loss 454.6241760253906\n",
      "epoch 58, train_loss 1276.9893798828125\n",
      "epoch 58, val_loss 454.6241760253906\n",
      "epoch 59, train_loss 1276.989501953125\n",
      "epoch 59, val_loss 454.6241149902344\n",
      "epoch 60, train_loss 1276.989501953125\n",
      "epoch 60, val_loss 454.62408447265625\n",
      "epoch 61, train_loss 1276.989501953125\n",
      "epoch 61, val_loss 454.62408447265625\n",
      "epoch 62, train_loss 1276.9896240234375\n",
      "epoch 62, val_loss 454.6240539550781\n",
      "epoch 63, train_loss 1276.989501953125\n",
      "epoch 63, val_loss 454.6240539550781\n",
      "epoch 64, train_loss 1276.9893798828125\n",
      "epoch 64, val_loss 454.6240539550781\n",
      "epoch 65, train_loss 1276.9893798828125\n",
      "epoch 65, val_loss 454.6239929199219\n",
      "epoch 66, train_loss 1276.9893798828125\n",
      "epoch 66, val_loss 454.6239929199219\n",
      "epoch 67, train_loss 1276.9893798828125\n",
      "epoch 67, val_loss 454.62396240234375\n",
      "epoch 68, train_loss 1276.9893798828125\n",
      "epoch 68, val_loss 454.6239318847656\n",
      "epoch 69, train_loss 1276.9892578125\n",
      "epoch 69, val_loss 454.6239318847656\n",
      "epoch 70, train_loss 1276.9892578125\n",
      "epoch 70, val_loss 454.6239318847656\n",
      "epoch 71, train_loss 1276.9892578125\n",
      "epoch 71, val_loss 454.6238708496094\n",
      "epoch 72, train_loss 1276.9892578125\n",
      "epoch 72, val_loss 454.62384033203125\n",
      "epoch 73, train_loss 1276.9891357421875\n",
      "epoch 73, val_loss 454.62384033203125\n",
      "epoch 74, train_loss 1276.9892578125\n",
      "epoch 74, val_loss 454.6237487792969\n",
      "epoch 75, train_loss 1276.9891357421875\n",
      "epoch 75, val_loss 454.6237487792969\n",
      "epoch 76, train_loss 1276.9891357421875\n",
      "epoch 76, val_loss 454.62371826171875\n",
      "epoch 77, train_loss 1276.9891357421875\n",
      "epoch 77, val_loss 454.6236572265625\n",
      "epoch 78, train_loss 1276.9891357421875\n",
      "epoch 78, val_loss 454.6236572265625\n",
      "epoch 79, train_loss 1276.989013671875\n",
      "epoch 79, val_loss 454.6236572265625\n",
      "epoch 80, train_loss 1276.9891357421875\n",
      "epoch 80, val_loss 454.62359619140625\n",
      "epoch 81, train_loss 1276.9891357421875\n",
      "epoch 81, val_loss 454.6236267089844\n",
      "epoch 82, train_loss 1276.989013671875\n",
      "epoch 82, val_loss 454.62359619140625\n",
      "epoch 83, train_loss 1276.9888916015625\n",
      "epoch 83, val_loss 454.62359619140625\n",
      "epoch 84, train_loss 1276.989013671875\n",
      "epoch 84, val_loss 454.62353515625\n",
      "epoch 85, train_loss 1276.989013671875\n",
      "epoch 85, val_loss 454.6235046386719\n",
      "epoch 86, train_loss 1276.9891357421875\n",
      "epoch 86, val_loss 454.6235046386719\n",
      "epoch 87, train_loss 1276.989013671875\n",
      "epoch 87, val_loss 454.6234436035156\n",
      "epoch 88, train_loss 1276.9888916015625\n",
      "epoch 88, val_loss 454.6234436035156\n",
      "epoch 89, train_loss 1276.989013671875\n",
      "epoch 89, val_loss 454.6234130859375\n",
      "epoch 90, train_loss 1276.9888916015625\n",
      "epoch 90, val_loss 454.6234130859375\n",
      "epoch 91, train_loss 1276.9888916015625\n",
      "epoch 91, val_loss 454.6233825683594\n",
      "epoch 92, train_loss 1276.9888916015625\n",
      "epoch 92, val_loss 454.6233215332031\n",
      "epoch 93, train_loss 1276.98876953125\n",
      "epoch 93, val_loss 454.623291015625\n",
      "epoch 94, train_loss 1276.9888916015625\n",
      "epoch 94, val_loss 454.6233215332031\n",
      "epoch 95, train_loss 1276.9888916015625\n",
      "epoch 95, val_loss 454.623291015625\n",
      "epoch 96, train_loss 1276.98876953125\n",
      "epoch 96, val_loss 454.6232604980469\n",
      "epoch 97, train_loss 1276.98876953125\n",
      "epoch 97, val_loss 454.6232604980469\n",
      "epoch 98, train_loss 1276.98876953125\n",
      "epoch 98, val_loss 454.6231994628906\n",
      "epoch 99, train_loss 1276.98876953125\n",
      "epoch 99, val_loss 454.6231384277344\n",
      "Parameter containing:\n",
      "tensor([4.3766e-34], requires_grad=True)\n",
      "iter 191, train_loss_regularization 0.683828592300415\n",
      "iter 191, val_loss_regularization 0.683828592300415\n",
      "epoch 0, train_loss 1276.9886474609375\n",
      "epoch 0, val_loss 454.6231384277344\n",
      "epoch 1, train_loss 1276.988525390625\n",
      "epoch 1, val_loss 454.6231384277344\n",
      "epoch 2, train_loss 1276.988525390625\n",
      "epoch 2, val_loss 454.6231384277344\n",
      "epoch 3, train_loss 1276.988525390625\n",
      "epoch 3, val_loss 454.6230773925781\n",
      "epoch 4, train_loss 1276.988525390625\n",
      "epoch 4, val_loss 454.6230773925781\n",
      "epoch 5, train_loss 1276.98876953125\n",
      "epoch 5, val_loss 454.62298583984375\n",
      "epoch 6, train_loss 1276.98876953125\n",
      "epoch 6, val_loss 454.6229553222656\n",
      "epoch 7, train_loss 1276.9888916015625\n",
      "epoch 7, val_loss 454.6229248046875\n",
      "epoch 8, train_loss 1276.988525390625\n",
      "epoch 8, val_loss 454.6229553222656\n",
      "epoch 9, train_loss 1276.988525390625\n",
      "epoch 9, val_loss 454.6229553222656\n",
      "epoch 10, train_loss 1276.988525390625\n",
      "epoch 10, val_loss 454.6229553222656\n",
      "epoch 11, train_loss 1276.9886474609375\n",
      "epoch 11, val_loss 454.62286376953125\n",
      "epoch 12, train_loss 1276.988525390625\n",
      "epoch 12, val_loss 454.62286376953125\n",
      "epoch 13, train_loss 1276.988525390625\n",
      "epoch 13, val_loss 454.6228332519531\n",
      "epoch 14, train_loss 1276.988525390625\n",
      "epoch 14, val_loss 454.6228332519531\n",
      "epoch 15, train_loss 1276.988525390625\n",
      "epoch 15, val_loss 454.6228332519531\n",
      "epoch 16, train_loss 1276.9884033203125\n",
      "epoch 16, val_loss 454.622802734375\n",
      "epoch 17, train_loss 1276.9884033203125\n",
      "epoch 17, val_loss 454.62274169921875\n",
      "epoch 18, train_loss 1276.988525390625\n",
      "epoch 18, val_loss 454.6227111816406\n",
      "epoch 19, train_loss 1276.98828125\n",
      "epoch 19, val_loss 454.62274169921875\n",
      "epoch 20, train_loss 1276.98828125\n",
      "epoch 20, val_loss 454.6226806640625\n",
      "epoch 21, train_loss 1276.98828125\n",
      "epoch 21, val_loss 454.6226806640625\n",
      "epoch 22, train_loss 1276.98828125\n",
      "epoch 22, val_loss 454.62261962890625\n",
      "epoch 23, train_loss 1276.98828125\n",
      "epoch 23, val_loss 454.6225280761719\n",
      "epoch 24, train_loss 1276.98828125\n",
      "epoch 24, val_loss 454.62249755859375\n",
      "epoch 25, train_loss 1276.9881591796875\n",
      "epoch 25, val_loss 454.62249755859375\n",
      "epoch 26, train_loss 1276.98828125\n",
      "epoch 26, val_loss 454.62249755859375\n",
      "epoch 27, train_loss 1276.98828125\n",
      "epoch 27, val_loss 454.62249755859375\n",
      "epoch 28, train_loss 1276.98828125\n",
      "epoch 28, val_loss 454.6224060058594\n",
      "epoch 29, train_loss 1276.98828125\n",
      "epoch 29, val_loss 454.6224060058594\n",
      "epoch 30, train_loss 1276.98828125\n",
      "epoch 30, val_loss 454.6224060058594\n",
      "epoch 31, train_loss 1276.98828125\n",
      "epoch 31, val_loss 454.62237548828125\n",
      "epoch 32, train_loss 1276.98828125\n",
      "epoch 32, val_loss 454.62237548828125\n",
      "epoch 33, train_loss 1276.98828125\n",
      "epoch 33, val_loss 454.6224060058594\n",
      "epoch 34, train_loss 1276.9881591796875\n",
      "epoch 34, val_loss 454.6223449707031\n",
      "epoch 35, train_loss 1276.9881591796875\n",
      "epoch 35, val_loss 454.62225341796875\n",
      "epoch 36, train_loss 1276.988037109375\n",
      "epoch 36, val_loss 454.6222839355469\n",
      "epoch 37, train_loss 1276.988037109375\n",
      "epoch 37, val_loss 454.62225341796875\n",
      "epoch 38, train_loss 1276.9879150390625\n",
      "epoch 38, val_loss 454.6222229003906\n",
      "epoch 39, train_loss 1276.988037109375\n",
      "epoch 39, val_loss 454.62225341796875\n",
      "epoch 40, train_loss 1276.988037109375\n",
      "epoch 40, val_loss 454.6221618652344\n",
      "epoch 41, train_loss 1276.988037109375\n",
      "epoch 41, val_loss 454.62213134765625\n",
      "epoch 42, train_loss 1276.988037109375\n",
      "epoch 42, val_loss 454.62213134765625\n",
      "epoch 43, train_loss 1276.988037109375\n",
      "epoch 43, val_loss 454.62213134765625\n",
      "epoch 44, train_loss 1276.9879150390625\n",
      "epoch 44, val_loss 454.6220703125\n",
      "epoch 45, train_loss 1276.9879150390625\n",
      "epoch 45, val_loss 454.6220397949219\n",
      "epoch 46, train_loss 1276.9879150390625\n",
      "epoch 46, val_loss 454.6220397949219\n",
      "epoch 47, train_loss 1276.98779296875\n",
      "epoch 47, val_loss 454.6219482421875\n",
      "epoch 48, train_loss 1276.98779296875\n",
      "epoch 48, val_loss 454.62200927734375\n",
      "epoch 49, train_loss 1276.98779296875\n",
      "epoch 49, val_loss 454.62188720703125\n",
      "epoch 50, train_loss 1276.98779296875\n",
      "epoch 50, val_loss 454.6219177246094\n",
      "epoch 51, train_loss 1276.98779296875\n",
      "epoch 51, val_loss 454.62188720703125\n",
      "epoch 52, train_loss 1276.98779296875\n",
      "epoch 52, val_loss 454.621826171875\n",
      "epoch 53, train_loss 1276.98779296875\n",
      "epoch 53, val_loss 454.6217956542969\n",
      "epoch 54, train_loss 1276.98779296875\n",
      "epoch 54, val_loss 454.6217956542969\n",
      "epoch 55, train_loss 1276.98779296875\n",
      "epoch 55, val_loss 454.621826171875\n",
      "epoch 56, train_loss 1276.9876708984375\n",
      "epoch 56, val_loss 454.6217956542969\n",
      "epoch 57, train_loss 1276.98779296875\n",
      "epoch 57, val_loss 454.6217956542969\n",
      "epoch 58, train_loss 1276.98779296875\n",
      "epoch 58, val_loss 454.62176513671875\n",
      "epoch 59, train_loss 1276.9876708984375\n",
      "epoch 59, val_loss 454.6217041015625\n",
      "epoch 60, train_loss 1276.98779296875\n",
      "epoch 60, val_loss 454.6217041015625\n",
      "epoch 61, train_loss 1276.9876708984375\n",
      "epoch 61, val_loss 454.6216735839844\n",
      "epoch 62, train_loss 1276.9876708984375\n",
      "epoch 62, val_loss 454.6216735839844\n",
      "epoch 63, train_loss 1276.9876708984375\n",
      "epoch 63, val_loss 454.62158203125\n",
      "epoch 64, train_loss 1276.987548828125\n",
      "epoch 64, val_loss 454.62158203125\n",
      "epoch 65, train_loss 1276.987548828125\n",
      "epoch 65, val_loss 454.6215515136719\n",
      "epoch 66, train_loss 1276.987548828125\n",
      "epoch 66, val_loss 454.6215515136719\n",
      "epoch 67, train_loss 1276.987548828125\n",
      "epoch 67, val_loss 454.6214904785156\n",
      "epoch 68, train_loss 1276.987548828125\n",
      "epoch 68, val_loss 454.6214904785156\n",
      "epoch 69, train_loss 1276.987548828125\n",
      "epoch 69, val_loss 454.6214599609375\n",
      "epoch 70, train_loss 1276.987548828125\n",
      "epoch 70, val_loss 454.6214294433594\n",
      "epoch 71, train_loss 1276.987548828125\n",
      "epoch 71, val_loss 454.6213684082031\n",
      "epoch 72, train_loss 1276.987548828125\n",
      "epoch 72, val_loss 454.6213684082031\n",
      "epoch 73, train_loss 1276.987548828125\n",
      "epoch 73, val_loss 454.6213684082031\n",
      "epoch 74, train_loss 1276.9873046875\n",
      "epoch 74, val_loss 454.6213684082031\n",
      "epoch 75, train_loss 1276.9873046875\n",
      "epoch 75, val_loss 454.621337890625\n",
      "epoch 76, train_loss 1276.9874267578125\n",
      "epoch 76, val_loss 454.621337890625\n",
      "epoch 77, train_loss 1276.987548828125\n",
      "epoch 77, val_loss 454.6212463378906\n",
      "epoch 78, train_loss 1276.9874267578125\n",
      "epoch 78, val_loss 454.6212463378906\n",
      "epoch 79, train_loss 1276.9874267578125\n",
      "epoch 79, val_loss 454.6212158203125\n",
      "epoch 80, train_loss 1276.9873046875\n",
      "epoch 80, val_loss 454.6212158203125\n",
      "epoch 81, train_loss 1276.9871826171875\n",
      "epoch 81, val_loss 454.62115478515625\n",
      "epoch 82, train_loss 1276.9873046875\n",
      "epoch 82, val_loss 454.62115478515625\n",
      "epoch 83, train_loss 1276.9873046875\n",
      "epoch 83, val_loss 454.6211242675781\n",
      "epoch 84, train_loss 1276.9873046875\n",
      "epoch 84, val_loss 454.62109375\n",
      "epoch 85, train_loss 1276.9873046875\n",
      "epoch 85, val_loss 454.6211242675781\n",
      "epoch 86, train_loss 1276.9871826171875\n",
      "epoch 86, val_loss 454.6211242675781\n",
      "epoch 87, train_loss 1276.9871826171875\n",
      "epoch 87, val_loss 454.62109375\n",
      "epoch 88, train_loss 1276.987060546875\n",
      "epoch 88, val_loss 454.6210021972656\n",
      "epoch 89, train_loss 1276.987060546875\n",
      "epoch 89, val_loss 454.6210021972656\n",
      "epoch 90, train_loss 1276.987060546875\n",
      "epoch 90, val_loss 454.6210021972656\n",
      "epoch 91, train_loss 1276.9873046875\n",
      "epoch 91, val_loss 454.6210021972656\n",
      "epoch 92, train_loss 1276.987060546875\n",
      "epoch 92, val_loss 454.6208801269531\n",
      "epoch 93, train_loss 1276.987060546875\n",
      "epoch 93, val_loss 454.62091064453125\n",
      "epoch 94, train_loss 1276.987060546875\n",
      "epoch 94, val_loss 454.62091064453125\n",
      "epoch 95, train_loss 1276.987060546875\n",
      "epoch 95, val_loss 454.6208190917969\n",
      "epoch 96, train_loss 1276.987060546875\n",
      "epoch 96, val_loss 454.62078857421875\n",
      "epoch 97, train_loss 1276.987060546875\n",
      "epoch 97, val_loss 454.6207580566406\n",
      "epoch 98, train_loss 1276.987060546875\n",
      "epoch 98, val_loss 454.6207580566406\n",
      "epoch 99, train_loss 1276.9869384765625\n",
      "epoch 99, val_loss 454.6207580566406\n",
      "Parameter containing:\n",
      "tensor([3.0645e-34], requires_grad=True)\n",
      "iter 192, train_loss_regularization 0.6837836503982544\n",
      "iter 192, val_loss_regularization 0.6837836503982544\n",
      "epoch 0, train_loss 1276.9869384765625\n",
      "epoch 0, val_loss 454.6206970214844\n",
      "epoch 1, train_loss 1276.9869384765625\n",
      "epoch 1, val_loss 454.6206359863281\n",
      "epoch 2, train_loss 1276.987060546875\n",
      "epoch 2, val_loss 454.6206359863281\n",
      "epoch 3, train_loss 1276.98681640625\n",
      "epoch 3, val_loss 454.62066650390625\n",
      "epoch 4, train_loss 1276.98681640625\n",
      "epoch 4, val_loss 454.6206359863281\n",
      "epoch 5, train_loss 1276.98681640625\n",
      "epoch 5, val_loss 454.6206359863281\n",
      "epoch 6, train_loss 1276.9869384765625\n",
      "epoch 6, val_loss 454.6205749511719\n",
      "epoch 7, train_loss 1276.9869384765625\n",
      "epoch 7, val_loss 454.62054443359375\n",
      "epoch 8, train_loss 1276.9869384765625\n",
      "epoch 8, val_loss 454.6205749511719\n",
      "epoch 9, train_loss 1276.98681640625\n",
      "epoch 9, val_loss 454.62054443359375\n",
      "epoch 10, train_loss 1276.98681640625\n",
      "epoch 10, val_loss 454.6205139160156\n",
      "epoch 11, train_loss 1276.9866943359375\n",
      "epoch 11, val_loss 454.6204528808594\n",
      "epoch 12, train_loss 1276.9866943359375\n",
      "epoch 12, val_loss 454.6204528808594\n",
      "epoch 13, train_loss 1276.9866943359375\n",
      "epoch 13, val_loss 454.62042236328125\n",
      "epoch 14, train_loss 1276.9869384765625\n",
      "epoch 14, val_loss 454.6203308105469\n",
      "epoch 15, train_loss 1276.9866943359375\n",
      "epoch 15, val_loss 454.6203308105469\n",
      "epoch 16, train_loss 1276.9866943359375\n",
      "epoch 16, val_loss 454.6203308105469\n",
      "epoch 17, train_loss 1276.98681640625\n",
      "epoch 17, val_loss 454.62030029296875\n",
      "epoch 18, train_loss 1276.986572265625\n",
      "epoch 18, val_loss 454.6202392578125\n",
      "epoch 19, train_loss 1276.9866943359375\n",
      "epoch 19, val_loss 454.6202087402344\n",
      "epoch 20, train_loss 1276.9866943359375\n",
      "epoch 20, val_loss 454.6202392578125\n",
      "epoch 21, train_loss 1276.9866943359375\n",
      "epoch 21, val_loss 454.6202087402344\n",
      "epoch 22, train_loss 1276.9866943359375\n",
      "epoch 22, val_loss 454.62017822265625\n",
      "epoch 23, train_loss 1276.986572265625\n",
      "epoch 23, val_loss 454.62017822265625\n",
      "epoch 24, train_loss 1276.986572265625\n",
      "epoch 24, val_loss 454.62017822265625\n",
      "epoch 25, train_loss 1276.986572265625\n",
      "epoch 25, val_loss 454.6200866699219\n",
      "epoch 26, train_loss 1276.9864501953125\n",
      "epoch 26, val_loss 454.6201171875\n",
      "epoch 27, train_loss 1276.9864501953125\n",
      "epoch 27, val_loss 454.62005615234375\n",
      "epoch 28, train_loss 1276.9864501953125\n",
      "epoch 28, val_loss 454.62005615234375\n",
      "epoch 29, train_loss 1276.9864501953125\n",
      "epoch 29, val_loss 454.62005615234375\n",
      "epoch 30, train_loss 1276.9864501953125\n",
      "epoch 30, val_loss 454.6199951171875\n",
      "epoch 31, train_loss 1276.9864501953125\n",
      "epoch 31, val_loss 454.6199035644531\n",
      "epoch 32, train_loss 1276.9864501953125\n",
      "epoch 32, val_loss 454.6199645996094\n",
      "epoch 33, train_loss 1276.9864501953125\n",
      "epoch 33, val_loss 454.6199645996094\n",
      "epoch 34, train_loss 1276.9864501953125\n",
      "epoch 34, val_loss 454.6199035644531\n",
      "epoch 35, train_loss 1276.986328125\n",
      "epoch 35, val_loss 454.619873046875\n",
      "epoch 36, train_loss 1276.986328125\n",
      "epoch 36, val_loss 454.6198425292969\n",
      "epoch 37, train_loss 1276.986328125\n",
      "epoch 37, val_loss 454.6198425292969\n",
      "epoch 38, train_loss 1276.9864501953125\n",
      "epoch 38, val_loss 454.6198425292969\n",
      "epoch 39, train_loss 1276.9862060546875\n",
      "epoch 39, val_loss 454.6198425292969\n",
      "epoch 40, train_loss 1276.9862060546875\n",
      "epoch 40, val_loss 454.6197509765625\n",
      "epoch 41, train_loss 1276.9864501953125\n",
      "epoch 41, val_loss 454.6196594238281\n",
      "epoch 42, train_loss 1276.986328125\n",
      "epoch 42, val_loss 454.6197509765625\n",
      "epoch 43, train_loss 1276.9862060546875\n",
      "epoch 43, val_loss 454.6196594238281\n",
      "epoch 44, train_loss 1276.9862060546875\n",
      "epoch 44, val_loss 454.61962890625\n",
      "epoch 45, train_loss 1276.986083984375\n",
      "epoch 45, val_loss 454.6195983886719\n",
      "epoch 46, train_loss 1276.986083984375\n",
      "epoch 46, val_loss 454.6195983886719\n",
      "epoch 47, train_loss 1276.986083984375\n",
      "epoch 47, val_loss 454.6195983886719\n",
      "epoch 48, train_loss 1276.9862060546875\n",
      "epoch 48, val_loss 454.6195983886719\n",
      "epoch 49, train_loss 1276.9862060546875\n",
      "epoch 49, val_loss 454.6195373535156\n",
      "epoch 50, train_loss 1276.9859619140625\n",
      "epoch 50, val_loss 454.6195068359375\n",
      "epoch 51, train_loss 1276.9859619140625\n",
      "epoch 51, val_loss 454.61944580078125\n",
      "epoch 52, train_loss 1276.9859619140625\n",
      "epoch 52, val_loss 454.6194152832031\n",
      "epoch 53, train_loss 1276.986083984375\n",
      "epoch 53, val_loss 454.61944580078125\n",
      "epoch 54, train_loss 1276.9862060546875\n",
      "epoch 54, val_loss 454.61944580078125\n",
      "epoch 55, train_loss 1276.9859619140625\n",
      "epoch 55, val_loss 454.61944580078125\n",
      "epoch 56, train_loss 1276.9859619140625\n",
      "epoch 56, val_loss 454.619384765625\n",
      "epoch 57, train_loss 1276.9859619140625\n",
      "epoch 57, val_loss 454.61932373046875\n",
      "epoch 58, train_loss 1276.9862060546875\n",
      "epoch 58, val_loss 454.61932373046875\n",
      "epoch 59, train_loss 1276.986083984375\n",
      "epoch 59, val_loss 454.61932373046875\n",
      "epoch 60, train_loss 1276.9859619140625\n",
      "epoch 60, val_loss 454.6192932128906\n",
      "epoch 61, train_loss 1276.986083984375\n",
      "epoch 61, val_loss 454.6192626953125\n",
      "epoch 62, train_loss 1276.9859619140625\n",
      "epoch 62, val_loss 454.6191711425781\n",
      "epoch 63, train_loss 1276.9859619140625\n",
      "epoch 63, val_loss 454.6191711425781\n",
      "epoch 64, train_loss 1276.9859619140625\n",
      "epoch 64, val_loss 454.6191711425781\n",
      "epoch 65, train_loss 1276.9859619140625\n",
      "epoch 65, val_loss 454.6191101074219\n",
      "epoch 66, train_loss 1276.9859619140625\n",
      "epoch 66, val_loss 454.61907958984375\n",
      "epoch 67, train_loss 1276.9859619140625\n",
      "epoch 67, val_loss 454.6191101074219\n",
      "epoch 68, train_loss 1276.98583984375\n",
      "epoch 68, val_loss 454.61907958984375\n",
      "epoch 69, train_loss 1276.98583984375\n",
      "epoch 69, val_loss 454.6190490722656\n",
      "epoch 70, train_loss 1276.98583984375\n",
      "epoch 70, val_loss 454.6190490722656\n",
      "epoch 71, train_loss 1276.98583984375\n",
      "epoch 71, val_loss 454.6189880371094\n",
      "epoch 72, train_loss 1276.9857177734375\n",
      "epoch 72, val_loss 454.6190490722656\n",
      "epoch 73, train_loss 1276.985595703125\n",
      "epoch 73, val_loss 454.61895751953125\n",
      "epoch 74, train_loss 1276.985595703125\n",
      "epoch 74, val_loss 454.6189270019531\n",
      "epoch 75, train_loss 1276.9857177734375\n",
      "epoch 75, val_loss 454.6188659667969\n",
      "epoch 76, train_loss 1276.9857177734375\n",
      "epoch 76, val_loss 454.6188659667969\n",
      "epoch 77, train_loss 1276.9857177734375\n",
      "epoch 77, val_loss 454.61883544921875\n",
      "epoch 78, train_loss 1276.9857177734375\n",
      "epoch 78, val_loss 454.61883544921875\n",
      "epoch 79, train_loss 1276.985595703125\n",
      "epoch 79, val_loss 454.6188659667969\n",
      "epoch 80, train_loss 1276.9857177734375\n",
      "epoch 80, val_loss 454.6188659667969\n",
      "epoch 81, train_loss 1276.985595703125\n",
      "epoch 81, val_loss 454.6187438964844\n",
      "epoch 82, train_loss 1276.9857177734375\n",
      "epoch 82, val_loss 454.6187438964844\n",
      "epoch 83, train_loss 1276.985595703125\n",
      "epoch 83, val_loss 454.6187438964844\n",
      "epoch 84, train_loss 1276.9854736328125\n",
      "epoch 84, val_loss 454.6187438964844\n",
      "epoch 85, train_loss 1276.9854736328125\n",
      "epoch 85, val_loss 454.6187438964844\n",
      "epoch 86, train_loss 1276.9854736328125\n",
      "epoch 86, val_loss 454.61865234375\n",
      "epoch 87, train_loss 1276.9854736328125\n",
      "epoch 87, val_loss 454.61859130859375\n",
      "epoch 88, train_loss 1276.9854736328125\n",
      "epoch 88, val_loss 454.6185302734375\n",
      "epoch 89, train_loss 1276.9854736328125\n",
      "epoch 89, val_loss 454.6185302734375\n",
      "epoch 90, train_loss 1276.9854736328125\n",
      "epoch 90, val_loss 454.6185302734375\n",
      "epoch 91, train_loss 1276.9854736328125\n",
      "epoch 91, val_loss 454.6185302734375\n",
      "epoch 92, train_loss 1276.9854736328125\n",
      "epoch 92, val_loss 454.61846923828125\n",
      "epoch 93, train_loss 1276.9853515625\n",
      "epoch 93, val_loss 454.61846923828125\n",
      "epoch 94, train_loss 1276.9853515625\n",
      "epoch 94, val_loss 454.618408203125\n",
      "epoch 95, train_loss 1276.9853515625\n",
      "epoch 95, val_loss 454.618408203125\n",
      "epoch 96, train_loss 1276.9854736328125\n",
      "epoch 96, val_loss 454.618408203125\n",
      "epoch 97, train_loss 1276.985107421875\n",
      "epoch 97, val_loss 454.618408203125\n",
      "epoch 98, train_loss 1276.9853515625\n",
      "epoch 98, val_loss 454.6183776855469\n",
      "epoch 99, train_loss 1276.9853515625\n",
      "epoch 99, val_loss 454.61834716796875\n",
      "Parameter containing:\n",
      "tensor([2.1458e-34], requires_grad=True)\n",
      "iter 193, train_loss_regularization 0.6837490797042847\n",
      "iter 193, val_loss_regularization 0.6837490797042847\n",
      "epoch 0, train_loss 1276.985107421875\n",
      "epoch 0, val_loss 454.61834716796875\n",
      "epoch 1, train_loss 1276.985107421875\n",
      "epoch 1, val_loss 454.6182861328125\n",
      "epoch 2, train_loss 1276.985107421875\n",
      "epoch 2, val_loss 454.6182861328125\n",
      "epoch 3, train_loss 1276.9852294921875\n",
      "epoch 3, val_loss 454.6182556152344\n",
      "epoch 4, train_loss 1276.9853515625\n",
      "epoch 4, val_loss 454.6182556152344\n",
      "epoch 5, train_loss 1276.985107421875\n",
      "epoch 5, val_loss 454.6181640625\n",
      "epoch 6, train_loss 1276.9852294921875\n",
      "epoch 6, val_loss 454.6181640625\n",
      "epoch 7, train_loss 1276.9852294921875\n",
      "epoch 7, val_loss 454.6181335449219\n",
      "epoch 8, train_loss 1276.9852294921875\n",
      "epoch 8, val_loss 454.6181640625\n",
      "epoch 9, train_loss 1276.985107421875\n",
      "epoch 9, val_loss 454.6180725097656\n",
      "epoch 10, train_loss 1276.985107421875\n",
      "epoch 10, val_loss 454.6180725097656\n",
      "epoch 11, train_loss 1276.985107421875\n",
      "epoch 11, val_loss 454.6180114746094\n",
      "epoch 12, train_loss 1276.985107421875\n",
      "epoch 12, val_loss 454.6180114746094\n",
      "epoch 13, train_loss 1276.9849853515625\n",
      "epoch 13, val_loss 454.6180114746094\n",
      "epoch 14, train_loss 1276.985107421875\n",
      "epoch 14, val_loss 454.6180114746094\n",
      "epoch 15, train_loss 1276.985107421875\n",
      "epoch 15, val_loss 454.6180114746094\n",
      "epoch 16, train_loss 1276.985107421875\n",
      "epoch 16, val_loss 454.6179504394531\n",
      "epoch 17, train_loss 1276.985107421875\n",
      "epoch 17, val_loss 454.6178894042969\n",
      "epoch 18, train_loss 1276.9849853515625\n",
      "epoch 18, val_loss 454.6178894042969\n",
      "epoch 19, train_loss 1276.98486328125\n",
      "epoch 19, val_loss 454.6177978515625\n",
      "epoch 20, train_loss 1276.98486328125\n",
      "epoch 20, val_loss 454.6178894042969\n",
      "epoch 21, train_loss 1276.98486328125\n",
      "epoch 21, val_loss 454.6177978515625\n",
      "epoch 22, train_loss 1276.98486328125\n",
      "epoch 22, val_loss 454.6177978515625\n",
      "epoch 23, train_loss 1276.9847412109375\n",
      "epoch 23, val_loss 454.6177062988281\n",
      "epoch 24, train_loss 1276.9847412109375\n",
      "epoch 24, val_loss 454.6177062988281\n",
      "epoch 25, train_loss 1276.9847412109375\n",
      "epoch 25, val_loss 454.6177062988281\n",
      "epoch 26, train_loss 1276.98486328125\n",
      "epoch 26, val_loss 454.61773681640625\n",
      "epoch 27, train_loss 1276.98486328125\n",
      "epoch 27, val_loss 454.6177062988281\n",
      "epoch 28, train_loss 1276.98486328125\n",
      "epoch 28, val_loss 454.61767578125\n",
      "epoch 29, train_loss 1276.98486328125\n",
      "epoch 29, val_loss 454.61761474609375\n",
      "epoch 30, train_loss 1276.9847412109375\n",
      "epoch 30, val_loss 454.6175842285156\n",
      "epoch 31, train_loss 1276.984619140625\n",
      "epoch 31, val_loss 454.61761474609375\n",
      "epoch 32, train_loss 1276.984619140625\n",
      "epoch 32, val_loss 454.6175842285156\n",
      "epoch 33, train_loss 1276.9847412109375\n",
      "epoch 33, val_loss 454.6175842285156\n",
      "epoch 34, train_loss 1276.9847412109375\n",
      "epoch 34, val_loss 454.61749267578125\n",
      "epoch 35, train_loss 1276.984619140625\n",
      "epoch 35, val_loss 454.617431640625\n",
      "epoch 36, train_loss 1276.984619140625\n",
      "epoch 36, val_loss 454.617431640625\n",
      "epoch 37, train_loss 1276.984619140625\n",
      "epoch 37, val_loss 454.61737060546875\n",
      "epoch 38, train_loss 1276.984619140625\n",
      "epoch 38, val_loss 454.61737060546875\n",
      "epoch 39, train_loss 1276.984619140625\n",
      "epoch 39, val_loss 454.61737060546875\n",
      "epoch 40, train_loss 1276.984619140625\n",
      "epoch 40, val_loss 454.6173400878906\n",
      "epoch 41, train_loss 1276.984619140625\n",
      "epoch 41, val_loss 454.6173400878906\n",
      "epoch 42, train_loss 1276.984619140625\n",
      "epoch 42, val_loss 454.6172790527344\n",
      "epoch 43, train_loss 1276.984375\n",
      "epoch 43, val_loss 454.6172790527344\n",
      "epoch 44, train_loss 1276.984375\n",
      "epoch 44, val_loss 454.6172790527344\n",
      "epoch 45, train_loss 1276.984375\n",
      "epoch 45, val_loss 454.61724853515625\n",
      "epoch 46, train_loss 1276.984375\n",
      "epoch 46, val_loss 454.6172180175781\n",
      "epoch 47, train_loss 1276.984375\n",
      "epoch 47, val_loss 454.6171569824219\n",
      "epoch 48, train_loss 1276.984375\n",
      "epoch 48, val_loss 454.6171569824219\n",
      "epoch 49, train_loss 1276.984375\n",
      "epoch 49, val_loss 454.6171569824219\n",
      "epoch 50, train_loss 1276.984375\n",
      "epoch 50, val_loss 454.61712646484375\n",
      "epoch 51, train_loss 1276.9844970703125\n",
      "epoch 51, val_loss 454.61712646484375\n",
      "epoch 52, train_loss 1276.984375\n",
      "epoch 52, val_loss 454.6170959472656\n",
      "epoch 53, train_loss 1276.984375\n",
      "epoch 53, val_loss 454.6170349121094\n",
      "epoch 54, train_loss 1276.984375\n",
      "epoch 54, val_loss 454.61700439453125\n",
      "epoch 55, train_loss 1276.984375\n",
      "epoch 55, val_loss 454.61700439453125\n",
      "epoch 56, train_loss 1276.984375\n",
      "epoch 56, val_loss 454.61700439453125\n",
      "epoch 57, train_loss 1276.984375\n",
      "epoch 57, val_loss 454.61700439453125\n",
      "epoch 58, train_loss 1276.984375\n",
      "epoch 58, val_loss 454.6169128417969\n",
      "epoch 59, train_loss 1276.984375\n",
      "epoch 59, val_loss 454.6169128417969\n",
      "epoch 60, train_loss 1276.984130859375\n",
      "epoch 60, val_loss 454.61688232421875\n",
      "epoch 61, train_loss 1276.9842529296875\n",
      "epoch 61, val_loss 454.61688232421875\n",
      "epoch 62, train_loss 1276.9842529296875\n",
      "epoch 62, val_loss 454.61688232421875\n",
      "epoch 63, train_loss 1276.9842529296875\n",
      "epoch 63, val_loss 454.6167907714844\n",
      "epoch 64, train_loss 1276.9842529296875\n",
      "epoch 64, val_loss 454.6167907714844\n",
      "epoch 65, train_loss 1276.984375\n",
      "epoch 65, val_loss 454.6167907714844\n",
      "epoch 66, train_loss 1276.984130859375\n",
      "epoch 66, val_loss 454.61669921875\n",
      "epoch 67, train_loss 1276.984130859375\n",
      "epoch 67, val_loss 454.61669921875\n",
      "epoch 68, train_loss 1276.984130859375\n",
      "epoch 68, val_loss 454.6166687011719\n",
      "epoch 69, train_loss 1276.9840087890625\n",
      "epoch 69, val_loss 454.6166687011719\n",
      "epoch 70, train_loss 1276.984130859375\n",
      "epoch 70, val_loss 454.6166687011719\n",
      "epoch 71, train_loss 1276.984130859375\n",
      "epoch 71, val_loss 454.61663818359375\n",
      "epoch 72, train_loss 1276.9840087890625\n",
      "epoch 72, val_loss 454.61663818359375\n",
      "epoch 73, train_loss 1276.9840087890625\n",
      "epoch 73, val_loss 454.6165771484375\n",
      "epoch 74, train_loss 1276.9840087890625\n",
      "epoch 74, val_loss 454.6165771484375\n",
      "epoch 75, train_loss 1276.984130859375\n",
      "epoch 75, val_loss 454.6165466308594\n",
      "epoch 76, train_loss 1276.9840087890625\n",
      "epoch 76, val_loss 454.6165466308594\n",
      "epoch 77, train_loss 1276.98388671875\n",
      "epoch 77, val_loss 454.616455078125\n",
      "epoch 78, train_loss 1276.98388671875\n",
      "epoch 78, val_loss 454.616455078125\n",
      "epoch 79, train_loss 1276.98388671875\n",
      "epoch 79, val_loss 454.616455078125\n",
      "epoch 80, train_loss 1276.9840087890625\n",
      "epoch 80, val_loss 454.6164245605469\n",
      "epoch 81, train_loss 1276.98388671875\n",
      "epoch 81, val_loss 454.6163330078125\n",
      "epoch 82, train_loss 1276.98388671875\n",
      "epoch 82, val_loss 454.6163330078125\n",
      "epoch 83, train_loss 1276.98388671875\n",
      "epoch 83, val_loss 454.6163330078125\n",
      "epoch 84, train_loss 1276.98388671875\n",
      "epoch 84, val_loss 454.6163024902344\n",
      "epoch 85, train_loss 1276.9837646484375\n",
      "epoch 85, val_loss 454.6162414550781\n",
      "epoch 86, train_loss 1276.9837646484375\n",
      "epoch 86, val_loss 454.6162414550781\n",
      "epoch 87, train_loss 1276.9837646484375\n",
      "epoch 87, val_loss 454.6162414550781\n",
      "epoch 88, train_loss 1276.9837646484375\n",
      "epoch 88, val_loss 454.6162109375\n",
      "epoch 89, train_loss 1276.9837646484375\n",
      "epoch 89, val_loss 454.6161804199219\n",
      "epoch 90, train_loss 1276.983642578125\n",
      "epoch 90, val_loss 454.6161804199219\n",
      "epoch 91, train_loss 1276.9837646484375\n",
      "epoch 91, val_loss 454.6161804199219\n",
      "epoch 92, train_loss 1276.9837646484375\n",
      "epoch 92, val_loss 454.6161193847656\n",
      "epoch 93, train_loss 1276.9837646484375\n",
      "epoch 93, val_loss 454.6161193847656\n",
      "epoch 94, train_loss 1276.983642578125\n",
      "epoch 94, val_loss 454.6160888671875\n",
      "epoch 95, train_loss 1276.983642578125\n",
      "epoch 95, val_loss 454.61602783203125\n",
      "epoch 96, train_loss 1276.983642578125\n",
      "epoch 96, val_loss 454.6160888671875\n",
      "epoch 97, train_loss 1276.9835205078125\n",
      "epoch 97, val_loss 454.6159973144531\n",
      "epoch 98, train_loss 1276.983642578125\n",
      "epoch 98, val_loss 454.6159973144531\n",
      "epoch 99, train_loss 1276.983642578125\n",
      "epoch 99, val_loss 454.615966796875\n",
      "Parameter containing:\n",
      "tensor([1.5026e-34], requires_grad=True)\n",
      "iter 194, train_loss_regularization 0.6837247610092163\n",
      "iter 194, val_loss_regularization 0.6837247610092163\n",
      "epoch 0, train_loss 1276.983642578125\n",
      "epoch 0, val_loss 454.61590576171875\n",
      "epoch 1, train_loss 1276.9835205078125\n",
      "epoch 1, val_loss 454.61590576171875\n",
      "epoch 2, train_loss 1276.9835205078125\n",
      "epoch 2, val_loss 454.61590576171875\n",
      "epoch 3, train_loss 1276.9835205078125\n",
      "epoch 3, val_loss 454.6158447265625\n",
      "epoch 4, train_loss 1276.9835205078125\n",
      "epoch 4, val_loss 454.6158447265625\n",
      "epoch 5, train_loss 1276.9835205078125\n",
      "epoch 5, val_loss 454.6158447265625\n",
      "epoch 6, train_loss 1276.9835205078125\n",
      "epoch 6, val_loss 454.6157531738281\n",
      "epoch 7, train_loss 1276.9833984375\n",
      "epoch 7, val_loss 454.6157531738281\n",
      "epoch 8, train_loss 1276.983642578125\n",
      "epoch 8, val_loss 454.6157531738281\n",
      "epoch 9, train_loss 1276.9835205078125\n",
      "epoch 9, val_loss 454.61572265625\n",
      "epoch 10, train_loss 1276.9833984375\n",
      "epoch 10, val_loss 454.61572265625\n",
      "epoch 11, train_loss 1276.9833984375\n",
      "epoch 11, val_loss 454.61566162109375\n",
      "epoch 12, train_loss 1276.9832763671875\n",
      "epoch 12, val_loss 454.6156311035156\n",
      "epoch 13, train_loss 1276.9832763671875\n",
      "epoch 13, val_loss 454.6156311035156\n",
      "epoch 14, train_loss 1276.9832763671875\n",
      "epoch 14, val_loss 454.6155700683594\n",
      "epoch 15, train_loss 1276.9832763671875\n",
      "epoch 15, val_loss 454.6155090332031\n",
      "epoch 16, train_loss 1276.9832763671875\n",
      "epoch 16, val_loss 454.6155090332031\n",
      "epoch 17, train_loss 1276.9832763671875\n",
      "epoch 17, val_loss 454.6155090332031\n",
      "epoch 18, train_loss 1276.9833984375\n",
      "epoch 18, val_loss 454.6155090332031\n",
      "epoch 19, train_loss 1276.9832763671875\n",
      "epoch 19, val_loss 454.6155090332031\n",
      "epoch 20, train_loss 1276.9832763671875\n",
      "epoch 20, val_loss 454.6155090332031\n",
      "epoch 21, train_loss 1276.983154296875\n",
      "epoch 21, val_loss 454.61541748046875\n",
      "epoch 22, train_loss 1276.983154296875\n",
      "epoch 22, val_loss 454.61541748046875\n",
      "epoch 23, train_loss 1276.9830322265625\n",
      "epoch 23, val_loss 454.61541748046875\n",
      "epoch 24, train_loss 1276.9830322265625\n",
      "epoch 24, val_loss 454.6153259277344\n",
      "epoch 25, train_loss 1276.9830322265625\n",
      "epoch 25, val_loss 454.6153259277344\n",
      "epoch 26, train_loss 1276.9830322265625\n",
      "epoch 26, val_loss 454.6153869628906\n",
      "epoch 27, train_loss 1276.9830322265625\n",
      "epoch 27, val_loss 454.6153259277344\n",
      "epoch 28, train_loss 1276.9830322265625\n",
      "epoch 28, val_loss 454.6152648925781\n",
      "epoch 29, train_loss 1276.9830322265625\n",
      "epoch 29, val_loss 454.61517333984375\n",
      "epoch 30, train_loss 1276.9830322265625\n",
      "epoch 30, val_loss 454.61517333984375\n",
      "epoch 31, train_loss 1276.98291015625\n",
      "epoch 31, val_loss 454.61517333984375\n",
      "epoch 32, train_loss 1276.9830322265625\n",
      "epoch 32, val_loss 454.61517333984375\n",
      "epoch 33, train_loss 1276.9830322265625\n",
      "epoch 33, val_loss 454.6151123046875\n",
      "epoch 34, train_loss 1276.9830322265625\n",
      "epoch 34, val_loss 454.6151123046875\n",
      "epoch 35, train_loss 1276.98291015625\n",
      "epoch 35, val_loss 454.6150817871094\n",
      "epoch 36, train_loss 1276.98291015625\n",
      "epoch 36, val_loss 454.6150817871094\n",
      "epoch 37, train_loss 1276.98291015625\n",
      "epoch 37, val_loss 454.61505126953125\n",
      "epoch 38, train_loss 1276.9830322265625\n",
      "epoch 38, val_loss 454.61505126953125\n",
      "epoch 39, train_loss 1276.9830322265625\n",
      "epoch 39, val_loss 454.61505126953125\n",
      "epoch 40, train_loss 1276.9830322265625\n",
      "epoch 40, val_loss 454.614990234375\n",
      "epoch 41, train_loss 1276.98291015625\n",
      "epoch 41, val_loss 454.6149597167969\n",
      "epoch 42, train_loss 1276.9830322265625\n",
      "epoch 42, val_loss 454.61492919921875\n",
      "epoch 43, train_loss 1276.9830322265625\n",
      "epoch 43, val_loss 454.61492919921875\n",
      "epoch 44, train_loss 1276.98291015625\n",
      "epoch 44, val_loss 454.61492919921875\n",
      "epoch 45, train_loss 1276.9827880859375\n",
      "epoch 45, val_loss 454.6148681640625\n",
      "epoch 46, train_loss 1276.9827880859375\n",
      "epoch 46, val_loss 454.6148681640625\n",
      "epoch 47, train_loss 1276.9827880859375\n",
      "epoch 47, val_loss 454.6147766113281\n",
      "epoch 48, train_loss 1276.9827880859375\n",
      "epoch 48, val_loss 454.6147766113281\n",
      "epoch 49, train_loss 1276.9827880859375\n",
      "epoch 49, val_loss 454.6147766113281\n",
      "epoch 50, train_loss 1276.982666015625\n",
      "epoch 50, val_loss 454.61474609375\n",
      "epoch 51, train_loss 1276.982666015625\n",
      "epoch 51, val_loss 454.61474609375\n",
      "epoch 52, train_loss 1276.982666015625\n",
      "epoch 52, val_loss 454.6147155761719\n",
      "epoch 53, train_loss 1276.982666015625\n",
      "epoch 53, val_loss 454.6146545410156\n",
      "epoch 54, train_loss 1276.982666015625\n",
      "epoch 54, val_loss 454.6146545410156\n",
      "epoch 55, train_loss 1276.982666015625\n",
      "epoch 55, val_loss 454.6146240234375\n",
      "epoch 56, train_loss 1276.9827880859375\n",
      "epoch 56, val_loss 454.6146240234375\n",
      "epoch 57, train_loss 1276.9825439453125\n",
      "epoch 57, val_loss 454.6146240234375\n",
      "epoch 58, train_loss 1276.9825439453125\n",
      "epoch 58, val_loss 454.6145324707031\n",
      "epoch 59, train_loss 1276.9825439453125\n",
      "epoch 59, val_loss 454.6144714355469\n",
      "epoch 60, train_loss 1276.9825439453125\n",
      "epoch 60, val_loss 454.6144714355469\n",
      "epoch 61, train_loss 1276.9825439453125\n",
      "epoch 61, val_loss 454.6144714355469\n",
      "epoch 62, train_loss 1276.9825439453125\n",
      "epoch 62, val_loss 454.614501953125\n",
      "epoch 63, train_loss 1276.9825439453125\n",
      "epoch 63, val_loss 454.6144714355469\n",
      "epoch 64, train_loss 1276.9825439453125\n",
      "epoch 64, val_loss 454.6144104003906\n",
      "epoch 65, train_loss 1276.9825439453125\n",
      "epoch 65, val_loss 454.61431884765625\n",
      "epoch 66, train_loss 1276.9825439453125\n",
      "epoch 66, val_loss 454.6143798828125\n",
      "epoch 67, train_loss 1276.9825439453125\n",
      "epoch 67, val_loss 454.61431884765625\n",
      "epoch 68, train_loss 1276.9825439453125\n",
      "epoch 68, val_loss 454.61431884765625\n",
      "epoch 69, train_loss 1276.9825439453125\n",
      "epoch 69, val_loss 454.61431884765625\n",
      "epoch 70, train_loss 1276.982421875\n",
      "epoch 70, val_loss 454.61431884765625\n",
      "epoch 71, train_loss 1276.982421875\n",
      "epoch 71, val_loss 454.6142578125\n",
      "epoch 72, train_loss 1276.9822998046875\n",
      "epoch 72, val_loss 454.61419677734375\n",
      "epoch 73, train_loss 1276.982177734375\n",
      "epoch 73, val_loss 454.6142578125\n",
      "epoch 74, train_loss 1276.982177734375\n",
      "epoch 74, val_loss 454.61419677734375\n",
      "epoch 75, train_loss 1276.9822998046875\n",
      "epoch 75, val_loss 454.61419677734375\n",
      "epoch 76, train_loss 1276.9822998046875\n",
      "epoch 76, val_loss 454.61419677734375\n",
      "epoch 77, train_loss 1276.9822998046875\n",
      "epoch 77, val_loss 454.61407470703125\n",
      "epoch 78, train_loss 1276.9822998046875\n",
      "epoch 78, val_loss 454.61407470703125\n",
      "epoch 79, train_loss 1276.9822998046875\n",
      "epoch 79, val_loss 454.6140441894531\n",
      "epoch 80, train_loss 1276.9822998046875\n",
      "epoch 80, val_loss 454.61407470703125\n",
      "epoch 81, train_loss 1276.982177734375\n",
      "epoch 81, val_loss 454.6140441894531\n",
      "epoch 82, train_loss 1276.982177734375\n",
      "epoch 82, val_loss 454.614013671875\n",
      "epoch 83, train_loss 1276.982177734375\n",
      "epoch 83, val_loss 454.61395263671875\n",
      "epoch 84, train_loss 1276.9820556640625\n",
      "epoch 84, val_loss 454.61395263671875\n",
      "epoch 85, train_loss 1276.982177734375\n",
      "epoch 85, val_loss 454.6139221191406\n",
      "epoch 86, train_loss 1276.982177734375\n",
      "epoch 86, val_loss 454.6138610839844\n",
      "epoch 87, train_loss 1276.9820556640625\n",
      "epoch 87, val_loss 454.6138610839844\n",
      "epoch 88, train_loss 1276.9820556640625\n",
      "epoch 88, val_loss 454.6138610839844\n",
      "epoch 89, train_loss 1276.9820556640625\n",
      "epoch 89, val_loss 454.6138610839844\n",
      "epoch 90, train_loss 1276.9820556640625\n",
      "epoch 90, val_loss 454.61383056640625\n",
      "epoch 91, train_loss 1276.982177734375\n",
      "epoch 91, val_loss 454.6137390136719\n",
      "epoch 92, train_loss 1276.982177734375\n",
      "epoch 92, val_loss 454.6137390136719\n",
      "epoch 93, train_loss 1276.982177734375\n",
      "epoch 93, val_loss 454.6137390136719\n",
      "epoch 94, train_loss 1276.982177734375\n",
      "epoch 94, val_loss 454.61370849609375\n",
      "epoch 95, train_loss 1276.982177734375\n",
      "epoch 95, val_loss 454.61370849609375\n",
      "epoch 96, train_loss 1276.98193359375\n",
      "epoch 96, val_loss 454.6136169433594\n",
      "epoch 97, train_loss 1276.9818115234375\n",
      "epoch 97, val_loss 454.6136169433594\n",
      "epoch 98, train_loss 1276.9818115234375\n",
      "epoch 98, val_loss 454.6136779785156\n",
      "epoch 99, train_loss 1276.9818115234375\n",
      "epoch 99, val_loss 454.6136169433594\n",
      "Parameter containing:\n",
      "tensor([1.0522e-34], requires_grad=True)\n",
      "iter 195, train_loss_regularization 0.6837104558944702\n",
      "iter 195, val_loss_regularization 0.6837104558944702\n",
      "epoch 0, train_loss 1276.9818115234375\n",
      "epoch 0, val_loss 454.61358642578125\n",
      "epoch 1, train_loss 1276.9818115234375\n",
      "epoch 1, val_loss 454.61358642578125\n",
      "epoch 2, train_loss 1276.9818115234375\n",
      "epoch 2, val_loss 454.61358642578125\n",
      "epoch 3, train_loss 1276.9818115234375\n",
      "epoch 3, val_loss 454.6134948730469\n",
      "epoch 4, train_loss 1276.98193359375\n",
      "epoch 4, val_loss 454.61346435546875\n",
      "epoch 5, train_loss 1276.98193359375\n",
      "epoch 5, val_loss 454.61346435546875\n",
      "epoch 6, train_loss 1276.9818115234375\n",
      "epoch 6, val_loss 454.61346435546875\n",
      "epoch 7, train_loss 1276.9818115234375\n",
      "epoch 7, val_loss 454.61346435546875\n",
      "epoch 8, train_loss 1276.981689453125\n",
      "epoch 8, val_loss 454.61334228515625\n",
      "epoch 9, train_loss 1276.981689453125\n",
      "epoch 9, val_loss 454.61334228515625\n",
      "epoch 10, train_loss 1276.981689453125\n",
      "epoch 10, val_loss 454.61334228515625\n",
      "epoch 11, train_loss 1276.981689453125\n",
      "epoch 11, val_loss 454.61328125\n",
      "epoch 12, train_loss 1276.981689453125\n",
      "epoch 12, val_loss 454.6132507324219\n",
      "epoch 13, train_loss 1276.981689453125\n",
      "epoch 13, val_loss 454.6132507324219\n",
      "epoch 14, train_loss 1276.981689453125\n",
      "epoch 14, val_loss 454.6132507324219\n",
      "epoch 15, train_loss 1276.981689453125\n",
      "epoch 15, val_loss 454.61322021484375\n",
      "epoch 16, train_loss 1276.981689453125\n",
      "epoch 16, val_loss 454.61322021484375\n",
      "epoch 17, train_loss 1276.981689453125\n",
      "epoch 17, val_loss 454.6131591796875\n",
      "epoch 18, train_loss 1276.981689453125\n",
      "epoch 18, val_loss 454.6131591796875\n",
      "epoch 19, train_loss 1276.981689453125\n",
      "epoch 19, val_loss 454.6131286621094\n",
      "epoch 20, train_loss 1276.9815673828125\n",
      "epoch 20, val_loss 454.6131286621094\n",
      "epoch 21, train_loss 1276.9815673828125\n",
      "epoch 21, val_loss 454.6131286621094\n",
      "epoch 22, train_loss 1276.9815673828125\n",
      "epoch 22, val_loss 454.613037109375\n",
      "epoch 23, train_loss 1276.9814453125\n",
      "epoch 23, val_loss 454.613037109375\n",
      "epoch 24, train_loss 1276.9814453125\n",
      "epoch 24, val_loss 454.613037109375\n",
      "epoch 25, train_loss 1276.9813232421875\n",
      "epoch 25, val_loss 454.6129455566406\n",
      "epoch 26, train_loss 1276.9813232421875\n",
      "epoch 26, val_loss 454.6129150390625\n",
      "epoch 27, train_loss 1276.9813232421875\n",
      "epoch 27, val_loss 454.6129150390625\n",
      "epoch 28, train_loss 1276.9814453125\n",
      "epoch 28, val_loss 454.6128845214844\n",
      "epoch 29, train_loss 1276.9814453125\n",
      "epoch 29, val_loss 454.6128845214844\n",
      "epoch 30, train_loss 1276.9814453125\n",
      "epoch 30, val_loss 454.6128234863281\n",
      "epoch 31, train_loss 1276.9814453125\n",
      "epoch 31, val_loss 454.6128234863281\n",
      "epoch 32, train_loss 1276.9814453125\n",
      "epoch 32, val_loss 454.6128234863281\n",
      "epoch 33, train_loss 1276.9814453125\n",
      "epoch 33, val_loss 454.6128234863281\n",
      "epoch 34, train_loss 1276.981201171875\n",
      "epoch 34, val_loss 454.6127624511719\n",
      "epoch 35, train_loss 1276.9813232421875\n",
      "epoch 35, val_loss 454.6127624511719\n",
      "epoch 36, train_loss 1276.981201171875\n",
      "epoch 36, val_loss 454.6127624511719\n",
      "epoch 37, train_loss 1276.981201171875\n",
      "epoch 37, val_loss 454.6127624511719\n",
      "epoch 38, train_loss 1276.9813232421875\n",
      "epoch 38, val_loss 454.6126708984375\n",
      "epoch 39, train_loss 1276.981201171875\n",
      "epoch 39, val_loss 454.6126708984375\n",
      "epoch 40, train_loss 1276.9813232421875\n",
      "epoch 40, val_loss 454.61260986328125\n",
      "epoch 41, train_loss 1276.9813232421875\n",
      "epoch 41, val_loss 454.6125793457031\n",
      "epoch 42, train_loss 1276.981201171875\n",
      "epoch 42, val_loss 454.61260986328125\n",
      "epoch 43, train_loss 1276.9814453125\n",
      "epoch 43, val_loss 454.6125793457031\n",
      "epoch 44, train_loss 1276.981201171875\n",
      "epoch 44, val_loss 454.6125793457031\n",
      "epoch 45, train_loss 1276.981201171875\n",
      "epoch 45, val_loss 454.612548828125\n",
      "epoch 46, train_loss 1276.981201171875\n",
      "epoch 46, val_loss 454.61248779296875\n",
      "epoch 47, train_loss 1276.981201171875\n",
      "epoch 47, val_loss 454.61248779296875\n",
      "epoch 48, train_loss 1276.981201171875\n",
      "epoch 48, val_loss 454.6124572753906\n",
      "epoch 49, train_loss 1276.98095703125\n",
      "epoch 49, val_loss 454.6124267578125\n",
      "epoch 50, train_loss 1276.98095703125\n",
      "epoch 50, val_loss 454.6124572753906\n",
      "epoch 51, train_loss 1276.9810791015625\n",
      "epoch 51, val_loss 454.6124267578125\n",
      "epoch 52, train_loss 1276.981201171875\n",
      "epoch 52, val_loss 454.6123352050781\n",
      "epoch 53, train_loss 1276.98095703125\n",
      "epoch 53, val_loss 454.6123352050781\n",
      "epoch 54, train_loss 1276.9810791015625\n",
      "epoch 54, val_loss 454.6123046875\n",
      "epoch 55, train_loss 1276.9810791015625\n",
      "epoch 55, val_loss 454.6123046875\n",
      "epoch 56, train_loss 1276.98095703125\n",
      "epoch 56, val_loss 454.6122131347656\n",
      "epoch 57, train_loss 1276.98095703125\n",
      "epoch 57, val_loss 454.6122131347656\n",
      "epoch 58, train_loss 1276.98095703125\n",
      "epoch 58, val_loss 454.6122131347656\n",
      "epoch 59, train_loss 1276.98095703125\n",
      "epoch 59, val_loss 454.6122131347656\n",
      "epoch 60, train_loss 1276.98095703125\n",
      "epoch 60, val_loss 454.6121520996094\n",
      "epoch 61, train_loss 1276.98095703125\n",
      "epoch 61, val_loss 454.61212158203125\n",
      "epoch 62, train_loss 1276.9808349609375\n",
      "epoch 62, val_loss 454.61212158203125\n",
      "epoch 63, train_loss 1276.9808349609375\n",
      "epoch 63, val_loss 454.6120910644531\n",
      "epoch 64, train_loss 1276.9808349609375\n",
      "epoch 64, val_loss 454.6120910644531\n",
      "epoch 65, train_loss 1276.9808349609375\n",
      "epoch 65, val_loss 454.6120300292969\n",
      "epoch 66, train_loss 1276.9808349609375\n",
      "epoch 66, val_loss 454.6120300292969\n",
      "epoch 67, train_loss 1276.9808349609375\n",
      "epoch 67, val_loss 454.61199951171875\n",
      "epoch 68, train_loss 1276.980712890625\n",
      "epoch 68, val_loss 454.61199951171875\n",
      "epoch 69, train_loss 1276.9808349609375\n",
      "epoch 69, val_loss 454.61199951171875\n",
      "epoch 70, train_loss 1276.980712890625\n",
      "epoch 70, val_loss 454.6119689941406\n",
      "epoch 71, train_loss 1276.980712890625\n",
      "epoch 71, val_loss 454.61187744140625\n",
      "epoch 72, train_loss 1276.9805908203125\n",
      "epoch 72, val_loss 454.61187744140625\n",
      "epoch 73, train_loss 1276.9805908203125\n",
      "epoch 73, val_loss 454.6119079589844\n",
      "epoch 74, train_loss 1276.98046875\n",
      "epoch 74, val_loss 454.6118469238281\n",
      "epoch 75, train_loss 1276.980712890625\n",
      "epoch 75, val_loss 454.6118469238281\n",
      "epoch 76, train_loss 1276.980712890625\n",
      "epoch 76, val_loss 454.6118469238281\n",
      "epoch 77, train_loss 1276.980712890625\n",
      "epoch 77, val_loss 454.61175537109375\n",
      "epoch 78, train_loss 1276.9805908203125\n",
      "epoch 78, val_loss 454.61175537109375\n",
      "epoch 79, train_loss 1276.9805908203125\n",
      "epoch 79, val_loss 454.61175537109375\n",
      "epoch 80, train_loss 1276.98046875\n",
      "epoch 80, val_loss 454.6116943359375\n",
      "epoch 81, train_loss 1276.9805908203125\n",
      "epoch 81, val_loss 454.6116638183594\n",
      "epoch 82, train_loss 1276.98046875\n",
      "epoch 82, val_loss 454.6116638183594\n",
      "epoch 83, train_loss 1276.98046875\n",
      "epoch 83, val_loss 454.61163330078125\n",
      "epoch 84, train_loss 1276.98046875\n",
      "epoch 84, val_loss 454.61163330078125\n",
      "epoch 85, train_loss 1276.98046875\n",
      "epoch 85, val_loss 454.611572265625\n",
      "epoch 86, train_loss 1276.98046875\n",
      "epoch 86, val_loss 454.611572265625\n",
      "epoch 87, train_loss 1276.98046875\n",
      "epoch 87, val_loss 454.6115417480469\n",
      "epoch 88, train_loss 1276.9803466796875\n",
      "epoch 88, val_loss 454.61151123046875\n",
      "epoch 89, train_loss 1276.98046875\n",
      "epoch 89, val_loss 454.61151123046875\n",
      "epoch 90, train_loss 1276.98046875\n",
      "epoch 90, val_loss 454.6114501953125\n",
      "epoch 91, train_loss 1276.98046875\n",
      "epoch 91, val_loss 454.6114501953125\n",
      "epoch 92, train_loss 1276.98046875\n",
      "epoch 92, val_loss 454.6114501953125\n",
      "epoch 93, train_loss 1276.9803466796875\n",
      "epoch 93, val_loss 454.6114196777344\n",
      "epoch 94, train_loss 1276.98046875\n",
      "epoch 94, val_loss 454.6114196777344\n",
      "epoch 95, train_loss 1276.9803466796875\n",
      "epoch 95, val_loss 454.6114196777344\n",
      "epoch 96, train_loss 1276.9803466796875\n",
      "epoch 96, val_loss 454.611328125\n",
      "epoch 97, train_loss 1276.9803466796875\n",
      "epoch 97, val_loss 454.6112976074219\n",
      "epoch 98, train_loss 1276.9803466796875\n",
      "epoch 98, val_loss 454.611328125\n",
      "epoch 99, train_loss 1276.98046875\n",
      "epoch 99, val_loss 454.611328125\n",
      "Parameter containing:\n",
      "tensor([7.3688e-35], requires_grad=True)\n",
      "iter 196, train_loss_regularization 0.6837059855461121\n",
      "iter 196, val_loss_regularization 0.6837059855461121\n",
      "epoch 0, train_loss 1276.980224609375\n",
      "epoch 0, val_loss 454.6112365722656\n",
      "epoch 1, train_loss 1276.980224609375\n",
      "epoch 1, val_loss 454.6112365722656\n",
      "epoch 2, train_loss 1276.980224609375\n",
      "epoch 2, val_loss 454.6112365722656\n",
      "epoch 3, train_loss 1276.980224609375\n",
      "epoch 3, val_loss 454.6111145019531\n",
      "epoch 4, train_loss 1276.980224609375\n",
      "epoch 4, val_loss 454.6111145019531\n",
      "epoch 5, train_loss 1276.980224609375\n",
      "epoch 5, val_loss 454.611083984375\n",
      "epoch 6, train_loss 1276.980224609375\n",
      "epoch 6, val_loss 454.611083984375\n",
      "epoch 7, train_loss 1276.980224609375\n",
      "epoch 7, val_loss 454.6110534667969\n",
      "epoch 8, train_loss 1276.980224609375\n",
      "epoch 8, val_loss 454.611083984375\n",
      "epoch 9, train_loss 1276.980224609375\n",
      "epoch 9, val_loss 454.6110534667969\n",
      "epoch 10, train_loss 1276.97998046875\n",
      "epoch 10, val_loss 454.6109924316406\n",
      "epoch 11, train_loss 1276.97998046875\n",
      "epoch 11, val_loss 454.6109924316406\n",
      "epoch 12, train_loss 1276.97998046875\n",
      "epoch 12, val_loss 454.6109924316406\n",
      "epoch 13, train_loss 1276.9798583984375\n",
      "epoch 13, val_loss 454.6109619140625\n",
      "epoch 14, train_loss 1276.97998046875\n",
      "epoch 14, val_loss 454.6109313964844\n",
      "epoch 15, train_loss 1276.97998046875\n",
      "epoch 15, val_loss 454.6108703613281\n",
      "epoch 16, train_loss 1276.97998046875\n",
      "epoch 16, val_loss 454.6108703613281\n",
      "epoch 17, train_loss 1276.97998046875\n",
      "epoch 17, val_loss 454.6108703613281\n",
      "epoch 18, train_loss 1276.97998046875\n",
      "epoch 18, val_loss 454.6108703613281\n",
      "epoch 19, train_loss 1276.97998046875\n",
      "epoch 19, val_loss 454.61077880859375\n",
      "epoch 20, train_loss 1276.97998046875\n",
      "epoch 20, val_loss 454.61077880859375\n",
      "epoch 21, train_loss 1276.97998046875\n",
      "epoch 21, val_loss 454.6107482910156\n",
      "epoch 22, train_loss 1276.9798583984375\n",
      "epoch 22, val_loss 454.6107177734375\n",
      "epoch 23, train_loss 1276.9798583984375\n",
      "epoch 23, val_loss 454.6107177734375\n",
      "epoch 24, train_loss 1276.9798583984375\n",
      "epoch 24, val_loss 454.6107177734375\n",
      "epoch 25, train_loss 1276.9798583984375\n",
      "epoch 25, val_loss 454.61065673828125\n",
      "epoch 26, train_loss 1276.9798583984375\n",
      "epoch 26, val_loss 454.61065673828125\n",
      "epoch 27, train_loss 1276.9798583984375\n",
      "epoch 27, val_loss 454.610595703125\n",
      "epoch 28, train_loss 1276.9798583984375\n",
      "epoch 28, val_loss 454.610595703125\n",
      "epoch 29, train_loss 1276.9796142578125\n",
      "epoch 29, val_loss 454.61053466796875\n",
      "epoch 30, train_loss 1276.9796142578125\n",
      "epoch 30, val_loss 454.61053466796875\n",
      "epoch 31, train_loss 1276.9796142578125\n",
      "epoch 31, val_loss 454.6105041503906\n",
      "epoch 32, train_loss 1276.9796142578125\n",
      "epoch 32, val_loss 454.6105041503906\n",
      "epoch 33, train_loss 1276.9796142578125\n",
      "epoch 33, val_loss 454.6105041503906\n",
      "epoch 34, train_loss 1276.9796142578125\n",
      "epoch 34, val_loss 454.61041259765625\n",
      "epoch 35, train_loss 1276.979736328125\n",
      "epoch 35, val_loss 454.6103820800781\n",
      "epoch 36, train_loss 1276.979736328125\n",
      "epoch 36, val_loss 454.6103820800781\n",
      "epoch 37, train_loss 1276.9796142578125\n",
      "epoch 37, val_loss 454.6103820800781\n",
      "epoch 38, train_loss 1276.9796142578125\n",
      "epoch 38, val_loss 454.6103820800781\n",
      "epoch 39, train_loss 1276.9794921875\n",
      "epoch 39, val_loss 454.6103210449219\n",
      "epoch 40, train_loss 1276.9794921875\n",
      "epoch 40, val_loss 454.61029052734375\n",
      "epoch 41, train_loss 1276.9794921875\n",
      "epoch 41, val_loss 454.61029052734375\n",
      "epoch 42, train_loss 1276.9796142578125\n",
      "epoch 42, val_loss 454.61029052734375\n",
      "epoch 43, train_loss 1276.9794921875\n",
      "epoch 43, val_loss 454.61029052734375\n",
      "epoch 44, train_loss 1276.9796142578125\n",
      "epoch 44, val_loss 454.61029052734375\n",
      "epoch 45, train_loss 1276.9796142578125\n",
      "epoch 45, val_loss 454.6101989746094\n",
      "epoch 46, train_loss 1276.9796142578125\n",
      "epoch 46, val_loss 454.6101989746094\n",
      "epoch 47, train_loss 1276.9796142578125\n",
      "epoch 47, val_loss 454.61016845703125\n",
      "epoch 48, train_loss 1276.9796142578125\n",
      "epoch 48, val_loss 454.61016845703125\n",
      "epoch 49, train_loss 1276.9794921875\n",
      "epoch 49, val_loss 454.6100769042969\n",
      "epoch 50, train_loss 1276.9794921875\n",
      "epoch 50, val_loss 454.6100769042969\n",
      "epoch 51, train_loss 1276.9793701171875\n",
      "epoch 51, val_loss 454.61004638671875\n",
      "epoch 52, train_loss 1276.9793701171875\n",
      "epoch 52, val_loss 454.6099548339844\n",
      "epoch 53, train_loss 1276.9793701171875\n",
      "epoch 53, val_loss 454.6099548339844\n",
      "epoch 54, train_loss 1276.9793701171875\n",
      "epoch 54, val_loss 454.6099548339844\n",
      "epoch 55, train_loss 1276.9793701171875\n",
      "epoch 55, val_loss 454.60992431640625\n",
      "epoch 56, train_loss 1276.979248046875\n",
      "epoch 56, val_loss 454.60992431640625\n",
      "epoch 57, train_loss 1276.9793701171875\n",
      "epoch 57, val_loss 454.60992431640625\n",
      "epoch 58, train_loss 1276.9793701171875\n",
      "epoch 58, val_loss 454.60992431640625\n",
      "epoch 59, train_loss 1276.9793701171875\n",
      "epoch 59, val_loss 454.6098327636719\n",
      "epoch 60, train_loss 1276.9793701171875\n",
      "epoch 60, val_loss 454.6098327636719\n",
      "epoch 61, train_loss 1276.979248046875\n",
      "epoch 61, val_loss 454.6098327636719\n",
      "epoch 62, train_loss 1276.9791259765625\n",
      "epoch 62, val_loss 454.6098327636719\n",
      "epoch 63, train_loss 1276.9791259765625\n",
      "epoch 63, val_loss 454.6098327636719\n",
      "epoch 64, train_loss 1276.9791259765625\n",
      "epoch 64, val_loss 454.60980224609375\n",
      "epoch 65, train_loss 1276.9791259765625\n",
      "epoch 65, val_loss 454.6097106933594\n",
      "epoch 66, train_loss 1276.979248046875\n",
      "epoch 66, val_loss 454.60968017578125\n",
      "epoch 67, train_loss 1276.979248046875\n",
      "epoch 67, val_loss 454.60968017578125\n",
      "epoch 68, train_loss 1276.979248046875\n",
      "epoch 68, val_loss 454.60968017578125\n",
      "epoch 69, train_loss 1276.9791259765625\n",
      "epoch 69, val_loss 454.60968017578125\n",
      "epoch 70, train_loss 1276.9791259765625\n",
      "epoch 70, val_loss 454.609619140625\n",
      "epoch 71, train_loss 1276.9791259765625\n",
      "epoch 71, val_loss 454.6095886230469\n",
      "epoch 72, train_loss 1276.9791259765625\n",
      "epoch 72, val_loss 454.6095275878906\n",
      "epoch 73, train_loss 1276.9791259765625\n",
      "epoch 73, val_loss 454.6095886230469\n",
      "epoch 74, train_loss 1276.9791259765625\n",
      "epoch 74, val_loss 454.6094970703125\n",
      "epoch 75, train_loss 1276.9791259765625\n",
      "epoch 75, val_loss 454.6094970703125\n",
      "epoch 76, train_loss 1276.9791259765625\n",
      "epoch 76, val_loss 454.6094970703125\n",
      "epoch 77, train_loss 1276.97900390625\n",
      "epoch 77, val_loss 454.6094665527344\n",
      "epoch 78, train_loss 1276.9788818359375\n",
      "epoch 78, val_loss 454.6094055175781\n",
      "epoch 79, train_loss 1276.9788818359375\n",
      "epoch 79, val_loss 454.6094055175781\n",
      "epoch 80, train_loss 1276.9788818359375\n",
      "epoch 80, val_loss 454.609375\n",
      "epoch 81, train_loss 1276.9788818359375\n",
      "epoch 81, val_loss 454.609375\n",
      "epoch 82, train_loss 1276.97900390625\n",
      "epoch 82, val_loss 454.609375\n",
      "epoch 83, train_loss 1276.9788818359375\n",
      "epoch 83, val_loss 454.6092834472656\n",
      "epoch 84, train_loss 1276.9788818359375\n",
      "epoch 84, val_loss 454.6092529296875\n",
      "epoch 85, train_loss 1276.9788818359375\n",
      "epoch 85, val_loss 454.6092529296875\n",
      "epoch 86, train_loss 1276.9788818359375\n",
      "epoch 86, val_loss 454.6092529296875\n",
      "epoch 87, train_loss 1276.9788818359375\n",
      "epoch 87, val_loss 454.6092224121094\n",
      "epoch 88, train_loss 1276.9788818359375\n",
      "epoch 88, val_loss 454.6092529296875\n",
      "epoch 89, train_loss 1276.9786376953125\n",
      "epoch 89, val_loss 454.6091613769531\n",
      "epoch 90, train_loss 1276.9788818359375\n",
      "epoch 90, val_loss 454.6091613769531\n",
      "epoch 91, train_loss 1276.9788818359375\n",
      "epoch 91, val_loss 454.609130859375\n",
      "epoch 92, train_loss 1276.9788818359375\n",
      "epoch 92, val_loss 454.609130859375\n",
      "epoch 93, train_loss 1276.9788818359375\n",
      "epoch 93, val_loss 454.609130859375\n",
      "epoch 94, train_loss 1276.978759765625\n",
      "epoch 94, val_loss 454.60906982421875\n",
      "epoch 95, train_loss 1276.9788818359375\n",
      "epoch 95, val_loss 454.609130859375\n",
      "epoch 96, train_loss 1276.978759765625\n",
      "epoch 96, val_loss 454.6090393066406\n",
      "epoch 97, train_loss 1276.978759765625\n",
      "epoch 97, val_loss 454.6090087890625\n",
      "epoch 98, train_loss 1276.978759765625\n",
      "epoch 98, val_loss 454.60894775390625\n",
      "epoch 99, train_loss 1276.9788818359375\n",
      "epoch 99, val_loss 454.6089172363281\n",
      "Parameter containing:\n",
      "tensor([5.1604e-35], requires_grad=True)\n",
      "iter 197, train_loss_regularization 0.6837114691734314\n",
      "iter 197, val_loss_regularization 0.6837114691734314\n",
      "epoch 0, train_loss 1276.9786376953125\n",
      "epoch 0, val_loss 454.6089172363281\n",
      "epoch 1, train_loss 1276.9786376953125\n",
      "epoch 1, val_loss 454.60882568359375\n",
      "epoch 2, train_loss 1276.978515625\n",
      "epoch 2, val_loss 454.60888671875\n",
      "epoch 3, train_loss 1276.9786376953125\n",
      "epoch 3, val_loss 454.6087951660156\n",
      "epoch 4, train_loss 1276.9786376953125\n",
      "epoch 4, val_loss 454.6087951660156\n",
      "epoch 5, train_loss 1276.9786376953125\n",
      "epoch 5, val_loss 454.60882568359375\n",
      "epoch 6, train_loss 1276.978515625\n",
      "epoch 6, val_loss 454.6087951660156\n",
      "epoch 7, train_loss 1276.978515625\n",
      "epoch 7, val_loss 454.6087646484375\n",
      "epoch 8, train_loss 1276.978515625\n",
      "epoch 8, val_loss 454.6087646484375\n",
      "epoch 9, train_loss 1276.978515625\n",
      "epoch 9, val_loss 454.6087646484375\n",
      "epoch 10, train_loss 1276.978515625\n",
      "epoch 10, val_loss 454.60870361328125\n",
      "epoch 11, train_loss 1276.978515625\n",
      "epoch 11, val_loss 454.60870361328125\n",
      "epoch 12, train_loss 1276.978515625\n",
      "epoch 12, val_loss 454.6086730957031\n",
      "epoch 13, train_loss 1276.9783935546875\n",
      "epoch 13, val_loss 454.6086120605469\n",
      "epoch 14, train_loss 1276.9783935546875\n",
      "epoch 14, val_loss 454.6086120605469\n",
      "epoch 15, train_loss 1276.9783935546875\n",
      "epoch 15, val_loss 454.60858154296875\n",
      "epoch 16, train_loss 1276.978271484375\n",
      "epoch 16, val_loss 454.6084899902344\n",
      "epoch 17, train_loss 1276.9783935546875\n",
      "epoch 17, val_loss 454.6085510253906\n",
      "epoch 18, train_loss 1276.9783935546875\n",
      "epoch 18, val_loss 454.6085510253906\n",
      "epoch 19, train_loss 1276.9783935546875\n",
      "epoch 19, val_loss 454.6085510253906\n",
      "epoch 20, train_loss 1276.9783935546875\n",
      "epoch 20, val_loss 454.60845947265625\n",
      "epoch 21, train_loss 1276.9783935546875\n",
      "epoch 21, val_loss 454.60845947265625\n",
      "epoch 22, train_loss 1276.9783935546875\n",
      "epoch 22, val_loss 454.60845947265625\n",
      "epoch 23, train_loss 1276.9783935546875\n",
      "epoch 23, val_loss 454.6084289550781\n",
      "epoch 24, train_loss 1276.9783935546875\n",
      "epoch 24, val_loss 454.6084289550781\n",
      "epoch 25, train_loss 1276.978271484375\n",
      "epoch 25, val_loss 454.60833740234375\n",
      "epoch 26, train_loss 1276.978271484375\n",
      "epoch 26, val_loss 454.60833740234375\n",
      "epoch 27, train_loss 1276.9781494140625\n",
      "epoch 27, val_loss 454.60833740234375\n",
      "epoch 28, train_loss 1276.97802734375\n",
      "epoch 28, val_loss 454.60833740234375\n",
      "epoch 29, train_loss 1276.97802734375\n",
      "epoch 29, val_loss 454.6082458496094\n",
      "epoch 30, train_loss 1276.97802734375\n",
      "epoch 30, val_loss 454.6082458496094\n",
      "epoch 31, train_loss 1276.97802734375\n",
      "epoch 31, val_loss 454.60821533203125\n",
      "epoch 32, train_loss 1276.97802734375\n",
      "epoch 32, val_loss 454.60821533203125\n",
      "epoch 33, train_loss 1276.9781494140625\n",
      "epoch 33, val_loss 454.608154296875\n",
      "epoch 34, train_loss 1276.9781494140625\n",
      "epoch 34, val_loss 454.6081237792969\n",
      "epoch 35, train_loss 1276.9781494140625\n",
      "epoch 35, val_loss 454.60809326171875\n",
      "epoch 36, train_loss 1276.9781494140625\n",
      "epoch 36, val_loss 454.6081237792969\n",
      "epoch 37, train_loss 1276.97802734375\n",
      "epoch 37, val_loss 454.60809326171875\n",
      "epoch 38, train_loss 1276.97802734375\n",
      "epoch 38, val_loss 454.6080322265625\n",
      "epoch 39, train_loss 1276.97802734375\n",
      "epoch 39, val_loss 454.6080322265625\n",
      "epoch 40, train_loss 1276.97802734375\n",
      "epoch 40, val_loss 454.6080322265625\n",
      "epoch 41, train_loss 1276.97802734375\n",
      "epoch 41, val_loss 454.60797119140625\n",
      "epoch 42, train_loss 1276.97802734375\n",
      "epoch 42, val_loss 454.6080017089844\n",
      "epoch 43, train_loss 1276.97802734375\n",
      "epoch 43, val_loss 454.60797119140625\n",
      "epoch 44, train_loss 1276.97802734375\n",
      "epoch 44, val_loss 454.60791015625\n",
      "epoch 45, train_loss 1276.9779052734375\n",
      "epoch 45, val_loss 454.60797119140625\n",
      "epoch 46, train_loss 1276.9779052734375\n",
      "epoch 46, val_loss 454.60791015625\n",
      "epoch 47, train_loss 1276.9779052734375\n",
      "epoch 47, val_loss 454.6078186035156\n",
      "epoch 48, train_loss 1276.977783203125\n",
      "epoch 48, val_loss 454.6078186035156\n",
      "epoch 49, train_loss 1276.977783203125\n",
      "epoch 49, val_loss 454.6077575683594\n",
      "epoch 50, train_loss 1276.977783203125\n",
      "epoch 50, val_loss 454.6077575683594\n",
      "epoch 51, train_loss 1276.9779052734375\n",
      "epoch 51, val_loss 454.6076965332031\n",
      "epoch 52, train_loss 1276.977783203125\n",
      "epoch 52, val_loss 454.6076965332031\n",
      "epoch 53, train_loss 1276.9776611328125\n",
      "epoch 53, val_loss 454.6076965332031\n",
      "epoch 54, train_loss 1276.9776611328125\n",
      "epoch 54, val_loss 454.607666015625\n",
      "epoch 55, train_loss 1276.9776611328125\n",
      "epoch 55, val_loss 454.607666015625\n",
      "epoch 56, train_loss 1276.9776611328125\n",
      "epoch 56, val_loss 454.6076354980469\n",
      "epoch 57, train_loss 1276.977783203125\n",
      "epoch 57, val_loss 454.6075744628906\n",
      "epoch 58, train_loss 1276.977783203125\n",
      "epoch 58, val_loss 454.6076354980469\n",
      "epoch 59, train_loss 1276.977783203125\n",
      "epoch 59, val_loss 454.6075744628906\n",
      "epoch 60, train_loss 1276.977783203125\n",
      "epoch 60, val_loss 454.6075439453125\n",
      "epoch 61, train_loss 1276.977783203125\n",
      "epoch 61, val_loss 454.6075439453125\n",
      "epoch 62, train_loss 1276.9776611328125\n",
      "epoch 62, val_loss 454.6074523925781\n",
      "epoch 63, train_loss 1276.9776611328125\n",
      "epoch 63, val_loss 454.6075134277344\n",
      "epoch 64, train_loss 1276.9775390625\n",
      "epoch 64, val_loss 454.6074523925781\n",
      "epoch 65, train_loss 1276.9775390625\n",
      "epoch 65, val_loss 454.6074523925781\n",
      "epoch 66, train_loss 1276.9775390625\n",
      "epoch 66, val_loss 454.607421875\n",
      "epoch 67, train_loss 1276.9775390625\n",
      "epoch 67, val_loss 454.60736083984375\n",
      "epoch 68, train_loss 1276.9774169921875\n",
      "epoch 68, val_loss 454.60736083984375\n",
      "epoch 69, train_loss 1276.9775390625\n",
      "epoch 69, val_loss 454.6073303222656\n",
      "epoch 70, train_loss 1276.9775390625\n",
      "epoch 70, val_loss 454.6073303222656\n",
      "epoch 71, train_loss 1276.9775390625\n",
      "epoch 71, val_loss 454.6073303222656\n",
      "epoch 72, train_loss 1276.9775390625\n",
      "epoch 72, val_loss 454.6073303222656\n",
      "epoch 73, train_loss 1276.9775390625\n",
      "epoch 73, val_loss 454.60723876953125\n",
      "epoch 74, train_loss 1276.9774169921875\n",
      "epoch 74, val_loss 454.60723876953125\n",
      "epoch 75, train_loss 1276.9774169921875\n",
      "epoch 75, val_loss 454.607177734375\n",
      "epoch 76, train_loss 1276.977294921875\n",
      "epoch 76, val_loss 454.607177734375\n",
      "epoch 77, train_loss 1276.977294921875\n",
      "epoch 77, val_loss 454.607177734375\n",
      "epoch 78, train_loss 1276.977294921875\n",
      "epoch 78, val_loss 454.6072082519531\n",
      "epoch 79, train_loss 1276.9774169921875\n",
      "epoch 79, val_loss 454.6070861816406\n",
      "epoch 80, train_loss 1276.977294921875\n",
      "epoch 80, val_loss 454.6070861816406\n",
      "epoch 81, train_loss 1276.977294921875\n",
      "epoch 81, val_loss 454.6070861816406\n",
      "epoch 82, train_loss 1276.977294921875\n",
      "epoch 82, val_loss 454.6070556640625\n",
      "epoch 83, train_loss 1276.977294921875\n",
      "epoch 83, val_loss 454.60699462890625\n",
      "epoch 84, train_loss 1276.977294921875\n",
      "epoch 84, val_loss 454.60699462890625\n",
      "epoch 85, train_loss 1276.977294921875\n",
      "epoch 85, val_loss 454.60699462890625\n",
      "epoch 86, train_loss 1276.977294921875\n",
      "epoch 86, val_loss 454.6069641113281\n",
      "epoch 87, train_loss 1276.977294921875\n",
      "epoch 87, val_loss 454.6069030761719\n",
      "epoch 88, train_loss 1276.977294921875\n",
      "epoch 88, val_loss 454.6069030761719\n",
      "epoch 89, train_loss 1276.977294921875\n",
      "epoch 89, val_loss 454.6069030761719\n",
      "epoch 90, train_loss 1276.977294921875\n",
      "epoch 90, val_loss 454.60687255859375\n",
      "epoch 91, train_loss 1276.9771728515625\n",
      "epoch 91, val_loss 454.60687255859375\n",
      "epoch 92, train_loss 1276.9771728515625\n",
      "epoch 92, val_loss 454.6068420410156\n",
      "epoch 93, train_loss 1276.97705078125\n",
      "epoch 93, val_loss 454.6067810058594\n",
      "epoch 94, train_loss 1276.97705078125\n",
      "epoch 94, val_loss 454.60675048828125\n",
      "epoch 95, train_loss 1276.97705078125\n",
      "epoch 95, val_loss 454.6068420410156\n",
      "epoch 96, train_loss 1276.97705078125\n",
      "epoch 96, val_loss 454.6067810058594\n",
      "epoch 97, train_loss 1276.9771728515625\n",
      "epoch 97, val_loss 454.60675048828125\n",
      "epoch 98, train_loss 1276.97705078125\n",
      "epoch 98, val_loss 454.6066589355469\n",
      "epoch 99, train_loss 1276.97705078125\n",
      "epoch 99, val_loss 454.60662841796875\n",
      "Parameter containing:\n",
      "tensor([3.6140e-35], requires_grad=True)\n",
      "iter 198, train_loss_regularization 0.6837265491485596\n",
      "iter 198, val_loss_regularization 0.6837265491485596\n",
      "epoch 0, train_loss 1276.97705078125\n",
      "epoch 0, val_loss 454.6065979003906\n",
      "epoch 1, train_loss 1276.97705078125\n",
      "epoch 1, val_loss 454.6065979003906\n",
      "epoch 2, train_loss 1276.97705078125\n",
      "epoch 2, val_loss 454.6065979003906\n",
      "epoch 3, train_loss 1276.9769287109375\n",
      "epoch 3, val_loss 454.6065979003906\n",
      "epoch 4, train_loss 1276.97705078125\n",
      "epoch 4, val_loss 454.6065979003906\n",
      "epoch 5, train_loss 1276.97705078125\n",
      "epoch 5, val_loss 454.60650634765625\n",
      "epoch 6, train_loss 1276.9769287109375\n",
      "epoch 6, val_loss 454.6064453125\n",
      "epoch 7, train_loss 1276.97705078125\n",
      "epoch 7, val_loss 454.6064453125\n",
      "epoch 8, train_loss 1276.97705078125\n",
      "epoch 8, val_loss 454.6064453125\n",
      "epoch 9, train_loss 1276.97705078125\n",
      "epoch 9, val_loss 454.6064453125\n",
      "epoch 10, train_loss 1276.9769287109375\n",
      "epoch 10, val_loss 454.6064453125\n",
      "epoch 11, train_loss 1276.976806640625\n",
      "epoch 11, val_loss 454.60638427734375\n",
      "epoch 12, train_loss 1276.9769287109375\n",
      "epoch 12, val_loss 454.6063232421875\n",
      "epoch 13, train_loss 1276.976806640625\n",
      "epoch 13, val_loss 454.6063232421875\n",
      "epoch 14, train_loss 1276.9769287109375\n",
      "epoch 14, val_loss 454.6062927246094\n",
      "epoch 15, train_loss 1276.976806640625\n",
      "epoch 15, val_loss 454.6063232421875\n",
      "epoch 16, train_loss 1276.976806640625\n",
      "epoch 16, val_loss 454.6062927246094\n",
      "epoch 17, train_loss 1276.976806640625\n",
      "epoch 17, val_loss 454.6062927246094\n",
      "epoch 18, train_loss 1276.9766845703125\n",
      "epoch 18, val_loss 454.606201171875\n",
      "epoch 19, train_loss 1276.9766845703125\n",
      "epoch 19, val_loss 454.606201171875\n",
      "epoch 20, train_loss 1276.9766845703125\n",
      "epoch 20, val_loss 454.6061706542969\n",
      "epoch 21, train_loss 1276.9766845703125\n",
      "epoch 21, val_loss 454.6061706542969\n",
      "epoch 22, train_loss 1276.9766845703125\n",
      "epoch 22, val_loss 454.6061706542969\n",
      "epoch 23, train_loss 1276.9766845703125\n",
      "epoch 23, val_loss 454.6061706542969\n",
      "epoch 24, train_loss 1276.9765625\n",
      "epoch 24, val_loss 454.60614013671875\n",
      "epoch 25, train_loss 1276.9765625\n",
      "epoch 25, val_loss 454.6060485839844\n",
      "epoch 26, train_loss 1276.9765625\n",
      "epoch 26, val_loss 454.6060791015625\n",
      "epoch 27, train_loss 1276.9766845703125\n",
      "epoch 27, val_loss 454.6059875488281\n",
      "epoch 28, train_loss 1276.9766845703125\n",
      "epoch 28, val_loss 454.6059875488281\n",
      "epoch 29, train_loss 1276.9765625\n",
      "epoch 29, val_loss 454.60595703125\n",
      "epoch 30, train_loss 1276.9765625\n",
      "epoch 30, val_loss 454.60595703125\n",
      "epoch 31, train_loss 1276.9765625\n",
      "epoch 31, val_loss 454.6059265136719\n",
      "epoch 32, train_loss 1276.9764404296875\n",
      "epoch 32, val_loss 454.6059265136719\n",
      "epoch 33, train_loss 1276.9764404296875\n",
      "epoch 33, val_loss 454.6058654785156\n",
      "epoch 34, train_loss 1276.9764404296875\n",
      "epoch 34, val_loss 454.6058654785156\n",
      "epoch 35, train_loss 1276.9765625\n",
      "epoch 35, val_loss 454.6058654785156\n",
      "epoch 36, train_loss 1276.9764404296875\n",
      "epoch 36, val_loss 454.6058349609375\n",
      "epoch 37, train_loss 1276.9764404296875\n",
      "epoch 37, val_loss 454.6058044433594\n",
      "epoch 38, train_loss 1276.9765625\n",
      "epoch 38, val_loss 454.6057434082031\n",
      "epoch 39, train_loss 1276.9765625\n",
      "epoch 39, val_loss 454.6057434082031\n",
      "epoch 40, train_loss 1276.9765625\n",
      "epoch 40, val_loss 454.6057434082031\n",
      "epoch 41, train_loss 1276.9765625\n",
      "epoch 41, val_loss 454.605712890625\n",
      "epoch 42, train_loss 1276.976318359375\n",
      "epoch 42, val_loss 454.605712890625\n",
      "epoch 43, train_loss 1276.976318359375\n",
      "epoch 43, val_loss 454.60565185546875\n",
      "epoch 44, train_loss 1276.9761962890625\n",
      "epoch 44, val_loss 454.6056213378906\n",
      "epoch 45, train_loss 1276.9761962890625\n",
      "epoch 45, val_loss 454.60565185546875\n",
      "epoch 46, train_loss 1276.9761962890625\n",
      "epoch 46, val_loss 454.6056213378906\n",
      "epoch 47, train_loss 1276.9761962890625\n",
      "epoch 47, val_loss 454.6055908203125\n",
      "epoch 48, train_loss 1276.9761962890625\n",
      "epoch 48, val_loss 454.6055908203125\n",
      "epoch 49, train_loss 1276.976318359375\n",
      "epoch 49, val_loss 454.6054992675781\n",
      "epoch 50, train_loss 1276.9761962890625\n",
      "epoch 50, val_loss 454.6054992675781\n",
      "epoch 51, train_loss 1276.9761962890625\n",
      "epoch 51, val_loss 454.60546875\n",
      "epoch 52, train_loss 1276.9761962890625\n",
      "epoch 52, val_loss 454.60540771484375\n",
      "epoch 53, train_loss 1276.9761962890625\n",
      "epoch 53, val_loss 454.60546875\n",
      "epoch 54, train_loss 1276.9761962890625\n",
      "epoch 54, val_loss 454.60540771484375\n",
      "epoch 55, train_loss 1276.9761962890625\n",
      "epoch 55, val_loss 454.60540771484375\n",
      "epoch 56, train_loss 1276.9761962890625\n",
      "epoch 56, val_loss 454.6053466796875\n",
      "epoch 57, train_loss 1276.9761962890625\n",
      "epoch 57, val_loss 454.6053466796875\n",
      "epoch 58, train_loss 1276.9761962890625\n",
      "epoch 58, val_loss 454.6053466796875\n",
      "epoch 59, train_loss 1276.9761962890625\n",
      "epoch 59, val_loss 454.60528564453125\n",
      "epoch 60, train_loss 1276.9759521484375\n",
      "epoch 60, val_loss 454.60528564453125\n",
      "epoch 61, train_loss 1276.9759521484375\n",
      "epoch 61, val_loss 454.6051940917969\n",
      "epoch 62, train_loss 1276.9759521484375\n",
      "epoch 62, val_loss 454.60516357421875\n",
      "epoch 63, train_loss 1276.9759521484375\n",
      "epoch 63, val_loss 454.6051940917969\n",
      "epoch 64, train_loss 1276.9759521484375\n",
      "epoch 64, val_loss 454.6051940917969\n",
      "epoch 65, train_loss 1276.9759521484375\n",
      "epoch 65, val_loss 454.60516357421875\n",
      "epoch 66, train_loss 1276.97607421875\n",
      "epoch 66, val_loss 454.60516357421875\n",
      "epoch 67, train_loss 1276.97607421875\n",
      "epoch 67, val_loss 454.60516357421875\n",
      "epoch 68, train_loss 1276.9759521484375\n",
      "epoch 68, val_loss 454.6050720214844\n",
      "epoch 69, train_loss 1276.9759521484375\n",
      "epoch 69, val_loss 454.60504150390625\n",
      "epoch 70, train_loss 1276.975830078125\n",
      "epoch 70, val_loss 454.60504150390625\n",
      "epoch 71, train_loss 1276.975830078125\n",
      "epoch 71, val_loss 454.60504150390625\n",
      "epoch 72, train_loss 1276.9757080078125\n",
      "epoch 72, val_loss 454.60504150390625\n",
      "epoch 73, train_loss 1276.9757080078125\n",
      "epoch 73, val_loss 454.60504150390625\n",
      "epoch 74, train_loss 1276.9757080078125\n",
      "epoch 74, val_loss 454.60491943359375\n",
      "epoch 75, train_loss 1276.9757080078125\n",
      "epoch 75, val_loss 454.60491943359375\n",
      "epoch 76, train_loss 1276.975830078125\n",
      "epoch 76, val_loss 454.6048889160156\n",
      "epoch 77, train_loss 1276.975830078125\n",
      "epoch 77, val_loss 454.6048278808594\n",
      "epoch 78, train_loss 1276.9759521484375\n",
      "epoch 78, val_loss 454.6048889160156\n",
      "epoch 79, train_loss 1276.9759521484375\n",
      "epoch 79, val_loss 454.6048278808594\n",
      "epoch 80, train_loss 1276.9759521484375\n",
      "epoch 80, val_loss 454.60479736328125\n",
      "epoch 81, train_loss 1276.9757080078125\n",
      "epoch 81, val_loss 454.6048278808594\n",
      "epoch 82, train_loss 1276.9759521484375\n",
      "epoch 82, val_loss 454.60479736328125\n",
      "epoch 83, train_loss 1276.9757080078125\n",
      "epoch 83, val_loss 454.604736328125\n",
      "epoch 84, train_loss 1276.9757080078125\n",
      "epoch 84, val_loss 454.6047058105469\n",
      "epoch 85, train_loss 1276.9757080078125\n",
      "epoch 85, val_loss 454.6047058105469\n",
      "epoch 86, train_loss 1276.9757080078125\n",
      "epoch 86, val_loss 454.60467529296875\n",
      "epoch 87, train_loss 1276.9757080078125\n",
      "epoch 87, val_loss 454.6047058105469\n",
      "epoch 88, train_loss 1276.9757080078125\n",
      "epoch 88, val_loss 454.6046142578125\n",
      "epoch 89, train_loss 1276.9757080078125\n",
      "epoch 89, val_loss 454.6046142578125\n",
      "epoch 90, train_loss 1276.9757080078125\n",
      "epoch 90, val_loss 454.6046142578125\n",
      "epoch 91, train_loss 1276.9757080078125\n",
      "epoch 91, val_loss 454.6045837402344\n",
      "epoch 92, train_loss 1276.9757080078125\n",
      "epoch 92, val_loss 454.6044921875\n",
      "epoch 93, train_loss 1276.9757080078125\n",
      "epoch 93, val_loss 454.6044921875\n",
      "epoch 94, train_loss 1276.9757080078125\n",
      "epoch 94, val_loss 454.6044616699219\n",
      "epoch 95, train_loss 1276.9757080078125\n",
      "epoch 95, val_loss 454.6044616699219\n",
      "epoch 96, train_loss 1276.9754638671875\n",
      "epoch 96, val_loss 454.6044616699219\n",
      "epoch 97, train_loss 1276.9754638671875\n",
      "epoch 97, val_loss 454.6044616699219\n",
      "epoch 98, train_loss 1276.9754638671875\n",
      "epoch 98, val_loss 454.6044616699219\n",
      "epoch 99, train_loss 1276.9755859375\n",
      "epoch 99, val_loss 454.60443115234375\n",
      "Parameter containing:\n",
      "tensor([2.5310e-35], requires_grad=True)\n",
      "iter 199, train_loss_regularization 0.6837512254714966\n",
      "iter 199, val_loss_regularization 0.6837512254714966\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "predicted = predicted_plotting(X_train, Y_train, model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.0577])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "a = model(X_train).detach().numpy()\r\n",
    "plt.plot(X_train, Y_train, 'go')\r\n",
    "# plt.plot(X_train, , label='Predictions', alpha=0.5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x142375ae548>]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# print(list(model.parameters()))\r\n",
    "print(list(model.params.values()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Parameter containing:\n",
      "tensor([0.9757], requires_grad=True), Parameter containing:\n",
      "tensor([0.0387], requires_grad=True), Parameter containing:\n",
      "tensor([0.2358], requires_grad=True)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(lambda_l2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\n",
      "tensor([8.9029e-15], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b8aae029aa4a300dc1633d12d51d7571ccf2a159ab722b8f137957ced566ff6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('.gr_wave': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}